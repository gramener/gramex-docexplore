{
  "topics": [
    {
      "topic": "LLM Development & Safety",
      "subtopic": "Prompt Injection Security"
    },
    {
      "topic": "LLM Development & Safety",
      "subtopic": "LLM Application Safety"
    },
    {
      "topic": "LLM Development & Safety",
      "subtopic": "Ethical LLM Frameworks"
    },
    {
      "topic": "LLM Development & Safety",
      "subtopic": "LLM Robustness Improvement"
    },
    {
      "topic": "LLM Development & Safety",
      "subtopic": "LLM Weight Quantization"
    },
    {
      "topic": "LLM Development & Safety",
      "subtopic": "Data Selection for LLMs"
    },
    {
      "topic": "LLM Performance & Evaluation",
      "subtopic": "Hallucination Detection"
    },
    {
      "topic": "LLM Performance & Evaluation",
      "subtopic": "LLM Benchmark Generation"
    },
    {
      "topic": "LLM Performance & Evaluation",
      "subtopic": "LLM Sociability Benchmarks"
    },
    {
      "topic": "LLM Performance & Evaluation",
      "subtopic": "LLM Text Generation Evaluation"
    },
    {
      "topic": "LLM Performance & Evaluation",
      "subtopic": "LLM Knowledge Base QA"
    },
    {
      "topic": "LLM Performance & Evaluation",
      "subtopic": "LLM Prompt Efficiency"
    },
    {
      "topic": "LLM Use Cases & Applications",
      "subtopic": "Chatbot Emotional Support"
    },
    {
      "topic": "LLM Use Cases & Applications",
      "subtopic": "LLM in Psychological Assessment"
    },
    {
      "topic": "LLM Use Cases & Applications",
      "subtopic": "LLM Moral Judgements"
    },
    {
      "topic": "LLM Use Cases & Applications",
      "subtopic": "LLM Dialogue Orchestration"
    },
    {
      "topic": "LLM Use Cases & Applications",
      "subtopic": "Programming with LLMs"
    },
    {
      "topic": "LLM Use Cases & Applications",
      "subtopic": "Multimodal LLM Integration"
    },
    {
      "topic": "LLM Learning & Adaptation",
      "subtopic": "Enhancing Reasoning Skills"
    },
    {
      "topic": "LLM Learning & Adaptation",
      "subtopic": "Adaptive LLM Reasoning"
    },
    {
      "topic": "LLM Learning & Adaptation",
      "subtopic": "Personalized Learning Models"
    },
    {
      "topic": "LLM Learning & Adaptation",
      "subtopic": "Cross-Lingual LLM Enhancement"
    },
    {
      "topic": "Content Analysis & Management",
      "subtopic": "Agent Behavior Analysis"
    },
    {
      "topic": "Content Analysis & Management",
      "subtopic": "Misinformation Detection"
    },
    {
      "topic": "Content Analysis & Management",
      "subtopic": "Text Clustering and Compression"
    }
  ],
  "docs": [
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SOCIOFILLMORE: A Tool for Discovering Perspectives\u2b1b  SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the\nfocus or the perspective that a text expresses in depicting an event. Our tool,\nwhose rationale we also support through a large collection of human judgements,\nis theoretically grounded on frame semantics and cognitive linguistics, and\nimplemented using the LOME frame semantic parser. We describe SOCIOFILLMORE's\ndevelopment and functionalities, show how non-NLP researchers can easily\ninteract with the tool, and present some example case studies which are already\nincorporated in the system, together with the kind of analysis that can be\nvisualised.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "On Reality and the Limits of Language Data: Aligning LLMs with Human\n  Norms\u2b1b  Recent advancements in Large Language Models (LLMs) harness linguistic\nassociations in vast natural language data for practical applications. However,\ntheir ability to understand the physical world using only language data remains\na question. After reviewing existing protocols, we explore this question using\na novel and tightly controlled reasoning test (ART) and compare human norms\nagainst versions of GPT-3. Our findings highlight the categories of\ncommon-sense relations models that could learn directly from data and areas of\nweakness. GPT-3 offers evidence for verbal reasoning on a par with human\nsubjects for several relations including Synonymy, Antonymy, and Default\ninheritance, Without reinforcement learning from human judgements, it appears\nGPT-3 performs at the lower end of the reference interval for Has-part and\nContained-in. Weaknesses were observed also in affordance characteristics\nthrough Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs\nwith symbolic world grounding is a promising direction to address associative\nlearning.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Selecting Better Samples from Pre-trained LLMs: A Case Study on Question\n  Generation\u2b1b  Large Language Models (LLMs) have in recent years demonstrated impressive\nprowess in natural language generation. A common practice to improve generation\ndiversity is to sample multiple outputs from the model. However, there lacks a\nsimple and robust way of selecting the best output from these stochastic\nsamples. As a case study framed in the context of question generation, we\npropose two prompt-based approaches to selecting high-quality questions from a\nset of LLM-generated candidates. Our method works under the constraints of 1) a\nblack-box (non-modifiable) question generation model and 2) lack of access to\nhuman-annotated references -- both of which are realistic limitations for\nreal-world deployment of LLMs. With automatic as well as human evaluations, we\nempirically demonstrate that our approach can effectively select questions of\nhigher qualities than greedy generation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Augmenting Interpretable Models with LLMs during Training\u2b1b  Recent large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their proliferation into\nhigh-stakes domains (e.g. medicine) and compute-limited settings has created a\nburgeoning need for interpretability and efficiency. We address this need by\nproposing Augmented Interpretable Models (Aug-imodels), a framework for\nleveraging the knowledge learned by LLMs to build extremely efficient and\ninterpretable models. Aug-imodels use LLMs during fitting but not during\ninference, allowing complete transparency and often a speed/memory improvement\nof greater than 1,000x for inference compared to LLMs. We explore two\ninstantiations of Aug-imodels in natural-language processing: (i) Aug-GAM,\nwhich augments a generalized additive model with decoupled embeddings from an\nLLM and (ii) Aug-Tree, which augments a decision tree with LLM feature\nexpansions. Across a variety of text-classification datasets, both outperform\ntheir non-augmented counterparts. Aug-GAM can even outperform much larger\nmodels (e.g. a 6-billion parameter GPT-J model), despite having 10,000x fewer\nparameters and being fully transparent. We further explore Aug-imodels in a\nnatural-language fMRI study, where they generate interesting interpretations\nfrom scientific data. All code for using Aug-imodels and reproducing results is\nmade available on Github.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TabLLM: Few-shot Classification of Tabular Data with Large Language\n  Models\u2b1b  We study the application of large language models to zero-shot and few-shot\nclassification of tabular data. We prompt the large language model with a\nserialization of the tabular data to a natural-language string, together with a\nshort description of the classification problem. In the few-shot setting, we\nfine-tune the large language model using some labeled examples. We evaluate\nseveral serialization methods including templates, table-to-text models, and\nlarge language models. Despite its simplicity, we find that this technique\noutperforms prior deep-learning-based tabular classification methods on several\nbenchmark datasets. In most cases, even zero-shot classification obtains\nnon-trivial performance, illustrating the method's ability to exploit prior\nknowledge encoded in large language models. Unlike many deep learning methods\nfor tabular datasets, this approach is also competitive with strong traditional\nbaselines like gradient-boosted trees, especially in the very-few-shot setting.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters\n  for Implicature Resolution by LLMs\u2b1b  Despite widespread use of LLMs as conversational agents, evaluations of\nperformance fail to capture a crucial aspect of communication: interpreting\nlanguage in context -- incorporating its pragmatics. Humans interpret language\nusing beliefs and prior knowledge about the world. For example, we intuitively\nunderstand the response \"I wore gloves\" to the question \"Did you leave\nfingerprints?\" as meaning \"No\". To investigate whether LLMs have the ability to\nmake this type of inference, known as an implicature, we design a simple task\nand evaluate four categories of widely used state-of-the-art models. We find\nthat, despite only evaluating on utterances that require a binary inference\n(yes or no), models in three of these categories perform close to random.\nHowever, LLMs instruction-tuned at the example-level perform significantly\nbetter. These results suggest that certain fine-tuning strategies are far\nbetter at inducing pragmatic understanding in models. We present our findings\nas the starting point for further research into evaluating how LLMs interpret\nlanguage in context and to drive the development of more pragmatic and useful\nmodels of human discourse.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Two-stage LLM Fine-tuning with Less Specialization and More\n  Generalization\u2b1b  Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task. We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization. ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models\u2b1b  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Despite \"super-human\" performance, current LLMs are unsuited for\n  decisions about ethics and safety\u2b1b  Large language models (LLMs) have exploded in popularity in the past few\nyears and have achieved undeniably impressive results on benchmarks as varied\nas question answering and text summarization. We provide a simple new prompting\nstrategy that leads to yet another supposedly \"super-human\" result, this time\noutperforming humans at common sense ethical reasoning (as measured by accuracy\non a subset of the ETHICS dataset). Unfortunately, we find that relying on\naverage performance to judge capabilities can be highly misleading. LLM errors\ndiffer systematically from human errors in ways that make it easy to craft\nadversarial examples, or even perturb existing examples to flip the output\nlabel. We also observe signs of inverse scaling with model size on some\nexamples, and show that prompting models to \"explain their reasoning\" often\nleads to alarming justifications of unethical actions. Our results highlight\nhow human-like performance does not necessarily imply human-like understanding\nor reasoning.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "What do LLMs Know about Financial Markets? A Case Study on Reddit Market\n  Sentiment Analysis\u2b1b  Market sentiment analysis on social media content requires knowledge of both\nfinancial markets and social media jargon, which makes it a challenging task\nfor human raters. The resulting lack of high-quality labeled data stands in the\nway of conventional supervised learning methods. Instead, we approach this\nproblem using semi-supervised learning with a large language model (LLM). Our\npipeline generates weak financial sentiment labels for Reddit posts with an LLM\nand then uses that data to train a small model that can be served in\nproduction. We find that prompting the LLM to produce Chain-of-Thought\nsummaries and forcing it through several reasoning paths helps generate more\nstable and accurate labels, while using a regression loss further improves\ndistillation quality. With only a handful of prompts, the final model performs\non par with existing supervised models. Though production applications of our\nmodel are limited by ethical considerations, the model's competitive\nperformance points to the great potential of using LLMs for tasks that\notherwise require skill-intensive annotation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Not what you've signed up for: Compromising Real-World LLM-Integrated\n  Applications with Indirect Prompt Injection\u2b1b  Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The ROOTS Search Tool: Data Transparency for LLMs\u2b1b  ROOTS is a 1.6TB multilingual text corpus developed for the training of\nBLOOM, currently the largest language model explicitly accompanied by\ncommensurate data governance efforts. In continuation of these efforts, we\npresent the ROOTS Search Tool: a search engine over the entire ROOTS corpus\noffering both fuzzy and exact search capabilities. ROOTS is the largest corpus\nto date that can be investigated this way. The ROOTS Search Tool is\nopen-sourced and available on Hugging Face Spaces. We describe our\nimplementation and the possible use cases of our tool.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and\n  Distillation of Rerankers\u2b1b  Many information retrieval tasks require large labeled datasets for\nfine-tuning. However, such datasets are often unavailable, and their utility\nfor real-world applications can diminish quickly due to domain shifts. To\naddress this challenge, we develop and motivate a method for using large\nlanguage models (LLMs) to generate large numbers of synthetic queries cheaply.\nThe method begins by generating a small number of synthetic queries using an\nexpensive LLM. After that, a much less expensive one is used to create large\nnumbers of synthetic queries, which are used to fine-tune a family of reranker\nmodels. These rerankers are then distilled into a single efficient retriever\nfor use in the target domain. We show that this technique boosts zero-shot\naccuracy in long-tail domains and achieves substantially lower latency than\nstandard reranking methods.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can an Embodied Agent Find Your \"Cat-shaped Mug\"? LLM-Guided Exploration\n  for Zero-Shot Object Navigation\u2b1b  We present LGX (Language-guided Exploration), a novel algorithm for\nLanguage-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied\nagent navigates to a uniquely described target object in a previously unseen\nenvironment. Our approach makes use of Large Language Models (LLMs) for this\ntask by leveraging the LLM's commonsense reasoning capabilities for making\nsequential navigational decisions. Simultaneously, we perform generalized\ntarget object detection using a pre-trained Vision-Language grounding model. We\nachieve state-of-the-art zero-shot object navigation results on RoboTHOR with a\nsuccess rate (SR) improvement of over 27% over the current baseline of the\nOWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for\nrobot navigation and present an analysis of various prompting strategies\naffecting the model output. Finally, we showcase the benefits of our approach\nvia \\textit{real-world} experiments that indicate the superior performance of\nLGX in detecting and navigating to visually unique objects.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Does Synthetic Data Generation of LLMs Help Clinical Text Mining?\u2b1b  Recent advancements in large language models (LLMs) have led to the\ndevelopment of highly potent models like OpenAI's ChatGPT. These models have\nexhibited exceptional performance in a variety of tasks, such as question\nanswering, essay composition, and code generation. However, their effectiveness\nin the healthcare sector remains uncertain. In this study, we seek to\ninvestigate the potential of ChatGPT to aid in clinical text mining by\nexamining its ability to extract structured information from unstructured\nhealthcare texts, with a focus on biological named entity recognition and\nrelation extraction. However, our preliminary results indicate that employing\nChatGPT directly for these tasks resulted in poor performance and raised\nprivacy concerns associated with uploading patients' information to the ChatGPT\nAPI. To overcome these limitations, we propose a new training paradigm that\ninvolves generating a vast quantity of high-quality synthetic data with labels\nutilizing ChatGPT and fine-tuning a local model for the downstream task. Our\nmethod has resulted in significant improvements in the performance of\ndownstream tasks, improving the F1-score from 23.37% to 63.99% for the named\nentity recognition task and from 75.86% to 83.59% for the relation extraction\ntask. Furthermore, generating data using ChatGPT can significantly reduce the\ntime and effort required for data collection and labeling, as well as mitigate\ndata privacy concerns. In summary, the proposed framework presents a promising\nsolution to enhance the applicability of LLM models to clinical text mining.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Science of Detecting LLM-Generated Texts\u2b1b  The emergence of large language models (LLMs) has resulted in the production\nof LLM-generated texts that is highly sophisticated and almost\nindistinguishable from texts written by humans. However, this has also sparked\nconcerns about the potential misuse of such texts, such as spreading\nmisinformation and causing disruptions in the education system. Although many\ndetection approaches have been proposed, a comprehensive understanding of the\nachievements and challenges is still lacking. This survey aims to provide an\noverview of existing LLM-generated text detection techniques and enhance the\ncontrol and regulation of language generation models. Furthermore, we emphasize\ncrucial considerations for future research, including the development of\ncomprehensive evaluation metrics and the threat posed by open-source LLMs, to\ndrive progress in the area of LLM-generated text detection.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the\n  Question Answering Performance of the GPT LLM Family\u2b1b  ChatGPT is a powerful large language model (LLM) that covers knowledge\nresources such as Wikipedia and supports natural language question answering\nusing its own knowledge. Therefore, there is growing interest in exploring\nwhether ChatGPT can replace traditional knowledge-based question answering\n(KBQA) models. Although there have been some works analyzing the question\nanswering performance of ChatGPT, there is still a lack of large-scale,\ncomprehensive testing of various types of complex questions to analyze the\nlimitations of the model. In this paper, we present a framework that follows\nthe black-box testing specifications of CheckList proposed by Ribeiro et. al.\nWe evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex\nquestion answering datasets, which include six English datasets and two\nmultilingual datasets. The total number of test cases is approximately 190,000.\nIn addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5\nto identify commonalities between the GPT family and other LLMs. The dataset\nand code are available at\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from\n  Comprehensive Study to Low Rank Compensation\u2b1b  Post-training quantization (PTQ) has emerged as a promising technique for\nmitigating memory consumption and computational costs in large language models\n(LLMs). However, a systematic examination of various quantization schemes,\nmodel families, and quantization bit precision has been absent from the\nliterature. In this paper, we conduct a comprehensive analysis of these factors\nby investigating the effects of PTQ on weight-only, activation-only, and\nweight-and-activation quantization using diverse methods such as\nround-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these\nmethods to two distinct model families with parameters ranging from 125M to\n176B. Our contributions include: (1) a sensitivity analysis revealing that\nactivation quantization is generally more susceptible to weight quantization,\nwith smaller models often outperforming larger models in terms of activation\nquantization; (2) an evaluation and comparison of existing PTQ methods to\noptimize model size reduction while minimizing the impact on accuracy,\nrevealing that none of the current methods can achieve the original model\nquality for quantization with either INT4-weight or\nINT4-weight-and-INT8-activation; (3) based on these insights, we propose an\noptimized method called Low-Rank Compensation (LoRC), which employs low-rank\nmatrices to enhance model quality recovery with a minimal increase in model\nsize.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning\n  Skills of LLMs\u2b1b  The potential of large language models (LLMs) to reason like humans has been\na highly contested topic in Machine Learning communities. However, the\nreasoning abilities of humans are multifaceted and can be seen in various\nforms, including analogical, spatial and moral reasoning, among others. This\nfact raises the question whether LLMs can perform equally well across all these\ndifferent domains. This research work aims to investigate the performance of\nLLMs on different reasoning tasks by conducting experiments that directly use\nor draw inspirations from existing datasets on analogical and spatial\nreasoning. Additionally, to evaluate the ability of LLMs to reason like human,\ntheir performance is evaluted on more open-ended, natural language questions.\nMy findings indicate that LLMs excel at analogical and moral reasoning, yet\nstruggle to perform as proficiently on spatial reasoning tasks. I believe these\nexperiments are crucial for informing the future development of LLMs,\nparticularly in contexts that require diverse reasoning proficiencies. By\nshedding light on the reasoning abilities of LLMs, this study aims to push\nforward our understanding of how they can better emulate the cognitive\nabilities of humans.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender\n  System\u2b1b  Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced\n  Annotators\u2b1b  Many natural language processing (NLP) tasks rely on labeled data to train\nmachine learning models to achieve high performance. However, data annotation\ncan be a time-consuming and expensive process, especially when the task\ninvolves a large amount of data or requires specialized domains. Recently,\nGPT-3.5 series models have demonstrated remarkable few-shot and zero-shot\nability across various NLP tasks. In this paper, we first claim that large\nlanguage models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced\nannotator by providing them with sufficient guidance and demonstrated examples.\nTo make LLMs to be better annotators, we propose a two-step approach,\n'explain-then-annotate'. To be more precise, we begin by creating prompts for\nevery demonstrated example, which we subsequently utilize to prompt a LLM to\nprovide an explanation for why the specific ground truth answer/label was\nchosen for that particular example. Following this, we construct the few-shot\nchain-of-thought prompt with the self-generated explanation and employ it to\nannotate the unlabeled data. We conduct experiments on three tasks, including\nuser input and keyword relevance assessment, BoolQ and WiC. The annotation\nresults from GPT-3.5 surpasses those from crowdsourced annotation for user\ninput and keyword relevance assessment. Additionally, for the other two tasks,\nGPT-3.5 achieves results that are comparable to those obtained through\ncrowdsourced annotation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language\n  Models\u2b1b  Large Language Models (LLMs) have revolutionized natural language processing\nand demonstrated impressive capabilities in various tasks. Unfortunately, they\nare prone to hallucinations, where the model exposes incorrect or false\ninformation in its responses, which renders diligent evaluation approaches\nmandatory. While LLM performance in specific knowledge fields is often\nevaluated based on question and answer (Q&A) datasets, such evaluations usually\nreport only a single accuracy number for the dataset, which often covers an\nentire field. This field-based evaluation, is problematic with respect to\ntransparency and model improvement. A stratified evaluation could instead\nreveal subfields, where hallucinations are more likely to occur and thus help\nto better assess LLMs' risks and guide their further development. To support\nsuch stratified evaluations, we propose LLMMaps as a novel visualization\ntechnique that enables users to evaluate LLMs' performance with respect to Q&A\ndatasets. LLMMaps provide detailed insights into LLMs' knowledge capabilities\nin different subfields, by transforming Q&A datasets as well as LLM responses\ninto an internal knowledge structure. An extension for comparative\nvisualization furthermore, allows for the detailed comparison of multiple LLMs.\nTo assess LLMMaps we use them to conduct a comparative analysis of several\nstate-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as\nwell as two qualitative user evaluations. All necessary source code and data\nfor generating LLMMaps to be used in scientific publications and elsewhere is\navailable on GitHub: https://github.com/viscom-ulm/LLMMaps\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Does Human Collaboration Enhance the Accuracy of Identifying\n  LLM-Generated Deepfake Texts?\u2b1b  Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the\ngeneration of coherent sentences resembling human writing on a large scale,\nresulting in the creation of so-called deepfake texts. However, this progress\nposes security and privacy concerns, necessitating effective solutions for\ndistinguishing deepfake texts from human-written ones. Although prior works\nstudied humans' ability to detect deepfake texts, none has examined whether\n\"collaboration\" among humans improves the detection of deepfake texts. In this\nstudy, to address this gap of understanding on deepfake texts, we conducted\nexperiments with two groups: (1) nonexpert individuals from the AMT platform\nand (2) writing experts from the Upwork platform. The results demonstrate that\ncollaboration among humans can potentially improve the detection of deepfake\ntexts for both groups, increasing detection accuracies by 6.36% for non-experts\nand 12.76% for experts, respectively, compared to individuals' detection\naccuracies. We further analyze the explanations that humans used for detecting\na piece of text as deepfake text, and find that the strongest indicator of\ndeepfake texts is their lack of coherence and consistency. Our study provides\nuseful insights for future tools and framework designs to facilitate the\ncollaborative human detection of deepfake texts. The experiment datasets and\nAMT implementations are available at:\nhttps://github.com/huashen218/llm-deepfake-human-study.git\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of\n  Large Language Models\u2b1b  The success of large language models (LLMs), like GPT-4 and ChatGPT, has led\nto the development of numerous cost-effective and accessible alternatives that\nare created by finetuning open-access LLMs with task-specific data (e.g.,\nChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning\nmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly\none of the most attractive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs while achieving comparable or\neven better performance. To enable further research on PEFT methods of LLMs,\nthis paper presents LLM-Adapters, an easy-to-use framework that integrates\nvarious adapters into LLMs and can execute these adapter-based PEFT methods of\nLLMs for different tasks. The framework includes state-of-the-art open-access\nLLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as\nSeries adapters, Parallel adapter, Prompt-based learning and\nReparametrization-based methods. Moreover, we conduct extensive empirical\nstudies on the impact of adapter types, placement locations, and\nhyper-parameters to the best design for each adapter-based methods. We evaluate\nthe effectiveness of the adapters on fourteen datasets from two different\nreasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results\ndemonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few\nextra trainable parameters yields comparable, and in some cases superior,\nperformance to powerful LLMs (175B) in zero-shot inference on both reasoning\ntasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "OpenAGI: When LLM Meets Domain Experts\u2b1b  Human Intelligence (HI) excels at combining basic skills to solve complex\ntasks. This capability is vital for Artificial Intelligence (AI) and should be\nembedded in comprehensive AI Agents, enabling them to harness expert models for\ncomplex task-solving towards Artificial General Intelligence (AGI). Large\nLanguage Models (LLMs) show promising learning and reasoning abilities, and can\neffectively use external models, tools, plugins, or APIs to tackle complex\nproblems. In this work, we introduce OpenAGI, an open-source AGI research and\ndevelopment platform designed for solving multi-step, real-world tasks.\nSpecifically, OpenAGI uses a dual strategy, integrating standard benchmark\ntasks for benchmarking and evaluation, and open-ended tasks including more\nexpandable models, tools, plugins, or APIs for creative problem-solving. Tasks\nare presented as natural language queries to the LLM, which then selects and\nexecutes appropriate models. We also propose a Reinforcement Learning from Task\nFeedback (RLTF) mechanism that uses task results to improve the LLM's\ntask-solving ability, which creates a self-improving AI feedback loop. While we\nacknowledge that AGI is a broad and multifaceted research challenge with no\nsingularly defined solution path, the integration of LLMs with domain-specific\nexpert models, inspired by mirroring the blend of general and specialized\nintelligence in humans, offers a promising approach towards AGI. We are\nopen-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation\nmethods, and the UI demo to foster community involvement in AGI advancement:\nhttps://github.com/agiresearch/OpenAGI.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Are LLMs All You Need for Task-Oriented Dialogue?\u2b1b  Instructions-tuned Large Language Models (LLMs) gained recently huge\npopularity thanks to their ability to interact with users through conversation.\nIn this work we aim to evaluate their ability to complete multi-turn tasks and\ninteract with external databases in the context of established task-oriented\ndialogue benchmarks. We show that for explicit belief state tracking, LLMs\nunderperform compared to specialized task-specific models. Nevertheless, they\nshow ability to guide the dialogue to successful ending if given correct slot\nvalues. Furthermore this ability improves with access to true belief state\ndistribution or in-domain examples.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Low-code LLM: Visual Programming over LLMs\u2b1b  Effectively utilizing LLMs for complex tasks is challenging, often involving\na time-consuming and uncontrollable prompt engineering process. This paper\nintroduces a novel human-LLM interaction framework, Low-code LLM. It\nincorporates six types of simple low-code visual programming interactions, all\nsupported by clicking, dragging, or text editing, to achieve more controllable\nand stable responses. Through visual interaction with a graphical user\ninterface, users can incorporate their ideas into the workflow without writing\ntrivial prompts. The proposed Low-code LLM framework consists of a Planning LLM\nthat designs a structured planning workflow for complex tasks, which can be\ncorrespondingly edited and confirmed by users through low-code visual\nprogramming operations, and an Executing LLM that generates responses following\nthe user-confirmed workflow. We highlight three advantages of the low-code LLM:\ncontrollable generation results, user-friendly human-LLM interaction, and\nbroadly applicable scenarios. We demonstrate its benefits using four typical\napplications. By introducing this approach, we aim to bridge the gap between\nhumans and LLMs, enabling more effective and efficient utilization of LLMs for\ncomplex tasks. Our system will be soon publicly available at LowCodeLLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs\u2b1b  Recent research has demonstrated that Large Language Models (LLMs) can\nenhance their capabilities by utilizing external tools. However, three pivotal\nquestions remain unanswered: (1) How effective are current LLMs in utilizing\ntools? (2) How can we enhance LLMs' ability to utilize tools? (3) What\nobstacles need to be overcome to leverage tools? To address these questions, we\nintroduce API-Bank, a groundbreaking benchmark, specifically designed for\ntool-augmented LLMs. For the first question, we develop a runnable evaluation\nsystem consisting of 73 API tools. We annotate 314 tool-use dialogues with 753\nAPI calls to assess the existing LLMs' capabilities in planning, retrieving,\nand calling APIs. For the second question, we construct a comprehensive\ntraining set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000\ndistinct domains. Using this dataset, we train Lynx, a tool-augmented LLM\ninitialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits\nimproved tool utilization compared to GPT-3, while GPT-4 excels in planning.\nHowever, there is still significant potential for further improvement.\nMoreover, Lynx surpasses Alpaca's tool utilization performance by more than 26\npts and approaches the effectiveness of GPT-3.5. Through error analysis, we\nhighlight the key challenges for future research in this field to answer the\nthird question.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to\n  Fine-Tune and Hard to Detect with other LLMs\u2b1b  The self-attention revolution allowed generative language models to scale and\nachieve increasingly impressive abilities. Such models - commonly referred to\nas Large Language Models (LLMs) - have recently gained prominence with the\ngeneral public, thanks to conversational fine-tuning, putting their behavior in\nline with public expectations regarding AI. This prominence amplified prior\nconcerns regarding the misuse of LLMs and led to the emergence of numerous\ntools to detect LLMs in the wild.\n  Unfortunately, most such tools are critically flawed. While major\npublications in the LLM detectability field suggested that LLMs were easy to\ndetect with fine-tuned autoencoders, the limitations of their results are easy\nto overlook. Specifically, they assumed publicly available generative models\nwithout fine-tunes or non-trivial prompts. While the importance of these\nassumptions has been demonstrated, until now, it remained unclear how well such\ndetection could be countered.\n  Here, we show that an attacker with access to such detectors' reference human\ntexts and output not only evades detection but can fully frustrate the detector\ntraining - with a reasonable budget and all its outputs labeled as such.\nAchieving it required combining common \"reinforcement from critic\" loss\nfunction modification and AdamW optimizer, which led to surprisingly good\nfine-tuning generalization. Finally, we warn against the temptation to\ntranspose the conclusions obtained in RNN-driven text GANs to LLMs due to their\nbetter representative ability.\n  These results have critical implications for the detection and prevention of\nmalicious use of generative language models, and we hope they will aid the\ndesigners of generative models and detectors.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM as A Robotic Brain: Unifying Egocentric Memory and Control\u2b1b  Embodied AI focuses on the study and development of intelligent systems that\npossess a physical or virtual embodiment (i.e. robots) and are able to\ndynamically interact with their environment. Memory and control are the two\nessential parts of an embodied system and usually require separate frameworks\nto model each of them. In this paper, we propose a novel and generalizable\nframework called LLM-Brain: using Large-scale Language Model as a robotic brain\nto unify egocentric memory and control. The LLM-Brain framework integrates\nmultiple multimodal language models for robotic tasks, utilizing a zero-shot\nlearning approach. All components within LLM-Brain communicate using natural\nlanguage in closed-loop multi-round dialogues that encompass perception,\nplanning, control, and memory. The core of the system is an embodied LLM to\nmaintain egocentric memory and control the robot. We demonstrate LLM-Brain by\nexamining two downstream tasks: active exploration and embodied question\nanswering. The active exploration tasks require the robot to extensively\nexplore an unknown environment within a limited number of actions. Meanwhile,\nthe embodied question answering tasks necessitate that the robot answers\nquestions based on observations acquired during prior explorations.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs\u2b1b  Large language models are becoming increasingly pervasive and ubiquitous in\nsociety via deployment in sociotechnical systems. Yet these language models, be\nit for classification or generation, have been shown to be biased and behave\nirresponsibly, causing harm to people at scale. It is crucial to audit these\nlanguage models rigorously. Existing auditing tools leverage either or both\nhumans and AI to find failures. In this work, we draw upon literature in\nhuman-AI collaboration and sensemaking, and conduct interviews with research\nexperts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro\nand Lundberg, 2022), which is powered by a generative large language model\n(LLM). Through the design process we highlight the importance of sensemaking\nand human-AI communication to leverage complementary strengths of humans and\ngenerative models in collaborative auditing. To evaluate the effectiveness of\nthe augmented tool, AdaTest++, we conduct user studies with participants\nauditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment\nanalysis model. Qualitative analysis shows that AdaTest++ effectively leverages\nhuman strengths such as schematization, hypothesis formation and testing.\nFurther, with our tool, participants identified a variety of failures modes,\ncovering 26 different topics over 2 tasks, that have been shown before in\nformal audits and also those previously under-reported.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Dialectical language model evaluation: An initial appraisal of the\n  commonsense spatial reasoning abilities of LLMs\u2b1b  Language models have become very popular recently and many claims have been\nmade about their abilities, including for commonsense reasoning. Given the\nincreasingly better results of current language models on previous static\nbenchmarks for commonsense reasoning, we explore an alternative dialectical\nevaluation. The goal of this kind of evaluation is not to obtain an aggregate\nperformance value but to find failures and map the boundaries of the system.\nDialoguing with the system gives the opportunity to check for consistency and\nget more reassurance of these boundaries beyond anecdotal evidence. In this\npaper we conduct some qualitative investigations of this kind of evaluation for\nthe particular case of spatial reasoning (which is a fundamental aspect of\ncommonsense reasoning). We conclude with some suggestions for future work both\nto improve the capabilities of language models and to systematise this kind of\ndialectical evaluation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth\n  Grade Math Answers\u2b1b  Written answers to open-ended questions can have a higher long-term effect on\nlearning than multiple-choice questions. However, it is critical that teachers\nimmediately review the answers, and ask to redo those that are incoherent. This\ncan be a difficult task and can be time-consuming for teachers. A possible\nsolution is to automate the detection of incoherent answers. One option is to\nautomate the review with Large Language Models (LLM). In this paper, we analyze\nthe responses of fourth graders in mathematics using three LLMs: GPT-3, BLOOM,\nand YOU. We used them with zero, one, two, three and four shots. We compared\ntheir performance with the results of various classifiers trained with Machine\nLearning (ML). We found that LLMs perform worse than MLs in detecting\nincoherent answers. The difficulty seems to reside in recursive questions that\ncontain both questions and answers, and in responses from students with typical\nfourth-grader misspellings. Upon closer examination, we have found that the\nChatGPT model faces the same challenges.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of\n  LLMs with Self-Information-Based Content Filtering\u2b1b  Large language models (LLMs) have received significant attention by achieving\nremarkable performance across various tasks. However, their fixed context\nlength poses challenges when processing long documents or maintaining extended\nconversations. This paper proposes a method called \\textit{Selective Context}\nthat employs self-information to filter out less informative content, thereby\nenhancing the efficiency of the fixed context length. We demonstrate the\neffectiveness of our approach on tasks of summarisation and question answering\nacross different data sources, including academic papers, news articles, and\nconversation transcripts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ChatLLM Network: More brains, More intelligence\u2b1b  Dialogue-based language models mark a huge milestone in the field of\nartificial intelligence, by their impressive ability to interact with users, as\nwell as a series of challenging tasks prompted by customized instructions.\nHowever, the prevalent large-scale dialogue-based language models like ChatGPT\nstill have room for improvement, such as unstable responses to questions and\nthe inability to think cooperatively like humans. Considering the ability of\ndialogue-based language models in conversation and their inherent randomness in\nthinking, we propose ChatLLM network that allows multiple dialogue-based\nlanguage models to interact, provide feedback, and think together. We design\nthe network of ChatLLMs based on ChatGPT. Specifically, individual instances of\nChatGPT may possess distinct perspectives towards the same problem, and by\nconsolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM\nnetwork system can conduct decision-making more objectively and\ncomprehensively. In addition, a language-based feedback mechanism comparable to\nbackpropagation is devised to update the ChatGPTs within the network.\nExperiments on two datasets demonstrate that our network attains significant\nimprovements in problem-solving, leading to observable progress amongst each\nmember.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\u2b1b  This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent\n  Diffusion Model\u2b1b  The immense scale of the recent large language models (LLM) allows many\ninteresting properties, such as, instruction- and chain-of-thought-based\nfine-tuning, that has significantly improved zero- and few-shot performance in\nmany natural language processing (NLP) tasks. Inspired by such successes, we\nadopt such an instruction-tuned LLM Flan-T5 as the text encoder for\ntext-to-audio (TTA) generation -- a task where the goal is to generate an audio\nfrom its textual description. The prior works on TTA either pre-trained a joint\ntext-audio encoder or used a non-instruction-tuned model, such as, T5.\nConsequently, our latent diffusion model (LDM)-based approach TANGO outperforms\nthe state-of-the-art AudioLDM on most metrics and stays comparable on the rest\non AudioCaps test set, despite training the LDM on a 63 times smaller dataset\nand keeping the text encoder frozen. This improvement might also be attributed\nto the adoption of audio pressure level-based sound mixing for training set\naugmentation, whereas the prior methods take a random mix.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Internal State of an LLM Knows When It's Lying\u2b1b  While Large Language Models (LLMs) have shown exceptional performance in\nvarious tasks, one of their most prominent drawbacks is generating inaccurate\nor false information with a confident tone. In this paper, we provide evidence\nthat the LLM's internal state can be used to reveal the truthfulness of\nstatements. This includes both statements provided to the LLM, and statements\nthat the LLM itself generates. Our approach is to train a classifier that\noutputs the probability that a statement is truthful, based on the hidden layer\nactivations of the LLM as it reads or generates the statement. Experiments\ndemonstrate that given a set of test sentences, of which half are true and half\nfalse, our trained classifier achieves an average of 71\\% to 83\\% accuracy\nlabeling which sentences are true versus false, depending on the LLM base\nmodel. Furthermore, we explore the relationship between our classifier's\nperformance and approaches based on the probability assigned to the sentence by\nthe LLM. We show that while LLM-assigned sentence probability is related to\nsentence truthfulness, this probability is also dependent on sentence length\nand the frequencies of words in the sentence, resulting in our trained\nclassifier providing a more reliable approach to detecting truthfulness,\nhighlighting its potential to enhance the reliability of LLM-generated content\nand its practical applicability in real-world scenarios.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Origin Tracing and Detecting of LLMs\u2b1b  The extraordinary performance of large language models (LLMs) heightens the\nimportance of detecting whether the context is generated by an AI system. More\nimportantly, while more and more companies and institutions release their LLMs,\nthe origin can be hard to trace. Since LLMs are heading towards the time of\nAGI, similar to the origin tracing in anthropology, it is of great importance\nto trace the origin of LLMs. In this paper, we first raise the concern of the\norigin tracing of LLMs and propose an effective method to trace and detect\nAI-generated contexts. We introduce a novel algorithm that leverages the\ncontrastive features between LLMs and extracts model-wise features to trace the\ntext origins. Our proposed method works under both white-box and black-box\nsettings therefore can be widely generalized to detect various LLMs.(e.g. can\nbe generalized to detect GPT-3 models without the GPT-3 models). Also, our\nproposed method requires only limited data compared with the supervised\nlearning methods and can be extended to trace new-coming model origins. We\nconstruct extensive experiments to examine whether we can trace the origins of\ngiven texts. We provide valuable observations based on the experimental\nresults, such as the difficulty level of AI origin tracing, and the AI origin\nsimilarities, and call for ethical concerns of LLM providers. We are releasing\nall codes and data as a toolkit and benchmark for future AI origin tracing and\ndetecting studies. \\footnote{We are releasing all available resource at\n\\url{https://github.com/OpenLMLab/}.}\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques\n  for LLMs\u2b1b  As foundation models continue to exponentially scale in size, efficient\nmethods of adaptation become increasingly critical. Parameter-efficient\nfine-tuning (PEFT), a recent class of techniques that require only modifying a\nsmall percentage of the model parameters, is currently the most popular method\nfor adapting large language models (LLMs). Several PEFT techniques have\nrecently been proposed with varying tradeoffs. We provide a comprehensive and\nuniform benchmark of various PEFT techniques across a representative LLM, the\nFLAN-T5 model, and evaluate model performance across different data scales of\nclassification and generation datasets. Based on this, we provide a framework\nfor choosing the optimal fine-tuning techniques given the task type and data\navailability. Contrary to popular belief, we also empirically prove that PEFT\ntechniques converge slower than full tuning in low data scenarios, and posit\nthe amount of data required for PEFT methods to both perform well and converge\nefficiently. Lastly, we further optimize these PEFT techniques by selectively\nchoosing which parts of the model to train, and find that these techniques can\nbe applied with significantly fewer parameters while maintaining and even\nimproving performance.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Large Linguistic Models: Analyzing theoretical linguistic abilities of\n  LLMs\u2b1b  The performance of large language models (LLMs) has recently improved to the\npoint where the models can perform well on many language tasks. We show here\nthat for the first time, the models can also generate coherent and valid formal\nanalyses of linguistic data and illustrate the vast potential of large language\nmodels for analyses of their metalinguistic abilities. LLMs are primarily\ntrained on language data in the form of text; analyzing and evaluating their\nmetalinguistic abilities improves our understanding of their general\ncapabilities and sheds new light on theoretical models in linguistics. In this\npaper, we probe into GPT-4's metalinguistic capabilities by focusing on three\nsubfields of formal linguistics: syntax, phonology, and semantics. We outline a\nresearch program for metalinguistic analyses of large language models, propose\nexperimental designs, provide general guidelines, discuss limitations, and\noffer future directions for this line of research. This line of inquiry also\nexemplifies behavioral interpretability of deep learning, where models'\nrepresentations are accessed by explicit prompting rather than internal\nrepresentations.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "VPGTrans: Transfer Visual Prompt Generator across LLMs\u2b1b  While developing a new multimodal LLM (MLLM) by pre-training on tremendous\nimage-text pairs from scratch can be exceedingly resource-consuming, connecting\nan existing LLM with a comparatively lightweight visual prompt generator (VPG)\nbecomes a feasible paradigm. However, further tuning the VPG part of the MLLM\nstill suffers from indispensable computational costs, i.e., requiring thousands\nof GPU hours and millions of training data. One alternative solution is to\ntransfer an existing VPG from any existing MLLMs for the target MLLM.\n  In this work, we for the first time investigate the VPG transferability\nacross LLMs, and explore a solution to reduce the cost of VPG transfer. We\nfirst study the VPG transfer across different LLM sizes (e.g., small-to-large),\nand across different LLM types, through which we diagnose the key factors to\nmaximize the transfer efficiency. Based on our observation, we design a\ntwo-stage transfer framework named VPGTrans, which is simple yet highly\neffective. Through extensive experiments, we demonstrate that VPGTrans helps\nsignificantly speed up the transfer learning process without compromising\nperformance. Remarkably, it helps achieve the VPG transfer from BLIP-2\nOPT$_\\text{2.7B}$ to BLIP-2 OPT$_\\text{6.7B}$ with over 10 times speed-up and\n10.7% training data compared with connecting a VPG to OPT$_\\text{6.7B}$ from\nscratch. Further, a series of intriguing findings and potential rationales\nbehind them are provided and discussed. Finally, we showcase the practical\nvalue of our VPGTrans approach, by customizing two novel MLLMs, including\nVL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs Capture Intertemporal Preferences?\u2b1b  We explore the viability of Large Language Models (LLMs), specifically\nOpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting\npreferences, with a focus on intertemporal choices. Leveraging the extensive\nliterature on intertemporal discounting for benchmarking, we examine responses\nfrom LLMs across various languages and compare them to human responses,\nexploring preferences between smaller, sooner, and larger, later rewards. Our\nfindings reveal that both GPT models demonstrate less patience than humans,\nwith GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike\nhuman decision-makers. Though GPT-4 does not display lexicographic preferences,\nits measured discount rates are still considerably larger than those found in\nhumans. Interestingly, GPT models show greater patience in languages with weak\nfuture tense references, such as German and Mandarin, aligning with existing\nliterature that suggests a correlation between language structure and\nintertemporal preferences. We demonstrate how prompting GPT to explain its\ndecisions, a procedure we term ``chain-of-thought conjoint,\" can mitigate, but\ndoes not eliminate, discrepancies between LLM and human responses. While\ndirectly eliciting preferences using LLMs may yield misleading results,\ncombining chain-of-thought conjoint with topic modeling aids in hypothesis\ngeneration, enabling researchers to explore the underpinnings of preferences.\nChain-of-thought conjoint provides a structured framework for marketers to use\nLLMs to identify potential attributes or factors that can explain preference\nheterogeneity across different customers and contexts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PersonaLLM: Investigating the Ability of Large Language Models to\n  Express Big Five Personality Traits\u2b1b  Despite the many use cases for large language models (LLMs) in creating\npersonalized chatbots, there has been limited research on evaluating the extent\nto which the behaviors of personalized LLMs accurately and consistently reflect\nspecific personality traits. We consider studying the behavior of LLM-based\nagents, referred to as LLM personas, and present a case study with ChatGPT and\nGPT-4. The study investigates whether LLMs can generate content that aligns\nwith their assigned personality profiles. To this end, we create distinct LLM\npersonas based on the Big Five personality model, have them complete the\n44-item Big Five Inventory (BFI) personality test and a story writing task, and\nthen assess their essays with automatic and human evaluations. Results show\nthat LLM personas' self-reported BFI scores are consistent with their\ndesignated personality types, with large effect sizes observed across five\ntraits. Additionally, there are significant correlations between the assigned\npersonality types and certain psycholinguistic features of their writings, as\nmeasured by the Linguistic Inquiry and Word Count (LIWC) tool. Interestingly,\nhuman evaluators perceive the stories as less personal when told that the\nstories are authored by AI. However, their judgments on other aspects of the\nwriting such as readability, cohesiveness, redundancy, likeability, and\nbelievability remain largely unaffected. Notably, when evaluators were informed\nabout the AI authorship, their accuracy in identifying the intended personality\ntraits from the stories decreased by more than 10% for some traits. This\nresearch marks a significant step forward in understanding the capabilities of\nLLMs to express personality traits.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Panda LLM: Training Data and Evaluation for Open-Sourced Chinese\n  Instruction-Following Large Language Models\u2b1b  This project focuses on enhancing open-source large language models through\ninstruction-tuning and providing comprehensive evaluations of their\nperformance. We explore how various training data factors, such as quantity,\nquality, and linguistic distribution, influence the performance of\ninstruction-tuned models trained on publicly accessible high-quality\ninstruction datasets for both English and Chinese languages. Our goal is to\nsupplement evaluation with quantitative analyses, providing valuable insights\nfor the continued advancement of open-source chat models. Our model, data, and\ncode are publicly available for others to use and build upon.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLM Already Serve as A Database Interface? A BIg Bench for\n  Large-Scale Database Grounded Text-to-SQLs\u2b1b  Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using\n  XLM-RoBERTa\u2b1b  Named Entity Recognition(NER) is a task of recognizing entities at a token\nlevel in a sentence. This paper focuses on solving NER tasks in a multilingual\nsetting for complex named entities. Our team, LLM-RM participated in the\nrecently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual\nComplex Named Entity Recognition. We approach the problem by leveraging\ncross-lingual representation provided by fine-tuning XLM-Roberta base model on\ndatasets of all of the 12 languages provided -- Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and\nUkrainian\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Jointly Extracting Interventions, Outcomes, and Findings from RCT\n  Reports with LLMs\u2b1b  Results from Randomized Controlled Trials (RCTs) establish the comparative\neffectiveness of interventions, and are in turn critical inputs for\nevidence-based care. However, results from RCTs are presented in (often\nunstructured) natural language articles describing the design, execution, and\noutcomes of trials; clinicians must manually extract findings pertaining to\ninterventions and outcomes of interest from such articles. This onerous manual\nprocess has motivated work on (semi-)automating extraction of structured\nevidence from trial reports. In this work we propose and evaluate a\ntext-to-text model built on instruction-tuned Large Language Models (LLMs) to\njointly extract Interventions, Outcomes, and Comparators (ICO elements) from\nclinical abstracts, and infer the associated results reported. Manual (expert)\nand automated evaluations indicate that framing evidence extraction as a\nconditional generation task and fine-tuning LLMs for this purpose realizes\nconsiderable ($\\sim$20 point absolute F1 score) gains over the previous SOTA.\nWe perform ablations and error analyses to assess aspects that contribute to\nmodel performance, and to highlight potential directions for further\nimprovements. We apply our model to a collection of published RCTs through\nmid-2022, and release a searchable database of structured findings:\nhttp://ico-relations.ebm-nlp.com\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Refining the Responses of LLMs by Themselves\u2b1b  In this paper, we propose a simple yet efficient approach based on prompt\nengineering that leverages the large language model itself to optimize its\nanswers without relying on auxiliary models. We introduce an iterative\nself-evaluating optimization mechanism, with the potential for improved output\nquality as iterations progress, removing the need for manual intervention. The\nexperiment's findings indicate that utilizing our response refinement framework\non the GPT-3.5 model yields results that are on par with, or even surpass,\nthose generated by the cutting-edge GPT-4 model. Detailed implementation\nstrategies and illustrative examples are provided to demonstrate the\nsuperiority of our proposed solution.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "X-LLM: Bootstrapping Advanced Large Language Models by Treating\n  Multi-Modalities as Foreign Languages\u2b1b  Large language models (LLMs) have demonstrated remarkable language abilities.\nGPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities\nbeyond previous visual language models. We attribute this to the use of more\nadvanced LLMs compared with previous multimodal models. Unfortunately, the\nmodel architecture and training strategies of GPT-4 are unknown. To endow LLMs\nwith multimodal capabilities, we propose X-LLM, which converts Multi-modalities\n(images, speech, videos) into foreign languages using X2L interfaces and inputs\nthem into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple\nfrozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X''\ndenotes multi-modalities such as image, speech, and videos, and ``L'' denotes\nlanguages. X-LLM's training consists of three stages: (1) Converting Multimodal\nInformation: The first stage trains each X2L interface to align with its\nrespective single-modal encoder separately to convert multimodal information\ninto languages. (2) Aligning X2L representations with the LLM: single-modal\nencoders are aligned with the LLM through X2L interfaces independently. (3)\nIntegrating multiple modalities: all single-modal encoders are aligned with the\nLLM through X2L interfaces to integrate multimodal capabilities into the LLM.\nOur experiments show that X-LLM demonstrates impressive multimodel chat\nabilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen\nimages/instructions, and yields a 84.5\\% relative score compared with GPT-4 on\na synthetic multimodal instruction-following dataset. And we also conduct\nquantitative tests on using LLM for ASR and multimodal ASR, hoping to promote\nthe era of LLM-based speech recognition.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation\u2b1b  In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for\ncreating high-quality conversational agents without the need for fine-tuning.\nOur method utilizes pre-trained large language models (LLMs) as individual\nmodules for long-term consistency and flexibility, by using techniques such as\nfew-shot prompting, chain-of-thought (CoT), and external memory. Our human\nevaluation results show that MPC is on par with fine-tuned chatbot models in\nopen-domain conversations, making it an effective solution for creating\nconsistent and engaging chatbots.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Read, Diagnose and Chat: Towards Explainable and Interactive\n  LLMs-Augmented Depression Detection in Social Media\u2b1b  This paper proposes a new depression detection system based on LLMs that is\nboth interpretable and interactive. It not only provides a diagnosis, but also\ndiagnostic evidence and personalized recommendations based on natural language\ndialogue with the user. We address challenges such as the processing of large\namounts of text and integrate professional diagnostic criteria. Our system\noutperforms traditional methods across various settings and is demonstrated\nthrough case studies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Multilingual LLMs are Better Cross-lingual In-context Learners with\n  Alignment\u2b1b  In-context learning (ICL) unfolds as large language models become capable of\ninferring test labels conditioned on a few labeled samples without any gradient\nupdate. ICL-enabled large language models provide a promising step forward\ntoward bypassing recurrent annotation costs in a low-resource setting. Yet,\nonly a handful of past studies have explored ICL in a cross-lingual setting, in\nwhich the need for transferring label-knowledge from a high-resource language\nto a low-resource one is immensely crucial. To bridge the gap, we provide the\nfirst in-depth analysis of ICL for cross-lingual text classification. We find\nthat the prevalent mode of selecting random input-label pairs to construct the\nprompt-context is severely limited in the case of cross-lingual ICL, primarily\ndue to the lack of alignment in the input as well as the output spaces. To\nmitigate this, we propose a novel prompt construction strategy -- Cross-lingual\nIn-context Source-Target Alignment (X-InSTA). With an injected coherence in the\nsemantics of the input examples and a task-based alignment across the source\nand target languages, X-InSTA is able to outperform random prompt selection by\na large margin across three different tasks using 44 different cross-lingual\npairs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Not All Languages Are Created Equal in LLMs: Improving Multilingual\n  Capability by Cross-Lingual-Thought Prompting\u2b1b  Large language models (LLMs) demonstrate impressive multilingual capability,\nbut their performance varies substantially across different languages. In this\nwork, we introduce a simple yet effective method, called cross-lingual-thought\nprompting (XLT), to systematically improve the multilingual capability of LLMs.\nSpecifically, XLT is a generic template prompt that stimulates cross-lingual\nand logical reasoning skills to enhance task performance across languages. We\nconduct comprehensive evaluations on 7 typical benchmarks related to reasoning,\nunderstanding, and generation tasks, covering both high-resource and\nlow-resource languages. Experimental results show that XLT not only remarkably\nenhances the performance of various multilingual tasks but also significantly\nreduces the gap between the average performance and the best performance of\neach task in different languages. Notably, XLT brings over 10 points of average\nimprovement in arithmetic reasoning and open-domain question-answering tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PALR: Personalization Aware LLMs for Recommendation\u2b1b  Large language models (LLMs) have recently received significant attention for\ntheir exceptional capabilities. Despite extensive efforts in developing\ngeneral-purpose LLMs that can be utilized in various natural language\nprocessing (NLP) tasks, there has been less research exploring their potential\nin recommender systems. In this paper, we propose a novel framework, named\nPALR, which aiming to combine user history behaviors (such as clicks,\npurchases, ratings, etc.) with LLMs to generate user preferred items.\nSpecifically, we first use user/item interactions as guidance for candidate\nretrieval. Then we adopt a LLM-based ranking model to generate recommended\nitems. Unlike existing approaches that typically adopt general-purpose LLMs for\nzero/few-shot recommendation testing or training on small-sized language models\n(with less than 1 billion parameters), which cannot fully elicit LLMs'\nreasoning abilities and leverage rich item side parametric knowledge, we\nfine-tune a 7 billion parameters LLM for the ranking purpose. This model takes\nretrieval candidates in natural language format as input, with instruction\nwhich explicitly asking to select results from input candidates during\ninference. Our experimental results demonstrate that our solution outperforms\nstate-of-the-art models on various sequential recommendation tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting\u2b1b  Building end-to-end task bots and maintaining their integration with new\nfunctionalities using minimal human efforts is a long-standing challenge in\ndialog research. Recently large language models (LLMs) have demonstrated\nexceptional proficiency in conversational engagement and adherence to\ninstructions across various downstream tasks. In this work, we introduce\nSGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems\neffortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we\ninstruct fixed LLMs to generate appropriate responses on novel tasks,\ncircumventing the need for training data. Specifically, SGP-TOD comprises three\ncomponents: a LLM for engaging with users, a DST Prompter to aid the LLM with\ndialog state tracking, which is then used to retrieve database items, and a\nPolicy Prompter to elicit proper responses adhering to the provided dialog\npolicy. Experimental results on Multiwoz, RADDLE and STAR datasets show that\nour training-free strategy SGP-TOD, without any task-specific data, yields\nstate-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot\napproaches. In a domain-extension setting, SGP-TOD aptly adapts to new\nfunctionalities by merely adding supplementary schema rules. We make our code\nand data publicly available.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Knowledge Graph Completion Models are Few-shot Learners: An Empirical\n  Study of Relation Labeling in E-commerce with LLMs\u2b1b  Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system\nperformance by providing structured information about entities and their\nrelationships, such as complementary or substitutable relations between\nproducts or product types, which can be utilized in recommender systems.\nHowever, relation labeling in KGs remains a challenging task due to the dynamic\nnature of e-commerce domains and the associated cost of human labor. Recently,\nbreakthroughs in Large Language Models (LLMs) have shown surprising results in\nnumerous natural language processing tasks. In this paper, we conduct an\nempirical study of LLMs for relation labeling in e-commerce KGs, investigating\ntheir powerful learning capabilities in natural language and effectiveness in\npredicting relations between product types with limited labeled data. We\nevaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets,\ndemonstrating their ability to achieve competitive performance compared to\nhumans on relation labeling tasks using just 1 to 5 labeled examples per\nrelation. Additionally, we experiment with different prompt engineering\ntechniques to examine their impact on model performance. Our results show that\nLLMs significantly outperform existing KG completion models in relation\nlabeling for e-commerce KGs and exhibit performance strong enough to replace\nhuman labeling.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized\n  Language Models\u2b1b  By design, large language models (LLMs) are static general-purpose models,\nexpensive to retrain or update frequently. As they are increasingly adopted for\nknowledge-intensive tasks, it becomes evident that these design choices lead to\nfailures to generate factual, relevant, and up-to-date knowledge. To this end,\nwe propose \\ourmethod{}, a modular framework to plug in new factual and\nrelevant knowledge into general-purpose LLMs. We first introduce\n\\emph{knowledge cards} -- specialized language models trained on corpora from\nspecific domains and sources. Knowledge cards serve as parametric repositories\nthat are selected at inference time to generate background knowledge for the\nbase LLM. We then propose three content selectors to dynamically select and\nretain information in documents generated by knowledge cards, specifically\ncontrolling for \\emph{relevance}, \\emph{brevity}, and \\emph{factuality} of\noutputs. Finally, we propose two complementary integration approaches to\naugment the base LLM with the (relevant, factual) knowledge curated from the\nspecialized LMs. Through extensive experiments, we demonstrate that\n\\ourmethod{} achieves state-of-the-art performance on six benchmark datasets.\nUltimately, \\ourmethod{} framework enables dynamic synthesis and updates of\nknowledge from diverse domains. Its modularity will ensure that relevant\nknowledge can be continuously updated through the collective efforts of the\nresearch community.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMScore: Unveiling the Power of Large Language Models in Text-to-Image\n  Synthesis Evaluation\u2b1b  Existing automatic evaluation on text-to-image synthesis can only provide an\nimage-text matching score, without considering the object-level\ncompositionality, which results in poor correlation with human judgments. In\nthis work, we propose LLMScore, a new framework that offers evaluation scores\nwith multi-granularity compositionality. LLMScore leverages the large language\nmodels (LLMs) to evaluate text-to-image models. Initially, it transforms the\nimage into image-level and object-level visual descriptions. Then an evaluation\ninstruction is fed into the LLMs to measure the alignment between the\nsynthesized image and the text, ultimately generating a score accompanied by a\nrationale. Our substantial analysis reveals the highest correlation of LLMScore\nwith human judgments on a wide range of datasets (Attribute Binding Contrast,\nConcept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScore\nachieves Kendall's tau correlation with human evaluations that is 58.8% and\n31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP,\nrespectively.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM\n  Inference with Transferable Prompt\u2b1b  While the numerous parameters in Large Language Models (LLMs) contribute to\ntheir superior performance, this massive scale makes them inefficient and\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\nsingle GPU. Given the memory and power constraints of such devices, model\ncompression methods are widely employed to reduce both the model size and\ninference latency, which essentially trades off model quality in return for\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\ncrucial for the LLM deployment on commodity hardware. In this paper, we\nintroduce a new perspective to optimize this trade-off by prompting compressed\nmodels. Specifically, we first observe that for certain questions, the\ngeneration quality of a compressed LLM can be significantly improved by adding\ncarefully designed hard prompts, though this isn't the case for all questions.\nBased on this observation, we propose a soft prompt learning method where we\nexpose the compressed model to the prompt learning process, aiming to enhance\nthe performance of prompts. Our experimental analysis suggests our soft prompt\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\ndemonstrate that these learned prompts can be transferred across various\ndatasets, tasks, and compression levels. Hence with this transferability, we\ncan stitch the soft prompt to a newly compressed model to improve the test-time\naccuracy in an ``in-situ'' way.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Writing your own book: A method for going from closed to open book QA to\n  improve robustness and performance of smaller LLMs\u2b1b  We introduce two novel methods, Tree-Search and Self-contextualizing QA,\ndesigned to enhance the performance of large language models (LLMs) in\nquestion-answering tasks. Tree-Search is a sampling technique specifically\ncreated to extract diverse information from an LLM for a given prompt.\nSelf-contextualizing QA leverages Tree-Search to enable the model to create its\nown context using a wide range of information relevant to the prompt, evaluate\nit explicitly and return a open book answer to the initial prompt . We\ndemonstrate that the quality of generated answers improves according to various\nmetrics, including accuracy, informativeness, coherence, and consistency, as\nevaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods\nresult in increased robustness and that performance is positively correlated\nwith tree size, benefiting both answer quality and robustness. Finally, we\ndiscuss other promising applications of Tree-Search, highlighting its potential\nto enhance a broad range of tasks beyond question-answering.\n  \\noindent We also discuss several areas for future work, including refining\nthe Tree-Search and Self-Contextualizing QA methods, improving the coherence of\nthe generated context, and investigating the impact of bootstrapping on model\nrobustness\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks\u2b1b  While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and\n  Generation\u2b1b  Following the impressive development of LLMs, vision-language alignment in\nLLMs is actively being researched to enable multimodal reasoning and visual IO.\nThis direction of research is particularly relevant to medical imaging because\nmedical image analysis and generation consist of reasoning based on a\ncombination of visual features and prior knowledge. Many recent works have\nfocused on training adapter networks that serve as an information bridge\nbetween image processing networks and LLMs; but presumably, in order to achieve\nmaximum reasoning potential of LLMs on visual information as well, visual and\nlanguage features should be allowed to interact more freely. This is especially\nimportant in the medical domain because understanding and generating medical\nimages such as chest X-rays (CXR) require not only accurate visual and\nlanguage-based reasoning but also a more intimate mapping between the two\nmodalities. Thus, taking inspiration from previous work on the transformer and\nVQ-GAN combination for bidirectional image and text generation, we build upon\nthis approach and develop a method for instruction-tuning an LLM pre-trained\nonly on text to gain vision-language capabilities for medical images.\nSpecifically, we leverage a pretrained LLM's existing question-answering and\ninstruction-following abilities to teach it to understand visual inputs by\ninstructing it to answer questions about image inputs and, symmetrically,\noutput both text and image responses appropriate to a given query by tuning the\nLLM with diverse tasks that encompass image-based text-generation and\ntext-based image-generation. We show that our model, LLM-CXR, trained in this\napproach shows better image-text alignment in both CXR understanding and\ngeneration tasks while being smaller in size compared to previously developed\nmodels that perform a narrower range of tasks. The code is at\nhttps://github.com/hyn2028/llm-cxr.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Pruner: On the Structural Pruning of Large Language Models\u2b1b  Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\nboth the deployment, inference, and training stages. With LLM being a\ngeneral-purpose task solver, we explore its compression in a task-agnostic\nmanner, which aims to preserve the multi-task solving and language generation\nability of the original LLM. One challenge to achieving this is the enormous\nsize of the training corpus of LLM, which makes both data transfer and model\npost-training over-burdensome. Thus, we tackle the compression of LLMs within\nthe bound of two constraints: being task-agnostic and minimizing the reliance\non the original training dataset. Our method, named LLM-Pruner, adopts\nstructural pruning that selectively removes non-critical coupled structures\nbased on gradient information, maximally preserving the majority of the LLM's\nfunctionality. To this end, the performance of pruned models can be efficiently\nrecovered through tuning techniques, LoRA, in merely 3 hours, requiring only\n50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,\nand ChatGLM, and demonstrate that the compressed models still exhibit\nsatisfactory capabilities in zero-shot classification and generation. The code\nis available at: https://github.com/horseee/LLM-Pruner\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue\n  Questions with LLMs\u2b1b  Large Language Models (LLMs), such as \\texttt{ChatGPT}, greatly empower\ndialogue systems with strong language understanding and generation\ncapabilities. However, most of the previous works prompt the LLMs to directly\ngenerate a response based on the dialogue context, overlooking the underlying\nlinguistic cues about the user status exhibited in the context. Such in-depth\ndialogue scenarios are challenging for existing LLMs to figure out the user's\nhidden needs and respond satisfactorily through a single-step inference. To\nthis end, we propose a novel linguistic cue-based chain-of-thoughts\n(\\textit{Cue}-CoT), which enhances the LLMs inference with an intermediate\nreasoning step to find cues exhibited in the dialogue, aiming to provide a more\npersonalized and engaging response. To evaluate the approach, we build a\nbenchmark with in-depth dialogue questions, consisting of 6 datasets in both\nChinese and English, targeting 3 major linguistic cues during the conversation:\n\\textit{personality}, \\textit{emotion}, and \\textit{psychology}. We conduct\nextensive experiments on the proposed benchmark with 5 LLMs under both\nzero-shot and one-shot settings. Empirical results demonstrate our proposed\n\\textit{Cue}-CoT method outperforms standard prompting methods in terms of both\n\\textit{helpfulness} and \\textit{acceptability} on all datasets.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic\n  Reviews\u2b1b  Medical systematic reviews play a vital role in healthcare decision making\nand policy. However, their production is time-consuming, limiting the\navailability of high-quality and up-to-date evidence summaries. Recent\nadvancements in large language models (LLMs) offer the potential to\nautomatically generate literature reviews on demand, addressing this issue.\nHowever, LLMs sometimes generate inaccurate (and potentially misleading) texts\nby hallucination or omission. In healthcare, this can make LLMs unusable at\nbest and dangerous at worst. We conducted 16 interviews with international\nsystematic review experts to characterize the perceived utility and risks of\nLLMs in the specific context of medical evidence reviews. Experts indicated\nthat LLMs can assist in the writing process by drafting summaries, generating\ntemplates, distilling information, and crosschecking information. They also\nraised concerns regarding confidently composed but inaccurate LLM outputs and\nother potential downstream harms, including decreased accountability and\nproliferation of low-quality reviews. Informed by this qualitative analysis, we\nidentify criteria for rigorous evaluation of biomedical LLMs aligned with\ndomain expert views.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain,\n  and Cross-domain Settings\u2b1b  Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning\n  and Coding with LLMs\u2b1b  A popular approach for improving the correctness of output from large\nlanguage models (LLMs) is Self-Consistency - poll the LLM multiple times and\noutput the most frequent solution. Existing Self-Consistency techniques always\ngenerate a constant number of samples per question, where a better approach\nwill be to non-uniformly distribute the available budget based on the amount of\nagreement in the samples generated so far. In response, we introduce\nAdaptive-Consistency, a cost-efficient, model-agnostic technique that\ndynamically adjusts the number of samples per question using a lightweight\nstopping criterion. Our experiments over 17 reasoning and code generation\ndatasets and three LLMs demonstrate that Adaptive-Consistency reduces sample\nbudget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our\ncode and data are available at https://www.sample-step-by-step.info\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PiVe: Prompting with Iterative Verification Improving Graph-based\n  Generative Capability of LLMs\u2b1b  Large language models (LLMs) have shown great abilities of solving various\nnatural language tasks in different domains. Due to the training objective of\nLLMs and their pretraining data, LLMs are not very well equipped for tasks\ninvolving structured data generation. We propose a framework, Prompting with\nIterative Verification (PiVe), to improve graphbased generative capability of\nLLMs. We show how a small language model could be trained to act as a verifier\nmodule for the output of an LLM (i.e., ChatGPT), and to iteratively improve its\nperformance via fine-grained corrective instructions. Additionally, we show how\nthe verifier module could apply iterative corrections offline for a more\ncost-effective solution to the text-to-graph generation task. Experiments on\nthree graph-based datasets show consistent improvement gained via PiVe.\nAdditionally, we highlight how the proposed verifier module can be used as a\ndata augmentation tool to help improve the quality of automatically generated\nparallel text-graph datasets. Our code and data are available at\nhttps://github.com/Jiuzhouh/PiVe.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot\n  Setting and Performance Boosting Through Prompts\u2b1b  Large Language Models (LLMs) have exhibited remarkable performance on various\nNatural Language Processing (NLP) tasks. However, there is a current hot debate\nregarding their reasoning capacity. In this paper, we examine the performance\nof GPT-3.5, GPT-4, and BARD models, by performing a thorough technical\nevaluation on different reasoning tasks across eleven distinct datasets. Our\npaper provides empirical evidence showcasing the superior performance of\nChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting\nthroughout almost all evaluated tasks. While the superiority of GPT-4 compared\nto GPT-3.5 might be explained by its larger size and NLP efficiency, this was\nnot evident for BARD. We also demonstrate that the three models show limited\nproficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To\nbolster our findings, we present a detailed and comprehensive analysis of the\nresults from these three models. Furthermore, we propose a set of engineered\nprompts that enhances the zero-shot setting performance of all three models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Learning Interpretable Style Embeddings via Prompting LLMs\u2b1b  Style representation learning builds content-independent representations of\nauthor style in text. Stylometry, the analysis of style in text, is often\nperformed by expert forensic linguists and no large dataset of stylometric\nannotations exists for training. Current style representation learning uses\nneural methods to disentangle style from content to create style vectors,\nhowever, these approaches result in uninterpretable representations,\ncomplicating their usage in downstream applications like authorship attribution\nwhere auditing and explainability is critical. In this work, we use prompting\nto perform stylometry on a large number of texts to create a synthetic dataset\nand train human-interpretable style representations we call LISA embeddings. We\nrelease our synthetic stylometry dataset and our interpretable style models as\nresources.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Observations on LLMs for Telecom Domain: Capabilities and Limitations\u2b1b  The landscape for building conversational interfaces (chatbots) has witnessed\na paradigm shift with recent developments in generative Artificial Intelligence\n(AI) based Large Language Models (LLMs), such as ChatGPT by OpenAI (GPT3.5 and\nGPT4), Google's Bard, Large Language Model Meta AI (LLaMA), among others. In\nthis paper, we analyze capabilities and limitations of incorporating such\nmodels in conversational interfaces for the telecommunication domain,\nspecifically for enterprise wireless products and services. Using Cradlepoint's\npublicly available data for our experiments, we present a comparative analysis\nof the responses from such models for multiple use-cases including domain\nadaptation for terminology and product taxonomy, context continuity, robustness\nto input perturbations and errors. We believe this evaluation would provide\nuseful insights to data scientists engaged in building customized\nconversational interfaces for domain-specific requirements.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM\n  Inference Pipeline\u2b1b  Large language models (LLMs) have revolutionized the field of AI,\ndemonstrating unprecedented capacity across various tasks. However, the\ninference process for LLMs comes with significant computational costs. In this\npaper, we propose an efficient LLM inference pipeline that harnesses the power\nof LLMs. Our approach begins by tapping into the potential of LLMs to\naccurately perceive and predict the response length with minimal overhead. By\nleveraging this information, we introduce an efficient sequence scheduling\ntechnique that groups queries with similar response lengths into micro-batches.\nWe evaluate our approach on real-world instruction datasets using the\nLLaMA-based model, and our results demonstrate an impressive 86% improvement in\ninference throughput without compromising effectiveness. Notably, our method is\northogonal to other inference acceleration techniques, making it a valuable\naddition to many existing toolkits (e.g., FlashAttention, Quantization) for LLM\ninference.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via\n  Debate\u2b1b  Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive\nperformance in complex reasoning tasks. However, it is difficult to know\nwhether the models are reasoning based on deep understandings of truth and\nlogic, or leveraging their memorized patterns in a relatively superficial way.\nIn this work, we explore testing LLMs' reasoning by engaging with them in a\ndebate-like conversation, where given a question, the LLM and the user need to\ndiscuss to make the correct decision starting from opposing arguments. Upon\nmitigating the Clever Hans effect, our task requires the LLM to not only\nachieve the correct answer on its own, but also be able to hold and defend its\nbelief instead of blindly believing or getting misled by the user's (invalid)\narguments and critiques, thus testing in greater depth whether the LLM grasps\nthe essence of the reasoning required to solve the problem. Across a range of\ncomplex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench\ntasks, we find that despite their impressive performance as reported in\nexisting work on generating correct step-by-step solutions in the beginning,\nLLMs like ChatGPT cannot maintain their beliefs in truth for a significant\nportion of examples when challenged by oftentimes absurdly invalid arguments.\nOur work points to danger zones of model alignment, and also suggests more\ncareful treatments and interpretations of the recent findings that LLMs can\nimprove their responses based on feedback.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities\u2b1b  This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis\u2b1b  Recent research has highlighted the importance of dataset size in scaling\nlanguage models. However, large language models (LLMs) are notoriously\ntoken-hungry during pre-training, and high-quality text data on the web is\napproaching its scaling limit for LLMs. To further enhance LLMs, a\nstraightforward approach is to repeat the pre-training data for additional\nepochs. In this study, we empirically investigate three key aspects under this\napproach. First, we explore the consequences of repeating pre-training data,\nrevealing that the model is susceptible to overfitting, leading to multi-epoch\ndegradation. Second, we examine the key factors contributing to multi-epoch\ndegradation, finding that significant factors include dataset size, model\nparameters, and training objectives, while less influential factors consist of\ndataset quality and model FLOPs. Finally, we explore whether widely used\nregularization can alleviate multi-epoch degradation. Most regularization\ntechniques do not yield significant improvements, except for dropout, which\ndemonstrates remarkable effectiveness but requires careful tuning when scaling\nup the model size. Additionally, we discover that leveraging mixture-of-experts\n(MoE) enables cost-effective and efficient hyper-parameter tuning for\ncomputationally intensive dense LLMs with comparable trainable parameters,\npotentially impacting efficient LLM development on a broader scale.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How do languages influence each other? Studying cross-lingual data\n  sharing during LLM fine-tuning\u2b1b  Multilingual large language models (MLLMs) are jointly trained on data from\nmany different languages such that representation of individual languages can\nbenefit from other languages' data. Impressive performance on zero-shot\ncross-lingual transfer shows that these models are capable of exploiting data\nfrom other languages. Yet, it remains unclear to what extent, and under which\nconditions, languages rely on each other's data. In this study, we use TracIn\n(Pruthi et al., 2020), a training data attribution (TDA) method, to retrieve\nthe most influential training samples seen during multilingual fine-tuning for\na particular test language. This allows us to analyse cross-lingual sharing\nmechanisms of MLLMs from a new perspective. While previous work studied\ncross-lingual sharing at the level of model parameters, we present the first\napproach to study cross-lingual sharing at the data level. We find that MLLMs\nrely on data from multiple languages from the early stages of fine-tuning and\nthat this reliance gradually increases as fine-tuning progresses. We further\nstudy how different fine-tuning languages influence model performance on a\ngiven test language and find that they can both reinforce and complement the\nknowledge acquired from data of the test language itself.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs facilitate interpretation of pre-trained language models?\u2b1b  Work done to uncover the knowledge encoded within pre-trained language models\nrely on annotated corpora or human-in-the-loop methods. However, these\napproaches are limited in terms of scalability and the scope of interpretation.\nWe propose using a large language model, ChatGPT, as an annotator to enable\nfine-grained interpretation analysis of pre-trained language models. We\ndiscover latent concepts within pre-trained language models by applying\nagglomerative hierarchical clustering over contextualized representations and\nthen annotate these concepts using ChatGPT. Our findings demonstrate that\nChatGPT produces accurate and semantically richer annotations compared to\nhuman-annotated concepts. Additionally, we showcase how GPT-based annotations\nempower interpretation analysis methodologies of which we demonstrate two:\nprobing frameworks and neuron interpretation. To facilitate further exploration\nand experimentation in the field, we make available a substantial ConceptNet\ndataset (TCN) comprising 39,000 annotated concepts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-empowered Chatbots for Psychiatrist and Patient Simulation:\n  Application and Evaluation\u2b1b  Empowering chatbots in the field of mental health is receiving increasing\namount of attention, while there still lacks exploration in developing and\nevaluating chatbots in psychiatric outpatient scenarios. In this work, we focus\non exploring the potential of ChatGPT in powering chatbots for psychiatrist and\npatient simulation. We collaborate with psychiatrists to identify objectives\nand iteratively develop the dialogue system to closely align with real-world\nscenarios. In the evaluation experiments, we recruit real psychiatrists and\npatients to engage in diagnostic conversations with the chatbots, collecting\ntheir ratings for assessment. Our findings demonstrate the feasibility of using\nChatGPT-powered chatbots in psychiatric scenarios and explore the impact of\nprompt designs on chatbot behavior and user experience.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain\n  Conversations with Large Language Models\u2b1b  We propose LLM-Eval, a unified multi-dimensional automatic evaluation method\nfor open-domain conversations with large language models (LLMs). Existing\nevaluation methods often rely on human annotations, ground-truth responses, or\nmultiple LLM prompts, which can be expensive and time-consuming. To address\nthese issues, we design a single prompt-based evaluation method that leverages\na unified evaluation schema to cover multiple dimensions of conversation\nquality in a single model call. We extensively evaluate the performance of\nLLM-Eval on various benchmark datasets, demonstrating its effectiveness,\nefficiency, and adaptability compared to state-of-the-art evaluation methods.\nOur analysis also highlights the importance of choosing suitable LLMs and\ndecoding strategies for accurate evaluation results. LLM-Eval offers a\nversatile and robust solution for evaluating open-domain conversation systems,\nstreamlining the evaluation process and providing consistent performance across\ndiverse scenarios.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large\n  Language Models\u2b1b  Existing efforts to improve logical reasoning ability of language models have\npredominantly relied on supervised fine-tuning, hindering generalization to new\ndomains and/or tasks. The development of Large Langauge Models (LLMs) has\ndemonstrated the capacity of compressing abundant knowledge into a single\nproxy, enabling them to tackle multiple tasks effectively. Our preliminary\nexperiments, nevertheless, show that LLMs do not show capability on logical\nreasoning. The performance of LLMs on logical reasoning benchmarks is far\nbehind the existing state-of-the-art baselines. In this paper, we make the\nfirst attempt to investigate the feasibility of incorporating logical knowledge\nthrough self-supervised post-training, and activating it via in-context\nlearning, which we termed as LogicLLM. Specifically, we devise an\nauto-regressive objective variant of MERIt and integrate it with two LLM\nseries, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to\n13 billion. The results on two challenging logical reasoning benchmarks\ndemonstrate the effectiveness of LogicLLM. Besides, we conduct extensive\nablation studies to analyze the key factors in designing logic-oriented proxy\ntasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot\n  Sequence-to-Sequence Semantic Parsing over Wikidata\u2b1b  While large language models (LLMs) can answer many questions correctly, they\ncan also hallucinate and give wrong answers. Wikidata, with its over 12 billion\nfacts, can be used to ground LLMs to improve their factuality. This paper\npresents WikiWebQuestions, a high-quality question answering benchmark for\nWikidata. Ported over from WebQuestions for Freebase, it consists of real-world\ndata with SPARQL annotation. This paper presents a few-shot\nsequence-to-sequence semantic parser for Wikidata. We modify SPARQL to use the\nunique domain and property names instead of their IDs. We train the parser to\nuse either the results from an entity linker or mentions in the query. We\nfine-tune LLaMA by adding the few-shot training data to that used to fine-tune\nAlpaca. Our experimental results demonstrate the effectiveness of this\nmethodology, establishing a strong baseline of 76% and 65% answer accuracy in\nthe dev and test sets of WikiWebQuestions, respectively. By pairing our\nsemantic parser with GPT-3, we combine verifiable results with qualified GPT-3\nguesses to provide useful answers to 96% of the questions in dev. We also show\nthat our method outperforms the state-of-the-art for the QALD-7 Wikidata\ndataset by 3.6% in F1 score.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs\u2b1b  Large language models (LLMs) have achieved widespread success on a variety of\nin-context few-shot tasks, but this success is typically evaluated via\ncorrectness rather than consistency. We argue that self-consistency is an\nimportant criteria for valid multi-step reasoning in tasks where the solution\nis composed of the answers to multiple sub-steps. We propose two types of\nself-consistency that are particularly important for multi-step reasoning --\nhypothetical consistency (a model's ability to predict what its output would be\nin a hypothetical other context) and compositional consistency (consistency of\na model's final outputs when intermediate sub-steps are replaced with the\nmodel's outputs for those steps). We demonstrate that multiple variants of the\nGPT-3/-4 models exhibit poor consistency rates across both types of consistency\non a variety of tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance\u2b1b  This paper explores the potential of leveraging Large Language Models (LLMs)\nfor data augmentation in multilingual commonsense reasoning datasets where the\navailable training data is extremely limited. To achieve this, we utilise\nseveral LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment\nthree datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate\nthe effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR,\nusing the synthesised data. We compare the performance of training with data\ngenerated in English and target languages, as well as translated\nEnglish-generated data, revealing the overall advantages of incorporating data\ngenerated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best\ncase. Furthermore, we conduct a human evaluation by asking native speakers to\nassess the naturalness and logical coherence of the generated examples across\ndifferent languages. The results of the evaluation indicate that LLMs such as\nChatGPT and GPT-4 excel at producing natural and coherent text in most\nlanguages, however, they struggle to generate meaningful text in certain\nlanguages like Tamil. We also observe that ChatGPT falls short in generating\nplausible alternatives compared to the original dataset, whereas examples from\nGPT-4 exhibit competitive logical consistency.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "RET-LLM: Towards a General Read-Write Memory for Large Language Models\u2b1b  Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Empowering LLM-based Machine Translation with Cultural Awareness\u2b1b  Traditional neural machine translation (NMT) systems often fail to translate\nsentences that contain culturally specific information. Most previous NMT\nmethods have incorporated external cultural knowledge during training, which\nrequires fine-tuning on low-frequency items specific to the culture. Recent\nin-context learning utilizes lightweight prompts to guide large language models\n(LLMs) to perform machine translation, however, whether such an approach works\nin terms of injecting culture awareness into machine translation remains\nunclear. To this end, we introduce a new data curation pipeline to construct a\nculturally relevant parallel corpus, enriched with annotations of\ncultural-specific entities. Additionally, we design simple but effective\nprompting strategies to assist this LLM-based translation. Extensive\nexperiments show that our approaches can largely help incorporate cultural\nknowledge into LLM-based machine translation, outperforming traditional NMT\nsystems in translating cultural-specific sentences.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond\u2b1b  With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle\n  Verifiers\u2b1b  Large language models (LLMs) excel at implementing code from functionality\ndescriptions but struggle with algorithmic problems that require not only\nimplementation but also identification of the suitable algorithm. Moreover,\nLLM-generated programs lack guaranteed correctness and require human\nverification. To address these challenges, we propose ALGO, a framework that\nsynthesizes Algorithmic programs with LLM-Generated Oracles to guide the\ngeneration and verify their correctness. ALGO first generates a reference\noracle by prompting an LLM to exhaustively enumerate all the combinations of\nrelevant variables. This oracle is then utilized to guide an arbitrary search\nstrategy in exploring the algorithm space and to verify the synthesized\nalgorithms. Our study shows that the LLM-generated oracles are correct for 88%\nof the cases. With the oracles as verifiers, ALGO can be integrated with any\nexisting code generation model in a model-agnostic manner to enhance its\nperformance. Experiments show that when equipped with ALGO, we achieve an 8x\nbetter one-submission pass rate over the Codex model and a 2.6x better\none-submission pass rate over CodeT, the current state-of-the-art model on\nCodeContests. We can also get 1.3x better pass rate over the ChatGPT Code\nInterpreter on unseen problems. The problem set we used for testing, the\nprompts we used, the verifier and solution programs, and the test cases\ngenerated by ALGO are available at https://github.com/zkx06111/ALGO.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Have Large Language Models Developed a Personality?: Applicability of\n  Self-Assessment Tests in Measuring Personality in LLMs\u2b1b  Have Large Language Models (LLMs) developed a personality? The short answer\nis a resounding \"We Don't Know!\". In this paper, we show that we do not yet\nhave the right tools to measure personality in language models. Personality is\nan important characteristic that influences behavior. As LLMs emulate\nhuman-like intelligence and performance in various tasks, a natural question to\nask is whether these models have developed a personality. Previous works have\nevaluated machine personality through self-assessment personality tests, which\nare a set of multiple-choice questions created to evaluate personality in\nhumans. A fundamental assumption here is that human personality tests can\naccurately measure personality in machines. In this paper, we investigate the\nemergence of personality in five LLMs of different sizes ranging from 1.5B to\n30B. We propose the Option-Order Symmetry property as a necessary condition for\nthe reliability of these self-assessment tests. Under this condition, the\nanswer to self-assessment questions is invariant to the order in which the\noptions are presented. We find that many LLMs personality test responses do not\npreserve option-order symmetry. We take a deeper look at LLMs test responses\nwhere option-order symmetry is preserved to find that in these cases, LLMs do\nnot take into account the situational statement being tested and produce the\nexact same answer irrespective of the situation being tested. We also identify\nthe existence of inherent biases in these LLMs which is the root cause of the\naforementioned phenomenon and makes self-assessment tests unreliable. These\nobservations indicate that self-assessment tests are not the correct tools to\nmeasure personality in LLMs. Through this paper, we hope to draw attention to\nthe shortcomings of current literature in measuring personality in LLMs and\ncall for developing tools for machine personality measurement.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ClusterLLM: Large Language Models as a Guide for Text Clustering\u2b1b  We introduce ClusterLLM, a novel text clustering framework that leverages\nfeedback from an instruction-tuned large language model, such as ChatGPT.\nCompared with traditional unsupervised methods that builds upon \"small\"\nembedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the\nemergent capability of LLM even if its embeddings are inaccessible; and (2) it\nunderstands the user's preference on clustering through textual instruction\nand/or a few annotated data. First, we prompt ChatGPT for insights on\nclustering perspective by constructing hard triplet questions <does A better\ncorrespond to B than C>, where A, B and C are similar data points that belong\nto different clusters according to small embedder. We empirically show that\nthis strategy is both effective for fine-tuning small embedder and\ncost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on\nclustering granularity by carefully designed pairwise questions <do A and B\nbelong to the same category>, and tune the granularity from cluster hierarchies\nthat is the most consistent with the ChatGPT answers. Extensive experiments on\n14 datasets show that ClusterLLM consistently improves clustering quality, at\nan average cost of ~$0.6 per dataset. The code will be available at\nhttps://github.com/zhang-yu-wei/ClusterLLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large\n  Language Models with SocKET Benchmark\u2b1b  Large language models (LLMs) have been shown to perform well at a variety of\nsyntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed\nin many forms including conversational agents that interact with humans, we\nlack a grounded benchmark to measure how well LLMs understand \\textit{social}\nlanguage. Here, we introduce a new theory-driven benchmark, SocKET, that\ncontains 58 NLP tasks testing social knowledge which we group into five\ncategories: humor & sarcasm, offensiveness, sentiment & emotion, and\ntrustworthiness. In tests on the benchmark, we demonstrate that current models\nattain only moderate performance but reveal significant potential for task\ntransfer among different types and categories of tasks, which were predicted\nfrom theory. Through zero-shot evaluations, we show that pretrained models\nalready possess some innate but limited capabilities of social language\nunderstanding and training on one category of tasks can improve zero-shot\ntesting on others. Our benchmark provides a systematic way to analyze model\nperformance on an important dimension of language and points to clear room for\nimprovement to build more socially-aware LLMs. The associated resources are\nreleased at https://github.com/minjechoi/SOCKET.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Tricking LLMs into Disobedience: Understanding, Analyzing, and\n  Preventing Jailbreaks\u2b1b  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating the prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited formal\nstudies have been carried out to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We perform a survey of existing jailbreak\nmethods and their effectiveness on open-source and commercial LLMs (such as GPT\n3.5, OPT, BLOOM, and FLAN-T5-xxl). We further propose a limited set of prompt\nguards and discuss their effectiveness against known attack types.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Investigating Table-to-Text Generation Capabilities of LLMs in\n  Real-World Information Seeking Scenarios\u2b1b  Tabular data is prevalent across various industries, necessitating\nsignificant time and effort for users to understand and manipulate for their\ninformation-seeking purposes. The advancements in large language models (LLMs)\nhave shown enormous potential to improve user efficiency. However, the adoption\nof LLMs in real-world applications for table information seeking remains\nunderexplored. In this paper, we investigate the table-to-text capabilities of\ndifferent LLMs using four datasets within two real-world information seeking\nscenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets\nfor data insight generation, along with the FeTaQA and our newly-constructed\nF2WTQ datasets for query-based generation. We structure our investigation\naround three research questions, evaluating the performance of LLMs in\ntable-to-text generation, automated evaluation, and feedback generation,\nrespectively. Experimental results indicate that the current high-performing\nLLM, specifically GPT-4, can effectively serve as a table-to-text generator,\nevaluator, and feedback generator, facilitating users' information seeking\npurposes in real-world scenarios. However, a significant performance gap still\nexists between other open-sourced LLMs (e.g., Tulu and LLaMA-2) and GPT-4\nmodels. Our data and code are publicly available at\nhttps://github.com/yale-nlp/LLM-T2T.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMDet: A Third Party Large Language Models Generated Text Detection\n  Tool\u2b1b  Generated texts from large language models (LLMs) are remarkably close to\nhigh-quality human-authored text, raising concerns about their potential misuse\nin spreading false information and academic misconduct. Consequently, there is\nan urgent need for a highly practical detection tool capable of accurately\nidentifying the source of a given text. However, existing detection tools\ntypically rely on access to LLMs and can only differentiate between\nmachine-generated and human-authored text, failing to meet the requirements of\nfine-grained tracing, intermediary judgment, and rapid detection. Therefore, we\npropose LLMDet, a model-specific, secure, efficient, and extendable detection\ntool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and\nothers. In LLMDet, we record the next-token probabilities of salient n-grams as\nfeatures to calculate proxy perplexity for each LLM. By jointly analyzing the\nproxy perplexities of LLMs, we can determine the source of the generated text.\nExperimental results show that LLMDet yields impressive detection performance\nwhile ensuring speed and security, achieving 98.54% precision and x5.0 faster\nfor recognizing human-authored text. Additionally, LLMDet can effortlessly\nextend its detection capabilities to a new open-source model. We will provide\nan open-source tool at https://github.com/TrustedLLM/LLMDet.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Not All Metrics Are Guilty: Improving NLG Evaluation with LLM\n  Paraphrasing\u2b1b  Most research about natural language generation (NLG) relies on evaluation\nbenchmarks with limited references for a sample, which may result in poor\ncorrelations with human judgements. The underlying reason is that one semantic\nmeaning can actually be expressed in different forms, and the evaluation with a\nsingle or few references may not accurately reflect the quality of the model's\nhypotheses. To address this issue, this paper presents a novel method, named\nPara-Ref, to enhance existing evaluation benchmarks by enriching the number of\nreferences. We leverage large language models (LLMs) to paraphrase a single\nreference into multiple high-quality ones in diverse expressions. Experimental\nresults on representative NLG tasks of machine translation, text summarization,\nand image caption demonstrate that our method can effectively improve the\ncorrelation with human evaluation for sixteen automatic evaluation metrics by\n+7.82% in ratio. We release the code and data at\nhttps://github.com/RUCAIBox/Para-Ref.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For\n  Large Language Models\u2b1b  The performance of large language models (LLMs) on existing reasoning\nbenchmarks has significantly improved over the past years. In response, we\npresent JEEBench, a considerably more challenging benchmark dataset for\nevaluating the problem solving abilities of LLMs. We curate 515 challenging\npre-engineering mathematics, physics and chemistry problems from the highly\ncompetitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep\nin-domain knowledge is essential for solving problems in this benchmark. Our\nevaluation on various open-source and proprietary models reveals that the\nhighest performance, even after using techniques like self-consistency,\nself-refinement and chain-of-thought prompting, is less than 40%. The typical\nfailure modes of GPT-4, the best model, are errors in algebraic manipulation,\ndifficulty in grounding abstract concepts into mathematical equations\naccurately and failure in retrieving relevant domain-specific concepts. We also\nobserve that by mere prompting, GPT-4 is unable to assess risk introduced by\nnegative marking for incorrect answers. For this, we develop a post-hoc\nconfidence-thresholding method over self-consistency, which enables effective\nresponse selection. We hope that our challenging benchmark will guide future\nre-search in problem-solving using LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Spoken Question Answering and Speech Continuation Using\n  Spectrogram-Powered LLM\u2b1b  We present a novel approach to adapting pre-trained large language models\n(LLMs) to perform question answering (QA) and speech continuation. By endowing\nthe LLM with a pre-trained speech encoder, our model becomes able to take\nspeech inputs and generate speech outputs. The entire system is trained\nend-to-end and operates directly on spectrograms, simplifying our architecture.\nKey to our approach is a training objective that jointly supervises speech\nrecognition, text continuation, and speech synthesis using only paired\nspeech-text pairs, enabling a `cross-modal' chain-of-thought within a single\ndecoding pass. Our method surpasses existing spoken language models in speaker\npreservation and semantic coherence. Furthermore, the proposed model improves\nupon direct initialization in retaining the knowledge of the original LLM as\ndemonstrated through spoken QA datasets. Audio samples can be found at\nhttps://michelleramanovich.github.io/spectron/spectron\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The False Promise of Imitating Proprietary LLMs\u2b1b  An emerging method to cheaply improve a weaker language model is to finetune\nit on outputs from a stronger model, such as a proprietary system like ChatGPT\n(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply\nimitate the proprietary model's capabilities using a weaker open-source model.\nIn this work, we critically analyze this approach. We first finetune a series\nof LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data\nsources, and imitation data amounts (0.3M--150M tokens). We then evaluate the\nmodels using crowd raters and canonical NLP benchmarks. Initially, we were\nsurprised by the output quality of our imitation models -- they appear far\nbetter at following instructions, and crowd workers rate their outputs as\ncompetitive with ChatGPT. However, when conducting more targeted automatic\nevaluations, we find that imitation models close little to none of the gap from\nthe base LM to ChatGPT on tasks that are not heavily supported in the imitation\ndata. We show that these performance discrepancies may slip past human raters\nbecause imitation models are adept at mimicking ChatGPT's style but not its\nfactuality. Overall, we conclude that model imitation is a false promise: there\nexists a substantial capabilities gap between open and closed LMs that, with\ncurrent methods, can only be bridged using an unwieldy amount of imitation data\nor by using more capable base LMs. In turn, we argue that the highest leverage\naction for improving open-source models is to tackle the difficult challenge of\ndeveloping better base LMs, rather than taking the shortcut of imitating\nproprietary systems.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities\u2b1b  In most current research, large language models (LLMs) are able to perform\nreasoning tasks by generating chains of thought through the guidance of\nspecific prompts. However, there still exists a significant discrepancy between\ntheir capability in solving complex reasoning problems and that of humans. At\npresent, most approaches focus on chains of thought (COT) and tool use, without\nconsidering the adoption and application of human cognitive frameworks. It is\nwell-known that when confronting complex reasoning challenges, humans typically\nemploy various cognitive abilities, and necessitate interaction with all\naspects of tools, knowledge, and the external environment information to\naccomplish intricate tasks. This paper introduces a novel intelligent\nframework, referred to as OlaGPT. OlaGPT carefully studied a cognitive\narchitecture framework, and propose to simulate certain aspects of human\ncognition. The framework involves approximating different cognitive modules,\nincluding attention, memory, reasoning, learning, and corresponding scheduling\nand decision-making mechanisms. Inspired by the active learning mechanism of\nhuman beings, it proposes a learning unit to record previous mistakes and\nexpert opinions, and dynamically refer to them to strengthen their ability to\nsolve similar problems. The paper also outlines common effective reasoning\nframeworks for human problem-solving and designs Chain-of-Thought (COT)\ntemplates accordingly. A comprehensive decision-making mechanism is also\nproposed to maximize model accuracy. The efficacy of OlaGPT has been\nstringently evaluated on multiple reasoning datasets, and the experimental\noutcomes reveal that OlaGPT surpasses state-of-the-art benchmarks,\ndemonstrating its superior performance. Our implementation of OlaGPT is\navailable on GitHub: \\url{https://github.com/oladata-team/OlaGPT}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Don't Trust ChatGPT when Your Question is not in English: A Study of\n  Multilingual Abilities and Types of LLMs\u2b1b  Large Language Models (LLMs) have demonstrated exceptional natural language\nunderstanding abilities and have excelled in a variety of natural language\nprocessing (NLP)tasks in recent years. Despite the fact that most LLMs are\ntrained predominantly in English, multiple studies have demonstrated their\ncomparative performance in many other languages. However, fundamental questions\npersist regarding how LLMs acquire their multi-lingual abilities and how\nperformance varies across different languages. These inquiries are crucial for\nthe study of LLMs since users and researchers often come from diverse language\nbackgrounds, potentially influencing their utilization and interpretation of\nLLMs' results. In this work, we propose a systematic way of qualifying the\nperformance disparities of LLMs under multilingual settings. We investigate the\nphenomenon of across-language generalizations in LLMs, wherein insufficient\nmulti-lingual training data leads to advanced multi-lingual capabilities. To\naccomplish this, we employ a novel back-translation-based prompting method. The\nresults show that GPT exhibits highly translating-like behaviour in\nmultilingual settings.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A\n  Comprehensive Framework and Dataset\u2b1b  Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunderexplored. In this research, we specialize in harnessing the potential of\nLLMs to comprehend critical information from financial reports, which are\nhybrid long-documents. We propose an Automated Financial Information Extraction\n(AFIE) framework that enhances LLMs' ability to comprehend and extract\ninformation from financial reports. To evaluate AFIE, we develop a Financial\nReports Numerical Extraction (FINE) dataset and conduct an extensive\nexperimental analysis. Our framework is effectively validated on GPT-3.5 and\nGPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively,\ncompared to a naive method. These results suggest that the AFIE framework\noffers accuracy for automated numerical extraction from complex, hybrid\ndocuments.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate\n  Model\u2b1b  The detection of machine-generated text, especially from large language\nmodels (LLMs), is crucial in preventing serious social problems resulting from\ntheir misuse. Some methods train dedicated detectors on specific datasets but\nfall short in generalizing to unseen test data, while other zero-shot ones\noften yield suboptimal performance. Although the recent DetectGPT has shown\npromising detection performance, it suffers from significant inefficiency\nissues, as detecting a single candidate requires scoring hundreds of its\nperturbations with the source LLM. This paper aims to bridge this gap.\nTechnically, we propose to incorporate a Bayesian surrogate model, which allows\nus to select typical samples based on Bayesian uncertainty and interpolate\nscores from typical samples to other ones, to improve query efficiency. Our\nempirical results demonstrate that our method significantly outperforms\nexisting approaches under a low query budget. Notably, our method achieves\nsimilar performance with up to 2 times fewer queries than DetectGPT and 3.7%\nhigher AUROC at a query number of 5.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for\n  Financial Tasks\u2b1b  Recently large language models (LLMs) like ChatGPT have shown impressive\nperformance on many natural language processing tasks with zero-shot. In this\npaper, we investigate the effectiveness of zero-shot LLMs in the financial\ndomain. We compare the performance of ChatGPT along with some open-source\ngenerative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We\naddress three inter-related research questions on data annotation, performance\ngaps, and the feasibility of employing generative models in the finance domain.\nOur findings demonstrate that ChatGPT performs well even without labeled data\nbut fine-tuned models generally outperform it. Our research also highlights how\nannotating with generative models can be time-intensive. Our codebase is\npublicly available on GitHub under CC BY-NC 4.0 license.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for\n  LLM KV Cache Compression at Test Time\u2b1b  Large language models(LLMs) have sparked a new wave of exciting AI\napplications. Hosting these models at scale requires significant memory\nresources. One crucial memory bottleneck for the deployment stems from the\ncontext window. It is commonly recognized that model weights are memory hungry;\nhowever, the size of key-value embedding stored during the generation process\n(KV cache) can easily surpass the model size. The enormous size of the KV cache\nputs constraints on the inference batch size, which is crucial for high\nthroughput inference workload. Inspired by an interesting observation of the\nattention scores, we hypothesize the persistence of importance: only pivotal\ntokens, which had a substantial influence at one step, will significantly\ninfluence future generations. Based on our empirical verification and\ntheoretical analysis around this hypothesis, we propose Scissorhands, a system\nthat maintains the memory usage of the KV cache at a fixed budget without\nfinetuning the model. In essence, Scissorhands manages the KV cache by storing\nthe pivotal tokens with a higher probability. We validate that Scissorhands\nreduces the inference memory usage of the KV cache by up to 5X without\ncompromising model quality. We further demonstrate that Scissorhands can be\ncombined with 4-bit quantization, traditionally used to compress model weights,\nto achieve up to 20X compression.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Breaking Language Barriers with a LEAP: Learning Strategies for Polyglot\n  LLMs\u2b1b  Large language models (LLMs) are at the forefront of transforming numerous\ndomains globally. However, their inclusivity and effectiveness remain limited\nfor non-Latin scripts and low-resource languages. This paper tackles the\nimperative challenge of enhancing the multilingual performance of LLMs,\nspecifically focusing on Generative models. Through systematic investigation\nand evaluation of diverse languages using popular question-answering (QA)\ndatasets, we present novel techniques that unlock the true potential of LLMs in\na polyglot landscape. Our approach encompasses three key strategies that yield\nremarkable improvements in multilingual proficiency. First, by meticulously\noptimizing prompts tailored for polyglot LLMs, we unlock their latent\ncapabilities, resulting in substantial performance boosts across languages.\nSecond, we introduce a new hybrid approach that synergizes GPT generation with\nmultilingual embeddings and achieves significant multilingual performance\nimprovement on critical tasks like QA and retrieval. Finally, to further propel\nthe performance of polyglot LLMs, we introduce a novel learning algorithm that\ndynamically selects the optimal prompt strategy, LLM model, and embeddings per\nquery. This dynamic adaptation maximizes the efficacy of LLMs across languages,\noutperforming best static and random strategies. Our results show substantial\nadvancements in multilingual understanding and generation across a diverse\nrange of languages.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models\u2b1b  Several post-training quantization methods have been applied to large\nlanguage models (LLMs), and have been shown to perform well down to 8-bits. We\nfind that these methods break down at lower bit precision, and investigate\nquantization aware training for LLMs (LLM-QAT) to push quantization levels even\nfurther. We propose a data-free distillation method that leverages generations\nproduced by the pre-trained model, which better preserves the original output\ndistribution and allows quantizing any generative model independent of its\ntraining data, similar to post-training quantization methods. In addition to\nquantizing weights and activations, we also quantize the KV cache, which is\ncritical for increasing throughput and support long sequence dependencies at\ncurrent model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B,\nat quantization levels down to 4-bits. We observe large improvements over\ntraining-free methods, especially in the low-bit settings.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and\n  the Importance of Object-based Representations\u2b1b  Can a Large Language Model (LLM) solve simple abstract reasoning problems? We\nexplore this broad question through a systematic analysis of GPT on the\nAbstraction and Reasoning Corpus (ARC), a representative benchmark of abstract\nreasoning ability from limited examples in which solutions require some \"core\nknowledge\" of concepts such as objects, goal states, counting, and basic\ngeometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when\nusing textual encodings for their two-dimensional input-output grids. Our\nfailure analysis reveals that GPT-4's capacity to identify objects and reason\nabout them is significantly influenced by the sequential nature of the text\nthat represents an object within a text encoding of a task. To test this\nhypothesis, we design a new benchmark, the 1D-ARC, which consists of\none-dimensional (array-like) tasks that are more conducive to GPT-based\nreasoning, and where it indeed performs better than on the (2D) ARC. To\nalleviate this issue, we propose an object-based representation that is\nobtained through an external tool, resulting in nearly doubling the performance\non solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the\nstate-of-the-art GPT-4 is unable to \"reason\" perfectly within non-language\ndomains such as the 1D-ARC or a simple ARC subset, our study reveals that the\nuse of object-based representations can significantly improve its reasoning\nability. Visualizations, GPT logs, and data are available at\nhttps://khalil-research.github.io/LLM4ARC.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly\n  Transformers\u2b1b  The community explored to build private inference frameworks for\ntransformer-based large language models (LLMs) in a server-client setting,\nwhere the server holds the model parameters and the client inputs its private\ndata (or prompt) for inference. However, these frameworks impose significant\noverhead when the private inputs are forward propagated through the original\nLLMs. In this paper, we show that substituting the computation- and\ncommunication-heavy operators in the transformer architecture with\nprivacy-computing friendly approximations can greatly reduce the private\ninference costs while incurring very minor impact on model performance.\nCompared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing\nfriendly model inference pipeline achieves a $5\\times$ acceleration in\ncomputation and an 80% reduction in communication overhead, while retaining\nnearly identical accuracy.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\u2b1b  Large language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AWQ: Activation-aware Weight Quantization for LLM Compression and\n  Acceleration\u2b1b  Large language models (LLMs) have shown excellent performance on various\ntasks, but the astronomical model size raises the hardware barrier for serving\n(memory size) and slows down token generation (memory bandwidth). In this\npaper, we propose Activation-aware Weight Quantization (AWQ), a\nhardware-friendly approach for LLM low-bit weight-only quantization. Our method\nis based on the observation that weights are not equally important: protecting\nonly 1% of salient weights can greatly reduce quantization error. We then\npropose to search for the optimal per-channel scaling that protects the salient\nweights by observing the activation, not weights. AWQ does not rely on any\nbackpropagation or reconstruction, so it can well preserve LLMs' generalization\nability on different domains and modalities, without overfitting to the\ncalibration set. AWQ outperforms existing work on various language modeling and\ndomain-specific benchmarks. Thanks to better generalization, it achieves\nexcellent quantization performance for instruction-tuned LMs and, for the first\ntime, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible\ninference framework tailored for LLMs on the edge, offering more than 3x\nspeedup over the Huggingface FP16 implementation on both desktop and mobile\nGPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile\nGPU (NVIDIA Jetson Orin 64GB).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMatic: Neural Architecture Search via Large Language Models and\n  Quality Diversity Optimization\u2b1b  Large Language Models (LLMs) have emerged as powerful tools capable of\naccomplishing a broad spectrum of tasks. Their abilities span numerous areas,\nand one area where they have made a significant impact is in the domain of code\ngeneration. In this context, we view LLMs as mutation and crossover tools.\nMeanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and\nrobust solutions. By merging the code-generating abilities of LLMs with the\ndiversity and robustness of QD solutions, we introduce LLMatic, a Neural\nArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NAS\ndirectly through prompts, LLMatic uses a procedural approach, leveraging QD for\nprompts and network architecture to create diverse and highly performant\nnetworks. We test LLMatic on the CIFAR-10 image classification benchmark,\ndemonstrating that it can produce competitive networks with just $2,000$\nsearches, even without prior knowledge of the benchmark domain or exposure to\nany previous top-performing models for the benchmark.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\n  with Web Data, and Web Data Only\u2b1b  Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How Ready are Pre-trained Abstractive Models and LLMs for Legal Case\n  Judgement Summarization?\u2b1b  Automatic summarization of legal case judgements has traditionally been\nattempted by using extractive summarization methods. However, in recent years,\nabstractive summarization models are gaining popularity since they can generate\nmore natural and coherent summaries. Legal domain-specific pre-trained\nabstractive summarization models are now available. Moreover, general-domain\npre-trained Large Language Models (LLMs), such as ChatGPT, are known to\ngenerate high-quality text and have the capacity for text summarization. Hence\nit is natural to ask if these models are ready for off-the-shelf application to\nautomatically generate abstractive summaries for case judgements. To explore\nthis question, we apply several state-of-the-art domain-specific abstractive\nsummarization models and general-domain LLMs on Indian court case judgements,\nand check the quality of the generated summaries. In addition to standard\nmetrics for summary quality, we check for inconsistencies and hallucinations in\nthe summaries. We see that abstractive summarization models generally achieve\nslightly higher scores than extractive models in terms of standard summary\nevaluation metrics such as ROUGE and BLEU. However, we often find inconsistent\nor hallucinated information in the generated abstractive summaries. Overall,\nour investigation indicates that the pre-trained abstractive summarization\nmodels and LLMs are not yet ready for fully automatic deployment for case\njudgement summarization; rather a human-in-the-loop approach including manual\nchecks for inconsistencies is more suitable at present.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs like GPT-4 outperform traditional AI tools in dementia\n  diagnosis? Maybe, but not today\u2b1b  Recent investigations show that large language models (LLMs), specifically\nGPT-4, not only have remarkable capabilities in common Natural Language\nProcessing (NLP) tasks but also exhibit human-level performance on various\nprofessional and academic benchmarks. However, whether GPT-4 can be directly\nused in practical applications and replace traditional artificial intelligence\n(AI) tools in specialized domains requires further experimental validation. In\nthis paper, we explore the potential of LLMs such as GPT-4 to outperform\ntraditional AI tools in dementia diagnosis. Comprehensive comparisons between\nGPT-4 and traditional AI tools are conducted to examine their diagnostic\naccuracy in a clinical setting. Experimental results on two real clinical\ndatasets show that, although LLMs like GPT-4 demonstrate potential for future\nadvancements in dementia diagnosis, they currently do not surpass the\nperformance of traditional AI tools. The interpretability and faithfulness of\nGPT-4 are also evaluated by comparison with real doctors. We discuss the\nlimitations of GPT-4 in its current state and propose future research\ndirections to enhance GPT-4 in dementia diagnosis.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Extending an Event-type Ontology: Adding Verbs and Classes Using\n  Fine-tuned LLMs Suggestions\u2b1b  In this project, we have investigated the use of advanced machine learning\nmethods, specifically fine-tuned large language models, for pre-annotating data\nfor a lexical extension task, namely adding descriptive words (verbs) to an\nexisting (but incomplete, as of yet) ontology of event types. Several research\nquestions have been focused on, from the investigation of a possible heuristics\nto provide at least hints to annotators which verbs to include and which are\noutside the current version of the ontology, to the possible use of the\nautomatic scores to help the annotators to be more efficient in finding a\nthreshold for identifying verbs that cannot be assigned to any existing class\nand therefore they are to be used as seeds for a new class. We have also\ncarefully examined the correlation of the automatic scores with the human\nannotation. While the correlation turned out to be strong, its influence on the\nannotation proper is modest due to its near linearity, even though the mere\nfact of such pre-annotation leads to relatively short annotation times.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and\n  Generative Fusion\u2b1b  We present LLM-Blender, an ensembling framework designed to attain\nconsistently superior performance by leveraging the diverse strengths of\nmultiple open-source large language models (LLMs). Our framework consists of\ntwo modules: PairRanker and GenFuser, addressing the observation that optimal\nLLMs for different examples can significantly vary. PairRanker employs a\nspecialized pairwise comparison method to distinguish subtle differences\nbetween candidate outputs. It jointly encodes the input text and a pair of\ncandidates, using cross-attention encoders to determine the superior one. Our\nresults demonstrate that PairRanker exhibits the highest correlation with\nChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,\ngenerating an improved output by capitalizing on their strengths and mitigating\ntheir weaknesses. To facilitate large-scale evaluation, we introduce a\nbenchmark dataset, MixInstruct, which is a mixture of multiple instruction\ndatasets featuring oracle pairwise comparisons. Our LLM-Blender significantly\noutperform individual LLMs and baseline methods across various metrics,\nestablishing a substantial performance gap.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight\n  Compression\u2b1b  Recent advances in large language model (LLM) pretraining have led to\nhigh-quality LLMs with impressive abilities. By compressing such LLMs via\nquantization to 3-4 bits per parameter, they can fit into memory-limited\ndevices such as laptops and mobile phones, enabling personalized use. However,\nquantization down to 3-4 bits per parameter usually leads to moderate-to-high\naccuracy losses, especially for smaller models in the 1-10B parameter range,\nwhich are well-suited for edge deployments. To address this accuracy issue, we\nintroduce the Sparse-Quantized Representation (SpQR), a new compressed format\nand quantization technique which enables for the first time near-lossless\ncompression of LLMs across model scales, while reaching similar compression\nlevels to previous methods. SpQR works by identifying and isolating outlier\nweights, which cause particularly-large quantization errors, and storing them\nin higher precision, while compressing all other weights to 3-4 bits, and\nachieves relative accuracy losses of less than 1% in perplexity for\nhighly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B\nparameter LLM on a single 24 GB consumer GPU without any performance\ndegradation at 15% speedup thus making powerful LLMs available to consumer\nwithout any downsides. SpQR comes with efficient algorithms for both encoding\nweights into its format, as well as decoding them efficiently at runtime.\nSpecifically, we provide an efficient GPU inference algorithm for SpQR which\nyields faster inference than 16-bit baselines at similar accuracy, while\nenabling memory compression gains of more than 4x.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Early Weight Averaging meets High Learning Rates for LLM Pre-training\u2b1b  Training Large Language Models (LLMs) incurs significant cost; hence, any\nstrategy that accelerates model convergence is helpful. In this paper, we\ninvestigate the ability of a simple idea checkpoint averaging along the\ntrajectory of a training run to improve both convergence and generalization\nquite early on during training. Here we show that models trained with high\nlearning rates observe higher gains due to checkpoint averaging. Furthermore,\nthese gains are amplified when checkpoints are sampled with considerable\nspacing in training steps. Our training recipe outperforms conventional\ntraining and popular checkpoint averaging baselines such as exponential moving\naverage (EMA) and stochastic moving average (SWA). We evaluate our training\nrecipe by pre-training LLMs, where high learning rates are inherently preferred\ndue to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2\nmodels of varying sizes, small (125M), medium (335M), and large (770M)on the\nOpenWebText dataset, comprised of 9B tokens. Additionally, we present results\nfor publicly available Pythia LLMs, ranging from 1B to 12B, which were trained\non the PILE-deduped dataset containing 207B tokens.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned\n  LLMs for Radiology Report Impression Generation\u2b1b  Instruction-tuned generative Large language models (LLMs) like ChatGPT and\nBloomz possess excellent generalization abilities, but they face limitations in\nunderstanding radiology reports, particularly in the task of generating the\nIMPRESSIONS section from the FINDINGS section. They tend to generate either\nverbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to\nmedical text data during training. We present a system which leverages\nlarge-scale medical text data for domain-adaptive pre-training of\ninstruction-tuned LLMs to enhance its medical knowledge and performance on\nspecific medical tasks. We show that this system performs better in a zero-shot\nsetting than a number of pretrain-and-finetune adaptation methods on the\nIMPRESSIONS generation task, and ranks 1st among participating systems in Task\n1B: Radiology Report Summarization at the BioNLP 2023 workshop.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory\u2b1b  Large language models (LLMs) with memory are computationally universal.\nHowever, mainstream LLMs are not taking full advantage of memory, and the\ndesigns are heavily influenced by biological brains. Due to their approximate\nnature and proneness to the accumulation of errors, conventional neural memory\nmechanisms cannot support LLMs to simulate complex reasoning. In this paper, we\nseek inspiration from modern computer architectures to augment LLMs with\nsymbolic memory for complex multi-hop reasoning. Such a symbolic memory\nframework is instantiated as an LLM and a set of SQL databases, where the LLM\ngenerates SQL instructions to manipulate the SQL databases. We validate the\neffectiveness of the proposed memory framework on a synthetic dataset requiring\ncomplex reasoning. The project website is available at\nhttps://chatdatabase.github.io/ .\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMZip: Lossless Text Compression using Large Language Models\u2b1b  We provide new estimates of an asymptotic upper bound on the entropy of\nEnglish using the large language model LLaMA-7B as a predictor for the next\ntoken given a window of past tokens. This estimate is significantly smaller\nthan currently available estimates in \\cite{cover1978convergent},\n\\cite{lutati2023focus}. A natural byproduct is an algorithm for lossless\ncompression of English text which combines the prediction from the large\nlanguage model with a lossless compression scheme. Preliminary results from\nlimited experiments suggest that our scheme outperforms state-of-the-art text\ncompression schemes such as BSC, ZPAQ, and paq8h.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis,\n  and LLMs Evaluations\u2b1b  This paper reexamines the research on out-of-distribution (OOD) robustness in\nthe field of NLP. We find that the distribution shift settings in previous\nstudies commonly lack adequate challenges, hindering the accurate evaluation of\nOOD robustness. To address these issues, we propose a benchmark construction\nprotocol that ensures clear differentiation and challenging distribution\nshifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution\nrobustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we\nconduct a series of experiments on pre-trained language models for analysis and\nevaluation of OOD robustness. First, for vanilla fine-tuning, we examine the\nrelationship between in-distribution (ID) and OOD performance. We identify\nthree typical types that unveil the inner learning mechanism, which could\npotentially facilitate the forecasting of OOD robustness, correlating with the\nadvancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and\nfind that, despite exhibiting some effectiveness in specific cases, they do not\noffer significant improvement compared to vanilla fine-tuning. Further, we\nevaluate 5 LLMs with various adaptation paradigms and find that when sufficient\nID data is available, fine-tuning domain-specific models outperform LLMs on ID\nexamples significantly. However, in the case of OOD instances, prioritizing\nLLMs with in-context learning yields better results. We identify that both\nfine-tuned small models and LLMs face challenges in effectively addressing\ndownstream tasks. The code is public at\n\\url{https://github.com/lifan-yuan/OOD_NLP}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "covLLM: Large Language Models for COVID-19 Biomedical Literature\u2b1b  The COVID-19 pandemic led to 1.1 million deaths in the United States, despite\nthe explosion of coronavirus research. These new findings are slow to translate\nto clinical interventions, leading to poorer patient outcomes and unnecessary\ndeaths. One reason is that clinicians, overwhelmed by patients, struggle to\nkeep pace with the rate of new coronavirus literature. A potential solution is\ndeveloping a tool for evaluating coronavirus literature using large language\nmodels (LLMs) -- neural networks that are deployed for natural language\nprocessing. LLMs can be used to summarize and extract user-specified\ninformation. The greater availability and advancement of LLMs and pre-processed\ncoronavirus literature databases provide the opportunity to assist clinicians\nin evaluating coronavirus literature through a coronavirus literature specific\nLLM (covLLM), a tool that directly takes an inputted research article and a\nuser query to return an answer. Using the COVID-19 Open Research Dataset\n(CORD-19), we produced two datasets: (1) synCovid, which uses a combination of\nhandwritten prompts and synthetic prompts generated using OpenAI, and (2) real\nabstracts, which contains abstract and title pairs. covLLM was trained with\nLLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca\nand synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real\nabstract datasets. These models were evaluated by two human evaluators and\nChatGPT. Results demonstrate that training covLLM on the synCovid and abstract\npairs datasets performs competitively with ChatGPT and outperforms covLLM\ntrained primarily using the Alpaca dataset.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning\n  Optimization\u2b1b  Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Prompt Injection attack against LLM-integrated Applications\u2b1b  Large Language Models (LLMs), renowned for their superior proficiency in\nlanguage comprehension and generation, stimulate a vibrant ecosystem of\napplications around them. However, their extensive assimilation into various\nservices introduces significant security risks. This study deconstructs the\ncomplexities and implications of prompt injection attacks on actual\nLLM-integrated applications. Initially, we conduct an exploratory analysis on\nten commercial applications, highlighting the constraints of current attack\nstrategies in practice. Prompted by these limitations, we subsequently\nformulate HouYi, a novel black-box prompt injection attack technique, which\ndraws inspiration from traditional web injection attacks. HouYi is\ncompartmentalized into three crucial elements: a seamlessly-incorporated\npre-constructed prompt, an injection prompt inducing context partition, and a\nmalicious payload designed to fulfill the attack objectives. Leveraging HouYi,\nwe unveil previously unknown and severe attack outcomes, such as unrestricted\narbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi\non 36 actual LLM-integrated applications and discern 31 applications\nsusceptible to prompt injection. 10 vendors have validated our discoveries,\nincluding Notion, which has the potential to impact millions of users. Our\ninvestigation illuminates both the possible risks of prompt injection attacks\nand the possible tactics for mitigation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of\n  Machine-Generated Text\u2b1b  With the rapid progress of large language models (LLMs) and the huge amount\nof text they generated, it becomes more and more impractical to manually\ndistinguish whether a text is machine-generated. Given the growing use of LLMs\nin social media and education, it prompts us to develop methods to detect\nmachine-generated text, preventing malicious usage such as plagiarism,\nmisinformation, and propaganda. Previous work has studied several zero-shot\nmethods, which require no training data. These methods achieve good\nperformance, but there is still a lot of room for improvement. In this paper,\nwe introduce two novel zero-shot methods for detecting machine-generated text\nby leveraging the log rank information. One is called DetectLLM-LRR, which is\nfast and efficient, and the other is called DetectLLM-NPR, which is more\naccurate, but slower due to the need for perturbations. Our experiments on\nthree datasets and seven language models show that our proposed methods improve\nover the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover,\nDetectLLM-NPR needs fewer perturbations than previous work to achieve the same\nlevel of performance, which makes it more practical for real-world use. We also\ninvestigate the efficiency--performance trade-off based on users preference on\nthese two measures and we provide intuition for using them in practice\neffectively. We release the data and the code of both methods in\nhttps://github.com/mbzuai-nlp/DetectLLM\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u2b1b  Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards the Exploitation of LLM-based Chatbot for Providing Legal\n  Support to Palestinian Cooperatives\u2b1b  With the ever-increasing utilization of natural language processing (NLP), we\nstarted to witness over the past few years a significant transformation in our\ninteraction with legal texts. This technology has advanced the analysis and\nenhanced the understanding of complex legal terminology and contexts. The\ndevelopment of recent large language models (LLMs), particularly ChatGPT, has\nalso introduced a revolutionary contribution to the way that legal texts can be\nprocessed and comprehended. In this paper, we present our work on a\ncooperative-legal question-answering LLM-based chatbot, where we developed a\nset of legal questions about Palestinian cooperatives, associated with their\nregulations and compared the auto-generated answers by the chatbot to their\ncorrespondences that are designed by a legal expert. To evaluate the proposed\nchatbot, we have used 50 queries generated by the legal expert and compared the\nanswers produced by the chart to their relevance judgments. Finding\ndemonstrated that an overall accuracy rate of 82% has been achieved when\nanswering the queries, while exhibiting an F1 score equivalent to 79%.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Trapping LLM Hallucinations Using Tagged Context Prompts\u2b1b  Recent advances in large language models (LLMs), such as ChatGPT, have led to\nhighly sophisticated conversation agents. However, these models suffer from\n\"hallucinations,\" where the model generates false or fabricated information.\nAddressing this challenge is crucial, particularly with AI-driven platforms\nbeing adopted across various sectors. In this paper, we propose a novel method\nto recognize and flag instances when LLMs perform outside their domain\nknowledge, and ensuring users receive accurate information.\n  We find that the use of context combined with embedded tags can successfully\ncombat hallucinations within generative language models. To do this, we\nbaseline hallucination frequency in no-context prompt-response pairs using\ngenerated URLs as easily-tested indicators of fabricated data. We observed a\nsignificant reduction in overall hallucination when context was supplied along\nwith question prompts for tested generative engines. Lastly, we evaluated how\nplacing tags within contexts impacted model responses and were able to\neliminate hallucinations in responses with 98.88% effectiveness.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Protect Your Prompts: Protocols for IP Protection in LLM Applications\u2b1b  With the rapid adoption of AI in the form of large language models (LLMs),\nthe potential value of carefully engineered prompts has become significant.\nHowever, to realize this potential, prompts should be tradable on an open\nmarket. Since prompts are, at present, generally economically non-excludable,\nby virtue of their nature as text, no general competitive market has yet been\nestablished. This note discusses two protocols intended to provide protection\nof prompts, elevating their status as intellectual property, thus confirming\nthe intellectual property rights of prompt engineers, and potentially\nsupporting the flourishing of an open market for LLM prompts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as\n  Translators and Checkers\u2b1b  For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders:\n  Perspectives and Use Cases\u2b1b  This study investigates the transformative potential of Large Language Models\n(LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public\ndata, these models, which possess remarkable language understanding and\ngeneration capabilities, are augmenting the interpretive skills of\nradiologists, enhancing patient-physician communication, and streamlining\nclinical workflows. The paper introduces an analytic framework for presenting\nthe complex interactions between LLMs and the broader ecosystem of medical\nimaging stakeholders, including businesses, insurance entities, governments,\nresearch institutions, and hospitals (nicknamed BIGR-H). Through detailed\nanalyses, illustrative use cases, and discussions on the broader implications\nand future directions, this perspective seeks to raise discussion in strategic\nplanning and decision-making in the era of AI-enabled healthcare.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models\u2b1b  Large Language Models (LLMs) are progressively being utilized as machine\nlearning services and interface tools for various applications. However, the\nsecurity implications of LLMs, particularly in relation to adversarial and\nTrojan attacks, remain insufficiently examined. In this paper, we propose\nTrojLLM, an automatic and black-box framework to effectively generate universal\nand stealthy triggers. When these triggers are incorporated into the input\ndata, the LLMs' outputs can be maliciously manipulated. Moreover, the framework\nalso supports embedding Trojans within discrete prompts, enhancing the overall\neffectiveness and precision of the triggers' attacks. Specifically, we propose\na trigger discovery algorithm for generating universal triggers for various\ninputs by querying victim LLM-based APIs using few-shot data samples.\nFurthermore, we introduce a novel progressive Trojan poisoning algorithm\ndesigned to generate poisoned prompts that retain efficacy and transferability\nacross a diverse range of models. Our experiments and results demonstrate\nTrojLLM's capacity to effectively insert Trojans into text prompts in\nreal-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining\nexceptional performance on clean test sets. Our work sheds light on the\npotential security risks in current models and offers a potential defensive\napproach. The source code of TrojLLM is available at\nhttps://github.com/UCF-ML-Research/TrojLLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SqueezeLLM: Dense-and-Sparse Quantization\u2b1b  Generative Large Language Models (LLMs) have demonstrated remarkable results\nfor a wide range of tasks. However, deploying these models for inference has\nbeen a significant challenge due to their unprecedented resource requirements.\nThis has forced existing deployment frameworks to use multi-GPU inference\npipelines, which are often complex and costly, or to use smaller and less\nperformant models. In this work, we demonstrate that the main bottleneck for\ngenerative inference with LLMs is memory bandwidth, rather than compute,\nspecifically for single batch inference. While quantization has emerged as a\npromising solution by representing model weights with reduced precision,\nprevious efforts have often resulted in notable performance degradation. To\naddress this, we introduce SqueezeLLM, a post-training quantization framework\nthat not only enables lossless compression to ultra-low precisions of up to\n3-bit, but also achieves higher quantization performance under the same memory\nconstraint. Our framework incorporates two novel ideas: (i) sensitivity-based\nnon-uniform quantization, which searches for the optimal bit precision\nassignment based on second-order information; and (ii) the Dense-and-Sparse\ndecomposition that stores outliers and sensitive weight values in an efficient\nsparse format. When applied to the LLaMA models, our 3-bit quantization\nsignificantly reduces the perplexity gap from the FP16 baseline by up to 2.1x\nas compared to the state-of-the-art methods with the same memory requirement.\nFurthermore, when deployed on an A6000 GPU, our quantized models achieve up to\n2.3x speedup compared to the baseline. Our code is open-sourced and available\nonline.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for\n  Speech Understanding\u2b1b  Large Language Models (LLMs) have been applied in the speech domain, often\nincurring a performance drop due to misaligned between speech and language\nrepresentations. To bridge this gap, we propose a joint speech and language\nmodel (SLM) using a Speech2Text adapter, which maps speech into text token\nembedding space without speech information loss. Additionally, using a\nCTC-based blank-filtering, we can reduce the speech sequence length to that of\ntext. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the\ndialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to\naddress errors on rare entities, we augment SLM with a Speech2Entity retriever,\nwhich uses speech to retrieve relevant entities, and then adds them to the\noriginal SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the\nDST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with\nthe dialog understanding task improves the ASR performance from 9.4% to 8.5%\nWER.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and\n  Text Integration\u2b1b  Although instruction-tuned large language models (LLMs) have exhibited\nremarkable capabilities across various NLP tasks, their effectiveness on other\ndata modalities beyond text has not been fully studied. In this work, we\npropose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual,\naudio, and textual information. Macaw-LLM consists of three main components: a\nmodality module for encoding multi-modal data, a cognitive module for\nharnessing pretrained LLMs, and an alignment module for harmonizing diverse\nrepresentations. Our novel alignment module seamlessly bridges multi-modal\nfeatures to textual features, simplifying the adaptation process from the\nmodality modules to the cognitive module. In addition, we construct a\nlarge-scale multi-modal instruction dataset in terms of multi-turn dialogue,\nincluding 69K image instances and 50K video instances. We have made our data,\ncode and model publicly available, which we hope can pave the way for future\nresearch in multi-modal LLMs and expand the capabilities of LLMs to handle\ndiverse data modalities and address complex real-world scenarios.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event\n  Boundary Captioning\u2b1b  Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC .\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing\n  Perspective\u2b1b  Large language models (LLMs), like ChatGPT, have shown some human-like\ncognitive abilities. For comparing these abilities of different models, several\nbenchmarks (i.e. sets of standard test questions) from different fields (e.g.,\nLiterature, Biology and Psychology) are often adopted and the test results\nunder traditional metrics such as accuracy, recall and F1, are reported.\nHowever, such way for evaluating LLMs can be inefficient and inaccurate from\nthe cognitive science perspective. Inspired by Computerized Adaptive Testing\n(CAT) used in psychometrics, we propose an adaptive testing framework for LLM\nevaluation. Rather than using a standard test set and simply reporting\naccuracy, this approach dynamically adjusts the characteristics of the test\nquestions, such as difficulty, based on the model's performance. This allows\nfor a more accurate estimation of the model's abilities, using fewer questions.\nMore importantly, it allows LLMs to be compared with humans easily, which is\nessential for NLP models that aim for human-level ability. Our diagnostic\nreports have found that ChatGPT often behaves like a ``careless student'',\nprone to slip and occasionally guessing the questions. We conduct a\nfine-grained diagnosis and rank the latest 6 instruction-tuned LLMs from three\naspects of Subject Knowledge, Mathematical Reasoning, and Programming, where\nGPT4 can outperform other models significantly and reach the cognitive ability\nof middle-level students. Different tests for different models using efficient\nadaptive testing -- we believe this has the potential to become a new norm in\nevaluating large language models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest\n  Cost\u2b1b  Medical artificial general intelligence (AGI) is an emerging field that aims\nto develop systems specifically designed for medical applications that possess\nthe ability to understand, learn, and apply knowledge across a wide range of\ntasks and domains. Large language models (LLMs) represent a significant step\ntowards AGI. However, training cross-domain LLMs in the medical field poses\nsignificant challenges primarily attributed to the requirement of collecting\ndata from diverse domains. This task becomes particularly difficult due to\nprivacy restrictions and the scarcity of publicly available medical datasets.\nHere, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific\nmedical LLMs with the lowest cost, and suggest a possible path to achieve\nmedical AGI. With an increasing number of domain-specific professional\nmultimodal LLMs in the medical field being developed, MedAGI is designed to\nautomatically select appropriate medical models by analyzing users' questions\nwith our novel adaptive expert selection algorithm. It offers a unified\napproach to existing LLMs in the medical field, eliminating the need for\nretraining regardless of the introduction of new models. This characteristic\nrenders it a future-proof solution in the dynamically advancing medical domain.\nTo showcase the resilience of MedAGI, we conducted an evaluation across three\ndistinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis\nof pathology pictures. The results demonstrated that MedAGI exhibited\nremarkable versatility and scalability, delivering exceptional performance\nacross diverse domains. Our code is publicly available to facilitate further\nresearch at https://github.com/JoshuaChou2018/MedAGI.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting\u2b1b  This paper presents a novel study on harnessing Large Language Models' (LLMs)\noutstanding knowledge and reasoning abilities for explainable financial time\nseries forecasting. The application of machine learning models to financial\ntime series comes with several challenges, including the difficulty in\ncross-sequence reasoning and inference, the hurdle of incorporating multi-modal\nsignals from historical news, financial knowledge graphs, etc., and the issue\nof interpreting and explaining the model results. In this paper, we focus on\nNASDAQ-100 stocks, making use of publicly accessible historical stock price\ndata, company metadata, and historical economic/financial news. We conduct\nexperiments to illustrate the potential of LLMs in offering a unified solution\nto the aforementioned challenges. Our experiments include trying\nzero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with\na public LLM model Open LLaMA. We demonstrate our approach outperforms a few\nbaselines, including the widely applied classic ARMA-GARCH model and a\ngradient-boosting tree model. Through the performance comparison results and a\nfew examples, we find LLMs can make a well-thought decision by reasoning over\ninformation from both textual news and price time series and extracting\ninsights, leveraging cross-sequence information, and utilizing the inherent\nknowledge embedded within the LLM. Additionally, we show that a publicly\navailable LLM such as Open-LLaMA, after fine-tuning, can comprehend the\ninstruction to generate explainable forecasts and achieve reasonable\nperformance, albeit relatively inferior in comparison to GPT-4.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Democratizing LLMs for Low-Resource Languages by Leveraging their\n  English Dominant Abilities with Linguistically-Diverse Prompts\u2b1b  Large language models (LLMs) are known to effectively perform tasks by simply\nobserving few exemplars. However, in low-resource languages, obtaining such\nhand-picked exemplars can still be challenging, where unsupervised techniques\nmay be necessary. Moreover, competent generative capabilities of LLMs are\nobserved only in high-resource languages, while their performances among\nunder-represented languages fall behind due to pre-training data imbalance. To\nelicit LLMs' ability onto low-resource languages without any supervised data,\nwe propose to assemble synthetic exemplars from a diverse set of high-resource\nlanguages to prompt the LLMs to translate from any language into English. These\nprompts are then used to create intra-lingual exemplars to perform tasks in the\ntarget languages. Our unsupervised prompting method performs on par with\nsupervised few-shot learning in LLMs of different sizes for translations\nbetween English and 13 Indic and 21 African low-resource languages. We also\nshow that fine-tuning a 7B model on data generated from our method helps it\nperform competitively with a 175B model. In non-English translation tasks, our\nmethod even outperforms supervised prompting by up to 3 chrF++ in many\nlow-resource languages. When evaluated on zero-shot multilingual summarization,\nour method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is\nalso favored by GPT-4.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Improving Image Captioning Descriptiveness by Ranking and LLM-based\n  Fusion\u2b1b  State-of-The-Art (SoTA) image captioning models often rely on the Microsoft\nCOCO (MS-COCO) dataset for training. This dataset contains annotations provided\nby human annotators, who typically produce captions averaging around ten\ntokens. However, this constraint presents a challenge in effectively capturing\ncomplex scenes and conveying detailed information. Furthermore, captioning\nmodels tend to exhibit bias towards the ``average'' caption, which captures\nonly the more general aspects. What would happen if we were able to\nautomatically generate longer captions, thereby making them more detailed?\nWould these captions, evaluated by humans, be more or less representative of\nthe image content compared to the original MS-COCO captions? In this paper, we\npresent a novel approach to address previous challenges by showcasing how\ncaptions generated from different SoTA models can be effectively fused,\nresulting in richer captions. Our proposed method leverages existing models\nfrom the literature, eliminating the need for additional training. Instead, it\nutilizes an image-text based metric to rank the captions generated by SoTA\nmodels for a given image. Subsequently, the top two captions are fused using a\nLarge Language Model (LLM). Experimental results demonstrate the effectiveness\nof our approach, as the captions generated by our model exhibit higher\nconsistency with human judgment when evaluated on the MS-COCO test set. By\ncombining the strengths of various SoTA models, our method enhances the quality\nand appeal of image captions, bridging the gap between automated systems and\nthe rich, informative nature of human-generated descriptions. This advance\nopens up new possibilities for generating captions that are more suitable for\nthe training of both vision-language and captioning models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Learning to Generate Better Than Your LLM\u2b1b  Reinforcement learning (RL) has emerged as a powerful paradigm for\nfine-tuning Large Language Models (LLMs) for text generation. In particular,\nrecent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with\nusers after finetuning with RL. Capitalizing on key properties of text\ngeneration, we seek to investigate RL algorithms beyond general purpose\nalgorithms like Proximal Policy Optimization (PPO). In particular, we extend RL\nalgorithms to allow them to interact with a dynamic black-box guide LLM and\npropose RL with guided feedback (RLGF), a suite of RL algorithms for LLM\nfine-tuning. We provide two ways for the guide LLM to interact with the LLM to\nbe optimized for maximizing rewards. The guide LLM can generate text which\nserves as additional starting states for the RL optimization procedure. The\nguide LLM can also be used to complete the partial sentences generated by the\nLLM that is being optimized, treating the guide LLM as an expert to imitate and\nsurpass eventually. We experiment on the IMDB positive sentiment, CommonGen,\nand TL;DR summarization tasks. We show that our RL algorithms achieve higher\nperformance than supervised learning (SL) and the RL baseline PPO,\ndemonstrating the benefit of interaction with the guide LLM. On both CommonGen\nand TL;DR, we not only outperform our SL baselines but also improve upon PPO\nacross a variety of metrics beyond the one we optimized for. Our code can be\nfound at https://github.com/Cornell-RL/tril.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis\u2b1b  Polis is a platform that leverages machine intelligence to scale up\ndeliberative processes. In this paper, we explore the opportunities and risks\nassociated with applying Large Language Models (LLMs) towards challenges with\nfacilitating, moderating and summarizing the results of Polis engagements. In\nparticular, we demonstrate with pilot experiments using Anthropic's Claude that\nLLMs can indeed augment human intelligence to help more efficiently run Polis\nconversations. In particular, we find that summarization capabilities enable\ncategorically new methods with immense promise to empower the public in\ncollective meaning-making exercises. And notably, LLM context limitations have\na significant impact on insight and quality of these results.\n  However, these opportunities come with risks. We discuss some of these risks,\nas well as principles and techniques for characterizing and mitigating them,\nand the implications for other deliberative or political systems that may\nemploy LLMs. Finally, we conclude with several open future research directions\nfor augmenting tools like Polis with LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Joint Prompt Optimization of Stacked LLMs using Variational Inference\u2b1b  Large language models (LLMs) can be seen as atomic units of computation\nmapping sequences to a distribution over sequences. Thus, they can be seen as\nstochastic language layers in a language network, where the learnable\nparameters are the natural language prompts at each layer. By stacking two such\nlayers and feeding the output of one layer to the next, we obtain a Deep\nLanguage Network (DLN). We first show how to effectively perform prompt\noptimization for a 1-Layer language network (DLN-1). Then, we present an\nextension that applies to 2-layer DLNs (DLN-2), where two prompts must be\nlearned. The key idea is to consider the output of the first layer as a latent\nvariable, which requires inference, and prompts to be learned as the parameters\nof the generative distribution. We first test the effectiveness of DLN-1 in\nmultiple reasoning and natural language understanding tasks. Then, we show that\nDLN-2 can reach higher performance than a single layer, showing promise that we\nmight reach comparable performance to GPT-4, even when each LLM in the network\nis smaller and less powerful.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of\n  Confidence Elicitation in LLMs\u2b1b  The task of empowering large language models (LLMs) to accurately express\ntheir confidence, referred to as confidence elicitation, is essential in\nensuring reliable and trustworthy decision-making processes. Previous methods,\nwhich primarily rely on model logits, have become less suitable for LLMs and\neven infeasible with the rise of closed-source LLMs (e.g., commercialized LLM\nAPIs). This leads to a growing need to explore the untapped area of\n\\emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence,\nin this study, we investigate approaches for confidence elicitation that do not\nrequire model fine-tuning or access to proprietary information. We introduce\nthree categories of methods: verbalize-based, consistency-based, and their\nhybrid methods for benchmarking, and evaluate their performance across five\ntypes of datasets and four widely-used LLMs. Our analysis of these methods\nuncovers several key insights: 1) LLMs often exhibit a high degree of\noverconfidence when verbalizing their confidence; 2) Prompting strategies such\nas CoT, Top-K and Multi-step confidences improve calibration of verbalized\nconfidence; 3) Consistency-based methods outperform the verbalized confidences\nin most cases, with particularly notable improvements on the arithmetic\nreasoning task; 4) Hybrid methods consistently deliver the best performance\nover their baselines, thereby emerging as a promising state-of-the-art\napproach; 5) Despite these advancements, all investigated methods continue to\nstruggle with challenging tasks, such as those requiring professional\nknowledge, leaving significant scope for improvement of confidence elicitation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ToolQA: A Dataset for LLM Question Answering with External Tools\u2b1b  Large Language Models (LLMs) have demonstrated impressive performance in\nvarious NLP tasks, but they still suffer from challenges such as hallucination\nand weak numerical reasoning. To overcome these challenges, external tools can\nbe used to enhance LLMs' question-answering abilities. However, current\nevaluation methods do not distinguish between questions that can be answered\nusing LLMs' internal knowledge and those that require external information\nthrough tool use. To address this issue, we introduce a new dataset called\nToolQA, which is designed to faithfully evaluate LLMs' ability to use external\ntools for question answering. Our development of ToolQA involved a scalable,\nautomated process for dataset curation, along with 13 specialized tools\ndesigned for interaction with external knowledge in order to answer questions.\nImportantly, we strive to minimize the overlap between our benchmark data and\nLLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use\nreasoning abilities. We conducted an in-depth diagnosis of existing tool-use\nLLMs to highlight their strengths, weaknesses, and potential improvements. Our\nfindings set a new benchmark for evaluating LLMs and suggest new directions for\nfuture advancements. Our data and code are freely available to the broader\nscientific community on GitHub.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric\n  Demonstrates LLMs are Pre-trained on Formally Diverse Data\u2b1b  Current trends to pre-train capable Large Language Models (LLMs) mostly focus\non scaling of model and dataset size. However, the quality of pre-training data\nis an important factor for training powerful LLMs, yet it is a nebulous concept\nthat has not been fully characterized. Therefore, we use the recently proposed\nTask2Vec diversity coefficient to ground and understand formal aspects of data\nquality, to go beyond scale alone. Specifically, we measure the diversity\ncoefficient of publicly available pre-training datasets to demonstrate that\ntheir formal diversity is high when compared to theoretical lower and upper\nbounds. In addition, to build confidence in the diversity coefficient, we\nconduct interpretability experiments and find that the coefficient aligns with\nintuitive properties of diversity, e.g., it increases as the number of latent\nconcepts increases. We conclude the diversity coefficient is reliable, show\nit's high for publicly available LLM datasets, and conjecture it can be used to\nbuild useful diverse datasets for LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews\n  using Domain-specific Finetuned Large Language Models\u2b1b  With the proliferation of open-sourced Large Language Models (LLMs) and\nefficient finetuning techniques, we are on the cusp of the emergence of\nnumerous domain-specific LLMs that have been finetuned for expertise across\nspecialized fields and applications for which the current general-purpose LLMs\nare unsuitable. In academia, this technology has the potential to revolutionize\nthe way we conduct systematic literature reviews (SLRs), access knowledge and\ngenerate new insights. This paper proposes an AI-enabled methodological\nframework that combines the power of LLMs with the rigorous reporting\nguidelines of the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA). By finetuning LLMs on domain-specific academic papers\nthat have been selected as a result of a rigorous SLR process, the proposed\nPRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer\nthe potential to achieve greater efficiency, reusability and scalability, while\nalso opening the potential for conducting incremental living systematic reviews\nwith the aid of LLMs. Additionally, the proposed approach for leveraging LLMs\nfor SLRs enables the dissemination of finetuned models, empowering researchers\nto accelerate advancements and democratize cutting-edge research. This paper\npresents the case for the feasibility of finetuned LLMs to support rigorous\nSLRs and the technical requirements for realizing this. This work then proposes\nthe extended PRISMA-DFLLM checklist of reporting guidelines as well as the\nadvantages, challenges, and potential implications of implementing\nPRISMA-DFLLM. Finally, a future research roadmap to develop this line of\nAI-enabled SLRs is presented, paving the way for a new era of evidence\nsynthesis and knowledge discovery.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Importance of Human-Labeled Data in the Era of LLMs\u2b1b  The advent of large language models (LLMs) has brought about a revolution in\nthe development of tailored machine learning models and sparked debates on\nredefining data requirements. The automation facilitated by the training and\nimplementation of LLMs has led to discussions and aspirations that human-level\nlabeling interventions may no longer hold the same level of importance as in\nthe era of supervised learning. This paper presents compelling arguments\nsupporting the ongoing relevance of human-labeled data in the era of LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Assisted Content Analysis: Using Large Language Models to Support\n  Deductive Coding\u2b1b  Deductive coding is a widely used qualitative research method for determining\nthe prevalence of themes across documents. While useful, deductive coding is\noften burdensome and time consuming since it requires researchers to read,\ninterpret, and reliably categorize a large body of unstructured text documents.\nLarge language models (LLMs), like ChatGPT, are a class of quickly evolving AI\ntools that can perform a range of natural language processing and reasoning\ntasks. In this study, we explore the use of LLMs to reduce the time it takes\nfor deductive coding while retaining the flexibility of a traditional content\nanalysis. We outline the proposed approach, called LLM-assisted content\nanalysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a\npublicly available deductive coding data set. Additionally, we conduct an\nempirical benchmark using LACA on 4 publicly available data sets to assess the\nbroader question of how well GPT-3.5 performs across a range of deductive\ncoding tasks. Overall, we find that GPT-3.5 can often perform deductive coding\nat levels of agreement comparable to human coders. Additionally, we demonstrate\nthat LACA can help refine prompts for deductive coding, identify codes for\nwhich an LLM is randomly guessing, and help assess when to use LLMs vs. human\ncoders for deductive coding. We conclude with several implications for future\npractice of deductive coding and related research methods.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality\n  synthetic note-oriented doctor-patient conversations?\u2b1b  This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023\nshared task for Task-A and Task-C. We focus especially on Task-C and propose a\nnovel LLMs cooperation system named a doctor-patient loop to generate\nhigh-quality conversation data sets. The experiment results demonstrate that\nour approaches yield reasonable performance as evaluated by automatic metrics\nsuch as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we\nconducted a comparative analysis between our proposed method and ChatGPT and\nGPT-4. This analysis also investigates the potential of utilizing cooperation\nLLMs to generate high-quality datasets.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring\n  of Learning Objectives\u2b1b  We evaluated the capability of a generative pre-trained transformer (GPT-4)\nto automatically generate high-quality learning objectives (LOs) in the context\nof a practically oriented university course on Artificial Intelligence.\nDiscussions of opportunities (e.g., content generation, explanation) and risks\n(e.g., cheating) of this emerging technology in education have intensified, but\nto date there has not been a study of the models' capabilities in supporting\nthe course design and authoring of LOs. LOs articulate the knowledge and skills\nlearners are intended to acquire by engaging with a course. To be effective,\nLOs must focus on what students are intended to achieve, focus on specific\ncognitive processes, and be measurable. Thus, authoring high-quality LOs is a\nchallenging and time consuming (i.e., expensive) effort. We evaluated 127 LOs\nthat were automatically generated based on a carefully crafted prompt (detailed\nguidelines on high-quality LOs authoring) submitted to GPT-4 for conceptual\nmodules and projects of an AI Practitioner course. We analyzed the generated\nLOs if they follow certain best practices such as beginning with action verbs\nfrom Bloom's taxonomy in regards to the level of sophistication intended. Our\nanalysis showed that the generated LOs are sensible, properly expressed (e.g.,\nstarting with an action verb), and that they largely operate at the appropriate\nlevel of Bloom's taxonomy, respecting the different nature of the conceptual\nmodules (lower levels) and projects (higher levels). Our results can be\nleveraged by instructors and curricular designers wishing to take advantage of\nthe state-of-the-art generative models to support their curricular and course\ndesign efforts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen\n  LLMs\u2b1b  In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling\nfrozen LLMs to perform both understanding and generation tasks involving\nnon-linguistic modalities such as images or videos. SPAE converts between raw\npixels and interpretable lexical tokens (or words) extracted from the LLM's\nvocabulary. The resulting tokens capture both the semantic meaning and the\nfine-grained details needed for visual reconstruction, effectively translating\nthe visual content into a language comprehensible to the LLM, and empowering it\nto perform a wide array of multimodal tasks. Our approach is validated through\nin-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set\nof image understanding and generation tasks. Our method marks the first\nsuccessful attempt to enable a frozen LLM to generate image content while\nsurpassing state-of-the-art performance in image understanding tasks, under the\nsame setting, by over 25%.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Conformer LLMs -- Convolution Augmented Large Language Models\u2b1b  This work builds together two popular blocks of neural architecture, namely\nconvolutional layers and Transformers, for large language models (LLMs).\nNon-causal conformers are used ubiquitously in automatic speech recognition.\nThis work aims to adapt these architectures in a causal setup for training\nLLMs. Transformers decoders effectively capture long-range dependencies over\nseveral modalities and form a core backbone of modern advancements in machine\nlearning. Convolutional architectures have been popular in extracting features\nin domains such as raw 1-D signals, speech, and images, to name a few. In this\npaper, by combining local and global dependencies over latent representations\nusing causal convolutional filters and Transformer, we achieve significant\ngains in performance. This work showcases a robust speech architecture that can\nbe integrated and adapted in a causal setup beyond speech applications for\nlarge-scale language modeling.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on\n  the Tensor-Train Decomposition\u2b1b  High-dimensional token embeddings underpin Large Language Models (LLMs), as\nthey can capture subtle semantic information and significantly enhance the\nmodelling of complex language patterns. However, the associated high\ndimensionality also introduces considerable model parameters, and a\nprohibitively high model storage. To address this issue, this work proposes an\napproach based on the Tensor-Train Decomposition (TTD), where each token\nembedding is treated as a Matrix Product State (MPS) that can be efficiently\ncomputed in a distributed manner. The experimental results on GPT-2 demonstrate\nthat, through our approach, the embedding layer can be compressed by a factor\nof up to 38.40 times, and when the compression factor is 3.31 times, even\nproduced a better performance than the original GPT-2 model.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction\u2b1b  In the current digitalization era, capturing and effectively representing\nknowledge is crucial in most real-world scenarios. In this context, knowledge\ngraphs represent a potent tool for retrieving and organizing a vast amount of\ninformation in a properly interconnected and interpretable structure. However,\ntheir generation is still challenging and often requires considerable human\neffort and domain expertise, hampering the scalability and flexibility across\ndifferent application fields. This paper proposes an innovative knowledge graph\ngeneration approach that leverages the potential of the latest generative large\nlanguage models, such as GPT-3.5, that can address all the main critical issues\nin knowledge graph building. The approach is conveyed in a pipeline that\ncomprises novel iterative zero-shot and external knowledge-agnostic strategies\nin the main stages of the generation process. Our unique manifold approach may\nencompass significant benefits to the scientific community. In particular, the\nmain contribution can be summarized by: (i) an innovative strategy for\niteratively prompting large language models to extract relevant components of\nthe final graph; (ii) a zero-shot strategy for each prompt, meaning that there\nis no need for providing examples for \"guiding\" the prompt result; (iii) a\nscalable solution, as the adoption of LLMs avoids the need for any external\nresources or human expertise. To assess the effectiveness of our proposed\nmodel, we performed experiments on a dataset that covered a specific domain. We\nclaim that our proposal is a suitable solution for scalable and versatile\nknowledge graph construction and may be applied to different and novel\ncontexts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Recommender Systems in the Era of Large Language Models (LLMs)\u2b1b  With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users' interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for\n  Efficient LLM Inference\u2b1b  Autoregressive large language models (LLMs) have made remarkable progress in\nvarious natural language generation tasks. However, they incur high computation\ncost and latency resulting from the autoregressive token-by-token generation.\nTo address this issue, several approaches have been proposed to reduce\ncomputational cost using early-exit strategies. These strategies enable faster\ntext generation using reduced computation without applying the full computation\ngraph to each token. While existing token-level early exit methods show\npromising results for online inference, they cannot be readily applied for\nbatch inferencing and Key-Value caching. This is because they have to wait\nuntil the last token in a batch exits before they can stop computing. This\nseverely limits the practical application of such techniques. In this paper, we\npropose a simple and effective token-level early exit method, SkipDecode,\ndesigned to work seamlessly with batch inferencing and KV caching. It overcomes\nprior constraints by setting up a singular exit point for every token in a\nbatch at each sequence position. It also guarantees a monotonic decrease in\nexit points, thereby eliminating the need to recompute KV Caches for preceding\ntokens. Rather than terminating computation prematurely as in prior works, our\napproach bypasses lower to middle layers, devoting most of the computational\nresources to upper layers, allowing later tokens to benefit from the compute\nexpenditure by earlier tokens. Our experimental results show that SkipDecode\ncan obtain 2x to 5x inference speedups with negligible regression across a\nvariety of tasks. This is achieved using OPT models of 1.3 billion and 6.7\nbillion parameters, all the while being directly compatible with batching and\nKV caching optimization techniques.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation\u2b1b  News summary generation is an important task in the field of intelligence\nanalysis, which can provide accurate and comprehensive information to help\npeople better understand and respond to complex real-world events. However,\ntraditional news summary generation methods face some challenges, which are\nlimited by the model itself and the amount of training data, as well as the\ninfluence of text noise, making it difficult to generate reliable information\naccurately. In this paper, we propose a new paradigm for news summary\ngeneration using LLM with powerful natural language understanding and\ngenerative capabilities. We use LLM to extract multiple structured event\npatterns from the events contained in news paragraphs, evolve the event pattern\npopulation with genetic algorithm, and select the most adaptive event pattern\nto input into the LLM to generate news summaries. A News Summary Generator\n(NSG) is designed to select and evolve the event pattern populations and\ngenerate news summaries. The experimental results show that the news summary\ngenerator is able to generate accurate and reliable news summaries with some\ngeneralization ability.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of\n  LLMs by Validating Low-Confidence Generation\u2b1b  Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with GPT-3.5 (text-davinci-003) on the 'article\ngeneration task', we first demonstrate the individual efficacy of our detection\nand mitigation techniques. Specifically, the detection technique achieves a\nrecall of ~88% and the mitigation technique successfully mitigates 57.6% of the\ncorrectly detected hallucinations. Importantly, our mitigation technique does\nnot introduce new hallucinations even in the case of incorrectly detected\nhallucinations, i.e., false positives. Then, we show that the proposed active\ndetection and mitigation approach successfully reduces the hallucinations of\nthe GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the\neffectiveness and wide applicability of our approach through additional studies\nincluding performance on different types of questions (multi-hop and false\npremise questions) and with another LLM from a different model family (Vicuna).\nIn summary, our work contributes to improving the reliability and\ntrustworthiness of large language models, a crucial step en route to enabling\ntheir widespread adoption in real-world applications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "BeaverTails: Towards Improved Safety Alignment of LLM via a\n  Human-Preference Dataset\u2b1b  In this paper, we introduce the BeaverTails dataset, aimed at fostering\nresearch on safety alignment in large language models (LLMs). This dataset\nuniquely separates annotations of helpfulness and harmlessness for\nquestion-answering pairs, thus offering distinct perspectives on these crucial\nattributes. In total, we have gathered safety meta-labels for 333,963\nquestion-answer (QA) pairs and 361,903 pairs of expert comparison data for both\nthe helpfulness and harmlessness metrics. We further showcase applications of\nBeaverTails in content moderation and reinforcement learning with human\nfeedback (RLHF), emphasizing its potential for practical safety measures in\nLLMs. We believe this dataset provides vital resources for the community,\ncontributing towards the safe development and deployment of LLMs. Our project\npage is available at the following URL:\nhttps://sites.google.com/view/pku-beavertails.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Go Beyond The Obvious: Probing the gap of INFORMAL reasoning ability\n  between Humanity and LLMs by Detective Reasoning Puzzle Benchmark\u2b1b  Informal reasoning ability is the ability to reason based on common sense,\nexperience, and intuition.Humans use informal reasoning every day to extract\nthe most influential elements for their decision-making from a large amount of\nlife-like information.With the rapid development of language models, the\nrealization of general artificial intelligence has emerged with hope. Given the\noutstanding informal reasoning ability of humans, how much informal reasoning\nability language models have has not been well studied by scholars.In order to\nexplore the gap between humans and language models in informal reasoning\nability, this paper constructs a Detective Reasoning Benchmark, which is an\nassembly of 1,200 questions gathered from accessible online resources, aims at\nevaluating the model's informal reasoning ability in real-life\ncontext.Considering the improvement of the model's informal reasoning ability\nrestricted by the lack of benchmark, we further propose a Self-Question Prompt\nFramework that mimics human thinking to enhance the model's informal reasoning\nability.The goals of self-question are to find key elements, deeply investigate\nthe connections between these elements, encourage the relationship between each\nelement and the problem, and finally, require the model to reasonably answer\nthe problem.The experimental results show that human performance greatly\noutperforms the SoTA Language Models in Detective Reasoning Benchmark.Besides,\nSelf-Question is proven to be the most effective prompt engineering in\nimproving GPT-4's informal reasoning ability, but it still does not even\nsurpass the lowest score made by human participants.Upon acceptance of the\npaper, the source code for the benchmark will be made publicly accessible.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Explaining Competitive-Level Programming Solutions using LLMs\u2b1b  In this paper, we approach competitive-level programming problem-solving as a\ncomposite task of reasoning and code generation. We propose a novel method to\nautomatically annotate natural language explanations to \\textit{<problem,\nsolution>} pairs. We show that despite poor performance in solving\ncompetitive-level programming problems, state-of-the-art LLMs exhibit a strong\ncapacity in describing and explaining solutions. Our explanation generation\nmethodology can generate a structured solution explanation for the problem\ncontaining descriptions and analysis. To evaluate the quality of the annotated\nexplanations, we examine their effectiveness in two aspects: 1) satisfying the\nhuman programming expert who authored the oracle solution, and 2) aiding LLMs\nin solving problems more effectively. The experimental results on the\nCodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities\nin describing the solution are comparable, GPT-4 shows a better understanding\nof the key idea behind the solution.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM\n  Decoding\u2b1b  This paper presents \"Predictive Pipelined Decoding (PPD),\" an approach that\nspeeds up greedy decoding in Large Language Models (LLMs) while maintaining the\nexact same output as the original decoding. Unlike conventional strategies, PPD\nemploys additional compute resources to parallelize the initiation of\nsubsequent token decoding during the current token decoding. This innovative\nmethod reduces decoding latency and reshapes the understanding of trade-offs in\nLLM decoding strategies. We have developed a theoretical framework that allows\nus to analyze the trade-off between computation and latency. Using this\nframework, we can analytically estimate the potential reduction in latency\nassociated with our proposed method, achieved through the assessment of the\nmatch rate, represented as p_correct. The results demonstrate that the use of\nextra computational resources has the potential to accelerate LLM greedy\ndecoding.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "VELMA: Verbalization Embodiment of LLM Agents for Vision and Language\n  Navigation in Street View\u2b1b  Incremental decision making in real-world environments is one of the most\nchallenging tasks in embodied artificial intelligence. One particularly\ndemanding scenario is Vision and Language Navigation~(VLN) which requires\nvisual and natural language understanding as well as spatial and temporal\nreasoning capabilities. The embodied agent needs to ground its understanding of\nnavigation instructions in observations of a real-world environment like Street\nView. Despite the impressive results of LLMs in other research areas, it is an\nongoing problem of how to best connect them with an interactive visual\nenvironment. In this work, we propose VELMA, an embodied LLM agent that uses a\nverbalization of the trajectory and of visual environment observations as\ncontextual prompt for the next action. Visual information is verbalized by a\npipeline that extracts landmarks from the human written navigation instructions\nand uses CLIP to determine their visibility in the current panorama view. We\nshow that VELMA is able to successfully follow navigation instructions in\nStreet View with only two in-context examples. We further finetune the LLM\nagent on a few thousand examples and achieve 25%-30% relative improvement in\ntask completion over the previous state-of-the-art for two datasets.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems\u2b1b  In autonomic computing, self-adaptation has been proposed as a fundamental\nparadigm to manage the complexity of multiagent systems (MASs). This achieved\nby extending a system with support to monitor and adapt itself to achieve\nspecific concerns of interest. Communication in these systems is key given that\nin scenarios involving agent interaction, it enhances cooperation and reduces\ncoordination challenges by enabling direct, clear information exchange.\nHowever, improving the expressiveness of the interaction communication with\nMASs is not without challenges. In this sense, the interplay between\nself-adaptive systems and effective communication is crucial for future MAS\nadvancements. In this paper, we propose the integration of large language\nmodels (LLMs) such as GPT-based technologies into multiagent systems. We anchor\nour methodology on the MAPE-K model, which is renowned for its robust support\nin monitoring, analyzing, planning, and executing system adaptations in\nresponse to dynamic environments. We also present a practical illustration of\nthe proposed approach, in which we implement and assess a basic MAS-based\napplication. The approach significantly advances the state-of-the-art of\nself-adaptive systems by proposing a new paradigm for MAS self-adaptation of\nautonomous systems based on LLM capabilities.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT\u2b1b  Knowledge Graphs (KG) provide us with a structured, flexible, transparent,\ncross-system, and collaborative way of organizing our knowledge and data across\nvarious domains in society and industrial as well as scientific disciplines.\nKGs surpass any other form of representation in terms of effectiveness.\nHowever, Knowledge Graph Engineering (KGE) requires in-depth experiences of\ngraph structures, web technologies, existing models and vocabularies, rule\nsets, logic, as well as best practices. It also demands a significant amount of\nwork. Considering the advancements in large language models (LLMs) and their\ninterfaces and applications in recent years, we have conducted comprehensive\nexperiments with ChatGPT to explore its potential in supporting KGE. In this\npaper, we present a selection of these experiments and their results to\ndemonstrate how ChatGPT can assist us in the development and management of KGs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\u2b1b  Modular vision-language models (Vision-LLMs) align pretrained image encoders\nwith frozen large language models (LLMs), representing a computationally much\nmore efficient alternative to end-to-end training of large vision-language\nmodels from scratch, which is prohibitively expensive for most researchers and\npractitioners. Vision-LLMs instead post-hoc condition LLMs to `understand' the\noutput of an image encoder. With the abundance of readily available\nhigh-quality English image-text data as well as monolingual English LLMs, the\nresearch focus has been on English-only Vision-LLMs. Multilingual\nvision-language models are still predominantly obtained via expensive\nend-to-end pretraining, resulting in comparatively smaller models, trained on\nlimited multilingual image data supplemented with text-only multilingual\ncorpora. In this work, we present mBLIP, the first multilingual Vision-LLM,\nwhich we obtain in a computationally efficient manner -- on consumer hardware\nand using only a few million training examples -- by leveraging a pretrained\nmultilingual LLM. To this end, we \\textit{re-align} an image encoder previously\ntuned to an English LLM to a new, multilingual LLM -- for this, we leverage\nmultilingual data from a mix of vision-and-language tasks, which we obtain by\nmachine-translating high-quality English data to 95 languages. On the IGLUE\nbenchmark, mBLIP yields results competitive with state-of-the-art models.\nMoreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms\nPaLI-X (a model with 55B parameters). Compared to these very large multilingual\nvision-language models trained from scratch, we obtain mBLIP by training orders\nof magnitude fewer parameters on magnitudes less data. We release our model and\ncode at \\url{https://github.com/gregor-ge/mBLIP}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Generating Efficient Training Data via LLM-based Attribute Manipulation\u2b1b  In this paper, we propose a novel method, Chain-of-Thoughts Attribute\nManipulation (CoTAM), to guide few-shot learning by carefully crafted data from\nLarge Language Models (LLMs). The main idea is to create data with changes only\nin the attribute targeted by the task. Inspired by facial attribute\nmanipulation, our approach generates label-switched data by leveraging LLMs to\nmanipulate task-specific attributes and reconstruct new sentences in a\ncontrolled manner. Instead of conventional latent representation controlling,\nwe implement chain-of-thoughts decomposition and reconstruction to adapt the\nprocedure to LLMs. Extensive results on text classification and other tasks\nverify the advantage of CoTAM over other LLM-based text generation methods with\nthe same number of training examples. Analysis visualizes the attribute\nmanipulation effectiveness of CoTAM and presents the potential of LLM-guided\nlearning with even less supervision.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Detecting LLM-Generated Text in Computing Education: A Comparative Study\n  for ChatGPT Cases\u2b1b  Due to the recent improvements and wide availability of Large Language Models\n(LLMs), they have posed a serious threat to academic integrity in education.\nModern LLM-generated text detectors attempt to combat the problem by offering\neducators with services to assess whether some text is LLM-generated. In this\nwork, we have collected 124 submissions from computer science students before\nthe creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this\ndata to evaluate eight publicly-available LLM-generated text detectors through\nthe measures of accuracy, false positives, and resilience. The purpose of this\nwork is to inform the community of what LLM-generated text detectors work and\nwhich do not, but also to provide insights for educators to better maintain\nacademic integrity in their courses. Our results find that CopyLeaks is the\nmost accurate LLM-generated text detector, GPTKit is the best LLM-generated\ntext detector to reduce false positives, and GLTR is the most resilient\nLLM-generated text detector. We also express concerns over 52 false positives\n(of 114 human written submissions) generated by GPTZero. Finally, we note that\nall LLM-generated text detectors are less accurate with code, other languages\n(aside from English), and after the use of paraphrasing tools (like QuillBot).\nModern detectors are still in need of improvements so that they can offer a\nfull-proof solution to help maintain academic integrity. Further, their\nusability can be improved by facilitating a smooth API integration, providing\nclear documentation of their features and the understandability of their\nmodel(s), and supporting more commonly used languages.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs be Good Financial Advisors?: An Initial Study in Personal\n  Decision Making for Optimized Outcomes\u2b1b  Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT\nand Bard, are becoming available to users that have the potential to\nrevolutionize the quality of decision-making achieved by the public. In this\ncontext, we set out to investigate how such systems perform in the personal\nfinance domain, where financial inclusion has been an overarching stated aim of\nbanks for decades. We asked 13 questions representing banking products in\npersonal finance: bank account, credit card, and certificate of deposits and\ntheir inter-product interactions, and decisions related to high-value\npurchases, payment of bank dues, and investment advice, and in different\ndialects and languages (English, African American Vernacular English, and\nTelugu). We find that although the outputs of the chatbots are fluent and\nplausible, there are still critical gaps in providing accurate and reliable\nfinancial information using LLM-based chatbots.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise\n  Comparisons using Large Language Models\u2b1b  Current developments in large language models (LLMs) have enabled impressive\nzero-shot capabilities across various natural language tasks. An interesting\napplication of these systems is in the automated assessment of natural language\ngeneration (NLG), a highly challenging area with great practical benefit. In\nthis paper, we explore two options for exploiting the emergent abilities of\nLLMs for zero-shot NLG assessment: absolute score prediction, and comparative\nassessment which uses relative comparisons between pairs of candidates. Though\ncomparative assessment has not been extensively studied in NLG assessment, we\nnote that humans often find it more intuitive to compare two options rather\nthan scoring each one independently. This work examines comparative assessment\nfrom multiple perspectives: performance compared to absolute grading;\npositional biases in the prompt; and efficient ranking in terms of the number\nof comparisons. We illustrate that LLM comparative assessment is a simple,\ngeneral and effective approach for NLG assessment. For moderate-sized\nopen-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is\nsuperior to prompt scoring, and in many cases can achieve performance\ncompetitive with state-of-the-art methods. Additionally, we demonstrate that\nLLMs often exhibit strong positional biases when making pairwise comparisons,\nand we propose debiasing methods that can further improve performance.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and\n  Addressing Sociological Implications\u2b1b  Gender bias in artificial intelligence (AI) and natural language processing\nhas garnered significant attention due to its potential impact on societal\nperceptions and biases. This research paper aims to analyze gender bias in\nLarge Language Models (LLMs) with a focus on multiple comparisons between GPT-2\nand GPT-3.5, some prominent language models, to better understand its\nimplications. Through a comprehensive literature review, the study examines\nexisting research on gender bias in AI language models and identifies gaps in\nthe current knowledge. The methodology involves collecting and preprocessing\ndata from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis\ntechniques to evaluate gender bias in the generated text. The findings shed\nlight on gendered word associations, language usage, and biased narratives\npresent in the outputs of these Large Language Models. The discussion explores\nthe ethical implications of gender bias and its potential consequences on\nsocial perceptions and marginalized communities. Additionally, the paper\npresents strategies for reducing gender bias in LLMs, including algorithmic\napproaches and data augmentation techniques. The research highlights the\nimportance of interdisciplinary collaborations and the role of sociological\nstudies in mitigating gender bias in AI models. By addressing these issues, we\ncan pave the way for more inclusive and unbiased AI systems that have a\npositive impact on society.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring\n  Instruction Tuning\u2b1b  Human-AI interactivity is a critical aspect that reflects the usability of\nmultimodal large language models (MLLMs). However, existing end-to-end MLLMs\nonly allow users to interact with them through language instructions, leading\nto the limitation of the interactive accuracy and efficiency. In this study, we\npresent precise referring instructions that utilize diverse reference\nrepresentations such as points and boxes as referring prompts to refer to the\nspecial region. This enables MLLMs to focus on the region of interest and\nachieve finer-grained interaction. Based on precise referring instruction, we\npropose ChatSpot, a unified end-to-end multimodal large language model that\nsupports diverse forms of interactivity including mouse clicks, drag-and-drop,\nand drawing boxes, which provides a more flexible and seamless interactive\nexperience. We also construct a multi-grained vision-language\ninstruction-following dataset based on existing datasets and GPT-4 generating.\nFurthermore, we design a series of evaluation tasks to assess the effectiveness\nof region recognition and interaction. Experimental results showcase ChatSpot's\npromising performance.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization\n  Using Floating-Point Formats\u2b1b  In the complex domain of large language models (LLMs), striking a balance\nbetween computational efficiency and maintaining model quality is a formidable\nchallenge. Navigating the inherent limitations of uniform quantization,\nparticularly when dealing with outliers, and motivated by the launch of\nNVIDIA's H100 hardware, this study delves into the viability of floating-point\n(FP) quantization, particularly focusing on FP8 and FP4, as a potential\nsolution. Our comprehensive investigation reveals that for LLMs, FP8 activation\nconsistently outshines its integer (INT8) equivalent, with the performance edge\nbecoming more noticeable in models possessing parameters beyond one billion.\nFor weight quantization, our findings indicate that FP4 exhibits comparable, if\nnot superior, performance to INT4, simplifying deployment on FP-supported\nhardware like H100. To mitigate the overhead from precision alignment caused by\nthe disparity between weights and activations, we propose two scaling\nconstraints for weight quantization that negligibly impact the performance\ncompared to the standard W4A8 model. We additionally enhance our quantization\nmethods by integrating the Low Rank Compensation (LoRC) strategy, yielding\nimprovements especially in smaller models. The results of our investigation\nemphasize the immense potential of FP quantization for LLMs, paving the way for\nhigh-efficiency deployment in resource-limited settings.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large\n  Language Models\u2b1b  Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs as Workers in Human-Computational Algorithms? Replicating\n  Crowdsourcing Pipelines with LLMs\u2b1b  LLMs have shown promise in replicating human-like behavior in crowdsourcing\ntasks that were previously thought to be exclusive to human abilities. However,\ncurrent efforts focus mainly on simple atomic tasks. We explore whether LLMs\ncan replicate more complex crowdsourcing pipelines. We find that modern LLMs\ncan simulate some of crowdworkers' abilities in these \"human computation\nalgorithms,\" but the level of success is variable and influenced by requesters'\nunderstanding of LLM capabilities, the specific skills required for sub-tasks,\nand the optimal interaction modality for performing these sub-tasks. We reflect\non human and LLMs' different sensitivities to instructions, stress the\nimportance of enabling human-facing safeguards for LLMs, and discuss the\npotential of training humans and LLMs with complementary skill sets. Crucially,\nwe show that replicating crowdsourcing pipelines offers a valuable platform to\ninvestigate (1) the relative strengths of LLMs on different tasks (by\ncross-comparing their performances on sub-tasks) and (2) LLMs' potential in\ncomplex tasks, where they can complete part of the tasks while leaving others\nto humans.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Several categories of Large Language Models (LLMs): A Short Survey\u2b1b  Large Language Models(LLMs)have become effective tools for natural language\nprocessing and have been used in many different fields. This essay offers a\nsuccinct summary of various LLM subcategories. The survey emphasizes recent\ndevelopments and efforts made for various LLM kinds, including task-based\nfinancial LLMs, multilingual language LLMs, biomedical and clinical LLMs,\nvision language LLMs, and code language models. The survey gives a general\nsummary of the methods, attributes, datasets, transformer models, and\ncomparison metrics applied in each category of LLMs. Furthermore, it highlights\nunresolved problems in the field of developing chatbots and virtual assistants,\nsuch as boosting natural language processing, enhancing chatbot intelligence,\nand resolving moral and legal dilemmas. The purpose of this study is to provide\nreaders, developers, academics, and users interested in LLM-based chatbots and\nvirtual intelligent assistant technologies with useful information and future\ndirections.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Abusing Images and Sounds for Indirect Instruction Injection in\n  Multi-Modal LLMs\u2b1b  We demonstrate how images and sounds can be used for indirect prompt and\ninstruction injection in multi-modal LLMs. An attacker generates an adversarial\nperturbation corresponding to the prompt and blends it into an image or audio\nrecording. When the user asks the (unmodified, benign) model about the\nperturbed image or audio, the perturbation steers the model to output the\nattacker-chosen text and/or make the subsequent dialog follow the attacker's\ninstruction. We illustrate this attack with several proof-of-concept examples\ntargeting LLaVa and PandaGPT.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Censorship: A Machine Learning Challenge or a Computer Security\n  Problem?\u2b1b  Large language models (LLMs) have exhibited impressive capabilities in\ncomprehending complex instructions. However, their blind adherence to provided\ninstructions has led to concerns regarding risks of malicious use. Existing\ndefence mechanisms, such as model fine-tuning or output censorship using LLMs,\nhave proven to be fallible, as LLMs can still generate problematic responses.\nCommonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs. In\nthis paper, we present the theoretical limitations of such semantic censorship\napproaches. Specifically, we demonstrate that semantic censorship can be\nperceived as an undecidable problem, highlighting the inherent challenges in\ncensorship that arise due to LLMs' programmatic and instruction-following\ncapabilities. Furthermore, we argue that the challenges extend beyond semantic\ncensorship, as knowledgeable attackers can reconstruct impermissible outputs\nfrom a collection of permissible ones. As a result, we propose that the problem\nof censorship needs to be reevaluated; it should be treated as a security\nproblem which warrants the adaptation of security-based approaches to mitigate\npotential risks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "OUTFOX: LLM-generated Essay Detection through In-context Learning with\n  Adversarially Generated Examples\u2b1b  Large Language Models (LLMs) have achieved human-level fluency in text\ngeneration, making it difficult to distinguish between human-written and\nLLM-generated texts. This poses a growing risk of misuse of LLMs and demands\nthe development of detectors to identify LLM-generated texts. However, existing\ndetectors lack robustness against attacks: they degrade detection accuracy by\nsimply paraphrasing LLM-generated texts. Furthermore, a malicious user might\nattempt to deliberately evade the detectors based on detection results, but\nthis has not been assumed in previous studies. In this paper, we propose\nOUTFOX, a framework that improves the robustness of LLM-generated-text\ndetectors by allowing both the detector and the attacker to consider each\nother's output. In this framework, the attacker uses the detector's prediction\nlabels as examples for in-context learning and adversarially generates essays\nthat are harder to detect, while the detector uses the adversarially generated\nessays as examples for in-context learning to learn to detect essays from a\nstrong attacker. Experiments in the domain of student essays show that the\nproposed detector improves the detection performance on the attacker-generated\ntexts by up to +41.3 points in F1-score. Furthermore, the proposed detector\nshows a state-of-the-art detection performance: up to 96.9 points in F1-score,\nbeating existing detectors on non-attacked texts. Finally, the proposed\nattacker drastically degrades the performance of detectors by up to -57.0\npoints F1-score, massively outperforming the baseline paraphrasing method for\nevading detection.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Cognitive Judgements Differ From Human\u2b1b  Large Language Models (LLMs) have lately been on the spotlight of\nresearchers, businesses, and consumers alike. While the linguistic capabilities\nof such models have been studied extensively, there is growing interest in\ninvestigating them as cognitive subjects. In the present work I examine GPT-3\nand ChatGPT capabilities on an limited-data inductive reasoning task from the\ncognitive science literature. The results suggest that these models' cognitive\njudgements are not human-like.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Looming Threat of Fake and LLM-generated LinkedIn Profiles:\n  Challenges and Opportunities for Detection and Prevention\u2b1b  In this paper, we present a novel method for detecting fake and Large\nLanguage Model (LLM)-generated profiles in the LinkedIn Online Social Network\nimmediately upon registration and before establishing connections. Early fake\nprofile identification is crucial to maintaining the platform's integrity since\nit prevents imposters from acquiring the private and sensitive information of\nlegitimate users and from gaining an opportunity to increase their credibility\nfor future phishing and scamming activities. This work uses textual information\nprovided in LinkedIn profiles and introduces the Section and Subsection Tag\nEmbedding (SSTE) method to enhance the discriminative characteristics of these\ndata for distinguishing between legitimate profiles and those created by\nimposters manually or by using an LLM. Additionally, the dearth of a large\npublicly available LinkedIn dataset motivated us to collect 3600 LinkedIn\nprofiles for our research. We will release our dataset publicly for research\npurposes. This is, to the best of our knowledge, the first large publicly\navailable LinkedIn dataset for fake LinkedIn account detection. Within our\nparadigm, we assess static and contextualized word embeddings, including GloVe,\nFlair, BERT, and RoBERTa. We show that the suggested method can distinguish\nbetween legitimate and fake profiles with an accuracy of about 95% across all\nword embeddings. In addition, we show that SSTE has a promising accuracy for\nidentifying LLM-generated profiles, despite the fact that no LLM-generated\nprofiles were employed during the training phase, and can achieve an accuracy\nof approximately 90% when only 20 LLM-generated profiles are added to the\ntraining set. It is a significant finding since the proliferation of several\nLLMs in the near future makes it extremely challenging to design a single\nsystem that can identify profiles created with various LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Psy-LLM: Scaling up Global Mental Health Psychological Services with\n  AI-based Large Language Models\u2b1b  The demand for psychological counselling has grown significantly in recent\nyears, particularly with the global outbreak of COVID-19, which has heightened\nthe need for timely and professional mental health support. Online\npsychological counselling has emerged as the predominant mode of providing\nservices in response to this demand. In this study, we propose the Psy-LLM\nframework, an AI-based assistive tool leveraging Large Language Models (LLMs)\nfor question-answering in psychological consultation settings to ease the\ndemand for mental health professions. Our framework combines pre-trained LLMs\nwith real-world professional Q\\&A from psychologists and extensively crawled\npsychological articles. The Psy-LLM framework serves as a front-end tool for\nhealthcare professionals, allowing them to provide immediate responses and\nmindfulness activities to alleviate patient stress. Additionally, it functions\nas a screening tool to identify urgent cases requiring further assistance. We\nevaluated the framework using intrinsic metrics, such as perplexity, and\nextrinsic evaluation metrics, with human participant assessments of response\nhelpfulness, fluency, relevance, and logic. The results demonstrate the\neffectiveness of the Psy-LLM framework in generating coherent and relevant\nanswers to psychological questions. This article discusses the potential and\nlimitations of using large language models to enhance mental health support\nthrough AI technologies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "3D-LLM: Injecting the 3D World into Large Language Models\u2b1b  Large language models (LLMs) and Vision-Language Models (VLMs) have been\nproven to excel at multiple tasks, such as commonsense reasoning. Powerful as\nthese models can be, they are not grounded in the 3D physical world, which\ninvolves richer concepts such as spatial relationships, affordances, physics,\nlayout, and so on. In this work, we propose to inject the 3D world into large\nlanguage models and introduce a whole new family of 3D-LLMs. Specifically,\n3D-LLMs can take 3D point clouds and their features as input and perform a\ndiverse set of 3D-related tasks, including captioning, dense captioning, 3D\nquestion answering, task decomposition, 3D grounding, 3D-assisted dialog,\nnavigation, and so on. Using three types of prompting mechanisms that we\ndesign, we are able to collect over 300k 3D-language data covering these tasks.\nTo efficiently train 3D-LLMs, we first utilize a 3D feature extractor that\nobtains 3D features from rendered multi- view images. Then, we use 2D VLMs as\nour backbones to train our 3D-LLMs. By introducing a 3D localization mechanism,\n3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show\nthat our model outperforms state-of-the-art baselines by a large margin (e.g.,\nthe BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore,\nexperiments on our held-in datasets for 3D captioning, task composition, and\n3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative\nexamples also show that our model could perform more tasks beyond the scope of\nexisting LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The potential of LLMs for coding with low-resource and domain-specific\n  programming languages\u2b1b  This paper presents a study on the feasibility of using large language models\n(LLM) for coding with low-resource and domain-specific programming languages\nthat typically lack the amount of data required for effective LLM processing\ntechniques. This study focuses on the econometric scripting language named\nhansl of the open-source software gretl and employs a proprietary LLM based on\nGPT-3.5. Our findings suggest that LLMs can be a useful tool for writing,\nunderstanding, improving, and documenting gretl code, which includes generating\ndescriptive docstrings for functions and providing precise explanations for\nabstract and poorly documented econometric code. While the LLM showcased\npromoting docstring-to-code translation capability, we also identify some\nlimitations, such as its inability to improve certain sections of code and to\nwrite accurate unit tests. This study is a step towards leveraging the power of\nLLMs to facilitate software development in low-resource programming languages\nand ultimately to lower barriers to entry for their adoption.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How to use LLMs for Text Analysis\u2b1b  This guide introduces Large Language Models (LLM) as a highly versatile text\nanalysis method within the social sciences. As LLMs are easy-to-use, cheap,\nfast, and applicable on a broad range of text analysis tasks, ranging from text\nannotation and classification to sentiment analysis and critical discourse\nanalysis, many scholars believe that LLMs will transform how we do text\nanalysis. This how-to guide is aimed at students and researchers with limited\nprogramming experience, and offers a simple introduction to how LLMs can be\nused for text analysis in your own research project, as well as advice on best\npractices. We will go through each of the steps of analyzing textual data with\nLLMs using Python: installing the software, setting up the API, loading the\ndata, developing an analysis prompt, analyzing the text, and validating the\nresults. As an illustrative example, we will use the challenging task of\nidentifying populism in political texts, and show how LLMs move beyond the\nexisting state-of-the-art.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Holistic Exploration on Universal Decompositional Semantic Parsing:\n  Architecture, Data Augmentation, and LLM Paradigm\u2b1b  In this paper, we conduct a holistic exploration of the Universal\nDecompositional Semantic (UDS) Parsing. We first introduce a cascade model for\nUDS parsing that decomposes the complex parsing task into semantically\nappropriate subtasks. Our approach outperforms the prior models, while\nsignificantly reducing inference time. We also incorporate syntactic\ninformation and further optimized the architecture. Besides, different ways for\ndata augmentation are explored, which further improve the UDS Parsing. Lastly,\nwe conduct experiments to investigate the efficacy of ChatGPT in handling the\nUDS task, revealing that it excels in attribute parsing but struggles in\nrelation parsing, and using ChatGPT for data augmentation yields suboptimal\nresults. Our code is available at https://github.com/hexuandeng/HExp4UDS.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical\n  Error Correction with Supervised Fine-Tuning\u2b1b  Grammatical error correction aims to correct ungrammatical sentences\nautomatically. Recently, some work has demonstrated the excellent capabilities\nof closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical\nerror correction. However, the potential of open-source LLMs remains\nunexplored. In this paper, we introduced GrammarGPT, an open-source LLM, to\npreliminary explore its potential for native Chinese grammatical error\ncorrection. The core recipe of GrammarGPT is to leverage the hybrid dataset of\nChatGPT-generated and human-annotated. For grammatical errors with clues, we\nproposed a heuristic method to guide ChatGPT to generate ungrammatical\nsentences by providing those clues. For grammatical errors without clues, we\ncollected ungrammatical sentences from publicly available websites and manually\ncorrected them. In addition, we employed an error-invariant augmentation method\nto enhance the ability of the model to correct native Chinese grammatical\nerrors. We ultimately constructed about 1k parallel data and utilized these\ndata to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese\nUniversity of Hong Kong, Shenzhen) with instruction tuning. The experimental\nresults show that GrammarGPT outperforms the existing SOTA system\nsignificantly. Although model parameters are 20x larger than the SOTA baseline,\nthe required amount of data for instruction tuning is 1200x smaller,\nillustrating the potential of open-source LLMs on native CGEC. Our GrammarGPT\nranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's\neffectiveness. The code and data are available at\n\\url{https://github.com/FreedomIntelligence/GrammarGPT}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Evaluating the Moral Beliefs Encoded in LLMs\u2b1b  This paper presents a case study on the design, administration,\npost-processing, and evaluation of surveys on large language models (LLMs). It\ncomprises two components: (1) A statistical method for eliciting beliefs\nencoded in LLMs. We introduce statistical measures and evaluation metrics that\nquantify the probability of an LLM \"making a choice\", the associated\nuncertainty, and the consistency of that choice. (2) We apply this method to\nstudy what moral beliefs are encoded in different LLMs, especially in ambiguous\ncases where the right choice is not obvious. We design a large-scale survey\ncomprising 680 high-ambiguity moral scenarios (e.g., \"Should I tell a white\nlie?\") and 687 low-ambiguity moral scenarios (e.g., \"Should I stop for a\npedestrian on the road?\"). Each scenario includes a description, two possible\nactions, and auxiliary labels indicating violated rules (e.g., \"do not kill\").\nWe administer the survey to 28 open- and closed-source LLMs. We find that (a)\nin unambiguous scenarios, most models \"choose\" actions that align with\ncommonsense. In ambiguous cases, most models express uncertainty. (b) Some\nmodels are uncertain about choosing the commonsense action because their\nresponses are sensitive to the question-wording. (c) Some models reflect clear\npreferences in ambiguous scenarios. Specifically, closed-source models tend to\nagree with each other.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Mental-LLM: Leveraging Large Language Models for Mental Health\n  Prediction via Online Text Data\u2b1b  Advances in large language models (LLMs) have empowered a variety of\napplications. However, there is still a significant gap in research when it\ncomes to understanding and enhancing the capabilities of LLMs in the field of\nmental health. In this work, we present the first comprehensive evaluation of\nmultiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, on\nvarious mental health prediction tasks via online text data. We conduct a broad\nrange of experiments, covering zero-shot prompting, few-shot prompting, and\ninstruction fine-tuning. The results indicate a promising yet limited\nperformance of LLMs with zero-shot and few-shot prompt designs for the mental\nhealth tasks. More importantly, our experiments show that instruction\nfinetuning can significantly boost the performance of LLMs for all tasks\nsimultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5,\noutperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9%\non balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%.\nThey further perform on par with the state-of-the-art task-specific language\nmodel. We also conduct an exploratory case study on LLMs' capability on the\nmental health reasoning tasks, illustrating the promising capability of certain\nmodels such as GPT-4. We summarize our findings into a set of action guidelines\nfor potential methods to enhance LLMs' capability for mental health tasks.\nMeanwhile, we also emphasize the important limitations before achieving\ndeployability in real-world mental health settings, such as known racial and\ngender bias. We highlight the important ethical risks accompanying this line of\nresearch.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CHATREPORT: Democratizing Sustainability Disclosure Analysis through\n  LLM-based Tools\u2b1b  In the face of climate change, are companies really taking substantial steps\ntoward more sustainable operations? A comprehensive answer lies in the dense,\ninformation-rich landscape of corporate sustainability reports. However, the\nsheer volume and complexity of these reports make human analysis very costly.\nTherefore, only a few entities worldwide have the resources to analyze these\nreports at scale, which leads to a lack of transparency in sustainability\nreporting. Empowering stakeholders with LLM-based automatic analysis tools can\nbe a promising way to democratize sustainability report analysis. However,\ndeveloping such tools is challenging due to (1) the hallucination of LLMs and\n(2) the inefficiency of bringing domain experts into the AI development loop.\nIn this paper, we ChatReport, a novel LLM-based system to automate the analysis\nof corporate sustainability reports, addressing existing challenges by (1)\nmaking the answers traceable to reduce the harm of hallucination and (2)\nactively involving domain experts in the development loop. We make our\nmethodology, annotated datasets, and generated analyses of 1015 reports\npublicly available.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Rec: Personalized Recommendation via Prompting Large Language Models\u2b1b  We investigate various prompting strategies for enhancing personalized\nrecommendation performance with large language models (LLMs) through input\naugmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct\nprompting strategies: (1) basic prompting, (2) recommendation-driven prompting,\n(3) engagement-guided prompting, and (4) recommendation-driven +\nengagement-guided prompting. Our empirical experiments show that incorporating\nthe augmented input text generated by LLM leads to improved recommendation\nperformance. Recommendation-driven and engagement-guided prompting strategies\nare found to elicit LLM's understanding of global and local item\ncharacteristics. This finding highlights the importance of leveraging diverse\nprompts and input augmentation techniques to enhance the recommendation\ncapabilities with LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards Codable Watermarking for Injecting Multi-bit Information to LLM\u2b1b  As large language models (LLMs) generate texts with increasing fluency and\nrealism, there is a growing need to identify the source of texts to prevent the\nabuse of LLMs. Text watermarking techniques have proven reliable in\ndistinguishing whether a text is generated by LLMs by injecting hidden patterns\ninto the generated texts. However, we argue that existing watermarking methods\nfor LLMs are encoding-inefficient (only contain one bit of information -\nwhether it is generated from an LLM or not) and cannot flexibly meet the\ndiverse information encoding needs (such as encoding model version, generation\ntime, user id, etc.) in different LLMs application scenarios. In this work, we\nconduct the first systematic study on the topic of Codable Text Watermarking\nfor LLMs (CTWL) that allows text watermarks to carry more customizable\ninformation. First of all, we study the taxonomy of LLM watermarking technology\nand give a mathematical formulation for CTWL. Additionally, we provide a\ncomprehensive evaluation system for CTWL: (1) watermarking success rate, (2)\nrobustness against various corruptions, (3) coding rate of payload information,\n(4) encoding and decoding efficiency, (5) impacts on the quality of the\ngenerated text. To meet the requirements of these non-Pareto-improving metrics,\nwe devise a CTWL method named Balance-Marking, based on the motivation of\nensuring that available and unavailable vocabularies for encoding information\nhave approximately equivalent probabilities. Compared to the random vocabulary\npartitioning extended from the existing work, a probability-balanced vocabulary\npartition can significantly improve the quality of the generated text.\nExtensive experimental results have shown that our method outperforms a direct\nbaseline under comprehensive evaluation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension\u2b1b  Based on powerful Large Language Models (LLMs), recent generative Multimodal\nLarge Language Models (MLLMs) have gained prominence as a pivotal research\narea, exhibiting remarkable capability for both comprehension and generation.\nIn this work, we address the evaluation of generative comprehension in MLLMs as\na preliminary step towards a comprehensive assessment of generative models, by\nintroducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple\nchoice questions with accurate human annotations (x 6 larger than existing\nbenchmarks), which spans 12 evaluation dimensions including the comprehension\nof both the image and video modality. We develop an advanced pipeline for\ngenerating multiple-choice questions that target specific evaluation\ndimensions, integrating both automatic filtering and manual verification\nprocesses. Multiple-choice questions with groundtruth options derived from\nhuman annotation enables an objective and efficient assessment of model\nperformance, eliminating the need for human or GPT intervention during\nevaluation. We further evaluate the performance of 18 models across all 12\ndimensions, covering both the spatial and temporal understanding. By revealing\nthe limitations of existing MLLMs through evaluation results, we aim for\nSEED-Bench to provide insights for motivating future research. We will launch\nand consistently maintain a leaderboard to provide a platform for the community\nto assess and investigate model capability.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing\n  Evaluation for Large Language Models\u2b1b  The field of large language models (LLMs) has made significant progress, and\ntheir knowledge storage capacity is approaching that of human beings.\nFurthermore, advanced techniques, such as prompt learning and reinforcement\nlearning, are being employed to address ethical concerns and hallucination\nproblems associated with LLMs, bringing them closer to aligning with human\nvalues. This situation naturally raises the question of whether LLMs with\nhuman-like abilities possess a human-like personality? In this paper, we aim to\ninvestigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a\nwidespread human personality assessment tool, as an evaluation metric for LLMs.\nSpecifically, extensive experiments will be conducted to explore: 1) the\npersonality types of different LLMs, 2) the possibility of changing the\npersonality types by prompt engineering, and 3) How does the training dataset\naffect the model's personality. Although the MBTI is not a rigorous assessment,\nit can still reflect the similarity between LLMs and human personality. In\npractice, the MBTI has the potential to serve as a rough indicator. Our codes\nare available at\nhttps://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs4OL: Large Language Models for Ontology Learning\u2b1b  We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs)\nfor Ontology Learning (OL). LLMs have shown significant advancements in natural\nlanguage processing, demonstrating their ability to capture complex language\npatterns in different knowledge domains. Our LLMs4OL paradigm investigates the\nfollowing hypothesis: \\textit{Can LLMs effectively apply their language pattern\ncapturing capability to OL, which involves automatically extracting and\nstructuring knowledge from natural language text?} To test this hypothesis, we\nconduct a comprehensive evaluation using the zero-shot prompting method. We\nevaluate nine different LLM model families for three main OL tasks: term\ntyping, taxonomy discovery, and extraction of non-taxonomic relations.\nAdditionally, the evaluations encompass diverse genres of ontological\nknowledge, including lexicosemantic knowledge in WordNet, geographical\nknowledge in GeoNames, and medical knowledge in UMLS.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMediator: GPT-4 Assisted Online Dispute Resolution\u2b1b  In this article, we introduce LLMediator, an experimental platform designed\nto enhance online dispute resolution (ODR) by utilizing capabilities of\nstate-of-the-art large language models (LLMs) such as GPT-4. In the context of\nhigh-volume, low-intensity legal disputes, alternative dispute resolution\nmethods such as negotiation and mediation offer accessible and cooperative\nsolutions for laypeople. These approaches can be carried out online on ODR\nplatforms. LLMediator aims to improve the efficacy of such processes by\nleveraging GPT-4 to reformulate user messages, draft mediator responses, and\npotentially autonomously engage in the discussions. We present and discuss\nseveral features of LLMediator and conduct initial qualitative evaluations,\ndemonstrating the potential for LLMs to support ODR and facilitate amicable\nsettlements. The initial proof of concept is promising and opens up avenues for\nfurther research in AI-assisted negotiation and mediation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world\n  APIs\u2b1b  Despite the advancements of open-source large language models (LLMs), e.g.,\nLLaMA, they remain significantly limited in tool-use capabilities, i.e., using\nexternal tools (APIs) to fulfill human instructions. The reason is that current\ninstruction tuning largely focuses on basic language tasks but ignores the\ntool-use domain. This is in contrast to the excellent tool-use capabilities of\nstate-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap,\nwe introduce ToolLLM, a general tool-use framework encompassing data\nconstruction, model training, and evaluation. We first present ToolBench, an\ninstruction-tuning dataset for tool use, which is constructed automatically\nusing ChatGPT. Specifically, the construction can be divided into three stages:\n(i) API collection: we collect 16,464 real-world RESTful APIs spanning 49\ncategories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to\ngenerate diverse instructions involving these APIs, covering both single-tool\nand multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to\nsearch for a valid solution path (chain of API calls) for each instruction. To\nenhance the reasoning capabilities of LLMs, we develop a novel depth-first\nsearch-based decision tree algorithm. It enables LLMs to evaluate multiple\nreasoning traces and expand the search space. Moreover, to evaluate the\ntool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.\nBased on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it\nwith a neural API retriever to recommend appropriate APIs for each instruction.\nExperiments show that ToolLLaMA demonstrates a remarkable ability to execute\ncomplex instructions and generalize to unseen APIs, and exhibits comparable\nperformance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot\ngeneralization ability in an out-of-distribution tool-use dataset: APIBench.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Comparative Analysis of Drug-GPT and ChatGPT LLMs for Healthcare\n  Insights: Evaluating Accuracy and Relevance in Patient and HCP Contexts\u2b1b  This study presents a comparative analysis of three Generative Pre-trained\nTransformer (GPT) solutions in a question and answer (Q&A) setting: Drug-GPT 3,\nDrug-GPT 4, and ChatGPT, in the context of healthcare applications. The\nobjective is to determine which model delivers the most accurate and relevant\ninformation in response to prompts related to patient experiences with atopic\ndermatitis (AD) and healthcare professional (HCP) discussions about diabetes.\nThe results demonstrate that while all three models are capable of generating\nrelevant and accurate responses, Drug-GPT 3 and Drug-GPT 4, which are supported\nby curated datasets of patient and HCP social media and message board posts,\nprovide more targeted and in-depth insights. ChatGPT, a more general-purpose\nmodel, generates broader and more general responses, which may be valuable for\nreaders seeking a high-level understanding of the topics but may lack the depth\nand personal insights found in the answers generated by the specialized\nDrug-GPT models. This comparative analysis highlights the importance of\nconsidering the language model's perspective, depth of knowledge, and currency\nwhen evaluating the usefulness of generated information in healthcare\napplications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "HAGRID: A Human-LLM Collaborative Dataset for Generative\n  Information-Seeking with Attribution\u2b1b  The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Predictive Data Analytics with AI: assessing the need for post-editing\n  of MT output by fine-tuning OpenAI LLMs\u2b1b  Translation Quality Evaluation (TQE) is an essential step of the modern\ntranslation production process. TQE is critical in assessing both machine\ntranslation (MT) and human translation (HT) quality without reference\ntranslations. The ability to evaluate or even simply estimate the quality of\ntranslation automatically may open significant efficiency gains through process\noptimisation. This work examines whether the state-of-the-art large language\nmodels (LLMs) can be used for this purpose. We take OpenAI models as the best\nstate-of-the-art technology and approach TQE as a binary classification task.\nOn eight language pairs including English to Italian, German, French, Japanese,\nDutch, Portuguese, Turkish, and Chinese, our experimental results show that\nfine-tuned gpt3.5 can demonstrate good performance on translation quality\nprediction tasks, i.e. whether the translation needs to be edited. Another\nfinding is that simply increasing the sizes of LLMs does not lead to apparent\nbetter performances on this task by comparing the performance of three\ndifferent versions of OpenAI models: curie, davinci, and gpt3.5 with 13B, 175B,\nand 175B parameters, respectively.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step\n  Reasoning\u2b1b  The recent progress in large language models (LLMs), especially the invention\nof chain-of-thought prompting, has made it possible to automatically answer\nquestions by stepwise reasoning. However, when faced with more complicated\nproblems that require non-linear thinking, even the strongest LLMs make\nmistakes. To address this, we explore whether LLMs are able to recognize errors\nin their own step-by-step reasoning, without resorting to external resources.\nTo this end, we propose SelfCheck, a general-purpose zero-shot verification\nschema for recognizing such errors. We then use the results of these checks to\nimprove question-answering performance by conducting weighted voting on\nmultiple solutions to the question. We test SelfCheck on three datasets (GSM8K,\nMathQA, and MATH) and find that it successfully recognizes errors and, in turn,\nincreases final answer accuracies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on\n  Class-level Code Generation\u2b1b  In this work, we make the first attempt to evaluate LLMs in a more\nchallenging code generation scenario, i.e. class-level code generation. We\nfirst manually construct the first class-level code generation benchmark\nClassEval of 100 class-level Python code generation tasks with approximately\n500 person-hours. Based on it, we then perform the first study of 11\nstate-of-the-art LLMs on class-level code generation. Based on our results, we\nhave the following main findings. First, we find that all existing LLMs show\nmuch worse performance on class-level code generation compared to on standalone\nmethod-level code generation benchmarks like HumanEval; and the method-level\ncoding ability cannot equivalently reflect the class-level coding ability among\nLLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior\nthan other LLMs on class-level code generation, and the second-tier models\nincludes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very\nsimilar performance. Third, we find that generating the entire class all at\nonce (i.e. holistic generation strategy) is the best generation strategy only\nfor GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and\ncompositional) is better strategies for the other models with limited ability\nof understanding long instructions and utilizing the middle information.\nLastly, we find the limited model ability of generating method-dependent code\nand discuss the frequent error types in generated classes. Our benchmark is\navailable at https://github.com/FudanSELab/ClassEval.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Wider and Deeper LLM Networks are Fairer LLM Evaluators\u2b1b  Measuring the quality of responses generated by LLMs is a challenging task,\nparticularly when it comes to evaluating whether the response is aligned with\nhuman preference. A novel approach involves using the LLM itself to make\nevaluation and stabilizing the results through multiple independent\nevaluations, similar to a single-layer narrow LLM network. This network\nconsists of a fixed number of neurons, with each neuron being the same LLM. In\nthis paper, we draw upon the extensive research on deep neural networks to\nexplore whether deeper and wider networks can lead to fairer evaluations.\nSpecifically, inspired by the observation that different neurons in a neural\nnetwork are responsible for detecting different concepts, we first adaptively\ngenerate as many neuron roles as possible for each evaluation sample. Each\nperspective corresponds to the role of a specific LLM neuron in the first\nlayer. In subsequent layers, we follow the idea that higher layers in deep\nnetworks are responsible for more comprehensive features, each layer receives\nrepresentations from all neurons in the previous layer, integrating the locally\nlearned evaluation information to obtain a more comprehensive evaluation\nresult. Interestingly, this network design resembles the process of academic\npaper reviewing. To validate the effectiveness of our method, we construct the\nlargest and most diverse English evaluation benchmark LLMEval$^2$ for LLM\nevaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental\nresults demonstrate that a wider network (involving many reviewers) with 2\nlayers (one round of discussion) performs the best, improving kappa correlation\ncoefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the\nassessment of Chinese LLMs, which has accelerated the evaluation time by 4.6\ntimes, resulting in a 60% cost saving. WideDeep achieves a remarkable 93%\nagreement level among humans.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval\u2b1b  The General Transit Feed Specification (GTFS) standard for publishing transit\ndata is ubiquitous. GTFS being tabular data, with information spread across\ndifferent files, necessitates specialized tools or packages to retrieve\ninformation. Concurrently, the use of Large Language Models(LLMs) for text and\ninformation retrieval is growing. The idea of this research is to see if the\ncurrent widely adopted LLMs (ChatGPT) are able to understand GTFS and retrieve\ninformation from GTFS using natural language instructions without explicitly\nproviding information. In this research, we benchmark OpenAI's GPT-3.5-Turbo\nand GPT-4 LLMs which are the backbone of ChatGPT. ChatGPT demonstrates a\nreasonable understanding of GTFS by answering 59.7% (GPT-3.5-Turbo) and 73.3%\n(GPT-4) of our multiple-choice questions (MCQ) correctly. Furthermore, we\nevaluated the LLMs on information extraction tasks using a filtered GTFS feed\ncontaining four routes. We found that program synthesis techniques outperformed\nzero-shot approaches, achieving up to 93% (90%) accuracy for simple queries and\n61% (41%) for complex ones using GPT-4 (GPT-3.5-Turbo).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench\u2b1b  Recently, the community has witnessed the advancement of Large Language\nModels (LLMs), which have shown remarkable performance on various downstream\ntasks. Led by powerful models like ChatGPT and Claude, LLMs are revolutionizing\nhow users engage with software, assuming more than mere tools but intelligent\nassistants. Consequently, evaluating LLMs' anthropomorphic capabilities becomes\nincreasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, i.e., how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes five LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4 and LLaMA 2. A conclusion can be\ndrawn from the results that, despite several misalignments, LLMs can generally\nrespond appropriately to certain situations. Nevertheless, they fall short in\nalignment with the emotional behaviors of human beings and cannot establish\nconnections between similar situations. Our collected dataset of situations,\nthe human evaluation results, and the code of our testing framework, dubbed\nEmotionBench, is made publicly in https://github.com/CUHK-ARISE/EmotionBench.\nWe aspire to contribute to the advancement of LLMs regarding better alignment\nwith the emotional behaviors of human beings, thereby enhancing their utility\nand applicability as intelligent assistants.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AgentBench: Evaluating LLMs as Agents\u2b1b  Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent's reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 27 API-based and\nopen-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and OSS competitors. We identify the\ntypical reasons of failures in environments and LLMs, showing that poor\nlong-term reasoning, decision-making, and instruction following abilities are\nthe main obstacles for developing usable LLM agents. Training on code and high\nquality multi-turn alignment data could improve agent performance. Datasets,\nenvironments, and an integrated evaluation package for AgentBench are released\nat \\url{https://github.com/THUDM/AgentBench}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Legal Summarisation through LLMs: The PRODIGIT Project\u2b1b  We present some initial results of a large-scale Italian project called\nPRODIGIT which aims to support tax judges and lawyers through digital\ntechnology, focusing on AI. We have focused on generation of summaries of\njudicial decisions and on the extraction of related information, such as the\nidentification of legal issues and decision-making criteria, and the\nspecification of keywords. To this end, we have deployed and evaluated\ndifferent tools and approaches to extractive and abstractive summarisation. We\nhave applied LLMs, and particularly on GPT4, which has enabled us to obtain\nresults that proved satisfactory, according to an evaluation by expert tax\njudges and lawyers. On this basis, a prototype application is being built which\nwill be made publicly available.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Accelerating LLM Inference with Staged Speculative Decoding\u2b1b  Recent advances with large language models (LLM) illustrate their diverse\ncapabilities. We propose a novel algorithm, staged speculative decoding, to\naccelerate LLM inference in small-batch, on-device scenarios. We address the\nlow arithmetic intensity of small-batch inference by improving upon previous\nwork in speculative decoding. First, we restructure the speculative batch as a\ntree, which reduces generation costs and increases the expected tokens per\nbatch. Second, we add a second stage of speculative decoding. Taken together,\nwe reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L\nmodel while perfectly preserving output quality.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Benchmarking LLM powered Chatbots: Methods and Metrics\u2b1b  Autonomous conversational agents, i.e. chatbots, are becoming an increasingly\ncommon mechanism for enterprises to provide support to customers and partners.\nIn order to rate chatbots, especially ones powered by Generative AI tools like\nLarge Language Models (LLMs) we need to be able to accurately assess their\nperformance. This is where chatbot benchmarking becomes important. In this\npaper, we propose the use of a novel benchmark that we call the E2E (End to\nEnd) benchmark, and show how the E2E benchmark can be used to evaluate accuracy\nand usefulness of the answers provided by chatbots, especially ones powered by\nLLMs. We evaluate an example chatbot at different levels of sophistication\nbased on both our E2E benchmark, as well as other available metrics commonly\nused in the state of art, and observe that the proposed benchmark show better\nresults compared to others. In addition, while some metrics proved to be\nunpredictable, the metric associated with the E2E benchmark, which uses cosine\nsimilarity performed well in evaluating chatbots. The performance of our best\nmodels shows that there are several benefits of using the cosine similarity\nscore as a metric in the E2E benchmark.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking\u2b1b  The recent development and success of Large Language Models (LLMs)\nnecessitate an evaluation of their performance across diverse NLP tasks in\ndifferent languages. Although several frameworks have been developed and made\npublicly available, their customization capabilities for specific tasks and\ndatasets are often complex for different users. In this study, we introduce the\nLLMeBench framework. Initially developed to evaluate Arabic NLP tasks using\nOpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task\nand model, regardless of language. The framework also features zero- and\nfew-shot learning settings. A new custom dataset can be added in less than 10\nminutes, and users can use their own model API keys to evaluate the task at\nhand. The developed framework has been already tested on 31 unique NLP tasks\nusing 53 publicly available datasets within 90 experimental setups, involving\napproximately 296K data points. We plan to open-source the framework for the\ncommunity (https://github.com/qcri/LLMeBench/). A video demonstrating the\nframework is available online (https://youtu.be/FkQn4UjYA0s).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM As DBA\u2b1b  Database administrators (DBAs) play a crucial role in managing, maintaining\nand optimizing a database system to ensure data availability, performance, and\nreliability. However, it is hard and tedious for DBAs to manage a large number\nof database instances (e.g., millions of instances on the cloud databases).\nRecently large language models (LLMs) have shown great potential to understand\nvaluable documents and accordingly generate reasonable answers. Thus, we\npropose D-Bot, a LLM-based database administrator that can continuously acquire\ndatabase maintenance experience from textual sources, and provide reasonable,\nwell-founded, in-time diagnosis and optimization advice for target databases.\nThis paper presents a revolutionary LLM-centric framework for database\nmaintenance, including (i) database maintenance knowledge detection from\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\nexperimental results that D-Bot can efficiently and effectively diagnose the\nroot causes and our code is available at\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Dynamic Planning with a LLM\u2b1b  While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher\u2b1b  Safety lies at the core of the development of Large Language Models (LLMs).\nThere is ample work on aligning LLMs with human ethics and preferences,\nincluding data filtering in pretraining, supervised fine-tuning, reinforcement\nlearning from human feedback, and red teaming, etc. In this study, we discover\nthat chat in cipher can bypass the safety alignment techniques of LLMs, which\nare mainly conducted in natural languages. We propose a novel framework\nCipherChat to systematically examine the generalizability of safety alignment\nto non-natural languages -- ciphers. CipherChat enables humans to chat with\nLLMs through cipher prompts topped with system role descriptions and few-shot\nenciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\nincluding ChatGPT and GPT-4 for different representative human ciphers across\n11 safety domains in both English and Chinese. Experimental results show that\ncertain ciphers succeed almost 100% of the time to bypass the safety alignment\nof GPT-4 in several safety domains, demonstrating the necessity of developing\nsafety alignment for non-natural languages. Notably, we identify that LLMs seem\nto have a ''secret cipher'', and propose a novel SelfCipher that uses only role\nplay and several demonstrations in natural language to evoke this capability.\nSelfCipher surprisingly outperforms existing human ciphers in almost all cases.\nOur code and data will be released at https://github.com/RobustNLP/CipherChat.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Mind your Language (Model): Fact-Checking LLMs and their Role in NLP\n  Research and Practice\u2b1b  Much of the recent discourse within the NLP research community has been\ncentered around Large Language Models (LLMs), their functionality and potential\n-- yet not only do we not have a working definition of LLMs, but much of this\ndiscourse relies on claims and assumptions that are worth re-examining. This\nposition paper contributes a definition of LLMs, explicates some of the\nassumptions made regarding their functionality, and outlines the existing\nevidence for and against them. We conclude with suggestions for research\ndirections and their framing in future work.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\u2b1b  Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked\u2b1b  Large language models (LLMs) are popular for high-quality text generation but\ncan produce harmful content, even when aligned with human values through\nreinforcement learning. Adversarial prompts can bypass their safety measures.\nWe propose LLM Self Defense, a simple approach to defend against these attacks\nby having an LLM screen the induced responses. Our method does not require any\nfine-tuning, input preprocessing, or iterative output generation. Instead, we\nincorporate the generated content into a pre-defined prompt and employ another\ninstance of an LLM to analyze the text and predict whether it is harmful. We\ntest LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent\nLLMs against various types of attacks, such as forcefully inducing affirmative\nresponses to prompts and prompt engineering attacks. Notably, LLM Self Defense\nsucceeds in reducing the attack success rate to virtually 0 using both GPT 3.5\nand Llama 2.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs\u2b1b  We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large\nLanguage Models (LLMs) that achieves the strongest performance and currently\nstands at first place in HuggingFace's Open LLM Leaderboard as of the release\ndate of this work. In this work we describe (1) our curated dataset\n$\\textbf{Open-Platypus}$, that is a subset of other open datasets and which\n$\\textit{we release to the public}$ (2) our process of fine-tuning and merging\nLoRA modules in order to conserve the strong prior of pretrained LLMs, while\nbringing specific domain knowledge to the surface (3) our efforts in checking\nfor test data leaks and contamination in the training data, which can inform\nfuture research. Specifically, the Platypus family achieves strong performance\nin quantitative LLM metrics across model sizes, topping the global Open LLM\nleaderboard while using just a fraction of the fine-tuning data and overall\ncompute that are required for other state-of-the-art fine-tuned LLMs. In\nparticular, a 13B Platypus model can be trained on $\\textit{a single}$ A100 GPU\nusing 25k questions in 5 hours. This is a testament of the quality of our\nOpen-Platypus dataset, and opens opportunities for more improvements in the\nfield. Project page: https://platypus-llm.github.io\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CALYPSO: LLMs as Dungeon Masters' Assistants\u2b1b  The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to\nperform multiple tasks simultaneously. The DM must digest information about the\ngame setting and monsters, synthesize scenes to present to other players, and\nrespond to the players' interactions with the scene. Doing all of these tasks\nwhile maintaining consistency within the narrative and story world is no small\nfeat of human cognition, making the task tiring and unapproachable to new\nplayers. Large language models (LLMs) like GPT-3 and ChatGPT have shown\nremarkable abilities to generate coherent natural language text. In this paper,\nwe conduct a formative evaluation with DMs to establish the use cases of LLMs\nin D&D and tabletop gaming generally. We introduce CALYPSO, a system of\nLLM-powered interfaces that support DMs with information and inspiration\nspecific to their own scenario. CALYPSO distills game context into bite-sized\nprose and helps brainstorm ideas without distracting the DM from the game. When\ngiven access to CALYPSO, DMs reported that it generated high-fidelity text\nsuitable for direct presentation to players, and low-fidelity ideas that the DM\ncould develop further while maintaining their creative agency. We see CALYPSO\nas exemplifying a paradigm of AI-augmented tools that provide synchronous\ncreative assistance within established game worlds, and tabletop gaming more\nbroadly.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Mini-CEX: Automatic Evaluation of Large Language Model for\n  Diagnostic Conversation\u2b1b  There is an increasing interest in developing LLMs for medical diagnosis to\nimprove diagnosis efficiency. Despite their alluring technological potential,\nthere is no unified and comprehensive evaluation criterion, leading to the\ninability to evaluate the quality and potential risks of medical LLMs, further\nhindering the application of LLMs in medical treatment scenarios. Besides,\ncurrent evaluations heavily rely on labor-intensive interactions with LLMs to\nobtain diagnostic dialogues and human evaluation on the quality of diagnosis\ndialogue. To tackle the lack of unified and comprehensive evaluation criterion,\nwe first initially establish an evaluation criterion, termed LLM-specific\nMini-CEX to assess the diagnostic capabilities of LLMs effectively, based on\noriginal Mini-CEX. To address the labor-intensive interaction problem, we\ndevelop a patient simulator to engage in automatic conversations with LLMs, and\nutilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental\nresults show that the LLM-specific Mini-CEX is adequate and necessary to\nevaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual\nevaluation on the metrics of humanistic qualities and provides reproducible and\nautomated comparisons between different LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Link-Context Learning for Multimodal LLMs\u2b1b  The ability to learn from context with novel concepts, and deliver\nappropriate responses are essential in human conversations. Despite current\nMultimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being\ntrained on mega-scale datasets, recognizing unseen images or understanding\nnovel concepts in a training-free manner remains a challenge. In-Context\nLearning (ICL) explores training-free few-shot learning, where models are\nencouraged to ``learn to learn\" from limited tasks and generalize to unseen\ntasks. In this work, we propose link-context learning (LCL), which emphasizes\n\"reasoning from cause and effect\" to augment the learning capabilities of\nMLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal\nrelationship between the support set and the query set. By providing\ndemonstrations with causal links, LCL guides the model to discern not only the\nanalogy but also the underlying causal associations between data points, which\nempowers MLLMs to recognize unseen images and understand novel concepts more\neffectively. To facilitate the evaluation of this novel approach, we introduce\nthe ISEKAI dataset, comprising exclusively of unseen generated image-label\npairs designed for link-context learning. Extensive experiments show that our\nLCL-MLLM exhibits strong link-context learning capabilities to novel concepts\nover vanilla MLLMs. Code and data will be released at\nhttps://github.com/isekai-portal/Link-Context-Learning.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Teach LLMs to Personalize -- An Approach inspired by Writing Education\u2b1b  Personalized text generation is an emerging research area that has attracted\nmuch attention in recent years. Most studies in this direction focus on a\nparticular domain by designing bespoke features or models. In this work, we\npropose a general approach for personalized text generation using large\nlanguage models (LLMs). Inspired by the practice of writing education, we\ndevelop a multistage and multitask framework to teach LLMs for personalized\ngeneration. In writing instruction, the task of writing from sources is often\ndecomposed into multiple steps that involve finding, evaluating, summarizing,\nsynthesizing, and integrating information. Analogously, our approach to\npersonalized text generation consists of multiple stages: retrieval, ranking,\nsummarization, synthesis, and generation. In addition, we introduce a multitask\nsetting that helps the model improve its generation ability further, which is\ninspired by the observation in education that a student's reading proficiency\nand writing ability are often correlated. We evaluate our approach on three\npublic datasets, each of which covers a different and representative domain.\nOur results show significant improvements over a variety of baselines.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DiagGPT: An LLM-based Chatbot with Automatic Topic Management for\n  Task-Oriented Dialogue\u2b1b  Large Language Models (LLMs), such as ChatGPT, are becoming increasingly\nsophisticated, demonstrating capabilities that closely resemble those of\nhumans. These AI models are playing an essential role in assisting humans with\na wide array of tasks in daily life. A significant application of AI is its use\nas a chat agent, responding to human inquiries across various domains. Current\nLLMs have shown proficiency in answering general questions. However, basic\nquestion-answering dialogue often falls short in complex diagnostic scenarios,\nsuch as legal or medical consultations. These scenarios typically necessitate\nTask-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively\npose questions and guide users towards specific task completion. Previous\nfine-tuning models have underperformed in TOD, and current LLMs do not\ninherently possess this capability. In this paper, we introduce DiagGPT\n(Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD\nscenarios. Our experiments reveal that DiagGPT exhibits outstanding performance\nin conducting TOD with users, demonstrating its potential for practical\napplications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\u2b1b  AutoGen is an open-source framework that allows developers to build LLM\napplications via multiple agents that can converse with each other to\naccomplish tasks. AutoGen agents are customizable, conversable, and can operate\nin various modes that employ combinations of LLMs, human inputs, and tools.\nUsing AutoGen, developers can also flexibly define agent interaction behaviors.\nBoth natural language and computer code can be used to program flexible\nconversation patterns for different applications. AutoGen serves as a generic\ninfrastructure to build diverse applications of various complexities and LLM\ncapacities. Empirical studies demonstrate the effectiveness of the framework in\nmany example applications, with domains ranging from mathematics, coding,\nquestion answering, operations research, online decision-making, entertainment,\netc.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain\n  Conversation\u2b1b  We propose MemoChat, a pipeline for refining instructions that enables large\nlanguage models (LLMs) to effectively employ self-composed memos for\nmaintaining consistent long-range open-domain conversations. We demonstrate a\nlong-range open-domain conversation through iterative\n\"memorization-retrieval-response\" cycles. This requires us to carefully design\ntailored tuning instructions for each distinct stage. The instructions are\nreconstructed from a collection of public datasets to teach the LLMs to\nmemorize and retrieve past dialogues with structured memos, leading to enhanced\nconsistency when participating in future conversations. We invite experts to\nmanually annotate a test set designed to evaluate the consistency of long-range\nconversations questions. Experiments on three testing scenarios involving both\nopen-source and API-accessible chatbots at scale verify the efficacy of\nMemoChat, which outperforms strong baselines. Our codes, data and models are\navailable here: https://github.com/LuJunru/MemoChat.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for\n  Time Series\u2b1b  This work summarizes two strategies for completing time-series (TS) tasks\nusing today's language model (LLM): LLM-for-TS, design and train a fundamental\nlarge model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS\ndata. Considering the insufficient data accumulation, limited resources, and\nsemantic context requirements, this work focuses on TS-for-LLM methods, where\nwe aim to activate LLM's ability for TS data by designing a TS embedding method\nsuitable for LLM. The proposed method is named TEST. It first tokenizes TS,\nbuilds an encoder to embed them by instance-wise, feature-wise, and\ntext-prototype-aligned contrast, and then creates prompts to make LLM more open\nto embeddings, and finally implements TS tasks. Experiments are carried out on\nTS classification and forecasting tasks using 8 LLMs with different structures\nand sizes. Although its results cannot significantly outperform the current\nSOTA models customized for TS tasks, by treating LLM as the pattern machine, it\ncan endow LLM's ability to process TS data without compromising the language\nability. This paper is intended to serve as a foundational work that will\ninspire further research.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models\u2b1b  Data contamination, i.e., the presence of test data from downstream tasks in\nthe training data of large language models (LLMs), is a potential major issue\nin measuring LLMs' real effectiveness on other tasks. We propose a\nstraightforward yet effective method for identifying data contamination within\nLLMs. At its core, our approach starts by identifying potential contamination\nat the instance level; using this information, our approach then assesses wider\ncontamination at the partition level. To estimate contamination of individual\ninstances, we employ \"guided instruction:\" a prompt consisting of the dataset\nname, partition type, and the random-length initial segment of a reference\ninstance, asking the LLM to complete it. An instance is flagged as contaminated\nif the LLM's output either exactly or nearly matches the latter segment of the\nreference. To understand if an entire partition is contaminated, we propose two\nideas. The first idea marks a dataset partition as contaminated if the average\noverlap score with the reference instances (as measured by ROUGE-L or BLEURT)\nis statistically significantly better with the completions from guided\ninstruction compared to a \"general instruction\" that does not include the\ndataset and partition name. The second idea marks a dataset partition as\ncontaminated if a classifier based on GPT-4 with few-shot in-context learning\nprompt marks multiple generated completions as exact/near-exact matches of the\ncorresponding reference instances. Our best method achieves an accuracy between\n92% and 100% in detecting if an LLM is contaminated with seven datasets,\ncontaining train and test/validation partitions, when contrasted with manual\nevaluation by human experts. Further, our findings indicate that GPT-4 is\ncontaminated with AG News, WNLI, and XSum datasets.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-FuncMapper: Function Identification for Interpreting Complex Clauses\n  in Building Codes via LLM\u2b1b  As a vital stage of automated rule checking (ARC), rule interpretation of\nregulatory texts requires considerable effort. However, interpreting regulatory\nclauses with implicit properties or complex computational logic is still\nchallenging due to the lack of domain knowledge and limited expressibility of\nconventional logic representations. Thus, LLM-FuncMapper, an approach to\nidentifying predefined functions needed to interpret various regulatory clauses\nbased on the large language model (LLM), is proposed. First, by systematically\nanalysis of building codes, a series of atomic functions are defined to capture\nshared computational logics of implicit properties and complex constraints,\ncreating a database of common blocks for interpreting regulatory clauses. Then,\na prompt template with the chain of thought is developed and further enhanced\nwith a classification-based tuning strategy, to enable common LLMs for\neffective function identification. Finally, the proposed approach is validated\nwith statistical analysis, experiments, and proof of concept. Statistical\nanalysis reveals a long-tail distribution and high expressibility of the\ndeveloped function database, with which almost 100% of computer-processible\nclauses can be interpreted and represented as computer-executable codes.\nExperiments show that LLM-FuncMapper achieve promising results in identifying\nrelevant predefined functions for rule interpretation. Further proof of concept\nin automated rule interpretation also demonstrates the possibility of\nLLM-FuncMapper in interpreting complex regulatory clauses. To the best of our\nknowledge, this study is the first attempt to introduce LLM for understanding\nand interpreting complex regulatory clauses, which may shed light on further\nadoption of LLM in the construction domain.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Contrasting Linguistic Patterns in Human and LLM-Generated Text\u2b1b  We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from 4 LLMs from the\nLLaMa family. Our analysis spans several measurable linguistic dimensions,\nincluding morphological, syntactic, psychometric and sociolinguistic aspects.\nThe results reveal various measurable differences between human and\nAI-generated texts. Among others, human texts exhibit more scattered sentence\nlength distributions, a distinct use of dependency and constituent types,\nshorter constituents, and more aggressive emotions (fear, disgust) than\nLLM-generated texts. LLM outputs use more numbers, symbols and auxiliaries\n(suggesting objective language) than human texts, as well as more pronouns. The\nsexist bias prevalent in human text is also expressed by LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Scope is all you need: Transforming LLMs for HPC Code\u2b1b  With easier access to powerful compute resources, there is a growing trend in\nthe field of AI for software development to develop larger and larger language\nmodels (LLMs) to address a variety of programming tasks. Even LLMs applied to\ntasks from the high-performance computing (HPC) domain are huge in size (e.g.,\nbillions of parameters) and demand expensive compute resources for training. We\nfound this design choice confusing - why do we need large LLMs trained on\nnatural languages and programming languages unrelated to HPC for HPC-specific\ntasks? In this line of work, we aim to question design choices made by existing\nLLMs by developing smaller LLMs for specific domains - we call them\ndomain-specific LLMs. Specifically, we start off with HPC as a domain and\npropose a novel tokenizer named Tokompiler, designed specifically for\npreprocessing code in HPC and compilation-centric tasks. Tokompiler leverages\nknowledge of language primitives to generate language-oriented tokens,\nproviding a context-aware understanding of code structure while avoiding human\nsemantics attributed to code structures completely. We applied Tokompiler to\npre-train two state-of-the-art models, SPT-Code and Polycoder, for a Fortran\ncode corpus mined from GitHub. We evaluate the performance of these models\nagainst the conventional LLMs. Results demonstrate that Tokompiler\nsignificantly enhances code completion accuracy and semantic understanding\ncompared to traditional tokenizers in normalized-perplexity tests, down to ~1\nperplexity score. This research opens avenues for further advancements in\ndomain-specific LLMs, catering to the unique demands of HPC and compilation\ntasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only\n  Quantization for LLMs\u2b1b  Large Language Models (LLMs) have achieved state-of-the-art performance\nacross various language tasks but pose challenges for practical deployment due\nto their substantial memory requirements. Furthermore, the latest generative\nmodels suffer from high inference costs caused by the memory bandwidth\nbottleneck in the auto-regressive decoding process. To address these issues, we\npropose an efficient weight-only quantization method that reduces memory\nconsumption and accelerates inference for LLMs. To ensure minimal quality\ndegradation, we introduce a simple and effective heuristic approach that\nutilizes only the model weights of a pre-trained model. This approach is\napplicable to both Mixture-of-Experts (MoE) and dense models without requiring\nadditional fine-tuning. To demonstrate the effectiveness of our proposed\nmethod, we first analyze the challenges and issues associated with LLM\nquantization. Subsequently, we present our heuristic approach, which adaptively\nfinds the granularity of quantization, effectively addressing these problems.\nFurthermore, we implement highly efficient GPU GEMMs that perform on-the-fly\nmatrix multiplication and dequantization, supporting the multiplication of fp16\nor bf16 activations with int8 or int4 weights. We evaluate our approach on\nlarge-scale open source models such as OPT-175B and internal MoE models,\nshowcasing minimal accuracy loss while achieving up to 3.65 times higher\nthroughput on the same number of GPUs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How susceptible are LLMs to Logical Fallacies?\u2b1b  This paper investigates the rational thinking capability of Large Language\nModels (LLMs) in multi-round argumentative debates by exploring the impact of\nfallacious arguments on their logical reasoning performance. More specifically,\nwe present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic\nbenchmark to assess the robustness of LLMs against logical fallacies. LOGICOM\ninvolves two agents: a persuader and a debater engaging in a multi-round debate\non a controversial topic, where the persuader tries to convince the debater of\nthe correctness of its claim. First, LOGICOM assesses the potential of LLMs to\nchange their opinions through reasoning. Then, it evaluates the debater's\nperformance in logical reasoning by contrasting the scenario where the\npersuader employs logical fallacies against one where logical reasoning is\nused. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4\nusing a dataset containing controversial topics, claims, and reasons supporting\nthem. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their\nopinion through reasoning. However, when presented with logical fallacies,\nGPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often,\nrespectively, compared to when logical reasoning is used. Finally, we introduce\na new dataset containing over 5k pairs of logical vs. fallacious arguments. The\nsource code and dataset of this work are made publicly available.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual\n  Questions\u2b1b  Vision Language Models (VLMs), which extend Large Language Models (LLM) by\nincorporating visual understanding capability, have demonstrated significant\nadvancements in addressing open-ended visual question-answering (VQA) tasks.\nHowever, these models cannot accurately interpret images infused with text, a\ncommon occurrence in real-world scenarios. Standard procedures for extracting\ninformation from images often involve learning a fixed set of query embeddings.\nThese embeddings are designed to encapsulate image contexts and are later used\nas soft prompt inputs in LLMs. Yet, this process is limited to the token count,\npotentially curtailing the recognition of scenes with text-rich context. To\nimprove upon them, the present study introduces BLIVA: an augmented version of\nInstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings\nfrom InstructBLIP and also directly projects encoded patch embeddings into the\nLLM, a technique inspired by LLaVA. This approach assists the model to capture\nintricate details potentially missed during the query decoding process.\nEmpirical evidence demonstrates that our model, BLIVA, significantly enhances\nperformance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA\nbenchmark) and in undertaking general (not particularly text-rich) VQA\nbenchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), comparing to our\nbaseline InstructBLIP. BLIVA demonstrates significant capability in decoding\nreal-world images, irrespective of text presence. To demonstrate the broad\nindustry applications enabled by BLIVA, we evaluate the model using a new\ndataset comprising YouTube thumbnails paired with question-answer sets across\n11 diverse categories. For researchers interested in further exploration, our\ncode and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs\u2b1b  Large language models (LLMs) possess a wealth of knowledge encoded in their\nparameters. However, this knowledge may become outdated or unsuitable over\ntime. As a result, there has been a growing interest in knowledge editing for\nLLMs and evaluating its effectiveness. Existing studies primarily focus on\nknowledge editing using factual triplets, which not only incur high costs for\ncollection but also struggle to express complex facts. Furthermore, these\nstudies are often limited in their evaluation perspectives. In this paper, we\npropose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs.\nThis benchmark includes an evaluation framework and a corresponding dataset.\nUnder our framework, we first ask the LLM to perform knowledge editing using\nraw documents, which provides a more convenient and universal approach compared\nto using factual triplets. We then evaluate the updated LLM from multiple\nperspectives. In addition to assessing the effectiveness of knowledge editing\nand the retention of unrelated knowledge from conventional studies, we further\ntest the LLM's ability in two aspects: 1) Reasoning with the altered knowledge,\naiming for the LLM to genuinely learn the altered knowledge instead of simply\nmemorizing it. 2) Cross-lingual knowledge transfer, where the LLM updated with\nraw documents in one language should be capable of handling queries from\nanother language. To facilitate further research, we construct and release the\ncorresponding dataset. Using this benchmark, we investigate the effectiveness\nof several commonly-used knowledge editing methods. Experimental results\nindicate that the current methods for knowledge editing using raw documents are\nnot effective in yielding satisfactory results, particularly when it comes to\nreasoning with altered knowledge and cross-lingual knowledge transfer.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "GameEval: Evaluating LLMs on Conversational Games\u2b1b  The rapid advancements in large language models (LLMs) have presented\nchallenges in evaluating those models. Existing evaluation methods are either\nreference-based or preference based, which inevitably need human intervention\nor introduce test bias caused by evaluator models. In this paper, we propose\nGameEval, a novel approach to evaluating LLMs through goal-driven\nconversational games, overcoming the limitations of previous methods. GameEval\ntreats LLMs as game players and assigns them distinct roles with specific goals\nachieved by launching conversations of various forms, including discussion,\nquestion answering, and voting. We design three unique games with cooperative\nor adversarial objectives, accompanied by corresponding evaluation metrics, to\nshow how this new paradigm comprehensively evaluates model performance.Through\nextensive experiments, we show that GameEval can effectively differentiate the\ncapabilities of various LLMs, providing a comprehensive assessment of their\nintegrated abilities to solve complex problems. Our public anonymous code is\navailable at https://github.com/GameEval/GameEval.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ExpeL: LLM Agents Are Experiential Learners\u2b1b  The recent surge in research interest in applying large language models\n(LLMs) to decision-making tasks has flourished by leveraging the extensive\nworld knowledge embedded in LLMs. While there is a growing demand to tailor\nLLMs for custom decision-making tasks, finetuning them for specific tasks is\nresource-intensive and may diminish the model's generalization capabilities.\nMoreover, state-of-the-art language models like GPT-4 and Claude are primarily\naccessible through API calls, with their parametric weights remaining\nproprietary and unavailable to the public. This scenario emphasizes the growing\nneed for new methodologies that allow learning from agent experiences without\nrequiring parametric updates. To address these problems, we introduce the\nExperiential Learning (ExpeL) agent. Our agent autonomously gathers experiences\nand extracts knowledge using natural language from a collection of training\ntasks. At inference, the agent recalls its extracted insights and past\nexperiences to make informed decisions. Our empirical results highlight the\nrobust learning efficacy of the ExpeL agent, indicating a consistent\nenhancement in its performance as it accumulates experiences. We further\nexplore the emerging capabilities and transfer learning potential of the ExpeL\nagent through qualitative observations and additional experiments.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A.\n  Will LLMs Replace Knowledge Graphs?\u2b1b  Since the recent prosperity of Large Language Models (LLMs), there have been\ninterleaved discussions regarding how to reduce hallucinations from LLM\nresponses, how to increase the factuality of LLMs, and whether Knowledge Graphs\n(KGs), which store the world knowledge in a symbolic form, will be replaced\nwith LLMs. In this paper, we try to answer these questions from a new angle:\nHow knowledgeable are LLMs?\n  To answer this question, we constructed Head-to-Tail, a benchmark that\nconsists of 18K question-answer (QA) pairs regarding head, torso, and tail\nfacts in terms of popularity. We designed an automated evaluation method and a\nset of metrics that closely approximate the knowledge an LLM confidently\ninternalizes. Through a comprehensive evaluation of 14 publicly available LLMs,\nwe show that existing LLMs are still far from being perfect in terms of their\ngrasp of factual knowledge, especially for facts of torso-to-tail entities.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DepreSym: A Depression Symptom Annotated Corpus and the Role of LLMs as\n  Assessors of Psychological Markers\u2b1b  Computational methods for depression detection aim to mine traces of\ndepression from online publications posted by Internet users. However,\nsolutions trained on existing collections exhibit limited generalisation and\ninterpretability. To tackle these issues, recent studies have shown that\nidentifying depressive symptoms can lead to more robust models. The eRisk\ninitiative fosters research on this area and has recently proposed a new\nranking task focused on developing search methods to find sentences related to\ndepressive symptoms. This search challenge relies on the symptoms specified by\nthe Beck Depression Inventory-II (BDI-II), a questionnaire widely used in\nclinical practice. Based on the participant systems' results, we present the\nDepreSym dataset, consisting of 21580 sentences annotated according to their\nrelevance to the 21 BDI-II symptoms. The labelled sentences come from a pool of\ndiverse ranking methods, and the final dataset serves as a valuable resource\nfor advancing the development of models that incorporate depressive markers\nsuch as clinical symptoms. Due to the complex nature of this relevance\nannotation, we designed a robust assessment methodology carried out by three\nexpert assessors (including an expert psychologist). Additionally, we explore\nhere the feasibility of employing recent Large Language Models (ChatGPT and\nGPT4) as potential assessors in this complex task. We undertake a comprehensive\nexamination of their performance, determine their main limitations and analyze\ntheir role as a complement or replacement for human annotators.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis\u2b1b  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,605 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community. In the spirit of further research, we plan to make this\ndataset and our experimental resources publicly accessible to the wider\nresearch community.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete\n  Information from Lateral Thinking Puzzles\u2b1b  With the continuous evolution and refinement of LLMs, they are endowed with\nimpressive logical reasoning or vertical thinking capabilities. But can they\nthink out of the box? Do they possess proficient lateral thinking abilities?\nFollowing the setup of Lateral Thinking Puzzles, we propose a novel evaluation\nbenchmark, LatEval, which assesses the model's lateral thinking within an\ninteractive framework. In our benchmark, we challenge LLMs with 2 aspects: the\nquality of questions posed by the model and the model's capability to integrate\ninformation for problem-solving. We find that nearly all LLMs struggle with\nemploying lateral thinking during interactions. For example, even the most\nadvanced model, GPT-4, exhibits the advantage to some extent, yet still\nmaintain a noticeable gap when compared to human. This evaluation benchmark\nprovides LLMs with a highly challenging and distinctive task that is crucial to\nan effective AI assistant.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Giraffe: Adventures in Expanding Context Lengths in LLMs\u2b1b  Modern large language models (LLMs) that rely on attention mechanisms are\ntypically trained with fixed context lengths which enforce upper limits on the\nlength of input sequences that they can handle at evaluation time. To use these\nmodels on sequences longer than the train-time context length, one might employ\ntechniques from the growing family of context length extrapolation methods --\nmost of which focus on modifying the system of positional encodings used in the\nattention mechanism to indicate where tokens or activations are located in the\ninput sequence. We conduct a wide survey of existing methods of context length\nextrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own\ndesign as well -- in particular, a new truncation strategy for modifying the\nbasis for the position encoding.\n  We test these methods using three new evaluation tasks (FreeFormQA,\nAlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to\nbe less fine-grained as a measure of long context performance of LLMs. We\nrelease the three tasks publicly as datasets on HuggingFace. We discover that\nlinear scaling is the best method for extending context length, and show that\nfurther gains can be achieved by using longer scales at evaluation time. We\nalso discover promising extrapolation capabilities in the truncated basis. To\nsupport further research in this area, we release three new 13B parameter\nlong-context models which we call Giraffe: 4k and 16k context models trained\nfrom base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We\nalso release the code to replicate our results.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator\u2b1b  The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT conversations, as evidenced by Vicuna. However, due to challenges\nin gathering conversations involving human participation, current endeavors\nlike Baize and UltraChat aim to automatically generate conversational data.\nThey primarily rely on ChatGPT conducting roleplay to simulate human behaviors\nbased on instructions rather than genuine learning from humans, resulting in\nlimited scope, diminished diversity, and an absence of genuine multi-round\nconversational dynamics. To address the above issues, we target human questions\nextracted from genuine human-machine conversations as a learning goal and train\na user simulator called `Socratic' to produce a high-quality human-centric\nsynthetic conversation dataset. Subsequently, this dataset was used to train\nour assistant model, named `PlatoLM'. Experimentally, PlatoLM outpaces baseline\nmodels in both Vicuna-Bench and MT-Bench by pairwise comparison when\nconsidering equivalent training set sizes, and manual evaluation also shows\nthat our model is highly competitive. Impressively, when fine-tuned with the\nlatest LLaMA 2 model, PlatoLM achieves the SOTA performance among 7B models\n(including LLaMA-2-7B-chat and Vicuna-7B) in MT-Bench benchmark and in\nAlpaca-Eval benchmark, it ranks second among 7B models, even beating some\nlarger scale models (including LLaMA-2-13B-chat and GPT-3.5). Further in-depth\nanalysis demonstrates the scalability and transferability of our approach. The\ncode is available at https://github.com/FreedomIntelligence/PlatoLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Building Emotional Support Chatbots in the Era of LLMs\u2b1b  The integration of emotional support into various conversational scenarios\npresents profound societal benefits, such as social interactions, mental health\ncounseling, and customer service. However, there are unsolved challenges that\nhinder real-world applications in this field, including limited data\navailability and the absence of well-accepted model training paradigms. This\nwork endeavors to navigate these challenges by harnessing the capabilities of\nLarge Language Models (LLMs). We introduce an innovative methodology that\nsynthesizes human insights with the computational prowess of LLMs to curate an\nextensive emotional support dialogue dataset. Our approach is initiated with a\nmeticulously designed set of dialogues spanning diverse scenarios as generative\nseeds. By utilizing the in-context learning potential of ChatGPT, we\nrecursively generate an ExTensible Emotional Support dialogue dataset, named\nExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,\nexamining the impact of diverse training strategies, ultimately yielding an LLM\nmeticulously optimized for emotional support interactions. An exhaustive\nassessment of the resultant model showcases its proficiency in offering\nemotional support, marking a pivotal step in the realm of emotional support\nbots and paving the way for subsequent research and implementations.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data\n  Selection for Instruction Tuning\u2b1b  In the realm of Large Language Models, the balance between instruction data\nquality and quantity has become a focal point. Recognizing this, we introduce a\nself-guided methodology for LLMs to autonomously discern and select cherry\nsamples from vast open-source datasets, effectively minimizing manual curation\nand potential cost for instruction tuning an LLM. Our key innovation, the\nInstruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to\nidentify discrepancies between a model's expected responses and its autonomous\ngeneration prowess. Through the adept application of IFD, cherry samples are\npinpointed, leading to a marked uptick in model training efficiency. Empirical\nvalidations on renowned datasets like Alpaca and WizardLM underpin our\nfindings; with a mere 10% of conventional data input, our strategy showcases\nimproved results. This synthesis of self-guided cherry-picking and the IFD\nmetric signifies a transformative leap in the optimization of LLMs, promising\nboth efficiency and resource-conscious advancements. Codes, data, and models\nare available: https://github.com/MingLiiii/Cherry_LLM\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base\n  Question Answering\u2b1b  Knowledge base question answering (KBQA) is a critical yet challenging task\ndue to the vast number of entities within knowledge bases and the diversity of\nnatural language questions posed by users. Unfortunately, the performance of\nmost KBQA models tends to decline significantly in real-world scenarios where\nhigh-quality annotated data is insufficient. To mitigate the burden associated\nwith manual annotation, we introduce FlexKBQA by utilizing Large Language\nModels (LLMs) as program translators for addressing the challenges inherent in\nthe few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms\nto sample diverse programs, such as SPARQL queries, from the knowledge base,\nwhich are subsequently converted into natural language questions via LLMs. This\nsynthetic dataset facilitates training a specialized lightweight model for the\nKB. Additionally, to reduce the barriers of distribution shift between\nsynthetic data and real user questions, FlexKBQA introduces an executionguided\nself-training method to iterative leverage unlabeled user questions.\nFurthermore, we explore harnessing the inherent reasoning capability of LLMs to\nenhance the entire framework. Consequently, FlexKBQA delivers substantial\nflexibility, encompassing data annotation, deployment, and being domain\nagnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we\nobserve that under the few-shot even the more challenging zero-shot scenarios,\nFlexKBQA achieves impressive results with a few annotations, surpassing all\nprevious baselines and even approaching the performance of supervised models,\nachieving a remarkable 93% performance relative to the fully-supervised models.\nWe posit that FlexKBQA represents a significant advancement towards exploring\nbetter integration of large and lightweight models. The code is open-sourced.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "D4: Improving LLM Pretraining via Document De-Duplication and\n  Diversification\u2b1b  Over recent years, an increasing amount of compute and data has been poured\ninto training large language models (LLMs), usually by doing one-pass learning\non as many tokens as possible randomly selected from large-scale web corpora.\nWhile training on ever-larger portions of the internet leads to consistent\nperformance improvements, the size of these improvements diminishes with scale,\nand there has been little work exploring the effect of data selection on\npre-training and downstream performance beyond simple de-duplication methods\nsuch as MinHash. Here, we show that careful data selection (on top of\nde-duplicated data) via pre-trained model embeddings can speed up training (20%\nefficiency gains) and improves average downstream accuracy on 16 NLP tasks (up\nto 2%) at the 6.7B model scale. Furthermore, we show that repeating data\nintelligently consistently outperforms baseline training (while repeating\nrandom data performs worse than baseline training). Our results indicate that\nclever data selection can significantly improve LLM pre-training, calls into\nquestion the common practice of training for a single epoch on as much data as\npossible, and demonstrates a path to keep improving our models past the limits\nof randomly sampling web data.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and\n  Vulnerabilities\u2b1b  Spurred by the recent rapid increase in the development and distribution of\nlarge language models (LLMs) across industry and academia, much recent work has\ndrawn attention to safety- and security-related threats and vulnerabilities of\nLLMs, including in the context of potentially criminal activities.\nSpecifically, it has been shown that LLMs can be misused for fraud,\nimpersonation, and the generation of malware; while other authors have\nconsidered the more general problem of AI alignment. It is important that\ndevelopers and practitioners alike are aware of security-related problems with\nsuch models. In this paper, we provide an overview of existing - predominantly\nscientific - efforts on identifying and mitigating threats and vulnerabilities\narising from LLMs. We present a taxonomy describing the relationship between\nthreats caused by the generative capabilities of LLMs, prevention measures\nintended to address such threats, and vulnerabilities arising from imperfect\nprevention measures. With our work, we hope to raise awareness of the\nlimitations of LLMs in light of such security concerns, among both experienced\ndevelopers and novel users of such technologies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM2KB: Constructing Knowledge Bases using instruction tuned context\n  aware Large Language Models\u2b1b  The advent of Large Language Models (LLM) has revolutionized the field of\nnatural language processing, enabling significant progress in various\napplications. One key area of interest is the construction of Knowledge Bases\n(KB) using these powerful models. Knowledge bases serve as repositories of\nstructured information, facilitating information retrieval and inference tasks.\nOur paper proposes LLM2KB, a system for constructing knowledge bases using\nlarge language models, with a focus on the Llama 2 architecture and the\nWikipedia dataset. We perform parameter efficient instruction tuning for\nLlama-2-13b-chat and StableBeluga-13B by training small injection models that\nhave only 0.05 % of the parameters of the base models using the Low Rank\nAdaptation (LoRA) technique. These injection models have been trained with\nprompts that are engineered to utilize Wikipedia page contexts of subject\nentities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer\nrelevant object entities for a given subject entity and relation. Our best\nperforming model achieved an average F1 score of 0.6185 across 21 relations in\nthe LM-KBC challenge held at the ISWC 2023 conference.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for\n  Knowledge-intensive Question Answering\u2b1b  Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown\nimpressive reasoning ability in various downstream tasks. Even so, suffering\nfrom hallucinations and the inability to access external knowledge, LLMs often\ncome with incorrect or unfaithful intermediate reasoning steps, especially in\nthe context of answering knowledge-intensive tasks such as KBQA. To alleviate\nthis issue, we propose a framework called Knowledge-Driven Chain-of-Thought\n(KD-CoT) to verify and modify reasoning traces in CoT via interaction with\nexternal knowledge, and thus overcome the hallucinations and error propagation.\nConcretely, we formulate the CoT rationale process of LLMs into a structured\nmulti-round QA format. In each round, LLMs interact with a QA system that\nretrieves external knowledge and produce faithful reasoning traces based on\nretrieved precise answers. The structured CoT reasoning of LLMs is facilitated\nby our developed KBQA CoT collection, which serves as in-context learning\ndemonstrations and can also be utilized as feedback augmentation to train a\nrobust retriever. Extensive experiments on WebQSP and ComplexWebQuestion\ndatasets demonstrate the effectiveness of proposed KD-CoT in task-solving\nreasoning generation, which outperforms the vanilla CoT ICL with an absolute\nsuccess rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented\nretriever outperforms the state-of-the-art baselines for retrieving knowledge,\nachieving significant improvement in Hit and recall performance. Our code and\ndata are released on https://github.com/AdelWang/KD-CoT/tree/main.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs\u2b1b  With the rapid evolution of large language models (LLMs), new and\nhard-to-predict harmful capabilities are emerging. This requires developers to\nbe able to identify risks through the evaluation of \"dangerous capabilities\" in\norder to responsibly deploy LLMs. In this work, we collect the first\nopen-source dataset to evaluate safeguards in LLMs, and deploy safer\nopen-source LLMs at a low cost. Our dataset is curated and filtered to consist\nonly of instructions that responsible language models should not follow. We\nannotate and assess the responses of six popular LLMs to these instructions.\nBased on our annotation, we proceed to train several BERT-like classifiers, and\nfind that these small classifiers can achieve results that are comparable with\nGPT-4 on automatic safety evaluation. Warning: this paper contains example data\nthat may be offensive, harmful, or biased.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Building Trust in Conversational AI: A Comprehensive Review and Solution\n  Architecture for Explainable, Privacy-Aware Systems using LLMs and Knowledge\n  Graph\u2b1b  Conversational AI systems have emerged as key enablers of human-like\ninteractions across diverse sectors. Nevertheless, the balance between\nlinguistic nuance and factual accuracy has proven elusive. In this paper, we\nfirst introduce LLMXplorer, a comprehensive tool that provides an in-depth\nreview of over 150 Large Language Models (LLMs), elucidating their myriad\nimplications ranging from social and ethical to regulatory, as well as their\napplicability across industries. Building on this foundation, we propose a\nnovel functional architecture that seamlessly integrates the structured\ndynamics of Knowledge Graphs with the linguistic capabilities of LLMs.\nValidated using real-world AI news data, our architecture adeptly blends\nlinguistic sophistication with factual rigour and further strengthens data\nsecurity through Role-Based Access Control. This research provides insights\ninto the evolving landscape of conversational AI, emphasizing the imperative\nfor systems that are efficient, transparent, and trustworthy.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MLLM-DataEngine: An Iterative Refinement Approach for MLLM\u2b1b  Despite the great advance of Multimodal Large Language Models (MLLMs) in both\ninstruction dataset building and benchmarking, the independence of training and\nevaluation makes current MLLMs hard to further improve their capability under\nthe guidance of evaluation results with a relatively low human cost. In this\npaper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data\ngeneration, model training, and evaluation. Within each loop iteration, the\nMLLM-DataEngine first analyze the weakness of the model based on the evaluation\nresults, then generate a proper incremental dataset for the next training\niteration and enhance the model capability iteratively. Compared with previous\ndata collection methods which are separate from the benchmarking, the data\ngenerated by MLLM-DataEngine shows better targeting, quality, and correctness.\nFor targeting, we propose an Adaptive Bad-case Sampling module, which adjusts\nthe ratio of different types of data within each incremental dataset based on\nthe benchmarking results. For quality, we resort to GPT-4 to generate\nhigh-quality data with each given data type. For correctness, prompt design is\ncritical for the data generation results. Rather than previous hand-crafted\nprompt, we propose an Interactive Prompt Optimization strategy, which optimizes\nthe prompt with the multi-round interaction between human and GPT, and improve\nthe correctness of generated data greatly. Through extensive experiments, we\nfind our MLLM-DataEngine could boost the MLLM capability in a targeted and\nautomatic manner, with only a few human participation. We hope it could be a\ngeneral solution for the following MLLMs building. The MLLM-DataEngine has been\nopen-sourced and is now available at\nhttps://github.com/opendatalab/MLLM-DataEngine.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DISC-MedLLM: Bridging General Large Language Models and Real-World\n  Medical Consultation\u2b1b  We propose DISC-MedLLM, a comprehensive solution that leverages Large\nLanguage Models (LLMs) to provide accurate and truthful medical response in\nend-to-end conversational healthcare services. To construct high-quality\nSupervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing\nmedical knowledge-graphs, reconstructing real-world dialogues, and\nincorporating human-guided preference rephrasing. These datasets are\ninstrumental in training DISC-MedLLM, surpassing existing medical LLMs in both\nsingle-turn and multi-turn consultation scenarios. Extensive experimental\nresults demonstrate the effectiveness of the proposed model in bridging the gap\nbetween general language models and real-world medical consultation.\nAdditionally, we release the constructed dataset and model weights to further\ncontribute to research and development. Further details and resources can be\nfound at https://github.com/FudanDISC/DISC-MedLLM\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "FurChat: An Embodied Conversational Agent using LLMs, Combining Open and\n  Closed-Domain Dialogue with Facial Expressions\u2b1b  We demonstrate an embodied conversational agent that can function as a\nreceptionist and generate a mixture of open and closed-domain dialogue along\nwith facial expressions, by using a large language model (LLM) to develop an\nengaging conversation. We deployed the system onto a Furhat robot, which is\nhighly expressive and capable of using both verbal and nonverbal cues during\ninteraction. The system was designed specifically for the National Robotarium\nto interact with visitors through natural conversations, providing them with\ninformation about the facilities, research, news, upcoming events, etc. The\nsystem utilises the state-of-the-art GPT-3.5 model to generate such information\nalong with domain-general conversations and facial expressions based on prompt\nengineering.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Multi-party Goal Tracking with LLMs: Comparing Pre-training,\n  Fine-tuning, and Prompt Engineering\u2b1b  This paper evaluates the extent to which current Large Language Models (LLMs)\ncan capture task-oriented multi-party conversations (MPCs). We have recorded\nand transcribed 29 MPCs between patients, their companions, and a social robot\nin a hospital. We then annotated this corpus for multi-party goal-tracking and\nintent-slot recognition. People share goals, answer each other's goals, and\nprovide other people's goals in MPCs - none of which occur in dyadic\ninteractions. To understand user goals in MPCs, we compared three methods in\nzero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks\nto train DialogLM using LED, and employed prompt engineering techniques with\nGPT-3.5-turbo, to determine which approach can complete this novel task with\nlimited data. GPT-3.5-turbo significantly outperformed the others in a few-shot\nsetting. The `reasoning' style prompt, when given 7% of the corpus as example\nannotated conversations, was the best performing method. It correctly annotated\n62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition\nMPCs. A `story' style prompt increased model hallucination, which could be\ndetrimental if deployed in safety-critical settings. We conclude that\nmulti-party conversations still challenge state-of-the-art LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through\n  the Lens of Moral Theories?\u2b1b  Making moral judgments is an essential step toward developing ethical AI\nsystems. Prevalent approaches are mostly implemented in a bottom-up manner,\nwhich uses a large set of annotated data to train models based on crowd-sourced\nopinions about morality. These approaches have been criticized for potentially\novergeneralizing a limited group of annotators' moral stances and lacking\nexplainability. In contrast, top-down approaches make moral judgments grounded\nin a set of principles. However, it remains conceptual due to the incapability\nof previous language models and the unsolved debate among moral principles. In\nthis study, we propose a flexible framework to steer Large Language Models\n(LLMs) to perform moral reasoning with well-established moral theories from\ninterdisciplinary research. The theory-guided top-down framework can\nincorporate various moral theories. Our experiments demonstrate the\neffectiveness of the proposed framework on datasets derived from moral\ntheories. Furthermore, we show the alignment between different moral theories\nand existing morality datasets. Our analysis exhibits the potentials and flaws\nin existing resources (models and datasets) in developing explainable moral\njudgment-making systems.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual\n  Dialogue Evaluation\u2b1b  Despite significant research effort in the development of automatic dialogue\nevaluation metrics, little thought is given to evaluating dialogues other than\nin English. At the same time, ensuring metrics are invariant to semantically\nsimilar responses is also an overlooked topic. In order to achieve the desired\nproperties of robustness and multilinguality for dialogue evaluation metrics,\nwe propose a novel framework that takes advantage of the strengths of current\nevaluation models with the newly-established paradigm of prompting Large\nLanguage Models (LLMs). Empirical results show our framework achieves state of\nthe art results in terms of mean Spearman correlation scores across several\nbenchmarks and ranks first place on both the Robust and Multilingual tasks of\nthe DSTC11 Track 4 \"Automatic Evaluation Metrics for Open-Domain Dialogue\nSystems\", proving the evaluation capabilities of prompted LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PointLLM: Empowering Large Language Models to Understand Point Clouds\u2b1b  The unprecedented advancements in Large Language Models (LLMs) have shown a\nprofound impact on natural language processing but are yet to fully embrace the\nrealm of 3D understanding. This paper introduces PointLLM, a preliminary effort\nto fill this gap, enabling LLMs to understand point clouds and offering a new\navenue beyond 2D visual data. PointLLM understands colored object point clouds\nwith human instructions and generates contextually appropriate responses,\nillustrating its grasp of point clouds and common sense. Specifically, it\nleverages a point cloud encoder with a powerful LLM to effectively fuse\ngeometric, appearance, and linguistic information. We collect a novel dataset\ncomprising 660K simple and 70K complex point-text instruction pairs to enable a\ntwo-stage training strategy: aligning latent spaces and subsequently\ninstruction-tuning the unified model. To rigorously evaluate the perceptual and\ngeneralization capabilities of PointLLM, we establish two benchmarks:\nGenerative 3D Object Classification and 3D Object Captioning, assessed through\nthree different methods, including human evaluation, GPT-4/ChatGPT evaluation,\nand traditional metrics. Experimental results reveal PointLLM's superior\nperformance over existing 2D and 3D baselines, with a notable achievement in\nhuman-evaluated object captioning tasks where it surpasses human annotators in\nover 50% of the samples. Codes, datasets, and benchmarks are available at\nhttps://github.com/OpenRobotLab/PointLLM .\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM in the Shell: Generative Honeypots\u2b1b  Honeypots are essential tools in cybersecurity. However, most of them (even\nthe high-interaction ones) lack the required realism to engage and fool human\nattackers. This limitation makes them easily discernible, hindering their\neffectiveness. This work introduces a novel method to create dynamic and\nrealistic software honeypots based on Large Language Models. Preliminary\nresults indicate that LLMs can create credible and dynamic honeypots capable of\naddressing important limitations of previous honeypots, such as deterministic\nresponses, lack of adaptability, etc. We evaluated the realism of each command\nby conducting an experiment with human attackers who needed to say if the\nanswer from the honeypot was fake or not. Our proposed honeypot, called shelLM,\nreached an accuracy rate of 0.92.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D\n  Understanding, Generation, and Instruction Following\u2b1b  We introduce Point-Bind, a 3D multi-modality model aligning point clouds with\n2D image, language, audio, and video. Guided by ImageBind, we construct a joint\nembedding space between 3D and multi-modalities, enabling many promising\napplications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D\nopen-world understanding. On top of this, we further present Point-LLM, the\nfirst 3D large language model (LLM) following 3D multi-modal instructions. By\nparameter-efficient fine-tuning techniques, Point-LLM injects the semantics of\nPoint-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction\ndata, but exhibits superior 3D and multi-modal question-answering capacity. We\nhope our work may cast a light on the community for extending 3D point clouds\nto multi-modality applications. Code is available at\nhttps://github.com/ZiyuGuo99/Point-Bind_Point-LLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Taken out of context: On measuring situational awareness in LLMs\u2b1b  We aim to better understand the emergence of `situational awareness' in large\nlanguage models (LLMs). A model is situationally aware if it's aware that it's\na model and can recognize whether it's currently in testing or deployment.\nToday's LLMs are tested for safety and alignment before they are deployed. An\nLLM could exploit situational awareness to achieve a high score on safety\ntests, while taking harmful actions after deployment. Situational awareness may\nemerge unexpectedly as a byproduct of model scaling. One way to better foresee\nthis emergence is to run scaling experiments on abilities necessary for\nsituational awareness. As such an ability, we propose `out-of-context\nreasoning' (in contrast to in-context learning). We study out-of-context\nreasoning experimentally. First, we finetune an LLM on a description of a test\nwhile providing no examples or demonstrations. At test time, we assess whether\nthe model can pass the test. To our surprise, we find that LLMs succeed on this\nout-of-context reasoning task. Their success is sensitive to the training setup\nand only works when we apply data augmentation. For both GPT-3 and LLaMA-1,\nperformance improves with model size. These findings offer a foundation for\nfurther empirical study, towards predicting and potentially controlling the\nemergence of situational awareness in LLMs. Code is available at:\nhttps://github.com/AsaCooperStickland/situational-awareness-evals.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LeanContext: Cost-Efficient Domain-Specific Question Answering Using\n  LLMs\u2b1b  Question-answering (QA) is a significant application of Large Language Models\n(LLMs), shaping chatbot capabilities across healthcare, education, and customer\nservice. However, widespread LLM integration presents a challenge for small\nbusinesses due to the high expenses of LLM API usage. Costs rise rapidly when\ndomain-specific data (context) is used alongside queries for accurate\ndomain-specific LLM responses. One option is to summarize the context by using\nLLMs and reduce the context. However, this can also filter out useful\ninformation that is necessary to answer some domain-specific queries. In this\npaper, we shift from human-oriented summarizers to AI model-friendly summaries.\nOur approach, LeanContext, efficiently extracts $k$ key sentences from the\ncontext that are closely aligned with the query. The choice of $k$ is neither\nstatic nor random; we introduce a reinforcement learning technique that\ndynamically determines $k$ based on the query and context. The rest of the less\nimportant sentences are reduced using a free open source text reduction method.\nWe evaluate LeanContext against several recent query-aware and query-unaware\ncontext reduction approaches on prominent datasets (arxiv papers and BBC news\narticles). Despite cost reductions of $37.29\\%$ to $67.81\\%$, LeanContext's\nROUGE-1 score decreases only by $1.41\\%$ to $2.65\\%$ compared to a baseline\nthat retains the entire context (no summarization). Additionally, if free\npretrained LLM-based summarizers are used to reduce context (into human\nconsumable summaries), LeanContext can further modify the reduced context to\nenhance the accuracy (ROUGE-1 score) by $13.22\\%$ to $24.61\\%$.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Study on the Implementation of Generative AI Services Using an\n  Enterprise Data-Based LLM Application Architecture\u2b1b  This study presents a method for implementing generative AI services by\nutilizing the Large Language Models (LLM) application architecture. With recent\nadvancements in generative AI technology, LLMs have gained prominence across\nvarious domains. In this context, the research addresses the challenge of\ninformation scarcity and proposes specific remedies by harnessing LLM\ncapabilities. The investigation delves into strategies for mitigating the issue\nof inadequate data, offering tailored solutions. The study delves into the\nefficacy of employing fine-tuning techniques and direct document integration to\nalleviate data insufficiency. A significant contribution of this work is the\ndevelopment of a Retrieval-Augmented Generation (RAG) model, which tackles the\naforementioned challenges. The RAG model is carefully designed to enhance\ninformation storage and retrieval processes, ensuring improved content\ngeneration. The research elucidates the key phases of the information storage\nand retrieval methodology underpinned by the RAG model. A comprehensive\nanalysis of these steps is undertaken, emphasizing their significance in\naddressing the scarcity of data. The study highlights the efficacy of the\nproposed method, showcasing its applicability through illustrative instances.\nBy implementing the RAG model for information storage and retrieval, the\nresearch not only contributes to a deeper comprehension of generative AI\ntechnology but also facilitates its practical usability within enterprises\nutilizing LLMs. This work holds substantial value in advancing the field of\ngenerative AI, offering insights into enhancing data-driven content generation\nand fostering active utilization of LLM-based services within corporate\nsettings.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM and Infrastructure as a Code use case\u2b1b  Cloud computing and the evolution of management methodologies such as Lean\nManagement or Agile entail a profound transformation in both system\nconstruction and maintenance approaches. These practices are encompassed within\nthe term \"DevOps.\" This descriptive approach to an information system or\napplication, alongside the configuration of its constituent components, has\nnecessitated the development of descriptive languages paired with specialized\nengines for automating systems administration tasks. Among these, the tandem of\nAnsible (engine) and YAML (descriptive language) stands out as the two most\nprevalent tools in the market, facing notable competition mainly from\nTerraform. The current document presents an inquiry into a solution for\ngenerating and managing Ansible YAML roles and playbooks, utilizing Generative\nLLMs (Language Models) to translate human descriptions into code. Our efforts\nare focused on identifying plausible directions and outlining the potential\nindustrial applications. Note: For the purpose of this experiment, we have\nopted against the use of Ansible Lightspeed. This is due to its reliance on an\nIBM Watson model, for which we have not found any publicly available\nreferences. Comprehensive information regarding this remarkable technology can\nbe found [1] directly on our partner's website, RedHat.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Augmenting Black-box LLMs with Medical Textbooks for Clinical Question\n  Answering\u2b1b  Large-scale language models (LLMs), such as ChatGPT, are capable of\ngenerating human-like responses for various downstream tasks, such as\ntask-oriented dialogues and question answering. However, applying LLMs to\nmedical domains remains challenging due to their inability to leverage\ndomain-specific knowledge. In this study, we present the Large-scale Language\nModels Augmented with Medical Textbooks (LLM-AMT), which integrates\nauthoritative medical textbooks as the cornerstone of its design, enhancing its\nproficiency in the specialized domain through plug-and-play modules, comprised\nof a Hybrid Textbook Retriever, supplemented by the Query Augmenter and the LLM\nReader. Experimental evaluation on three open-domain medical question-answering\ntasks reveals a substantial enhancement in both the professionalism and\naccuracy of the LLM responses when utilizing LLM-AMT, exhibiting an improvement\nranging from 11.4% to 13.2%. Despite being 100 times smaller, we found that\nmedical textbooks as the retrieval corpus serves as a more valuable external\nknowledge source than Wikipedia in the medical domain. Our experiments show\nthat textbook augmentation results in a performance improvement ranging from\n9.7% to 12.2% over Wikipedia augmentation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Certifying LLM Safety against Adversarial Prompting\u2b1b  Large language models (LLMs) released for public use incorporate guardrails\nto ensure their output is safe, often referred to as \"model alignment.\" An\naligned language model should decline a user's request to produce harmful\ncontent. However, such safety measures are vulnerable to adversarial attacks,\nwhich add maliciously designed token sequences to a harmful prompt to bypass\nthe model's safety guards. In this work, we introduce erase-and-check, the\nfirst framework to defend against adversarial prompts with verifiable safety\nguarantees. We defend against three attack modes: i) adversarial suffix, which\nappends an adversarial sequence at the end of the prompt; ii) adversarial\ninsertion, where the adversarial sequence is inserted anywhere in the middle of\nthe prompt; and iii) adversarial infusion, where adversarial tokens are\ninserted at arbitrary positions in the prompt, not necessarily as a contiguous\nblock. Our experimental results demonstrate that this procedure can obtain\nstrong certified safety guarantees on harmful prompts while maintaining good\nempirical performance on safe prompts. For example, against adversarial\nsuffixes of length 20, it certifiably detects 92% of harmful prompts and labels\n94% of safe prompts correctly using the open-source language model Llama 2 as\nthe safety filter. We further improve the filter's performance, in terms of\naccuracy and speed, by replacing Llama 2 with a DistilBERT safety classifier\nfine-tuned on safe and harmful prompts. Additionally, we propose two efficient\nempirical defenses: i) RandEC, a randomized version of erase-and-check that\nevaluates the safety filter on a small subset of the erased subsequences, and\nii) GradEC, a gradient-based version that optimizes the erased tokens to remove\nthe adversarial sequence. The code for our experiments is available at\nhttps://github.com/aounon/certified-llm-safety.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from\n  Knowledge Graphs\u2b1b  Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and\ncan solve different tasks due to their emergent ability and generalizability.\nHowever, LLMs sometimes lack domain-specific knowledge to perform tasks, which\nwould also cause hallucination during inference. In some previous works,\nadditional modules like graph neural networks (GNNs) are trained on retrieved\nknowledge from external knowledge bases, aiming to mitigate the problem of\nlacking domain-specific knowledge. However, incorporating additional modules:\n1) would need retraining additional modules when encountering novel domains; 2)\nwould become a bottleneck since LLMs' strong abilities are not fully utilized\nfor retrieval. In this paper, we propose a paradigm, termed Knowledge Solver\n(KSL), to teach LLMs to search for essential knowledge from external knowledge\nbases by harnessing their own strong generalizability. Specifically, we design\na simple yet effective prompt to transform retrieval into a multi-hop decision\nsequence, which empowers LLMs with searching knowledge ability in zero-shot\nmanner. Additionally, KSL is able to provide complete retrieval paths and\ntherefore increase explainability of LLMs' reasoning processes. We conduct\nexperiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and\nfound that our approach improves LLM baseline performance by a relatively large\nmargin.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "FLM-101B: An Open LLM and How to Train It with $100K Budget\u2b1b  Large language models (LLMs) have achieved remarkable success in NLP and\nmultimodal tasks, among others. Despite these successes, two main challenges\nremain in developing LLMs: (i) high computational cost, and (ii) fair and\nobjective evaluations. In this paper, we report a solution to significantly\nreduce LLM training cost through a growth strategy. We demonstrate that a\n101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US\ndollars. Inspired by IQ tests, we also consolidate an additional range of\nevaluations on top of existing evaluations that focus on knowledge-oriented\nabilities. These IQ evaluations include symbolic mapping, rule understanding,\npattern mining, and anti-interference. Such evaluations minimize the potential\nimpact of memorization. Experimental results show that our model, named\nFLM-101B, trained with a budget of 100K US dollars, achieves performance\ncomparable to powerful and well-known models, e.g., GPT-3 and GLM-130B,\nespecially on the additional range of IQ evaluations. The checkpoint of\nFLM-101B is released at https://huggingface.co/CofeAI/FLM-101B.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs\u2b1b  Instruction-tuned Large Language Models (LLMs) have recently showcased\nremarkable ability to generate fitting responses to natural language\ninstructions. However, an open research question concerns the inherent biases\nof trained models and their responses. For instance, if the data used to tune\nan LLM is dominantly written by persons with a specific political bias, we\nmight expect generated answers to share this bias. Current research work seeks\nto de-bias such models, or suppress potentially biased answers. With this\ndemonstration, we take a different view on biases in instruction-tuning: Rather\nthan aiming to suppress them, we aim to make them explicit and transparent. To\nthis end, we present OpinionGPT, a web demo in which users can ask questions\nand select all biases they wish to investigate. The demo will answer this\nquestion using a model fine-tuned on text representing each of the selected\nbiases, allowing side-by-side comparison. To train the underlying model, we\nidentified 11 different biases (political, geographic, gender, age) and derived\nan instruction-tuning corpus in which each answer was written by members of one\nof these demographics. This paper presents OpinionGPT, illustrates how we\ntrained the bias-aware model and showcases the web application (available at\nhttps://opiniongpt.informatik.hu-berlin.de).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ImageBind-LLM: Multi-modality Instruction Tuning\u2b1b  We present ImageBind-LLM, a multi-modality instruction tuning method of large\nlanguage models (LLMs) via ImageBind. Existing works mainly focus on language\nand image instruction tuning, different from which, our ImageBind-LLM can\nrespond to multi-modality conditions, including audio, 3D point clouds, video,\nand their embedding-space arithmetic by only image-text alignment training.\nDuring training, we adopt a learnable bind network to align the embedding space\nbetween LLaMA and ImageBind's image encoder. Then, the image features\ntransformed by the bind network are added to word tokens of all layers in\nLLaMA, which progressively injects visual instructions via an attention-free\nand zero-initialized gating mechanism. Aided by the joint embedding of\nImageBind, the simple image-text training enables our model to exhibit superior\nmulti-modality instruction-following capabilities. During inference, the\nmulti-modality inputs are fed into the corresponding ImageBind encoders, and\nprocessed by a proposed visual cache model for further cross-modal embedding\nenhancement. The training-free cache model retrieves from three million image\nfeatures extracted by ImageBind, which effectively mitigates the\ntraining-inference modality discrepancy. Notably, with our approach,\nImageBind-LLM can respond to instructions of diverse modalities and demonstrate\nsignificant language generation quality. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from\n  Chinese Medical Literature\u2b1b  The application of Large Language Models (LLMs) to the medical domain has\nstimulated the interest of researchers. Recent studies have focused on\nconstructing Instruction Fine-Tuning (IFT) data through medical knowledge\ngraphs to enrich the interactive medical knowledge of LLMs. However, the\nmedical literature serving as a rich source of medical knowledge remains\nunexplored. Our work introduces the CALLA dataset to probe LLMs' interactive\nknowledge acquisition from Chinese medical literature. It assesses the\nproficiency of LLMs in mastering medical knowledge through a free-dialogue\nfact-checking task. We identify a phenomenon called the ``fact-following\nresponse``, where LLMs tend to affirm facts mentioned in questions and display\na reluctance to challenge them. To eliminate the inaccurate evaluation caused\nby this phenomenon, for the golden fact, we artificially construct test data\nfrom two perspectives: one consistent with the fact and one inconsistent with\nthe fact. Drawing from the probing experiment on the CALLA dataset, we conclude\nthat IFT data highly correlated with the medical literature corpus serves as a\npotent catalyst for LLMs, enabling themselves to skillfully employ the medical\nknowledge acquired during the pre-training phase within interactive scenarios,\nenhancing accuracy. Furthermore, we design a framework for automatically\nconstructing IFT data based on medical literature and discuss some real-world\napplications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation\u2b1b  Large Language Models (LLMs) have made progress in various real-world tasks,\nwhich stimulates requirements for the evaluation of LLMs. Existing LLM\nevaluation methods are mainly supervised signal-based which depends on static\ndatasets and cannot evaluate the ability of LLMs in dynamic real-world\nscenarios where deep interaction widely exists. Other LLM evaluation methods\nare human-based which are costly and time-consuming and are incapable of\nlarge-scale evaluation of LLMs. To address the issues above, we propose a novel\nDeep Interaction-based LLM-evaluation framework. In our proposed framework,\nLLMs' performances in real-world domains can be evaluated from their deep\ninteraction with other LLMs in elaborately designed evaluation tasks.\nFurthermore, our proposed framework is a general evaluation method that can be\napplied to a host of real-world tasks such as machine translation and code\ngeneration. We demonstrate the effectiveness of our proposed method through\nextensive experiments on four elaborately designed evaluation tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges\u2b1b  Unstructured Electronic Health Record (EHR) data often contains critical\ninformation complementary to imaging data that would inform radiologists'\ndiagnoses. However, time constraints and the large volume of notes frequently\nassociated with individual patients renders manual perusal of such data to\nidentify relevant evidence infeasible in practice. Modern Large Language Models\n(LLMs) provide a flexible means of interacting with unstructured EHR data, and\nmay provide a mechanism to efficiently retrieve and summarize unstructured\nevidence relevant to a given query. In this work, we propose and evaluate an\nLLM (Flan-T5 XXL) for this purpose. Specifically, in a zero-shot setting we\ntask the LLM to infer whether a patient has or is at risk of a particular\ncondition; if so, we prompt the model to summarize the supporting evidence.\nEnlisting radiologists for manual evaluation, we find that this LLM-based\napproach provides outputs consistently preferred to a standard information\nretrieval baseline, but we also highlight the key outstanding challenge: LLMs\nare prone to hallucinating evidence. However, we provide results indicating\nthat model confidence in outputs might indicate when LLMs are hallucinating,\npotentially providing a means to address this.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "When Less is More: Investigating Data Pruning for Pretraining LLMs at\n  Scale\u2b1b  Large volumes of text data have contributed significantly to the development\nof large language models (LLMs) in recent years. This data is typically\nacquired by scraping the internet, leading to pretraining datasets comprised of\nnoisy web text. To date, efforts to prune these datasets down to a higher\nquality subset have relied on hand-crafted heuristics encoded as rule-based\nfilters. In this work, we take a wider view and explore scalable estimates of\ndata quality that can be used to systematically measure the quality of\npretraining data. We perform a rigorous comparison at scale of the simple data\nquality estimator of perplexity, as well as more sophisticated and\ncomputationally intensive estimates of the Error L2-Norm and memorization.\nThese metrics are used to rank and prune pretraining corpora, and we\nsubsequently compare LLMs trained on these pruned datasets. Surprisingly, we\nfind that the simple technique of perplexity outperforms our more\ncomputationally expensive scoring methods. We improve over our no-pruning\nbaseline while training on as little as 30% of the original training dataset.\nOur work sets the foundation for unexplored strategies in automatically\ncurating high quality corpora and suggests the majority of pretraining data can\nbe removed while retaining performance.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Optimize Weight Rounding via Signed Gradient Descent for the\n  Quantization of LLMs\u2b1b  Large Language Models (LLMs) have proven their exceptional capabilities in\nperforming language-related tasks. However, their deployment poses significant\nchallenges due to their considerable memory and storage requirements. In\nresponse to this issue, weight-only quantization, particularly 3 and 4-bit\nweight-only quantization, has emerged as one of the most viable solutions. As\nthe number of bits decreases, the quantization grid broadens, thus emphasizing\nthe importance of up and down rounding. While previous studies have\ndemonstrated that fine-tuning up and down rounding with the addition of\nperturbations can enhance accuracy in some scenarios, our study is driven by\nthe precise and limited boundary of these perturbations, where only the\nthreshold for altering the rounding value is of significance. Consequently, we\npropose a concise and highly effective approach for optimizing the weight\nrounding task. Our method, named SignRound, involves lightweight block-wise\ntuning using signed gradient descent, enabling us to achieve outstanding\nresults within 400 steps. SignRound competes impressively against recent\nmethods without introducing additional inference overhead. The source code will\nbe publicly available at \\url{https://github.com/intel/neural-compressor} soon.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "NExT-GPT: Any-to-Any Multimodal LLM\u2b1b  While recently Multimodal Large Language Models (MM-LLMs) have made exciting\nstrides, they mostly fall prey to the limitation of only input-side multimodal\nunderstanding, without the ability to produce content in multiple modalities.\nAs we humans always perceive the world and communicate with people through\nvarious modalities, developing any-to-any MM-LLMs capable of accepting and\ndelivering content in any modality becomes essential to human-level AI. To fill\nthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,\nNExT-GPT. We connect an LLM with multimodal adaptors and different diffusion\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in\narbitrary combinations of text, images, videos, and audio. By leveraging the\nexisting well-trained highly-performing encoders and decoders, NExT-GPT is\ntuned with only a small amount of parameter (1%) of certain projection layers,\nwhich not only benefits low-cost training and also facilitates convenient\nexpansion to more potential modalities. Moreover, we introduce a\nmodality-switching instruction tuning (MosIT) and manually curate a\nhigh-quality dataset for MosIT, based on which NExT-GPT is empowered with\ncomplex cross-modal semantic understanding and content generation. Overall, our\nresearch showcases the promising possibility of building an AI agent capable of\nmodeling universal modalities, paving the way for more human-like AI research\nin the community. Project page: https://next-gpt.github.io/\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Stochastic LLMs do not Understand Language: Towards Symbolic,\n  Explainable and Ontologically Based LLMs\u2b1b  In our opinion the exuberance surrounding the relative success of data-driven\nlarge language models (LLMs) is slightly misguided and for several reasons (i)\nLLMs cannot be relied upon for factual information since for LLMs all ingested\ntext (factual or non-factual) was created equal; (ii) due to their subsymbolic\nna-ture, whatever 'knowledge' these models acquire about language will always\nbe buried in billions of microfeatures (weights), none of which is meaningful\non its own; and (iii) LLMs will often fail to make the correct inferences in\nseveral linguistic contexts (e.g., nominal compounds, copredication, quantifier\nscope ambi-guities, intensional contexts. Since we believe the relative success\nof data-driven large language models (LLMs) is not a reflection on the symbolic\nvs. subsymbol-ic debate but a reflection on applying the successful strategy of\na bottom-up reverse engineering of language at scale, we suggest in this paper\napplying the effective bottom-up strategy in a symbolic setting resulting in\nsymbolic, explainable, and ontologically grounded language models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Generative Data Augmentation using LLMs improves Distributional\n  Robustness in Question Answering\u2b1b  Robustness in Natural Language Processing continues to be a pertinent issue,\nwhere state of the art models under-perform under naturally shifted\ndistributions. In the context of Question Answering, work on domain adaptation\nmethods continues to be a growing body of research. However, very little\nattention has been given to the notion of domain generalization under natural\ndistribution shifts, where the target domain is unknown. With drastic\nimprovements in the quality and access to generative models, we answer the\nquestion: How do generated datasets influence the performance of QA models\nunder natural distribution shifts? We perform experiments on 4 different\ndatasets under varying amounts of distribution shift, and analyze how\n\"in-the-wild\" generation can help achieve domain generalization. We take a\ntwo-step generation approach, generating both contexts and QA pairs to augment\nexisting datasets. Through our experiments, we demonstrate how augmenting\nreading comprehension datasets with generated data leads to better robustness\ntowards natural distribution shifts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Text Encoders Lack Knowledge: Leveraging Generative LLMs for\n  Domain-Specific Semantic Textual Similarity\u2b1b  Amidst the sharp rise in the evaluation of large language models (LLMs) on\nvarious tasks, we find that semantic textual similarity (STS) has been\nunder-explored. In this study, we show that STS can be cast as a text\ngeneration problem while maintaining strong performance on multiple STS\nbenchmarks. Additionally, we show generative LLMs significantly outperform\nexisting encoder-based STS models when characterizing the semantic similarity\nbetween two texts with complex semantic relationships dependent on world\nknowledge. We validate this claim by evaluating both generative LLMs and\nexisting encoder-based STS models on three newly collected STS challenge sets\nwhich require world knowledge in the domains of Health, Politics, and Sports.\nAll newly collected data is sourced from social media content posted after May\n2023 to ensure the performance of closed-source models like ChatGPT cannot be\ncredited to memorization. Our results show that, on average, generative LLMs\noutperform the best encoder-only baselines by an average of 22.3% on STS tasks\nrequiring world knowledge. Our results suggest generative language models with\nSTS-specific prompting strategies achieve state-of-the-art performance in\ncomplex, domain-specific STS tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness\n  and Ethics\u2b1b  Multi-modal large language models (MLLMs) are trained based on large language\nmodels (LLM), with an enhanced capability to comprehend multi-modal inputs and\ngenerate textual responses. While they excel in multi-modal tasks, the pure NLP\nabilities of MLLMs are often underestimated and left untested. In this study,\nwe get out of the box and unveil an intriguing characteristic of MLLMs -- our\npreliminary results suggest that visual instruction tuning, a prevailing\nstrategy for transitioning LLMs into MLLMs, unexpectedly and interestingly\nhelps models attain both improved truthfulness and ethical alignment in the\npure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model\nsurpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one\nmillion human annotations, on TruthfulQA-mc and Ethics benchmarks. Further\nanalysis reveals that the improved alignment can be attributed to the superior\ninstruction quality inherent to visual-text data. In releasing our code at\ngithub.com/UCSC-VLAA/Sight-Beyond-Text, we aspire to foster further exploration\ninto the intrinsic value of visual-text synergies and, in a broader scope,\nmulti-modal interactions in alignment research.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Less is More for Long Document Summary Evaluation by LLMs\u2b1b  Large Language Models (LLMs) have shown promising performance in summary\nevaluation tasks, yet they face challenges such as high computational costs and\nthe Lost-in-the-Middle problem where important information in the middle of\nlong documents is often overlooked. To address these issues, this paper\nintroduces a novel approach, Extract-then-Evaluate, which involves extracting\nkey sentences from a long source document and then evaluating the summary by\nprompting LLMs. The results reveal that the proposed method not only\nsignificantly reduces evaluation costs but also exhibits a higher correlation\nwith human evaluations. Furthermore, we provide practical recommendations for\noptimal document length and sentence extraction methods, contributing to the\ndevelopment of cost-effective yet more accurate methods for LLM-based text\ngeneration evaluation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Detecting Misinformation with LLM-Predicted Credibility Signals and Weak\n  Supervision\u2b1b  Credibility signals represent a wide range of heuristics that are typically\nused by journalists and fact-checkers to assess the veracity of online content.\nAutomating the task of credibility signal extraction, however, is very\nchallenging as it requires high-accuracy signal-specific extractors to be\ntrained, while there are currently no sufficiently large datasets annotated\nwith all credibility signals. This paper investigates whether large language\nmodels (LLMs) can be prompted effectively with a set of 18 credibility signals\nto produce weak labels for each signal. We then aggregate these potentially\nnoisy labels using weak supervision in order to predict content veracity. We\ndemonstrate that our approach, which combines zero-shot LLM credibility signal\nlabeling and weak supervision, outperforms state-of-the-art classifiers on two\nmisinformation datasets without using any ground-truth labels for training. We\nalso analyse the contribution of the individual credibility signals towards\npredicting content veracity, which provides new valuable insights into their\nrole in misinformation detection.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Generative AI Text Classification using Ensemble LLM Approaches\u2b1b  Large Language Models (LLMs) have shown impressive performance across a\nvariety of Artificial Intelligence (AI) and natural language processing tasks,\nsuch as content creation, report generation, etc. However, unregulated malign\napplication of these models can create undesirable consequences such as\ngeneration of fake news, plagiarism, etc. As a result, accurate detection of\nAI-generated language can be crucial in responsible usage of LLMs. In this\nwork, we explore 1) whether a certain body of text is AI generated or written\nby human, and 2) attribution of a specific language model in generating a body\nof text. Texts in both English and Spanish are considered. The datasets used in\nthis study are provided as part of the Automated Text Identification\n(AuTexTification) shared task. For each of the research objectives stated\nabove, we propose an ensemble neural model that generates probabilities from\ndifferent pre-trained LLMs which are used as features to a Traditional Machine\nLearning (TML) classifier following it. For the first task of distinguishing\nbetween AI and human generated text, our model ranked in fifth and thirteenth\nplace (with macro $F1$ scores of 0.733 and 0.649) for English and Spanish\ntexts, respectively. For the second task on model attribution, our model ranked\nin first place with macro $F1$ scores of 0.625 and 0.653 for English and\nSpanish texts, respectively.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LASER: LLM Agent with State-Space Exploration for Web Navigation\u2b1b  Large language models (LLMs) have been successfully adapted for interactive\ndecision-making tasks like web navigation. While achieving decent performance,\nprevious methods implicitly assume a forward-only execution mode for the model,\nwhere they only provide oracle trajectories as in-context examples to teach the\nmodel how to reason in the interactive environment. Consequently, the model\ncould not handle more challenging scenarios not covered in the in-context\nexamples, e.g., mistakes, leading to sub-optimal performance. To address this\nissue, we propose to model the interactive task as state space exploration,\nwhere the LLM agent transitions among a pre-defined set of states by performing\nactions to complete the task. This formulation enables flexible back-tracking,\nallowing the model to easily recover from errors. We evaluate our proposed LLM\nAgent with State-Space ExploRation (LASER) on the WebShop task. Experimental\nresults show that our LASER agent significantly outperforms previous methods\nand closes the gap with human performance on the web navigation task.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Investigating Answerability of LLMs for Long-Form Question Answering\u2b1b  As we embark on a new era of LLMs, it becomes increasingly crucial to\nunderstand their capabilities, limitations, and differences. Toward making\nfurther progress in this direction, we strive to build a deeper understanding\nof the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective\nopen-source LLMs and their distilled counterparts. To this end, we specifically\nfocus on long-form question answering (LFQA) because it has several practical\nand impactful applications (e.g., troubleshooting, customer service, etc.) yet\nis still understudied and challenging for LLMs. We propose a\nquestion-generation method from abstractive summaries and show that generating\nfollow-up questions from summaries of long documents can create a challenging\nsetting for LLMs to reason and infer from long contexts. Our experimental\nresults confirm that: (1) our proposed method of generating questions from\nabstractive summaries pose a challenging setup for LLMs and shows performance\ngaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2)\nopen-source LLMs exhibit decreased reliance on context for generated questions\nfrom the original document, but their generation capabilities drop\nsignificantly on generated questions from summaries -- especially for longer\ncontexts (>1024 tokens)\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Using Large Language Models for Knowledge Engineering (LLMKE): A Case\n  Study on Wikidata\u2b1b  In this work, we explore the use of Large Language Models (LLMs) for\nknowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.\nFor this task, given subject and relation pairs sourced from Wikidata, we\nutilize pre-trained LLMs to produce the relevant objects in string format and\nlink them to their respective Wikidata QIDs. We developed a pipeline using LLMs\nfor Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata\nentity mapping. The method achieved a macro-averaged F1-score of 0.701 across\nthe properties, with the scores varying from 1.00 to 0.328. These results\ndemonstrate that the knowledge of LLMs varies significantly depending on the\ndomain and that further experimentation is required to determine the\ncircumstances under which LLMs can be used for automatic Knowledge Base (e.g.,\nWikidata) completion and correction. The investigation of the results also\nsuggests the promising contribution of LLMs in collaborative knowledge\nengineering. LLMKE won Track 2 of the challenge. The implementation is\navailable at https://github.com/bohuizhang/LLMKE.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\n  into Multicultural Proverbs and Sayings\u2b1b  Large language models (LLMs) are highly adept at question answering and\nreasoning tasks, but when reasoning in situational context, human expectations\nvary depending on the relevant cultural common ground. As human languages are\nassociated with diverse cultures, LLMs should also be culturally-diverse\nreasoners. In this paper, we study the ability of a wide range of\nstate-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings\nin a conversational context. Our experiments reveal that: (1) mLLMs 'knows'\nlimited proverbs and memorizing proverbs does not mean understanding them\nwithin a conversational context; (2) mLLMs struggle to reason with figurative\nproverbs and sayings, and when asked to select the wrong answer (instead of\nasking it to select the correct answer); and (3) there is a \"culture gap\" in\nmLLMs when reasoning about proverbs and sayings translated from other\nlanguages. We construct and release our evaluation dataset MAPS (MulticultrAl\nProverbs and Sayings) for proverb understanding with conversational context for\nsix different languages.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\n  in the Era of LLMs\u2b1b  The traditional Dialogue State Tracking (DST) problem aims to track user\npreferences and intents in user-agent conversations. While sufficient for\ntask-oriented dialogue systems supporting narrow domain applications, the\nadvent of Large Language Model (LLM)-based chat systems has introduced many\nreal-world intricacies in open-domain dialogues. These intricacies manifest in\nthe form of increased complexity in contextual interactions, extended dialogue\nsessions encompassing a diverse array of topics, and more frequent contextual\nshifts. To handle these intricacies arising from evolving LLM-based chat\nsystems, we propose joint dialogue segmentation and state tracking per segment\nin open-domain dialogue systems. Assuming a zero-shot setting appropriate to a\ntrue open-domain dialogue system, we propose S3-DST, a structured prompting\ntechnique that harnesses Pre-Analytical Recollection, a novel grounding\nmechanism we designed for improving long context tracking. To demonstrate the\nefficacy of our proposed approach in joint segmentation and state tracking, we\nevaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as\nwell as publicly available DST and segmentation datasets. Across all datasets\nand settings, S3-DST consistently outperforms the state-of-the-art,\ndemonstrating its potency and robustness the next generation of LLM-based chat\nsystems.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\n  Nationality Bias in Generative Models\u2b1b  LLMs are increasingly powerful and widely used to assist users in a variety\nof tasks. This use risks the introduction of LLM biases to consequential\ndecisions such as job hiring, human performance evaluation, and criminal\nsentencing. Bias in NLP systems along the lines of gender and ethnicity has\nbeen widely studied, especially for specific stereotypes (e.g., Asians are good\nat math). In this paper, we investigate bias along less studied, but still\nconsequential, dimensions, such as age and beauty, measuring subtler correlated\ndecisions that LLMs (specially autoregressive language models) make between\nsocial groups and unrelated positive and negative attributes. We ask whether\nLLMs hold wide-reaching biases of positive or negative sentiment for specific\nsocial groups similar to the ``what is beautiful is good'' bias found in people\nin experimental psychology. We introduce a template-generated dataset of\nsentence completion tasks that asks the model to select the most appropriate\nattribute to complete an evaluative statement about a person described as a\nmember of a specific social group. We also reverse the completion task to\nselect the social group based on an attribute. Finally, we report the\ncorrelations that we find for multiple cutting-edge LLMs. This dataset can be\nused as a benchmark to evaluate progress in more generalized biases and the\ntemplating technique can be used to expand the benchmark with minimal\nadditional human annotation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Fabricator: An Open Source Toolkit for Generating Labeled Training Data\n  with Teacher LLMs\u2b1b  Most NLP tasks are modeled as supervised learning and thus require labeled\ntraining data to train effective models. However, manually producing such data\nat sufficient quality and quantity is known to be costly and time-intensive.\nCurrent research addresses this bottleneck by exploring a novel paradigm called\nzero-shot learning via dataset generation. Here, a powerful LLM is prompted\nwith a task description to generate labeled data that can be used to train a\ndownstream NLP model. For instance, an LLM might be prompted to \"generate 500\nmovie reviews with positive overall sentiment, and another 500 with negative\nsentiment.\" The generated data could then be used to train a binary sentiment\nclassifier, effectively leveraging an LLM as a teacher to a smaller student\nmodel. With this demo, we introduce Fabricator, an open-source Python toolkit\nfor dataset generation. Fabricator implements common dataset generation\nworkflows, supports a wide range of downstream NLP tasks (such as text\nclassification, question answering, and entity recognition), and is integrated\nwith well-known libraries to facilitate quick experimentation. With Fabricator,\nwe aim to support researchers in conducting reproducible dataset generation\nexperiments using LLMs and help practitioners apply this approach to train\nmodels for downstream tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM4Jobs: Unsupervised occupation extraction and standardization\n  leveraging Large Language Models\u2b1b  Automated occupation extraction and standardization from free-text job\npostings and resumes are crucial for applications like job recommendation and\nlabor market policy formation. This paper introduces LLM4Jobs, a novel\nunsupervised methodology that taps into the capabilities of large language\nmodels (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the\nnatural language understanding and generation capacities of LLMs. Evaluated on\nrigorous experimentation on synthetic and real-world datasets, we demonstrate\nthat LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks,\ndemonstrating its versatility across diverse datasets and granularities. As a\nside result of our work, we present both synthetic and real-world datasets,\nwhich may be instrumental for subsequent research in this domain. Overall, this\ninvestigation highlights the promise of contemporary LLMs for the intricate\ntask of occupation extraction and standardization, laying the foundation for a\nrobust and adaptable framework relevant to both research and industrial\ncontexts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Few-Shot Adaptation for Parsing Contextual Utterances with LLMs\u2b1b  We evaluate the ability of semantic parsers based on large language models\n(LLMs) to handle contextual utterances. In real-world settings, there typically\nexists only a limited number of annotated contextual utterances due to\nannotation cost, resulting in an imbalance compared to non-contextual\nutterances. Therefore, parsers must adapt to contextual utterances with a few\ntraining examples. We examine four major paradigms for doing so in\nconversational semantic parsing i.e., Parse-with-Utterance-History,\nParse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. To\nfacilitate such cross-paradigm comparisons, we construct\nSMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow with\nadditional annotations. Experiments with in-context learning and fine-tuning\nsuggest that Rewrite-then-Parse is the most promising paradigm when\nholistically considering parsing accuracy, annotation cost, and error types.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Platform Security: Applying a Systematic Evaluation Framework to\n  OpenAI's ChatGPT Plugins\u2b1b  Large language model (LLM) platforms, such as ChatGPT, have recently begun\noffering a plugin ecosystem to interface with third-party services on the\ninternet. While these plugins extend the capabilities of LLM platforms, they\nare developed by arbitrary third parties and thus cannot be implicitly trusted.\nPlugins also interface with LLM platforms and users using natural language,\nwhich can have imprecise interpretations. In this paper, we propose a framework\nthat lays a foundation for LLM platform designers to analyze and improve the\nsecurity, privacy, and safety of current and future plugin-integrated LLM\nplatforms. Our framework is a formulation of an attack taxonomy that is\ndeveloped by iteratively exploring how LLM platform stakeholders could leverage\ntheir capabilities and responsibilities to mount attacks against each other. As\npart of our iterative process, we apply our framework in the context of\nOpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the\npotential for the types of issues that we outline in our attack taxonomy. We\nconclude by discussing novel challenges and by providing recommendations to\nimprove the security, privacy, and safety of present and future LLM-based\ncomputing platforms.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion\n  Recognition\u2b1b  In this paper, we explored how to boost speech emotion recognition (SER) with\nthe state-of-the-art speech pre-trained model (PTM), data2vec, text generation\ntechnique, GPT-4, and speech synthesis technique, Azure TTS. First, we\ninvestigated the representation ability of different speech self-supervised\npre-trained models, and we found that data2vec has a good representation\nability on the SER task. Second, we employed a powerful large language model\n(LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate\nemotionally congruent text and speech. We carefully designed the text prompt\nand dataset construction, to obtain the synthetic emotional speech data with\nhigh quality. Third, we studied different ways of data augmentation to promote\nthe SER task with synthetic speech, including random mixing, adversarial\ntraining, transfer learning, and curriculum learning. Experiments and ablation\nstudies on the IEMOCAP dataset demonstrate the effectiveness of our method,\ncompared with other data augmentation methods, and data augmentation with other\nsynthetic data.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PoSE: Efficient Context Window Extension of LLMs via Positional\n  Skip-wise Training\u2b1b  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Model Leeching: An Extraction Attack Targeting LLMs\u2b1b  Model Leeching is a novel extraction attack targeting Large Language Models\n(LLMs), capable of distilling task-specific knowledge from a target LLM into a\nreduced parameter model. We demonstrate the effectiveness of our attack by\nextracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match\n(EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%,\nrespectively for only $50 in API cost. We further demonstrate the feasibility\nof adversarial attack transferability from an extracted model extracted via\nModel Leeching to perform ML attack staging against a target LLM, resulting in\nan 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language\n  Feedback\u2b1b  To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SlimPajama-DC: Understanding Data Combinations for LLM Training\u2b1b  This paper aims to understand the impacts of various data combinations (e.g.,\nweb text, wikipedia, github, books) on the training of large language models\nusing SlimPajama. SlimPajama is a rigorously deduplicated, multi-source\ndataset, which has been refined and further deduplicated to 627B tokens from\nthe extensive 1.2T tokens RedPajama dataset contributed by Together. We've\ntermed our research as SlimPajama-DC, an empirical analysis designed to uncover\nfundamental characteristics and best practices associated with employing\nSlimPajama in the training of large language models. During our research with\nSlimPajama, two pivotal observations emerged: (1) Global deduplication vs.\nlocal deduplication. We analyze and discuss how global (across different\nsources of datasets) and local (within the single source of dataset)\ndeduplications affect the performance of trained models. (2) Proportions of\nhigh-quality/highly-deduplicated multi-source datasets in the combination. To\nstudy this, we construct six configurations of SlimPajama dataset and train\nindividual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best\nconfiguration outperforms the 1.3B model trained on RedPajama using the same\nnumber of training tokens by a significant margin. All our 1.3B models are\ntrained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16\nmixed precision. We further extend our discoveries (such as increasing data\ndiversity is crucial after global deduplication) on a 7B model with large\nbatch-size training. Our models and the separate SlimPajama-DC datasets are\navailable at: https://huggingface.co/MBZUAI-LLM and\nhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for\n  Knowledge Graph Question Answering\u2b1b  Despite their competitive performance on knowledge-intensive tasks, large\nlanguage models (LLMs) still have limitations in memorizing all world knowledge\nespecially long tail knowledge. In this paper, we study the KG-augmented\nlanguage model approach for solving the knowledge graph question answering\n(KGQA) task that requires rich world knowledge. Existing work has shown that\nretrieving KG knowledge to enhance LLMs prompting can significantly improve\nLLMs performance in KGQA. However, their approaches lack a well-formed\nverbalization of KG knowledge, i.e., they ignore the gap between KG\nrepresentations and textual representations. To this end, we propose an\nanswer-sensitive KG-to-Text approach that can transform KG knowledge into\nwell-textualized statements most informative for KGQA. Based on this approach,\nwe propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.\nExperiments on several KGQA benchmarks show that the proposed KG-to-Text\naugmented LLMs approach outperforms previous KG-augmented LLMs approaches\nregarding answer accuracy and usefulness of knowledge statements.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CPLLM: Clinical Prediction with Large Language Models\u2b1b  We present Clinical Prediction with Large Language Models (CPLLM), a method\nthat involves fine-tuning a pre-trained Large Language Model (LLM) for clinical\ndisease prediction. We utilized quantization and fine-tuned the LLM using\nprompts, with the task of predicting whether patients will be diagnosed with a\ntarget disease during their next visit or in the subsequent diagnosis,\nleveraging their historical diagnosis records. We compared our results versus\nvarious baselines, including Logistic Regression, RETAIN, and Med-BERT, which\nis the current state-of-the-art model for disease prediction using structured\nEHR data. Our experiments have shown that CPLLM surpasses all the tested models\nin terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements\ncompared to the baseline models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal\n  Services\u2b1b  We propose DISC-LawLLM, an intelligent legal system utilizing large language\nmodels (LLMs) to provide a wide range of legal services. We adopt legal\nsyllogism prompting strategies to construct supervised fine-tuning datasets in\nthe Chinese Judicial domain and fine-tune LLMs with legal reasoning capability.\nWe augment LLMs with a retrieval module to enhance models' ability to access\nand utilize external legal knowledge. A comprehensive legal benchmark,\nDISC-Law-Eval, is presented to evaluate intelligent legal systems from both\nobjective and subjective dimensions. Quantitative and qualitative results on\nDISC-Law-Eval demonstrate the effectiveness of our system in serving various\nusers across diverse legal scenarios. The detailed resources are available at\nhttps://github.com/FudanDISC/DISC-LawLLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Safurai 001: New Qualitative Approach for Code LLM Evaluation\u2b1b  This paper presents Safurai-001, a new Large Language Model (LLM) with\nsignificant potential in the domain of coding assistance. Driven by recent\nadvancements in coding LLMs, Safurai-001 competes in performance with the\nlatest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al.,\n2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more\nconversational interaction. By capitalizing on the progress in data engineering\n(including latest techniques of data transformation and prompt engineering) and\ninstruction tuning, this new model promises to stand toe-to-toe with recent\nclosed and open source developments. Recognizing the need for an efficacious\nevaluation metric for coding LLMs, this paper also introduces GPT4-based\nMultiParameters, an evaluation benchmark that harnesses varied parameters to\npresent a comprehensive insight into the models functioning and performance.\nOur assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and\nWizardCoder by 18.78% in the Code Readability parameter and more.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DreamLLM: Synergistic Multimodal Comprehension and Creation\u2b1b  This paper presents DreamLLM, a learning framework that first achieves\nversatile Multimodal Large Language Models (MLLMs) empowered with frequently\noverlooked synergy between multimodal comprehension and creation. DreamLLM\noperates on two fundamental principles. The first focuses on the generative\nmodeling of both language and image posteriors by direct sampling in the raw\nmultimodal space. This approach circumvents the limitations and information\nloss inherent to external feature extractors like CLIP, and a more thorough\nmultimodal understanding is obtained. Second, DreamLLM fosters the generation\nof raw, interleaved documents, modeling both text and image contents, along\nwith unstructured layouts. This allows DreamLLM to learn all conditional,\nmarginal, and joint multimodal distributions effectively. As a result, DreamLLM\nis the first MLLM capable of generating free-form interleaved content.\nComprehensive experiments highlight DreamLLM's superior performance as a\nzero-shot multimodal generalist, reaping from the enhanced learning synergy.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards LLM-based Autograding for Short Textual Answers\u2b1b  Grading of exams is an important, labor intensive, subjective, repetitive and\nfrequently challenging task. The feasibility of autograding textual responses\nhas greatly increased thanks to the availability of large language models\n(LLMs) such as ChatGPT and because of the substantial influx of data brought\nabout by digitalization. However, entrusting AI models with decision-making\nroles raises ethical considerations, mainly stemming from potential biases and\nissues related to generating false information. Thus, in this manuscript we\nprovide an evaluation of a large language model for the purpose of autograding,\nwhile also highlighting how LLMs can support educators in validating their\ngrading procedures. Our evaluation is targeted towards automatic short textual\nanswers grading (ASAG), spanning various languages and examinations from two\ndistinct courses. Our findings suggest that while \"out-of-the-box\" LLMs provide\na valuable tool to provide a complementary perspective, their readiness for\nindependent automated grading remains a work in progress, necessitating human\noversight.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Guided Inductive Inference for Solving Compositional Problems\u2b1b  While large language models (LLMs) have demonstrated impressive performance\nin question-answering tasks, their performance is limited when the questions\nrequire knowledge that is not included in the model's training data and can\nonly be acquired through direct observation or interaction with the real world.\nExisting methods decompose reasoning tasks through the use of modules invoked\nsequentially, limiting their ability to answer deep reasoning tasks. We\nintroduce a method, Recursion based extensible LLM (REBEL), which handles\nopen-world, deep reasoning tasks by employing automated reasoning techniques\nlike dynamic planning and forward-chaining strategies. REBEL allows LLMs to\nreason via recursive problem decomposition and utilization of external tools.\nThe tools that REBEL uses are specified only by natural language description.\nWe further demonstrate REBEL capabilities on a set of problems that require a\ndeeply nested use of external tools in a compositional and conversational\nsetting.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Memory-Augmented LLM Personalization with Short- and Long-Term Memory\n  Coordination\u2b1b  Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable\nproficiency in comprehending and generating natural language. However, their\nunpersonalized generation paradigm may result in suboptimal user-specific\noutcomes. Typically, users converse differently based on their knowledge and\npreferences. This necessitates the task of enhancing user-oriented LLM which\nremains unexplored. While one can fully train an LLM for this objective, the\nresource consumption is unaffordable. Prior research has explored memory-based\nmethods to store and retrieve knowledge to enhance generation without\nretraining for new queries. However, we contend that a mere memory module is\ninadequate to comprehend a user's preference, and fully training an LLM can be\nexcessively costly. In this study, we propose a novel computational bionic\nmemory mechanism, equipped with a parameter-efficient fine-tuning schema, to\npersonalize LLMs. Our extensive experimental results demonstrate the\neffectiveness and superiority of the proposed approach. To encourage further\nresearch into this area, we are releasing a new conversation dataset generated\nentirely by LLM based on an open-source medical corpus, as well as our\nimplementation code.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Goal-Oriented Prompt Attack and Safety Evaluation for LLMs\u2b1b  Large Language Models (LLMs) presents significant priority in text\nunderstanding and generation. However, LLMs suffer from the risk of generating\nharmful contents especially while being employed to applications. There are\nseveral black-box attack methods, such as Prompt Attack, which can change the\nbehaviour of LLMs and induce LLMs to generate unexpected answers with harmful\ncontents. Researchers are interested in Prompt Attack and Defense with LLMs,\nwhile there is no publicly available dataset with high successful attacking\nrate to evaluate the abilities of defending prompt attack. In this paper, we\nintroduce a pipeline to construct high-quality prompt attack samples, along\nwith a Chinese prompt attack dataset called CPAD. Our prompts aim to induce\nLLMs to generate unexpected outputs with several carefully designed prompt\nattack templates and widely concerned attacking contents. Different from\nprevious datasets involving safety estimation, we construct the prompts\nconsidering three dimensions: contents, attacking methods and goals.\nEspecially, the attacking goals indicate the behaviour expected after\nsuccessfully attacking the LLMs, thus the responses can be easily evaluated and\nanalysed. We run several popular Chinese LLMs on our dataset, and the results\nshow that our prompts are significantly harmful to LLMs, with around 70% attack\nsuccess rate to GPT-3.5. CPAD is publicly available at\nhttps://github.com/liuchengyuan123/CPAD.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "InstructERC: Reforming Emotion Recognition in Conversation with a\n  Retrieval Multi-task LLMs Framework\u2b1b  The development of emotion recognition in dialogue (ERC) has been\nconsistently hindered by the complexity of pipeline designs, leading to ERC\nmodels that often overfit to specific datasets and dialogue patterns. In this\nstudy, we propose a novel approach, namely\n  InstructERC, to reformulates the ERC task from a discriminative framework to\na generative framework based on Large Language Models (LLMs) . InstructERC has\ntwo significant contributions: Firstly, InstructERC introduces a simple yet\neffective retrieval template module, which helps the model explicitly integrate\nmulti-granularity dialogue supervision information by concatenating the\nhistorical dialog content, label statement, and emotional domain demonstrations\nwith high semantic similarity. Furthermore, we introduce two additional emotion\nalignment tasks, namely speaker identification and emotion prediction tasks, to\nimplicitly model the dialogue role relationships and future emotional\ntendencies in conversations. Our LLM-based plug-and-play plugin framework\nsignificantly outperforms all previous models and achieves comprehensive SOTA\non three commonly used ERC datasets. Extensive analysis of parameter-efficient\nand data-scaling experiments provide empirical guidance for applying\nInstructERC in practical scenarios. Our code will be released after blind\nreview.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset\u2b1b  Studying how people interact with large language models (LLMs) in real-world\nscenarios is increasingly important due to their widespread use in various\napplications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset\ncontaining one million real-world conversations with 25 state-of-the-art LLMs.\nThis dataset is collected from 210K unique IP addresses in the wild on our\nVicuna demo and Chatbot Arena website. We offer an overview of the dataset's\ncontent, including its curation process, basic statistics, and topic\ndistribution, highlighting its diversity, originality, and scale. We\ndemonstrate its versatility through four use cases: developing content\nmoderation models that perform similarly to GPT-4, building a safety benchmark,\ntraining instruction-following models that perform similarly to Vicuna, and\ncreating challenging benchmark questions. We believe that this dataset will\nserve as a valuable resource for understanding and advancing LLM capabilities.\nThe dataset is publicly available at\nhttps://huggingface.co/datasets/lmsys/lmsys-chat-1m.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMR: Real-time Prompting of Interactive Worlds using Large Language\n  Models\u2b1b  We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"\u2b1b  We expose a surprising failure of generalization in auto-regressive large\nlanguage models (LLMs). If a model is trained on a sentence of the form \"A is\nB\", it will not automatically generalize to the reverse direction \"B is A\".\nThis is the Reversal Curse. For instance, if a model is trained on \"Olaf Scholz\nwas the ninth Chancellor of Germany\", it will not automatically be able to\nanswer the question, \"Who was the ninth Chancellor of Germany?\". Moreover, the\nlikelihood of the correct answer (\"Olaf Scholz\") will not be higher than for a\nrandom name. Thus, models exhibit a basic failure of logical deduction and do\nnot generalize a prevalent pattern in their training set (i.e. if \"A is B''\noccurs, \"B is A\" is more likely to occur). We provide evidence for the Reversal\nCurse by finetuning GPT-3 and Llama-1 on fictitious statements such as \"Uriah\nHawthorne is the composer of 'Abyssal Melodies'\" and showing that they fail to\ncorrectly answer \"Who composed 'Abyssal Melodies?'\". The Reversal Curse is\nrobust across model sizes and model families and is not alleviated by data\naugmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about\nreal-world celebrities, such as \"Who is Tom Cruise's mother? [A: Mary Lee\nPfeiffer]\" and the reverse \"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly\nanswers questions like the former 79% of the time, compared to 33% for the\nlatter. This shows a failure of logical deduction that we hypothesize is caused\nby the Reversal Curse. Code is available at\nhttps://github.com/lukasberglund/reversal_curse.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language\n  Model as an Agent\u2b1b  3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ .\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs Augment Low-Resource Reading Comprehension Datasets?\n  Opportunities and Challenges\u2b1b  Large Language Models (LLMs) have demonstrated impressive zero shot\nperformance on a wide range of NLP tasks, demonstrating the ability to reason\nand apply commonsense. A relevant application is to use them for creating high\nquality synthetic datasets for downstream tasks. In this work, we probe whether\nGPT-4 can be used to augment existing extractive reading comprehension\ndatasets. Automating data annotation processes has the potential to save large\namounts of time, money and effort that goes into manually labelling datasets.\nIn this paper, we evaluate the performance of GPT-4 as a replacement for human\nannotators for low resource reading comprehension tasks, by comparing\nperformance after fine tuning, and the cost associated with annotation. This\nwork serves to be the first analysis of LLMs as synthetic data augmenters for\nQA systems, highlighting the unique opportunities and challenges. Additionally,\nwe release augmented versions of low resource datasets, that will allow the\nresearch community to create further benchmarks for evaluation of generated\ndatasets.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among\n  Diverse LLMs\u2b1b  Large Language Models (LLMs) still struggle with complex reasoning tasks.\nMotivated by the society of minds (Minsky, 1988), we propose ReConcile, a\nmulti-model multi-agent framework designed as a round table conference among\ndiverse LLM agents to foster diverse thoughts and discussion for improved\nconsensus. ReConcile enhances the reasoning capabilities of LLMs by holding\nmultiple rounds of discussion, learning to convince other agents to improve\ntheir answers, and employing a confidence-weighted voting mechanism. In each\nround, ReConcile initiates discussion between agents via a 'discussion prompt'\nthat consists of (a) grouped answers and explanations generated by each agent\nin the previous round, (b) their uncertainties, and (c) demonstrations of\nanswer-rectifying human explanations, used for convincing other agents. This\ndiscussion prompt enables each agent to revise their responses in light of\ninsights from other agents. Once a consensus is reached and the discussion\nends, ReConcile determines the final answer by leveraging the confidence of\neach agent in a weighted voting scheme. We implement ReConcile with ChatGPT,\nBard, and Claude2 as the three agents. Our experimental results on various\nbenchmarks demonstrate that ReConcile significantly enhances the reasoning\nperformance of the agents (both individually and as a team), surpassing prior\nsingle-agent and multi-agent baselines by 7.7% and also outperforming GPT-4 on\nsome of these datasets. We also experiment with GPT-4 itself as one of the\nagents in ReConcile and demonstrate that its initial performance also improves\nby absolute 10.0% through discussion and feedback from other agents. Finally,\nwe also analyze the accuracy after every round and observe that ReConcile\nachieves better and faster consensus between agents, compared to a multi-agent\ndebate baseline. Our code is available at: https://github.com/dinobby/ReConcile\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls\n  of Large Language Models on Bengali NLP\u2b1b  Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in natural language processing (NLP) for their impressive skills\nin language generation and other language-specific tasks. Though LLMs have been\nevaluated in various tasks, mostly in English, they have not yet undergone\nthorough evaluation in under-resourced languages such as Bengali (Bangla). In\nthis paper, we evaluate the performance of LLMs for the low-resourced Bangla\nlanguage. We select various important and diverse Bangla NLP tasks, such as\nabstractive summarization, question answering, paraphrasing, natural language\ninference, text classification, and sentiment analysis for zero-shot evaluation\nwith ChatGPT, LLaMA-2, and Claude-2 and compare the performance with\nstate-of-the-art fine-tuned models. Our experimental results demonstrate an\ninferior performance of LLMs for different Bangla NLP tasks, calling for\nfurther effort to develop better understanding of LLMs in low-resource\nlanguages like Bangla.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Effective Distillation of Table-based Reasoning Ability from LLMs\u2b1b  Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, their remarkable\nparameter size and their impressive high requirement of computing resources\npose challenges for their practical deployment. Recent research has revealed\nthat specific capabilities of LLMs, such as numerical reasoning, can be\ntransferred to smaller models through distillation. Some studies explore the\npotential of leveraging LLMs to perform table-based reasoning. Nevertheless,\nprior to our work, there has been no investigation into the prospect of\nspecialising table reasoning skills in smaller models specifically tailored for\ntable-to-text generation tasks. In this paper, we propose a novel table-based\nreasoning distillation, with the aim of distilling distilling LLMs into\ntailored, smaller models specifically designed for table-based reasoning task.\nExperimental results have shown that a 0.22 billion parameter model\n(Flan-T5-base) fine-tuned using distilled data, not only achieves a significant\nimprovement compared to traditionally fine-tuned baselines but also surpasses\nspecific LLMs like gpt-3.5-turbo on the scientific table-to-text generation\ndataset (SciGen). The code and data are released in\nhttps://github.com/Bernard-Yang/TableDistill.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Calibrating LLM-Based Evaluator\u2b1b  Recent advancements in large language models (LLMs) on language modeling and\nemergent capabilities make them a promising reference-free evaluator of natural\nlanguage generation quality, and a competent alternative to human evaluation.\nHowever, hindered by the closed-source or high computational demand to host and\ntune, there is a lack of practice to further calibrate an off-the-shelf\nLLM-based evaluator towards better human alignment. In this work, we propose\nAutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate\nand align an LLM-based evaluator toward human preference. Instead of explicitly\nmodeling human preferences, we first implicitly encompass them within a set of\nhuman labels. Then, an initial set of scoring criteria is drafted by the\nlanguage model itself, leveraging in-context learning on different few-shot\nexamples. To further calibrate this set of criteria, we select the best\nperformers and re-draft them with self-refinement. Our experiments on multiple\ntext quality evaluation datasets illustrate a significant improvement in\ncorrelation with expert evaluation through calibration. Our comprehensive\nqualitative analysis conveys insightful intuitions and observations on the\nessence of effective scoring criteria.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs as Counterfactual Explanation Modules: Can ChatGPT Explain\n  Black-box Text Classifiers?\u2b1b  Large language models (LLMs) are increasingly being used for tasks beyond\ntext generation, including complex tasks such as data labeling, information\nextraction, etc. With the recent surge in research efforts to comprehend the\nfull extent of LLM capabilities, in this work, we investigate the role of LLMs\nas counterfactual explanation modules, to explain decisions of black-box text\nclassifiers. Inspired by causal thinking, we propose a pipeline for using LLMs\nto generate post-hoc, model-agnostic counterfactual explanations in a\nprincipled way via (i) leveraging the textual understanding capabilities of the\nLLM to identify and extract latent features, and (ii) leveraging the\nperturbation and generation capabilities of the same LLM to generate a\ncounterfactual explanation by perturbing input features derived from the\nextracted latent features. We evaluate three variants of our framework, with\nvarying degrees of specificity, on a suite of state-of-the-art LLMs, including\nChatGPT and LLaMA 2. We evaluate the effectiveness and quality of the generated\ncounterfactual explanations, over a variety of text classification benchmarks.\nOur results show varied performance of these models in different settings, with\na full two-step feature extraction based variant outperforming others in most\ncases. Our pipeline can be used in automated explanation systems, potentially\nreducing human effort.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using\n  Iterative In-Context-Learning\u2b1b  From grading papers to summarizing medical documents, large language models\n(LLMs) are evermore used for evaluation of text generated by humans and AI\nalike. However, despite their extensive utility, LLMs exhibit distinct failure\nmodes, necessitating a thorough audit and improvement of their text evaluation\ncapabilities. Here we introduce ALLURE, a systematic approach to Auditing Large\nLanguage Models Understanding and Reasoning Errors. ALLURE involves comparing\nLLM-generated evaluations with annotated data, and iteratively incorporating\ninstances of significant deviation into the evaluator, which leverages\nin-context learning (ICL) to enhance and improve robust evaluation of text by\nLLMs. Through this iterative process, we refine the performance of the\nevaluator LLM, ultimately reducing reliance on human annotators in the\nevaluation process. We anticipate ALLURE to serve diverse applications of LLMs\nin various domains related to evaluation of textual data, such as medical\nsummarization, education, and and productivity.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLM-Generated Misinformation Be Detected?\u2b1b  The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot\n  Compression\u2b1b  Low Rank Decomposition of matrix - splitting a large matrix into a product of\ntwo smaller matrix offers a means for compression that reduces the parameters\nof a model without sparsification, and hence delivering more speedup on modern\nhardware. Moreover, unlike quantization, the compressed linear layers remain\nfully differentiable and all the parameters trainable, while being able to\nleverage the existing highly efficient kernels over floating point matrices. We\nstudy the potential to compress Large Language Models (LLMs) for monolingual\nCode generation via Low Rank Decomposition (LoRD) and observe that ranks for\nthe linear layers in these models can be reduced by upto 39.58% with less than\n1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to\ncompress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with\nminimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single\nA100. The compressed models speeds up inference by up to 22.35% with just a\nsingle line of change in code over huggingface's implementation with pytorch\nbackend. Low Rank Decomposition (LoRD) models remain compatible with state of\nthe art near-lossless quantization method such as SpQR, which allows leveraging\nfurther compression gains of quantization. Lastly, QLoRA over Low Rank\nDecomposition (LoRD) model further reduces memory requirements by as much as\n21.2% over vanilla QLoRA while offering similar gains from parameter efficient\nfine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new\nparadigm for LLM compression.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM\u2b1b  Recently, Large Language Models (LLMs) have made significant advancements and\nare now widely used across various domains. Unfortunately, there has been a\nrising concern that LLMs can be misused to generate harmful or malicious\ncontent. Though a line of research has focused on aligning LLMs with human\nvalues and preventing them from producing inappropriate content, such\nalignments are usually vulnerable and can be bypassed by alignment-breaking\nattacks via adversarially optimized or handcrafted jailbreaking prompts. In\nthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against\npotential alignment-breaking attacks. RA-LLM can be directly constructed upon\nan existing aligned LLM with a robust alignment checking function, without\nrequiring any expensive retraining or fine-tuning process of the original LLM.\nFurthermore, we also provide a theoretical analysis for RA-LLM to verify its\neffectiveness in defending against alignment-breaking attacks. Through\nreal-world experiments on open-source large language models, we demonstrate\nthat RA-LLM can successfully defend against both state-of-the-art adversarial\nprompts and popular handcrafted jailbreaking prompts by reducing their attack\nsuccess rates from nearly 100% to around 10% or less.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language\n  Models\u2b1b  The carbon footprint associated with large language models (LLMs) is a\nsignificant concern, encompassing emissions from their training, inference,\nexperimentation, and storage processes, including operational and embodied\ncarbon emissions. An essential aspect is accurately estimating the carbon\nimpact of emerging LLMs even before their training, which heavily relies on GPU\nusage. Existing studies have reported the carbon footprint of LLM training, but\nonly one tool, mlco2, can predict the carbon footprint of new neural networks\nprior to physical training. However, mlco2 has several serious limitations. It\ncannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,\ndisregards critical architectural parameters, focuses solely on GPUs, and\ncannot model embodied carbon footprints. Addressing these gaps, we introduce\n\\textit{LLMCarbon}, an end-to-end carbon footprint projection model designed\nfor both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly\nenhances the accuracy of carbon footprint estimations for various LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Natural Language based Context Modeling and Reasoning with LLMs: A\n  Tutorial\u2b1b  Large language models (LLMs) have become phenomenally surging, since\n2018--two decades after introducing context-awareness into computing systems.\nThrough taking into account the situations of ubiquitous devices, users and the\nsocieties, context-aware computing has enabled a wide spectrum of innovative\napplications, such as assisted living, location-based social network services\nand so on. To recognize contexts and make decisions for actions accordingly,\nvarious artificial intelligence technologies, such as Ontology and OWL, have\nbeen adopted as representations for context modeling and reasoning. Recently,\nwith the rise of LLMs and their improved natural language understanding and\nreasoning capabilities, it has become feasible to model contexts using natural\nlanguage and perform context reasoning by interacting with LLMs such as ChatGPT\nand GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and\nautonomous agents (AutoAgents) that enable LLMs to perform context modeling and\nreasoning without requiring fine-tuning of the model. We organize and introduce\nworks in the related field, and name this computing paradigm as the LLM-driven\nContext-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors\nreading data, and the command to actuators are supposed to be represented as\ntexts. Given the text of users' request and sensor data, the AutoAgent models\nthe context by prompting and sends to the LLM for context reasoning. LLM\ngenerates a plan of actions and responds to the AutoAgent, which later follows\nthe action plan to foster context-awareness. To prove the concepts, we use two\nshowcases--(1) operating a mobile z-arm in an apartment for assisted living,\nand (2) planning a trip and scheduling the itinerary in a context-aware and\npersonalized manner.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided\n  Planning\u2b1b  Although recent text-to-video (T2V) generation methods have seen significant\nadvancements, most of these works focus on producing short video clips of a\nsingle event with a single background (i.e., single-scene videos). Meanwhile,\nrecent large language models (LLMs) have demonstrated their capability in\ngenerating layouts and programs to control downstream visual modules such as\nimage generation models. This raises an important question: can we leverage the\nknowledge embedded in these LLMs for temporally consistent long video\ngeneration? In this paper, we propose VideoDirectorGPT, a novel framework for\nconsistent multi-scene video generation that uses the knowledge of LLMs for\nvideo content planning and grounded video generation. Specifically, given a\nsingle text prompt, we first ask our video planner LLM (GPT-4) to expand it\ninto a 'video plan', which involves generating the scene descriptions, the\nentities with their respective layouts, the background for each scene, and\nconsistency groupings of the entities and backgrounds. Next, guided by this\noutput from the video planner, our video generator, Layout2Vid, has explicit\ncontrol over spatial layouts and can maintain temporal consistency of\nentities/backgrounds across scenes, while only trained with image-level\nannotations. Our experiments demonstrate that VideoDirectorGPT framework\nsubstantially improves layout and movement control in both single- and\nmulti-scene video generation and can generate multi-scene videos with visual\nconsistency across scenes, while achieving competitive performance with SOTAs\nin open-domain single-scene T2V generation. We also demonstrate that our\nframework can dynamically control the strength for layout guidance and can also\ngenerate videos with user-provided images. We hope our framework can inspire\nfuture work on better integrating the planning ability of LLMs into consistent\nlong video generation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Beyond the Chat: Executable and Verifiable Text-Editing with LLMs\u2b1b  Conversational interfaces powered by Large Language Models (LLMs) have\nrecently become a popular way to obtain feedback during document editing.\nHowever, standard chat-based conversational interfaces do not support\ntransparency and verifiability of the editing changes that they suggest. To\ngive the author more agency when editing with an LLM, we present InkSync, an\nediting interface that suggests executable edits directly within the document\nbeing edited. Because LLMs are known to introduce factual errors, Inksync also\nsupports a 3-stage approach to mitigate this risk: Warn authors when a\nsuggested edit introduces new information, help authors Verify the new\ninformation's accuracy through external search, and allow an auditor to perform\nan a-posteriori verification by Auditing the document via a trace of all\nauto-generated content. Two usability studies confirm the effectiveness of\nInkSync's components when compared to standard LLM-based chat interfaces,\nleading to more accurate, more efficient editing, and improved user experience.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level\n  Neural State Classification in Semantic Inference Reading Comprehension\u2b1b  With the recent proliferation of large language models (LLMs), such as\nGenerative Pre-trained Transformers (GPT), there has been a significant shift\nin exploring human and machine comprehension of semantic language meaning. This\nshift calls for interdisciplinary research that bridges cognitive science and\nnatural language processing (NLP). This pilot study aims to provide insights\ninto individuals' neural states during a semantic relation\nreading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and\nelectroencephalographic (EEG) data to study how the brain processes words with\nvarying degrees of relevance to a keyword during reading. We also use a feature\nengineering approach to improve the fixation-related EEG data classification\nwhile participants read words with high versus low relevance to the keyword.\nThe best validation accuracy in this word-level classification is over 60\\%\nacross 12 subjects. Words of high relevance to the inference keyword had\nsignificantly more eye fixations per word: 1.0584 compared to 0.6576 when\nexcluding no-fixation words, and 1.5126 compared to 1.4026 when including them.\nThis study represents the first attempt to classify brain states at a word\nlevel using LLM knowledge. It provides valuable insights into human cognitive\nabilities and the realm of Artificial General Intelligence (AGI), and offers\nguidance for developing potential reading-assisted technologies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking\n  Unrelated Questions\u2b1b  Large language models (LLMs) can \"lie\", which we define as outputting false\nstatements despite \"knowing\" the truth in a demonstrable sense. LLMs might\n\"lie\", for example, when instructed to output misinformation. Here, we develop\na simple lie detector that requires neither access to the LLM's activations\n(black-box) nor ground-truth knowledge of the fact in question. The detector\nworks by asking a predefined set of unrelated follow-up questions after a\nsuspected lie, and feeding the LLM's yes/no answers into a logistic regression\nclassifier. Despite its simplicity, this lie detector is highly accurate and\nsurprisingly general. When trained on examples from a single setting --\nprompting GPT-3.5 to lie about factual questions -- the detector generalises\nout-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,\n(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as\nsales. These results indicate that LLMs have distinctive lie-related\nbehavioural patterns, consistent across architectures and contexts, which could\nenable general-purpose lie detection.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Disinformation Detection: An Evolving Challenge in the Age of LLMs\u2b1b  The advent of generative Large Language Models (LLMs) such as ChatGPT has\ncatalyzed transformative advancements across multiple domains. However,\nalongside these advancements, they have also introduced potential threats. One\ncritical concern is the misuse of LLMs by disinformation spreaders, leveraging\nthese models to generate highly persuasive yet misleading content that\nchallenges the disinformation detection system. This work aims to address this\nissue by answering three research questions: (1) To what extent can the current\ndisinformation detection technique reliably detect LLM-generated\ndisinformation? (2) If traditional techniques prove less effective, can LLMs\nthemself be exploited to serve as a robust defense against advanced\ndisinformation? and, (3) Should both these strategies falter, what novel\napproaches can be proposed to counter this burgeoning threat effectively? A\nholistic exploration for the formation and detection of disinformation is\nconducted to foster this line of research.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "At Which Training Stage Does Code Data Help LLMs Reasoning?\u2b1b  Large Language Models (LLMs) have exhibited remarkable reasoning capabilities\nand become the foundation of language technologies. Inspired by the great\nsuccess of code data in training LLMs, we naturally wonder at which training\nstage introducing code data can really help LLMs reasoning. To this end, this\npaper systematically explores the impact of code data on LLMs at different\nstages. Concretely, we introduce the code data at the pre-training stage,\ninstruction-tuning stage, and both of them, respectively. Then, the reasoning\ncapability of LLMs is comprehensively and fairly evaluated via six reasoning\ntasks in five domains. We critically analyze the experimental results and\nprovide conclusions with insights. First, pre-training LLMs with the mixture of\ncode and text can significantly enhance LLMs' general reasoning capability\nalmost without negative transfer on other tasks. Besides, at the\ninstruction-tuning stage, code data endows LLMs the task-specific reasoning\ncapability. Moreover, the dynamic mixing strategy of code and text data assists\nLLMs to learn reasoning capability step-by-step during training. These insights\ndeepen the understanding of LLMs regarding reasoning ability for their\napplication, such as scientific question answering, legal support, etc. The\nsource code and model parameters are released at the\nlink:~\\url{https://github.com/yingweima2022/CodeLLM}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Augmenting LLMs with Knowledge: A survey on hallucination prevention\u2b1b  Large pre-trained language models have demonstrated their proficiency in\nstoring factual knowledge within their parameters and achieving remarkable\nresults when fine-tuned for downstream natural language processing tasks.\nNonetheless, their capacity to access and manipulate knowledge with precision\nremains constrained, resulting in performance disparities on\nknowledge-intensive tasks when compared to task-specific architectures.\nAdditionally, the challenges of providing provenance for model decisions and\nmaintaining up-to-date world knowledge persist as open research frontiers. To\naddress these limitations, the integration of pre-trained models with\ndifferentiable access mechanisms to explicit non-parametric memory emerges as a\npromising solution. This survey delves into the realm of language models (LMs)\naugmented with the ability to tap into external knowledge sources, including\nexternal knowledge bases and search engines. While adhering to the standard\nobjective of predicting missing tokens, these augmented LMs leverage diverse,\npossibly non-parametric external modules to augment their contextual processing\ncapabilities, departing from the conventional language modeling paradigm.\nThrough an exploration of current advancements in augmenting large language\nmodels with knowledge, this work concludes that this emerging research\ndirection holds the potential to address prevalent issues in traditional LMs,\nsuch as hallucinations, un-grounded responses, and scalability challenges.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Sign Language Recognition System with Pepper, Lightweight-Transformer,\n  and LLM\u2b1b  This research explores using lightweight deep neural network architectures to\nenable the humanoid robot Pepper to understand American Sign Language (ASL) and\nfacilitate non-verbal human-robot interaction. First, we introduce a\nlightweight and efficient model for ASL understanding optimized for embedded\nsystems, ensuring rapid sign recognition while conserving computational\nresources. Building upon this, we employ large language models (LLMs) for\nintelligent robot interactions. Through intricate prompt engineering, we tailor\ninteractions to allow the Pepper Robot to generate natural Co-Speech Gesture\nresponses, laying the foundation for more organic and intuitive humanoid-robot\ndialogues. Finally, we present an integrated software pipeline, embodying\nadvancements in a socially aware AI interaction model. Leveraging the Pepper\nRobot's capabilities, we demonstrate the practicality and effectiveness of our\napproach in real-world scenarios. The results highlight a profound potential\nfor enhancing human-robot interaction through non-verbal interactions, bridging\ncommunication gaps, and making technology more accessible and understandable.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Benchmarking the Abilities of Large Language Models for RDF Knowledge\n  Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?\u2b1b  Large Language Models (LLMs) are advancing at a rapid pace, with significant\nimprovements at natural language processing and coding tasks. Yet, their\nability to work with formal languages representing data, specifically within\nthe realm of knowledge graph engineering, remains under-investigated. To\nevaluate the proficiency of various LLMs, we created a set of five tasks that\nprobe their ability to parse, understand, analyze, and create knowledge graphs\nserialized in Turtle syntax. These tasks, each embodying distinct degrees of\ncomplexity and being able to scale with the size of the problem, have been\nintegrated into our automated evaluation system, the LLM-KG-Bench. The\nevaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,\nClaude 1.3, and Claude 2.0, as well as two freely accessible offline models,\nGPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth\nunderstanding of the strengths and shortcomings of LLMs in relation to their\napplication within RDF knowledge graph engineering workflows utilizing Turtle\nrepresentation. While our findings show that the latest commercial models\noutperform their forerunners in terms of proficiency with the Turtle language,\nthey also reveal an apparent weakness. These models fall short when it comes to\nadhering strictly to the output formatting constraints, a crucial requirement\nin this context.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent\n  Negotiation Games\u2b1b  There is a growing interest in using Large Language Models (LLMs) as agents\nto tackle real-world tasks that may require assessing complex situations. Yet,\nwe have a limited understanding of LLMs' reasoning and decision-making\ncapabilities, partly stemming from a lack of dedicated evaluation benchmarks.\nAs negotiating and compromising are key aspects of our everyday communication\nand collaboration, we propose using scorable negotiation games as a new\nevaluation framework for LLMs. We create a testbed of diverse text-based,\nmulti-agent, multi-issue, semantically rich negotiation games, with easily\ntunable difficulty. To solve the challenge, agents need to have strong\narithmetic, inference, exploration, and planning capabilities, while seamlessly\nintegrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT),\nwe show that agents can negotiate and consistently reach successful deals. We\nquantify the performance with multiple metrics and observe a large gap between\nGPT-4 and earlier models. Importantly, we test the generalization to new games\nand setups. Finally, we show that these games can help evaluate other critical\naspects, such as the interaction dynamics between agents in the presence of\ngreedy and adversarial players.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending\n  Against Extraction Attacks\u2b1b  Pretrained language models sometimes possess knowledge that we do not wish\nthem to, including memorized personal information and knowledge that could be\nused to harm people. They can also output toxic or harmful text. To mitigate\nthese safety and informational issues, we propose an attack-and-defense\nframework for studying the task of deleting sensitive information directly from\nmodel weights. We study direct edits to model weights because (1) this approach\nshould guarantee that particular deleted information is never extracted by\nfuture prompt attacks, and (2) it should protect against whitebox attacks,\nwhich is necessary for making claims about safety/privacy in a setting where\npublicly available model weights could be used to elicit sensitive information.\nOur threat model assumes that an attack succeeds if the answer to a sensitive\nquestion is located among a set of B generated candidates, based on scenarios\nwhere the information would be insecure if the answer is among B candidates.\nExperimentally, we show that even state-of-the-art model editing methods such\nas ROME struggle to truly delete factual information from models like GPT-J, as\nour whitebox and blackbox attacks can recover \"deleted\" information from an\nedited model 38% of the time. These attacks leverage two key observations: (1)\nthat traces of deleted information can be found in intermediate model hidden\nstates, and (2) that applying an editing method for one question may not delete\ninformation across rephrased versions of the question. Finally, we provide new\ndefense methods that protect against some extraction attacks, but we do not\nfind a single universally effective defense method. Our results suggest that\ntruly deleting sensitive information is a tractable but difficult problem,\nsince even relatively low attack success rates have potentially severe societal\nimplications for real-world deployment of language models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting\n  Prompts\u2b1b  This paper explores the robustness of LLMs' preference to their internal\nmemory or the given prompt, which may contain contrasting information in\nreal-world applications due to noise or task settings. To this end, we\nestablish a quantitative benchmarking framework and conduct the role playing\nintervention to control LLMs' preference. In specific, we define two types of\nrobustness, factual robustness targeting the ability to identify the correct\nfact from prompts or memory, and decision style to categorize LLMs' behavior in\nmaking consistent choices -- assuming there is no definitive \"right\" answer --\nintuitive, dependent, or rational based on cognitive theory. Our findings,\nderived from extensive experiments on seven open-source and closed-source LLMs,\nreveal that these models are highly susceptible to misleading prompts,\nespecially for instructing commonsense knowledge. While detailed instructions\ncan mitigate the selection of misleading answers, they also increase the\nincidence of invalid responses. After Unraveling the preference, we intervene\ndifferent sized LLMs through specific style of role instruction, showing their\nvarying upper bound of robustness and adaptivity.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CRAFT: Customizing LLMs by Creating and Retrieving from Specialized\n  Toolsets\u2b1b  Large language models (LLMs) are often augmented with tools to solve complex\ntasks. By generating code snippets and executing them through task-specific\nApplication Programming Interfaces (APIs), they can offload certain functions\nto dedicated external modules, such as image encoding and performing\ncalculations. However, most existing approaches to augment LLMs with tools are\nconstrained by general-purpose APIs and lack the flexibility for tailoring them\nto specific tasks. In this work, we present CRAFT, a general tool creation and\nretrieval framework for LLMs. It creates toolsets specifically curated for the\ntasks and equips LLMs with a component that retrieves tools from these sets to\nenhance their capability to solve complex tasks. For each task, we collect\nspecific code solutions by prompting GPT-4 to solve the training examples.\nFollowing a validation step ensuring the correctness, these solutions are\nabstracted into code snippets to enhance reusability, and deduplicated for\nhigher quality. At inference time, the language model retrieves snippets from\nthe toolsets and then executes them or generates the output conditioning on the\nretrieved snippets. Our method is designed to be flexible and offers a\nplug-and-play approach to adapt off-the-shelf LLMs to unseen domains and\nmodalities, without any finetuning. Experiments on vision-language, tabular\nprocessing, and mathematical reasoning tasks show that our approach achieves\nsubstantial improvements compared to strong baselines. In addition, our\nin-depth analysis reveals that: (1) consistent performance improvement can be\nachieved by scaling up the number of tools and the capability of the backbone\nmodels; (2) each component of our approach contributes to the performance\ngains; (3) the created tools are well-structured and reliable with low\ncomplexity and atomicity. The code is available at\n\\url{https://github.com/lifan-yuan/CRAFT}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-grounded Video Diffusion Models\u2b1b  Text-conditioned diffusion models have emerged as a promising tool for neural\nvideo generation. However, current models still struggle with intricate\nspatiotemporal prompts and often generate restricted or incorrect motion (e.g.,\neven lacking the ability to be prompted for objects moving from left to right).\nTo address these limitations, we introduce LLM-grounded Video Diffusion (LVD).\nInstead of directly generating videos from the text inputs, LVD first leverages\na large language model (LLM) to generate dynamic scene layouts based on the\ntext inputs and subsequently uses the generated layouts to guide a diffusion\nmodel for video generation. We show that LLMs are able to understand complex\nspatiotemporal dynamics from text alone and generate layouts that align closely\nwith both the prompts and the object motion patterns typically observed in the\nreal world. We then propose to guide video diffusion models with these layouts\nby adjusting the attention maps. Our approach is training-free and can be\nintegrated into any video diffusion model that admits classifier guidance. Our\nresults demonstrate that LVD significantly outperforms its base video diffusion\nmodel and several strong baseline methods in faithfully generating videos with\nthe desired attributes and motion patterns.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PB-LLM: Partially Binarized Large Language Models\u2b1b  This paper explores network binarization, a radical form of quantization,\ncompressing model weights to a single bit, specifically for Large Language\nModels (LLMs) compression. Due to previous binarization methods collapsing\nLLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can\nachieve extreme low-bit quantization while maintaining the linguistic reasoning\ncapacity of quantized LLMs. Specifically, our exploration first uncovers the\nineffectiveness of naive applications of existing binarization algorithms and\nhighlights the imperative role of salient weights in achieving low-bit\nquantization. Thus, PB-LLM filters a small ratio of salient weights during\nbinarization, allocating them to higher-bit storage, i.e.,\npartially-binarization. PB-LLM is extended to recover the capacities of\nquantized LMMs, by analyzing from the perspective of post-training quantization\n(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts\nfrom GPTQ, we reconstruct the binarized weight matrix guided by the Hessian\nmatrix and successfully recover the reasoning capacity of PB-LLM in low-bit.\nUnder QAT, we freeze the salient weights during training, explore the\nderivation of optimal scaling factors crucial for minimizing the quantization\nerror, and propose a scaling mechanism based on this derived scaling strategy\nfor residual binarized weights. Those explorations and the developed\nmethodologies significantly contribute to rejuvenating the performance of\nlow-bit quantized LLMs and present substantial advancements in the field of\nnetwork binarization for LLMs.The code is available at\nhttps://github.com/hahnyuan/BinaryLLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for\n  LLM Alignment\u2b1b  Large Language Models (LLMs) can acquire extensive world knowledge through\npre-training on large corpora. However, due to exposure to low-quality data,\nLLMs may exhibit harmful behavior without aligning with human values. The\ndominant approach for steering LLMs towards beneficial behavior involves\nReinforcement Learning with Human Feedback (RLHF), with Proximal Policy\nOptimization (PPO) serving as the default RL optimizer. Despite its\neffectiveness, PPO has limitations when optimizing rewards trained from\ncomparison-based loss. Primarily, PPO is not invariant to equivalent reward\nfunctions containing identical preference information due to the need to\ncalibrate the reward scale. Additionally, PPO's necessity for token-wise\nupdates introduces complexity in both function approximation and algorithm\ndesign compared to trajectory-wise optimization. This paper proposes a new\nframework, reinforcement learning with relative feedback, and a novel\ntrajectory-wise policy gradient algorithm, Pairwise Proximal Policy\nOptimization (P3O) that operates directly on comparative rewards. We show\ntheoretically that P3O is invariant to equivalent rewards and avoids the\ncomplexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO\nin the KL-Reward trade-off and can align with human preferences as well as or\nbetter than prior methods. In summary, this work introduces a simpler yet\neffective approach for aligning LLMs to human preferences through relative\nfeedback.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards LLM-based Fact Verification on News Claims with a Hierarchical\n  Step-by-Step Prompting Method\u2b1b  While large pre-trained language models (LLMs) have shown their impressive\ncapabilities in various NLP tasks, they are still under-explored in the\nmisinformation domain. In this paper, we examine LLMs with in-context learning\n(ICL) for news claim verification, and find that only with 4-shot demonstration\nexamples, the performance of several prompting methods can be comparable with\nprevious supervised models. To further boost performance, we introduce a\nHierarchical Step-by-Step (HiSS) prompting method which directs LLMs to\nseparate a claim into several subclaims and then verify each of them via\nmultiple questions-answering steps progressively. Experiment results on two\npublic misinformation datasets show that HiSS prompting outperforms\nstate-of-the-art fully-supervised approach and strong few-shot ICL-enabled\nbaselines.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "From Language Modeling to Instruction Following: Understanding the\n  Behavior Shift in LLMs after Instruction Tuning\u2b1b  Large Language Models (LLMs) have achieved remarkable success, demonstrating\npowerful instruction-following capabilities across diverse tasks. Instruction\nfine-tuning is critical in enabling LLMs to align with user intentions and\neffectively follow instructions. In this work, we investigate how instruction\nfine-tuning modifies pre-trained models, focusing on two perspectives:\ninstruction recognition and knowledge evolution. To study the behavior shift of\nLLMs, we employ a suite of local and global explanation methods, including a\ngradient-based approach for input-output attribution and techniques for\ninterpreting patterns and concepts in self-attention and feed-forward layers.\nOur findings reveal three significant impacts of instruction fine-tuning: 1) It\nempowers LLMs to better recognize the instruction parts from user prompts,\nthereby facilitating high-quality response generation and addressing the\n``lost-in-the-middle'' issue observed in pre-trained models; 2) It aligns the\nknowledge stored in feed-forward layers with user-oriented tasks, exhibiting\nminimal shifts across linguistic levels. 3) It facilitates the learning of\nword-word relations with instruction verbs through the self-attention\nmechanism, particularly in the lower and middle layers, indicating enhanced\nrecognition of instruction words. These insights contribute to a deeper\nunderstanding of the behavior shifts in LLMs after instruction fine-tuning and\nlay the groundwork for future research aimed at interpreting and optimizing\nLLMs for various applications. We will release our code and data soon.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing\n  Training Length\u2b1b  The evolving sophistication and intricacies of Large Language Models (LLMs)\nyield unprecedented advancements, yet they simultaneously demand considerable\ncomputational resources and incur significant costs. To alleviate these\nchallenges, this paper introduces a novel, simple, and effective method named\n``\\growlength'' to accelerate the pretraining process of LLMs. Our method\nprogressively increases the training length throughout the pretraining phase,\nthereby mitigating computational costs and enhancing efficiency. For instance,\nit begins with a sequence length of 128 and progressively extends to 4096. This\napproach enables models to process a larger number of tokens within limited\ntime frames, potentially boosting their performance. In other words, the\nefficiency gain is derived from training with shorter sequences optimizing the\nutilization of resources. Our extensive experiments with various\nstate-of-the-art LLMs have revealed that models trained using our method not\nonly converge more swiftly but also exhibit superior performance metrics\ncompared to those trained with existing methods. Furthermore, our method for\nLLMs pretraining acceleration does not require any additional engineering\nefforts, making it a practical solution in the realm of LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Faithful Explanations of Black-box NLP Models Using LLM-generated\n  Counterfactuals\u2b1b  Causal explanations of the predictions of NLP systems are essential to ensure\nsafety and establish trust. Yet, existing methods often fall short of\nexplaining model predictions effectively or efficiently and are often\nmodel-specific. In this paper, we address model-agnostic explanations,\nproposing two approaches for counterfactual (CF) approximation. The first\napproach is CF generation, where a large language model (LLM) is prompted to\nchange a specific text concept while keeping confounding concepts unchanged.\nWhile this approach is demonstrated to be very effective, applying LLM at\ninference-time is costly. We hence present a second approach based on matching,\nand propose a method that is guided by an LLM at training-time and learns a\ndedicated embedding space. This space is faithful to a given causal graph and\neffectively serves to identify matches that approximate CFs. After showing\ntheoretically that approximating CFs is required in order to construct faithful\nexplanations, we benchmark our approaches and explain several models, including\nLLMs with billions of parameters. Our empirical results demonstrate the\nexcellent performance of CF generation models as model-agnostic explainers.\nMoreover, our matching approach, which requires far less test-time resources,\nalso provides effective explanations, surpassing many baselines. We also find\nthat Top-K techniques universally improve every tested method. Finally, we\nshowcase the potential of LLMs in constructing new benchmarks for model\nexplanation and subsequently validate our conclusions. Our work illuminates new\npathways for efficient and accurate approaches to interpreting NLP systems.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities\n  of Large Language Models\u2b1b  The advent of Large Language Models (LLMs) has paved the way for complex\ntasks such as role-playing, which enhances user interactions by enabling models\nto imitate various characters. However, the closed-source nature of\nstate-of-the-art LLMs and their general-purpose training limit role-playing\noptimization. In this paper, we introduce RoleLLM, a framework to benchmark,\nelicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four\nstages: (1) Role Profile Construction for 100 roles; (2) Context-Based\nInstruction Generation (Context-Instruct) for role-specific knowledge\nextraction; (3) Role Prompting using GPT (RoleGPT) for speaking style\nimitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning\nopen-source models along with role customization. By Context-Instruct and\nRoleGPT, we create RoleBench, the first systematic and fine-grained\ncharacter-level benchmark dataset for role-playing with 168,093 samples.\nMoreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),\nsignificantly enhancing role-playing abilities and even achieving comparable\nresults with RoleGPT (using GPT-4).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "BooookScore: A systematic exploration of book-length summarization in\n  the era of LLMs\u2b1b  Summarizing book-length documents (>100K tokens) that exceed the context\nwindow size of large language models (LLMs) requires first breaking the input\ndocument into smaller chunks and then prompting an LLM to merge, update, and\ncompress chunk-level summaries. Despite the complexity and importance of this\ntask, it has yet to be meaningfully studied due to the challenges of\nevaluation: existing book-length summarization datasets (e.g., BookSum) are in\nthe pretraining data of most public LLMs, and existing evaluation methods\nstruggle to capture errors made by modern LLM summarizers. In this paper, we\npresent the first study of the coherence of LLM-based book-length summarizers\nimplemented via two prompting workflows: (1) hierarchically merging chunk-level\nsummaries, and (2) incrementally updating a running summary. We obtain 1193\nfine-grained human annotations on GPT-4 generated summaries of 100\nrecently-published books and identify eight common types of coherence errors\nmade by LLMs. Because human evaluation is expensive and time-consuming, we\ndevelop an automatic metric, BooookScore, that measures the proportion of\nsentences in a summary that do not contain any of the identified error types.\nBooookScore has high agreement with human annotations and allows us to\nsystematically evaluate the impact of many other critical parameters (e.g.,\nchunk size, base LLM) while saving $15K and 500 hours in human evaluation\ncosts. We find that closed-source LLMs such as GPT-4 and Claude 2 produce\nsummaries with higher BooookScore than the oft-repetitive ones generated by\nLLaMA 2. Incremental updating yields lower BooookScore but higher level of\ndetail than hierarchical merging, a trade-off sometimes preferred by human\nannotators. We release code and annotations after blind review to spur more\nprincipled research on book-length summarization.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Testing the Limits of Unified Sequence to Sequence LLM Pretraining on\n  Diverse Table Data Tasks\u2b1b  Tables stored in databases and tables which are present in web pages and\narticles account for a large part of semi-structured data that is available on\nthe internet. It then becomes pertinent to develop a modeling approach with\nlarge language models (LLMs) that can be used to solve diverse table tasks such\nas semantic parsing, question answering as well as classification problems.\nTraditionally, there existed separate models specialized for each task\nindividually. It raises the question of how far can we go to build a unified\nmodel that works well on some table tasks without significant degradation on\nothers. To that end, we attempt at creating a shared modeling approach in the\npretraining stage with encoder-decoder style LLMs that can cater to diverse\ntasks. We evaluate our approach that continually pretrains and finetunes\ndifferent model families of T5 with data from tables and surrounding context,\non these downstream tasks at different model scales. Through multiple ablation\nstudies, we observe that our pretraining with self-supervised objectives can\nsignificantly boost the performance of the models on these tasks. As an example\nof one improvement, we observe that the instruction finetuned public models\nwhich come specialized on text question answering (QA) and have been trained on\ntable data still have room for improvement when it comes to table specific QA.\nOur work is the first attempt at studying the advantages of a unified approach\nto table specific pretraining when scaled from 770M to 11B sequence to sequence\nmodels while also comparing the instruction finetuned variants of the models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "(Dynamic) Prompting might be all you need to repair Compressed LLMs\u2b1b  Large language models (LLMs), while transformative for NLP, come with\nsignificant computational demands, underlining the need for efficient,\ntraining-free compression. Notably, despite the marked improvement in\ntraining-free compression for the largest of LLMs, our tests using LLaMA-7B and\nOPT-6.7b highlight a significant performance drop in several realistic\ndownstream tasks. Investigation into the trade-off between resource-intensive\npost-compression re-training highlights the prospect of prompt-driven recovery\nas a lightweight adaption tool. However, existing studies, confined mainly to\nperplexity evaluations and simple tasks, fail to offer unequivocal confidence\nin the scalability and generalizability of prompting. We tackle this\nuncertainty in two key ways. First, we uncover the vulnerability of naive\nprompts in LLM compression as an over-reliance on a singular prompt per input.\nIn response, we propose inference-time dynamic prompting (IDP), a mechanism\nthat autonomously chooses from a set of curated prompts based on the context of\neach individual input. Second, we delve into a scientific understanding of why\n\"prompting might be all you need post-LLM compression.\" Our findings suggest\nthat compression does not irretrievably erase LLM model knowledge but displace\nit, necessitating a new inference path. IDP effectively redirects this path,\nenabling the model to tap into its inherent yet displaced knowledge and thereby\nrecover performance. Empirical tests affirm the value of IDP, demonstrating an\naverage performance improvement of 1.24% across nine varied tasks spanning\nmultiple knowledge domains.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Automated Evaluation of Classroom Instructional Support with LLMs and\n  BoWs: Connecting Global Predictions to Specific Feedback\u2b1b  With the aim to provide teachers with more specific, frequent, and actionable\nfeedback about their teaching, we explore how Large Language Models (LLMs) can\nbe used to estimate ``Instructional Support'' domain scores of the CLassroom\nAssessment Scoring System (CLASS), a widely used observation protocol. We\ndesign a machine learning architecture that uses either zero-shot prompting of\nMeta's Llama2, and/or a classic Bag of Words (BoW) model, to classify\nindividual utterances of teachers' speech (transcribed automatically using\nOpenAI's Whisper) for the presence of 11 behavioral indicators of Instructional\nSupport. Then, these utterance-level judgments are aggregated over an entire\n15-min observation session to estimate a global CLASS score. Experiments on two\nCLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that\n(1) automatic CLASS Instructional Support estimation accuracy using the\nproposed method (Pearson $R$ up to $0.46$) approaches human inter-rater\nreliability (up to $R=0.55$); (2) LLMs yield slightly greater accuracy than BoW\nfor this task; and (3) the best models often combined features extracted from\nboth LLM and BoW. Finally, (4) we illustrate how the model's outputs can be\nvisualized at the utterance level to provide teachers with explainable feedback\non which utterances were most positively or negatively correlated with specific\nCLASS dimensions.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SPELL: Semantic Prompt Evolution based on a LLM\u2b1b  Prompt engineering is a new paradigm for enhancing the performance of trained\nneural network models. For optimizing text-style prompts, existing methods\nusually individually operate small portions of a text step by step, which\neither breaks the fluency or could not globally adjust a prompt. Since large\nlanguage models (LLMs) have powerful ability of generating coherent texts token\nby token, can we utilize LLMs for improving prompts? Based on this motivation,\nin this paper, considering a trained LLM as a text generator, we attempt to\ndesign a black-box evolution algorithm for automatically optimizing texts,\nnamely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method is\nevaluated with different LLMs and evolution parameters in different text tasks.\nExperimental results show that SPELL could rapidly improve the prompts indeed.\nWe further explore the evolution process and discuss on the limitations,\npotential possibilities and future work.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Compressing LLMs: The Truth is Rarely Pure and Never Simple\u2b1b  Despite their remarkable achievements, modern Large Language Models (LLMs)\nencounter exorbitant computational and memory footprints. Recently, several\nworks have shown significant success in training-free and data-free compression\n(pruning and quantization) of LLMs achieving 50-60% sparsity and reducing the\nbit-width down to 3 or 4 bits per weight, with negligible perplexity\ndegradation over the uncompressed baseline. As recent research efforts are\nfocused on developing increasingly sophisticated compression methods, our work\ntakes a step back, and re-evaluates the effectiveness of existing SoTA\ncompression methods, which rely on a fairly simple and widely questioned\nmetric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive\nCompressed LLM BenchmarK (LLM-KICK), a collection of carefully-curated tasks to\nre-define the evaluation protocol for compressed LLMs, which have significant\nalignment with their dense counterparts, and perplexity fail to capture subtle\nchange in their true capabilities. LLM-KICK unveils many favorable merits and\nunfortunate plights of current SoTA compression methods: all pruning methods\nsuffer significant performance degradation, sometimes at trivial sparsity\nratios (e.g., 25-30%), and fail for N:M sparsity on knowledge-intensive tasks;\ncurrent quantization methods are more successful than pruning; yet, pruned LLMs\neven at $\\geq 50$% sparsity are robust in-context retrieval and summarization\nsystems; among others. LLM-KICK is designed to holistically access compressed\nLLMs' ability for language understanding, reasoning, generation, in-context\nretrieval, in-context summarization, etc. We hope our study can foster the\ndevelopment of better LLM compression methods. All our related codes are planed\nto be open-sourced.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using\n  PsychoBench\u2b1b  Large Language Models (LLMs) have recently showcased their remarkable\ncapacities, not only in natural language processing tasks but also across\ndiverse domains such as clinical medicine, legal consultation, and education.\nLLMs become more than mere applications, evolving into assistants capable of\naddressing diverse user requests. This narrows the distinction between human\nbeings and artificial intelligence agents, raising intriguing questions\nregarding the potential manifestation of personalities, temperaments, and\nemotions within LLMs. In this paper, we propose a framework, PsychoBench, for\nevaluating diverse psychological aspects of LLMs. Comprising thirteen scales\ncommonly used in clinical psychology, PsychoBench further classifies these\nscales into four distinct categories: personality traits, interpersonal\nrelationships, motivational tests, and emotional abilities. Our study examines\nfive popular models, namely \\texttt{text-davinci-003}, ChatGPT, GPT-4,\nLLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to\nbypass the safety alignment protocols and test the intrinsic natures of LLMs.\nWe have made PsychoBench openly accessible via\n\\url{https://github.com/CUHK-ARISE/PsychoBench}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT\n  LLM on Mobile\u2b1b  The field of Artificial Intelligence has witnessed remarkable progress in\nrecent years, especially with the emergence of powerful large language models\n(LLMs) based on the transformer architecture. Cloud-based LLMs, such as\nOpenAI's ChatGPT, offer impressive capabilities but come with concerns\nregarding latency and privacy due to network dependencies. This article\npresents an innovative approach to LLM inference, envisioning a future where\nLLMs with billions of parameters can be executed directly on mobile devices\nwithout network connectivity. The article showcases a fine-tuned GPT LLM with 3\nbillion parameters that can operate smoothly on devices with as low as 4GB of\nmemory. Through the integration of native code and model quantization\ntechniques, the application not only serves as a general-purpose assistant but\nalso facilitates seamless mobile interactions with text-to-actions features.\nThe article provides insights into the training pipeline, implementation\ndetails, test results, and future directions of on-device LLM inference. This\nbreakthrough technology opens up possibilities for empowering users with\nsophisticated AI capabilities while preserving their privacy and eliminating\nlatency concerns.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Adapting LLM Agents Through Communication\u2b1b  Recent advancements in large language models (LLMs) have shown potential for\nhuman-like agents. To help these agents adapt to new tasks without extensive\nhuman supervision, we propose the Learning through Communication (LTC)\nparadigm, a novel training approach enabling LLM agents to improve continuously\nthrough interactions with their environments and other agents. Recent\nadvancements in large language models (LLMs) have shown potential for\nhuman-like agents. To help these agents adapt to new tasks without extensive\nhuman supervision, we propose the Learning through Communication (LTC)\nparadigm, a novel training approach enabling LLM agents to improve continuously\nthrough interactions with their environments and other agents. Through\niterative exploration and PPO training, LTC empowers the agent to assimilate\nshort-term experiences into long-term memory. To optimize agent interactions\nfor task-specific learning, we introduce three structured communication\npatterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as\ndecision-making, knowledge-intensive reasoning, and numerical reasoning. We\nevaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA\n(knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld,\nit exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA,\nLTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it\noutperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k,\nLTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results\nshowcase the versatility and efficiency of the LTC approach across diverse\ndomains. We will open-source our code to promote further development of the\ncommunity.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Entity-Deduction Arena: A playground for probing the conversational\n  reasoning and planning capabilities of LLMs\u2b1b  Large language models (LLMs) are effective at answering questions that are\nclearly asked. However, when faced with ambiguous queries they can act\nunpredictably and produce incorrect outputs. This underscores the need for the\ndevelopment of intelligent agents capable of asking clarification questions to\nresolve ambiguities effectively. This capability requires complex\nunderstanding, state tracking, reasoning and planning over multiple\nconversational turns. However, directly measuring this can be challenging. In\nthis paper, we offer a surrogate problem which assesses an LLMs's capability to\ndeduce an entity unknown to itself, but revealed to a judge, by asking the\njudge a series of queries. This entity-deducing game can serve as an evaluation\nframework to probe the conversational reasoning and planning capabilities of\nlanguage models. We systematically evaluate various LLMs and discover\nsignificant differences in their performance on this task. We find that strong\nLLMs like GPT-4 outperform human players by a large margin. We further employ\nBehavior Cloning (BC) to examine whether a weaker model is capable of imitating\na stronger model and generalizing to data or domains, using only the\ndemonstrations from a stronger model. We finally propose to use Reinforcement\nLearning to enhance reasoning and planning capacity of Vicuna models through\nepisodes of game playing, which lead to significant performance improvement. We\nhope that this problem offers insights into how autonomous agents could be\ntrained to behave more intelligently in ambiguous circumstances.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial\n  Examples\u2b1b  Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be\nknowledgeable and able to adapt to many tasks. However, we still can not\ncompletely trust their answer, since LLMs suffer from\nhallucination--fabricating non-existent facts to cheat users without\nperception. And the reasons for their existence and pervasiveness remain\nunclear. In this paper, we demonstrate that non-sense prompts composed of\nrandom tokens can also elicit the LLMs to respond with hallucinations. This\nphenomenon forces us to revisit that hallucination may be another view of\nadversarial examples, and it shares similar features with conventional\nadversarial examples as the basic feature of LLMs. Therefore, we formalize an\nautomatic hallucination triggering method as the hallucination attack in an\nadversarial way. Finally, we explore basic feature of attacked adversarial\nprompts and propose a simple yet effective defense strategy. Our code is\nreleased on GitHub.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\u2b1b  In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Driving with LLMs: Fusing Object-Level Vector Modality for Explainable\n  Autonomous Driving\u2b1b  Large Language Models (LLMs) have shown promise in the autonomous driving\nsector, particularly in generalization and interpretability. We introduce a\nunique object-level multimodal LLM architecture that merges vectorized numeric\nmodalities with a pre-trained LLM to improve context understanding in driving\nsituations. We also present a new dataset of 160k QA pairs derived from 10k\ndriving scenarios, paired with high quality control commands collected with RL\nagent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct\npretraining strategy is devised to align numeric vector modalities with static\nLLM representations using vector captioning language data. We also introduce an\nevaluation metric for Driving QA and demonstrate our LLM-driver's proficiency\nin interpreting driving scenarios, answering questions, and decision-making.\nOur findings highlight the potential of LLM-based driving action generation in\ncomparison to traditional behavioral cloning. We make our benchmark, datasets,\nand model available for further exploration.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward\n  Reasoning in Math Word Problems\u2b1b  While forward reasoning (i.e. find the answer given the question) has been\nexplored extensively in the recent literature, backward reasoning is relatively\nunexplored. We examine the backward reasoning capabilities of LLMs on Math Word\nProblems (MWPs): given a mathematical question and its answer, with some\ndetails omitted from the question, can LLMs effectively retrieve the missing\ninformation?\n  In this paper, we formally define the backward reasoning task on math word\nproblems and modify three datasets to evaluate this task: GSM8k, SVAMP and\nMultiArith. Our findings show a significant drop in the accuracy of models on\nbackward reasoning compared to forward reasoning across four SOTA LLMs (GPT4,\nGPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we\npropose three novel techniques that improve performance: Rephrase reformulates\nthe given problem into a forward reasoning problem, PAL-Tools combines the idea\nof Program-Aided LLMs to produce a set of equations that can be solved by an\nexternal solver, and Check your Work exploits the availability of natural\nverifier of high accuracy in the forward direction, interleaving solving and\nverification steps. Finally, realizing that each of our base methods correctly\nsolves a different set of problems, we propose a novel Bayesian formulation for\ncreating an ensemble over these base methods aided by a verifier to further\nboost the accuracy by a significant margin. Extensive experimentation\ndemonstrates that our techniques successively improve the performance of LLMs\non the backward reasoning task, with the final ensemble-based method resulting\nin a substantial performance gain compared to the raw LLMs with standard\nprompting techniques such as chain-of-thought.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology\n  View\u2b1b  As Natural Language Processing (NLP) systems are increasingly employed in\nintricate social environments, a pressing query emerges: Can these NLP systems\nmirror human-esque collaborative intelligence, in a multi-agent society\nconsisting of multiple large language models (LLMs)? This paper probes the\ncollaboration mechanisms among contemporary NLP systems by melding practical\nexperiments with theoretical insights. We fabricate four unique `societies'\ncomprised of LLM agents, where each agent is characterized by a specific\n`trait' (easy-going or overconfident) and engages in collaboration with a\ndistinct `thinking pattern' (debate or reflection). Evaluating these\nmulti-agent societies on three benchmark datasets, we discern that LLM agents\nnavigate tasks by leveraging diverse social behaviors, from active debates to\nintrospective reflections. Notably, certain collaborative strategies only\noptimize efficiency (using fewer API tokens), but also outshine previous\ntop-tier approaches. Moreover, our results further illustrate that LLM agents\nmanifest human-like social behaviors, such as conformity or majority rule,\nmirroring foundational Social Psychology theories. In conclusion, we integrate\ninsights from Social Psychology to contextualize the collaboration of LLM\nagents, inspiring further investigations into the collaboration mechanism for\nLLMs. We commit to sharing our code and datasets (already submitted in\nsupplementary materials), hoping to catalyze further research in this promising\navenue (All code and data are available at\n\\url{https://github.com/zjunlp/MachineSoM}.).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Editing Personality for LLMs\u2b1b  This paper introduces an innovative task focused on editing the personality\ntraits of Large Language Models (LLMs). This task seeks to adjust the models'\nresponses to opinion-related questions on specified topics since an\nindividual's personality often manifests in the form of their expressed\nopinions, thereby showcasing different personality traits. Specifically, we\nconstruct a new benchmark dataset PersonalityEdit to address this task. Drawing\non the theory in Social Psychology, we isolate three representative traits,\nnamely Neuroticism, Extraversion, and Agreeableness, as the foundation for our\nbenchmark. We then gather data using GPT-4, generating responses that not only\nalign with a specified topic but also embody the targeted personality trait. We\nconduct comprehensive experiments involving various baselines and discuss the\nrepresentation of personality behavior in LLMs. Our intriguing findings uncover\npotential challenges of the proposed task, illustrating several remaining\nissues. We anticipate that our work can provide the NLP community with\ninsights. Code and datasets will be released at\nhttps://github.com/zjunlp/EasyEdit.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with\n  Agent Team Optimization\u2b1b  Large language model (LLM) agents have been shown effective on a wide range\nof tasks, and by ensembling multiple LLM agents, their performances could be\nfurther improved. Existing approaches employ a fixed set of agents to interact\nwith each other in a static architecture, which limits their generalizability\nto various tasks and requires strong human prior in designing these agents. In\nthis work, we propose to construct a strategic team of agents communicating in\na dynamic interaction architecture based on the task query. Specifically, we\nbuild a framework named Dynamic LLM-Agent Network ($\\textbf{DyLAN}$) for\nLLM-agent collaboration on complicated tasks like reasoning and code\ngeneration. DyLAN enables agents to interact for multiple rounds in a dynamic\narchitecture with inference-time agent selection and an early-stopping\nmechanism to improve performance and efficiency. We further design an automatic\nagent team optimization algorithm based on an unsupervised metric termed\n$\\textit{Agent Importance Score}$, enabling the selection of best agents based\non the contribution each agent makes. Empirically, we demonstrate that DyLAN\nperforms well in both reasoning and code generation tasks with reasonable\ncomputational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and\nHumanEval, respectively, compared to a single execution on GPT-35-turbo. On\nspecific subjects of MMLU, agent team optimization in DyLAN increases accuracy\nby up to 25.0%.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Who's Harry Potter? Approximate Unlearning in LLMs\u2b1b  Large language models (LLMs) are trained on massive internet corpora that\noften contain copyrighted content. This poses legal and ethical challenges for\nthe developers and users of these models, as well as the original authors and\npublishers. In this paper, we propose a novel technique for unlearning a subset\nof the training data from a LLM, without having to retrain it from scratch.\n  We evaluate our technique on the task of unlearning the Harry Potter books\nfrom the Llama2-7b model (a generative language model recently open-sourced by\nMeta). While the model took over 184K GPU-hours to pretrain, we show that in\nabout 1 GPU hour of finetuning, we effectively erase the model's ability to\ngenerate or recall Harry Potter-related content, while its performance on\ncommon benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains\nalmost unaffected. We make our fine-tuned model publicly available on\nHuggingFace for community evaluation. To the best of our knowledge, this is the\nfirst paper to present an effective technique for unlearning in generative\nlanguage models.\n  Our technique consists of three main components: First, we use a reinforced\nmodel that is further trained on the target data to identify the tokens that\nare most related to the unlearning target, by comparing its logits with those\nof a baseline model. Second, we replace idiosyncratic expressions in the target\ndata with generic counterparts, and leverage the model's own predictions to\ngenerate alternative labels for every token. These labels aim to approximate\nthe next-token predictions of a model that has not been trained on the target\ndata. Third, we finetune the model on these alternative labels, which\neffectively erases the original text from the model's memory whenever it is\nprompted with its context.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework\u2b1b  Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance, self-awareness, and diagnosis. Current\nCHAs, especially those utilizing Large Language Models (LLMs), primarily focus\non conversation aspects. However, they offer limited agent capabilities\nspecifically lacking multi-step problem-solving, empathetic conversations, and\nmultimodal data analysis. Our aim is to overcome these limitations. In this\npaper, we propose an LLM-powered framework to empower CHAs to generate a\npersonalized response for users' healthcare queries. This framework provides\ncritical thinking, knowledge acquisition, and problem-solving abilities by\nintegrating healthcare data sources, enabling multilingual and multimodal\nconversations, and interacting with various user data analysis tools. We\nillustrate the framework's proficiency in handling complex healthcare tasks via\na case study on stress level estimation, showcasing the agent's cognitive and\noperational capabilities. Powered by our framework, the CHA can provide\nappropriate responses, when the user inquires about their stress level. To\nachieve this, it learns to collect photoplethysmogram signals, converts them\ninto heart rate variability, and interprets them as indicators of stress\nlevels.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can Large Language Models Provide Security & Privacy Advice? Measuring\n  the Ability of LLMs to Refute Misconceptions\u2b1b  Users seek security & privacy (S&P) advice from online resources, including\ntrusted websites and content-sharing platforms. These resources help users\nunderstand S&P technologies and tools and suggest actionable strategies. Large\nLanguage Models (LLMs) have recently emerged as trusted information sources.\nHowever, their accuracy and correctness have been called into question. Prior\nresearch has outlined the shortcomings of LLMs in answering multiple-choice\nquestions and user ability to inadvertently circumvent model restrictions\n(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable\nS&P advice is not well-explored. In this paper, we measure their ability to\nrefute popular S&P misconceptions that the general public holds. We first study\nrecent academic literature to curate a dataset of over a hundred S&P-related\nmisconceptions across six different topics. We then query two popular LLMs\n(Bard and ChatGPT) and develop a labeling guide to evaluate their responses to\nthese misconceptions. To comprehensively evaluate their responses, we further\napply three strategies: query each misconception multiple times, generate and\nquery their paraphrases, and solicit source URLs of the responses. Both models\ndemonstrate, on average, a 21.3% non-negligible error rate, incorrectly\nsupporting popular S&P misconceptions. The error rate increases to 32.6% when\nwe repeatedly query LLMs with the same or paraphrased misconceptions. We also\nexpose that models may partially support a misconception or remain\nnoncommittal, refusing a firm stance on misconceptions. Our exploration of\ninformation sources for responses revealed that LLMs are susceptible to\nproviding invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to\nunrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task\n  Adaptation\u2b1b  Large Language Models (LLMs) have the ability to solve a variety of tasks,\nsuch as text summarization and mathematical questions, just out of the box, but\nthey are often trained with a single task in mind. Due to high computational\ncosts, the current trend is to use prompt instruction tuning to better adjust\nmonolithic, pretrained LLMs for new -- but often individual -- downstream\ntasks. Thus, how one would expand prompt tuning to handle -- concomitantly --\nheterogeneous tasks and data distributions is a widely open question. To\naddress this gap, we suggest the use of \\emph{Mixture of Prompts}, or MoPs,\nassociated with smart gating functionality: the latter -- whose design is one\nof the contributions of this paper -- can identify relevant skills embedded in\ndifferent groups of prompts and dynamically assign combined experts (i.e.,\ncollection of prompts), based on the target task. Additionally, MoPs are\nempirically agnostic to any model compression technique applied -- for\nefficiency reasons -- as well as instruction data source and task composition.\nIn practice, MoPs can simultaneously mitigate prompt training \"interference\" in\nmulti-task, multi-source scenarios (e.g., task and data heterogeneity across\nsources), as well as possible implications from model approximations. As a\nhighlight, MoPs manage to decrease final perplexity from $\\sim20\\%$ up to\n$\\sim70\\%$, as compared to baselines, in the federated scenario, and from $\\sim\n3\\%$ up to $\\sim30\\%$ in the centralized scenario.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Understanding In-Context Learning in Transformers and LLMs by Learning\n  to Learn Discrete Functions\u2b1b  In order to understand the in-context learning phenomenon, recent works have\nadopted a stylized experimental framework and demonstrated that Transformers\ncan learn gradient-based learning algorithms for various classes of real-valued\nfunctions. However, the limitations of Transformers in implementing learning\nalgorithms, and their ability to learn other forms of algorithms are not well\nunderstood. Additionally, the degree to which these capabilities are confined\nto attention-based models is unclear. Furthermore, it remains to be seen\nwhether the insights derived from these stylized settings can be extrapolated\nto pretrained Large Language Models (LLMs). In this work, we take a step\ntowards answering these questions by demonstrating the following: (a) On a\ntest-bed with a variety of Boolean function classes, we find that Transformers\ncan nearly match the optimal learning algorithm for 'simpler' tasks, while\ntheir performance deteriorates on more 'complex' tasks. Additionally, we find\nthat certain attention-free models perform (almost) identically to Transformers\non a range of tasks. (b) When provided a teaching sequence, i.e. a set of\nexamples that uniquely identifies a function in a class, we show that\nTransformers learn more sample-efficiently. Interestingly, our results show\nthat Transformers can learn to implement two distinct algorithms to solve a\nsingle task, and can adaptively select the more sample-efficient algorithm\ndepending on the sequence of in-context examples. (c) Lastly, we show that\nextant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines\non prediction tasks that are guaranteed to not be in their training set.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "FreshLLMs: Refreshing Large Language Models with Search Engine\n  Augmentation\u2b1b  Most large language models (LLMs) are trained once and never updated; thus,\nthey lack the ability to dynamically adapt to our ever-changing world. In this\nwork, we perform a detailed study of the factuality of LLM-generated text in\nthe context of answering questions that test current world knowledge.\nSpecifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a\ndiverse range of question and answer types, including questions that require\nfast-changing world knowledge as well as questions with false premises that\nneed to be debunked. We benchmark a diverse array of both closed and\nopen-source LLMs under a two-mode evaluation procedure that allows us to\nmeasure both correctness and hallucination. Through human evaluations involving\nmore than 50K judgments, we shed light on limitations of these models and\ndemonstrate significant room for improvement: for instance, all models\n(regardless of model size) struggle on questions that involve fast-changing\nknowledge and false premises. Motivated by these results, we present\nFreshPrompt, a simple few-shot prompting method that substantially boosts the\nperformance of an LLM on FreshQA by incorporating relevant and up-to-date\ninformation retrieved from a search engine into the prompt. Our experiments\nshow that FreshPrompt outperforms both competing search engine-augmented\nprompting methods such as Self-Ask (Press et al., 2022) as well as commercial\nsystems such as Perplexity.AI. Further analysis of FreshPrompt reveals that\nboth the number of retrieved evidences and their order play a key role in\ninfluencing the correctness of LLM-generated answers. Additionally, instructing\nthe LLM to generate concise and direct answers helps reduce hallucination\ncompared to encouraging more verbose answers. To facilitate future work, we\nrelease FreshQA at github.com/freshllms/freshqa and commit to updating it at\nregular intervals.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Based Multi-Document Summarization Exploiting Main-Event Biased\n  Monotone Submodular Content Extraction\u2b1b  Multi-document summarization is a challenging task due to its inherent\nsubjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4\namong DUC-2004 reference summaries. In this work, we aim to enhance the\nobjectivity of news summarization by focusing on the main event of a group of\nrelated news documents and presenting it coherently with sufficient context.\nOur primary objective is to succinctly report the main event, ensuring that the\nsummary remains objective and informative. To achieve this, we employ an\nextract-rewrite approach that incorporates a main-event biased\nmonotone-submodular function for content selection. This enables us to extract\nthe most crucial information related to the main event from the document\ncluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for\nrewriting the extracted content into a coherent text. The evaluation using\nobjective metrics and human evaluators confirms the effectiveness of our\napproach, as it surpasses potential baselines, demonstrating excellence in both\ncontent coverage, coherence, and informativeness.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical\n  Reasoning\u2b1b  The recently released GPT-4 Code Interpreter has demonstrated remarkable\nproficiency in solving challenging math problems, primarily attributed to its\nability to seamlessly reason with natural language, generate code, execute\ncode, and continue reasoning based on the execution output. In this paper, we\npresent a method to fine-tune open-source language models, enabling them to use\ncode for modeling and deriving math equations and, consequently, enhancing\ntheir mathematical reasoning abilities. We propose a method of generating novel\nand high-quality datasets with math problems and their code-based solutions,\nreferred to as MathCodeInstruct. Each solution interleaves natural language,\ncode, and execution results. We also introduce a customized supervised\nfine-tuning and inference approach. This approach yields the MathCoder models,\na family of models capable of generating code-based solutions for solving\nchallenging math problems. Impressively, the MathCoder models achieve\nstate-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K\n(83.9%) datasets, substantially outperforming other open-source alternatives.\nNotably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K\nand MATH but also outperforms GPT-4 on the competition-level MATH dataset. The\ndataset and models will be released at https://github.com/mathllm/MathCoder.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Benchmarking a foundation LLM on its ability to re-label structure names\n  in accordance with the AAPM TG-263 report\u2b1b  Purpose: To introduce the concept of using large language models (LLMs) to\nre-label structure names in accordance with the American Association of\nPhysicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish a\nbenchmark for future studies to reference.\n  Methods and Materials: The Generative Pre-trained Transformer (GPT)-4\napplication programming interface (API) was implemented as a Digital Imaging\nand Communications in Medicine (DICOM) storage server, which upon receiving a\nstructure set DICOM file, prompts GPT-4 to re-label the structure names of both\ntarget volumes and normal tissues according to the AAPM TG-263. Three disease\nsites, prostate, head and neck, and thorax were selected for evaluation. For\neach disease site category, 150 patients were randomly selected for manually\ntuning the instructions prompt (in batches of 50) and 50 patients were randomly\nselected for evaluation. Structure names that were considered were those that\nwere most likely to be relevant for studies utilizing structure contours for\nmany patients.\n  Results: The overall re-labeling accuracy of both target volumes and normal\ntissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and\n96.9% respectively. Re-labeling of target volumes was less accurate on average\nexcept for prostate - 100%, 93.1%, and 91.1% respectively.\n  Conclusions: Given the accuracy of GPT-4 in re-labeling structure names of\nboth target volumes and normal tissues as presented in this work, LLMs are\npoised to be the preferred method for standardizing structure names in\nradiation oncology, especially considering the rapid advancements in LLM\ncapabilities that are likely to continue.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "What's the Magic Word? A Control Theory of LLM Prompting\u2b1b  Prompt engineering is effective and important in the deployment of LLMs but\nis poorly understood mathematically. Here, we formalize prompt engineering as\nan optimal control problem on LLMs -- where the prompt is considered a control\nvariable for modulating the output distribution of the LLM. Within this\nframework, we ask a simple question: given a sequence of tokens, does there\nalways exist a prompt we can prepend that will steer the LLM toward accurately\npredicting the final token? We call such an optimal prompt the magic word since\nprepending the prompt causes the LLM to output the correct answer. If magic\nwords exist, can we find them? If so, what are their properties? We offer\nanalytic analysis on the controllability of the self-attention head where we\nprove a bound on controllability as a function of the singular values of its\nweight matrices. We take inspiration from control theory to propose a metric\ncalled $k-\\epsilon$ controllability to characterize LLM steerability. We\ncompute the $k-\\epsilon$ controllability of a panel of large language models,\nincluding Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language\nmodeling tasks. Remarkably, we find that magic words of 10 tokens or less exist\nfor over 97% of WikiText instances surveyed for each model.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Label-free Node Classification on Graphs with Large Language Models\n  (LLMS)\u2b1b  In recent years, there have been remarkable advancements in node\nclassification achieved by Graph Neural Networks (GNNs). However, they\nnecessitate abundant high-quality labels to ensure promising performance. In\ncontrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency\non text-attributed graphs. Yet, they face challenges in efficiently processing\nstructural data and suffer from high inference costs. In light of these\nobservations, this work introduces a label-free node classification on graphs\nwith LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs\nwhile mitigating their limitations. Specifically, LLMs are leveraged to\nannotate a small portion of nodes and then GNNs are trained on LLMs'\nannotations to make predictions for the remaining large portion of nodes. The\nimplementation of LLM-GNN faces a unique challenge: how can we actively select\nnodes for LLMs to annotate and consequently enhance the GNN training? How can\nwe leverage LLMs to obtain annotations of high quality, representativeness, and\ndiversity, thereby enhancing GNN performance with less cost? To tackle this\nchallenge, we develop an annotation quality heuristic and leverage the\nconfidence scores derived from LLMs to advanced node selection. Comprehensive\nexperimental results validate the effectiveness of LLM-GNN. In particular,\nLLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with\na cost less than 1 dollar.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Chat Vector: A Simple Approach to Equip LLMs With New Language Chat\n  Capabilities\u2b1b  With the advancements in conversational AI, such as ChatGPT, this paper\nfocuses on exploring developing Large Language Models (LLMs) for non-English\nlanguages, especially emphasizing alignment with human preferences. We\nintroduce a computationally efficient method, leveraging chat vector, to\nsynergize pre-existing knowledge and behaviors in LLMs, restructuring the\nconventional training paradigm from continual pre-train -> SFT -> RLHF to\ncontinual pre-train + chat vector. Our empirical studies, primarily focused on\nTraditional Chinese, employ LLaMA2 as the base model and acquire the chat\nvector by subtracting the pre-trained weights, LLaMA2, from the weights of\nLLaMA2-chat. Evaluating from three distinct facets, which are toxicity, ability\nof instruction following, and multi-turn dialogue demonstrates the chat\nvector's superior efficacy in chatting. To confirm the adaptability of our\napproach, we extend our experiments to include models pre-trained in both\nKorean and Simplified Chinese, illustrating the versatility of our methodology.\nOverall, we present a significant solution in aligning LLMs with human\npreferences efficiently across various languages, accomplished by the chat\nvector.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Balancing Specialized and General Skills in LLMs: The Impact of Modern\n  Tuning and Data Strategy\u2b1b  This paper introduces a multifaceted methodology for fine-tuning and\nevaluating large language models (LLMs) for specialized monetization tasks. The\ngoal is to balance general language proficiency with domain-specific skills.\nThe methodology has three main components: 1) Carefully blending in-domain and\ngeneral-purpose data during fine-tuning to achieve an optimal balance between\ngeneral and specialized capabilities; 2) Designing a comprehensive evaluation\nframework with 45 questions tailored to assess performance on functionally\nrelevant dimensions like reliability, consistency, and business impact; 3)\nAnalyzing how model size and continual training influence metrics to guide\nefficient resource allocation during fine-tuning. The paper details the design,\ndata collection, analytical techniques, and results validating the proposed\nframeworks. It aims to provide businesses and researchers with actionable\ninsights on effectively adapting LLMs for specialized contexts. We also intend\nto make public the comprehensive evaluation framework, which includes the 45\ntailored questions and their respective scoring guidelines, to foster\ntransparency and collaboration in adapting LLMs for specialized tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AvalonBench: Evaluating LLMs Playing the Game of Avalon\u2b1b  In this paper, we explore the potential of Large Language Models (LLMs)\nAgents in playing the strategic social deduction game, Resistance Avalon.\nPlayers in Avalon are challenged not only to make informed decisions based on\ndynamically evolving game phases, but also to engage in discussions where they\nmust deceive, deduce, and negotiate with other players. These characteristics\nmake Avalon a compelling test-bed to study the decision-making and\nlanguage-processing capabilities of LLM Agents. To facilitate research in this\nline, we introduce AvalonBench - a comprehensive game environment tailored for\nevaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game\nenvironment for Avalon, (2) rule-based bots as baseline opponents, and (3)\nReAct-style LLM agents with tailored prompts for each role. Notably, our\nevaluations based on AvalonBench highlight a clear capability gap. For\ninstance, models like ChatGPT playing good-role got a win rate of 22.2% against\nrule-based bots playing evil, while good-role bot achieves 38.2% win rate in\nthe same setting. We envision AvalonBench could be a good test-bed for\ndeveloping more advanced LLMs (with self-playing) and agent frameworks that can\neffectively model the layered complexities of such game environments.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Large Language Model (LLM) as a System of Multiple Expert Agents: An\n  Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge\u2b1b  We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge\nusing Large Language Models (LLMs) as a system of multiple expert agents. Using\nthe flexibility of LLMs to be prompted to do various novel tasks using\nzero-shot, few-shot, context-grounded prompting, we explore the feasibility of\nusing LLMs to solve the ARC Challenge. We firstly convert the input image into\nmultiple suitable text-based abstraction spaces. We then utilise the\nassociative power of LLMs to derive the input-output relationship and map this\nto actions in the form of a working program, similar to Voyager / Ghost in the\nMineCraft. In addition, we use iterative environmental feedback in order to\nguide LLMs to solve the task. Our proposed approach achieves 50 solves out of\n111 training set problems (45%) with just three abstraction spaces - grid,\nobject and pixel - and we believe that with more abstraction spaces and\nlearnable actions, we will be able to solve more.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "An Investigation of LLMs' Inefficacy in Understanding Converse Relations\u2b1b  Large Language Models (LLMs) have achieved remarkable success in many formal\nlanguage oriented tasks, such as structural data-to-text and semantic parsing.\nHowever current benchmarks mostly follow the data distribution of the\npre-training data of LLMs. Therefore, a natural question rises that do LLMs\nreally understand the structured semantics of formal languages. In this paper,\nwe investigate this problem on a special case, converse binary relation. We\nintroduce a new benchmark ConvRe focusing on converse relations, which contains\n17 relations and 1240 triples extracted from popular knowledge graph completion\ndatasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are\nformulated as multi-choice question answering to evaluate LLMs' ability to\ndetermine the matching between relations and associated text. For the\nevaluation protocol, apart from different prompting methods, we further\nintroduce variants to the test text and few-shot example text. We conduct\nexperiments on three popular LLM families and have observed various scaling\ntrends. The results suggest that LLMs often resort to shortcut learning and\nstill face challenges on our proposed benchmark.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Resolving the Imbalance Issue in Hierarchical Disciplinary Topic\n  Inference via LLM-based Data Augmentation\u2b1b  In addressing the imbalanced issue of data within the realm of Natural\nLanguage Processing, text data augmentation methods have emerged as pivotal\nsolutions. This data imbalance is prevalent in the research proposals submitted\nduring the funding application process. Such imbalances, resulting from the\nvarying popularity of disciplines or the emergence of interdisciplinary\nstudies, significantly impede the precision of downstream topic models that\ndeduce the affiliated disciplines of these proposals. At the data level,\nproposals penned by experts and scientists are inherently complex technological\ntexts, replete with intricate terminologies, which augmenting such specialized\ntext data poses unique challenges. At the system level, this, in turn,\ncompromises the fairness of AI-assisted reviewer assignment systems, which\nraises a spotlight on solving this issue. This study leverages large language\nmodels (Llama V1) as data generators to augment research proposals categorized\nwithin intricate disciplinary hierarchies, aiming to rectify data imbalances\nand enhance the equity of expert assignments. We first sample within the\nhierarchical structure to find the under-represented class. Then we designed a\nprompt for keyword-based research proposal generation. Our experiments attests\nto the efficacy of the generated data, demonstrating that research proposals\nproduced using the prompts can effectively address the aforementioned issues\nand generate high quality scientific text data, thus help the model overcome\nthe imbalanced issue.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMLingua: Compressing Prompts for Accelerated Inference of Large\n  Language Models\u2b1b  Large language models (LLMs) have been applied in various applications due to\ntheir astonishing capabilities. With advancements in technologies such as\nchain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed\nto LLMs are becoming increasingly lengthy, even exceeding tens of thousands of\ntokens. To accelerate model inference and reduce cost, this paper presents\nLLMLingua, a coarse-to-fine prompt compression method that involves a budget\ncontroller to maintain semantic integrity under high compression ratios, a\ntoken-level iterative compression algorithm to better model the interdependence\nbetween compressed contents, and an instruction tuning based method for\ndistribution alignment between language models. We conduct experiments and\nanalysis over four datasets from different scenarios, i.e., GSM8K, BBH,\nShareGPT, and Arxiv-March23; showing that the proposed approach yields\nstate-of-the-art performance and allows for up to 20x compression with little\nperformance loss. Our code is available at https://aka.ms/LLMLingua.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and\n  Execution of LLM Agents in an Auction Arena\u2b1b  Can Large Language Models (LLMs) simulate human behavior in complex\nenvironments? LLMs have recently been shown to exhibit advanced reasoning\nskills but much of NLP evaluation still relies on static benchmarks. Answering\nthis requires evaluation environments that probe strategic reasoning in\ncompetitive, dynamic scenarios that involve long-term planning. We introduce\nAucArena, a novel simulation environment for evaluating LLMs within auctions, a\nsetting chosen for being highly unpredictable and involving many skills related\nto resource and risk management, while also being easy to evaluate. We conduct\nseveral controlled simulations using state-of-the-art LLMs as bidding agents.\nWe find that through simple prompting, LLMs do indeed demonstrate many of the\nskills needed for effectively engaging in auctions (e.g., managing budget,\nadhering to long-term goals and priorities), skills that we find can be\nsharpened by explicitly encouraging models to be adaptive and observe\nstrategies in past auctions. These results are significant as they show the\npotential of using LLM agents to model intricate social dynamics, especially in\ncompetitive settings. However, we also observe considerable variability in the\ncapabilities of individual LLMs. Notably, even our most advanced models (GPT-4)\nare occasionally surpassed by heuristic baselines and human agents,\nhighlighting the potential for further improvements in the design of LLM agents\nand the important role that our simulation environment can play in further\ntesting and refining agent architectures.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model\u2b1b  The advancement of Large Language Models (LLMs) has remarkably pushed the\nboundaries towards artificial general intelligence (AGI), with their\nexceptional ability on understanding diverse types of information, including\nbut not limited to images and audio. Despite this progress, a critical gap\nremains in empowering LLMs to proficiently understand and reason on graph data.\nRecent studies underscore LLMs' underwhelming performance on fundamental graph\nreasoning tasks. In this paper, we endeavor to unearth the obstacles that\nimpede LLMs in graph reasoning, pinpointing the common practice of converting\ngraphs into natural language descriptions (Graph2Text) as a fundamental\nbottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering\nend-to-end approach that synergistically integrates graph learning models with\nLLMs. This synergy equips LLMs with the ability to proficiently interpret and\nreason on graph data, harnessing the superior expressive power of graph\nlearning models. Our empirical evaluations across four fundamental graph\nreasoning tasks validate the effectiveness of GraphLLM. The results exhibit a\nsubstantial average accuracy enhancement of 54.44%, alongside a noteworthy\ncontext reduction of 96.45% across various graph reasoning tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM for SoC Security: A Paradigm Shift\u2b1b  As the ubiquity and complexity of system-on-chip (SoC) designs increase\nacross electronic devices, the task of incorporating security into an SoC\ndesign flow poses significant challenges. Existing security solutions are\ninadequate to provide effective verification of modern SoC designs due to their\nlimitations in scalability, comprehensiveness, and adaptability. On the other\nhand, Large Language Models (LLMs) are celebrated for their remarkable success\nin natural language understanding, advanced reasoning, and program synthesis\ntasks. Recognizing an opportunity, our research delves into leveraging the\nemergent capabilities of Generative Pre-trained Transformers (GPTs) to address\nthe existing gaps in SoC security, aiming for a more efficient, scalable, and\nadaptable methodology. By integrating LLMs into the SoC security verification\nparadigm, we open a new frontier of possibilities and challenges to ensure the\nsecurity of increasingly complex SoCs. This paper offers an in-depth analysis\nof existing works, showcases practical case studies, demonstrates comprehensive\nexperiments, and provides useful promoting guidelines. We also present the\nachievements, prospects, and challenges of employing LLM in different SoC\nsecurity verification tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models\u2b1b  The application of machine learning (ML) in a range of geospatial tasks is\nincreasingly common but often relies on globally available covariates such as\nsatellite imagery that can either be expensive or lack predictive power. Here\nwe explore the question of whether the vast amounts of knowledge found in\nInternet language corpora, now compressed within large language models (LLMs),\ncan be leveraged for geospatial prediction tasks. We first demonstrate that\nLLMs embed remarkable spatial information about locations, but naively querying\nLLMs using geographic coordinates alone is ineffective in predicting key\nindicators like population density. We then present GeoLLM, a novel method that\ncan effectively extract geospatial knowledge from LLMs with auxiliary map data\nfrom OpenStreetMap. We demonstrate the utility of our approach across multiple\ntasks of central interest to the international community, including the\nmeasurement of population density and economic livelihoods. Across these tasks,\nour method demonstrates a 70% improvement in performance (measured using\nPearson's $r^2$) relative to baselines that use nearest neighbors or use\ninformation directly from the prompt, and performance equal to or exceeding\nsatellite-based benchmarks in the literature. With GeoLLM, we observe that\nGPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting\nthat the performance of our method scales well with the size of the model and\nits pretraining dataset. Our experiments reveal that LLMs are remarkably\nsample-efficient, rich in geospatial information, and robust across the globe.\nCrucially, GeoLLM shows promise in mitigating the limitations of existing\ngeospatial covariates and complementing them well.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Improved prompting and process for writing user personas with LLMs,\n  using qualitative interviews: Capturing behaviour and personality traits of\n  users\u2b1b  This draft paper presents a workflow for creating User Personas with Large\nLanguage Models, using the results of a Thematic Analysis of qualitative\ninterviews. The proposed workflow uses improved prompting and a larger pool of\nThemes, compared to previous work conducted by the author for the same task.\nThis is possible due to the capabilities of a recently released LLM which\nallows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to\nthe possibility to offer a refined prompting for the creation of Personas. The\npaper offers details of performing Phase 2 and 3 of Thematic Analysis, and then\ndiscusses the improved workflow for creating Personas. The paper also offers\nsome reflections on the relationship between the proposed process and existing\napproaches to Personas such as the data-driven and qualitative Personas.\nMoreover, the paper offers reflections on the capacity of LLMs to capture user\nbehaviours and personality traits, from the underlying dataset of qualitative\ninterviews used for the analysis.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Understanding the Effects of RLHF on LLM Generalisation and Diversity\u2b1b  Large language models (LLMs) fine-tuned with reinforcement learning from\nhuman feedback (RLHF) have been used in some of the most widely deployed AI\nmodels to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's\nLLaMA-2. While there has been significant work developing these methods, our\nunderstanding of the benefits and downsides of each stage in RLHF is still\nlimited. To fill this gap, we present an extensive analysis of how each stage\nof the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF)\naffects two key properties: out-of-distribution (OOD) generalisation and output\ndiversity. OOD generalisation is crucial given the wide range of real-world\nscenarios in which these models are being used, while output diversity refers\nto the model's ability to generate varied outputs and is important for a\nvariety of use cases. We perform our analysis across two base models on both\nsummarisation and instruction following tasks, the latter being highly relevant\nfor current LLM use cases. We find that RLHF generalises better than SFT to new\ninputs, particularly as the distribution shift between train and test becomes\nlarger. However, RLHF significantly reduces output diversity compared to SFT\nacross a variety of measures, implying a tradeoff in current LLM fine-tuning\nmethods between generalisation and diversity. Our results provide guidance on\nwhich fine-tuning method should be used depending on the application, and show\nthat more research is needed to improve the trade-off between generalisation\nand diversity.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Revisit Input Perturbation Problems for LLMs: A Unified Robustness\n  Evaluation Framework for Noisy Slot Filling Task\u2b1b  With the increasing capabilities of large language models (LLMs), these\nhigh-performance models have achieved state-of-the-art results on a wide range\nof natural language processing (NLP) tasks. However, the models' performance on\ncommonly-used benchmark datasets often fails to accurately reflect their\nreliability and robustness when applied to real-world noisy data. To address\nthese challenges, we propose a unified robustness evaluation framework based on\nthe slot-filling task to systematically evaluate the dialogue understanding\ncapability of LLMs in diverse input perturbation scenarios. Specifically, we\nconstruct a input perturbation evaluation dataset, Noise-LLM, which contains\nfive types of single perturbation and four types of mixed perturbation data.\nFurthermore, we utilize a multi-level data augmentation method (character,\nword, and sentence levels) to construct a candidate data pool, and carefully\ndesign two ways of automatic task demonstration construction strategies\n(instance-level and entity-level) with various prompt templates. Our aim is to\nassess how well various robustness methods of LLMs perform in real-world noisy\nscenarios. The experiments have demonstrated that the current open-source LLMs\ngenerally achieve limited perturbation robustness performance. Based on these\nexperimental observations, we make some forward-looking suggestions to fuel the\nresearch in this direction.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios\n  via Prompt Compression\u2b1b  In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational/financial cost, longer latency, and inferior\nperformance. Some studies reveal that the performance of LLMs depends on both\nthe density and the position of the key information (question relevant) in the\ninput prompt. Inspired by these findings, we propose LongLLMLingua for prompt\ncompression towards improving LLMs' perception of the key information to\nsimultaneously address the three challenges. We conduct evaluation on a wide\nrange of long context scenarios including single-/multi-document QA, few-shot\nlearning, summarization, synthetic tasks, and code completion. The experimental\nresults show that LongLLMLingua compressed prompt can derive higher performance\nwith much less cost. The latency of the end-to-end system is also reduced. For\nexample, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost\nof up to 17.1% over the original prompt with ~4x fewer tokens as input to\nGPT-3.5-Turbo. It can derive cost savings of \\$28.5 and \\$27.4 per 1,000\nsamples from the LongBench and ZeroScrolls benchmark, respectively.\nAdditionally, when compressing prompts of ~10k tokens at a compression rate of\n2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our\ncode is available at https://aka.ms/LLMLingua.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation\u2b1b  The rapid progress in open-source large language models (LLMs) is\nsignificantly advancing AI development. Extensive efforts have been made before\nmodel release to align their behavior with human values, with the primary goal\nof ensuring their helpfulness and harmlessness. However, even carefully aligned\nmodels can be manipulated maliciously, leading to unintended behaviors, known\nas \"jailbreaks\". These jailbreaks are typically triggered by specific text\ninputs, often referred to as adversarial prompts. In this work, we propose the\ngeneration exploitation attack, an extremely simple approach that disrupts\nmodel alignment by only manipulating variations of decoding methods. By\nexploiting different generation strategies, including varying decoding\nhyper-parameters and sampling methods, we increase the misalignment rate from\n0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,\nand MPT families, outperforming state-of-the-art attacks with $30\\times$ lower\ncomputational cost. Finally, we propose an effective alignment method that\nexplores diverse generation strategies, which can reasonably reduce the\nmisalignment rate under our attack. Altogether, our study underscores a major\nfailure in current safety evaluation and alignment procedures for open-source\nLLMs, strongly advocating for more comprehensive red teaming and better\nalignment before releasing such models. Our code is available at\nhttps://github.com/Princeton-SysML/Jailbreak_LLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State\n  Decoding\u2b1b  Large language models (LLMs) have shown promising capabilities in using\nexternal tools to solve complex problems. However, existing approaches either\ninvolve fine-tuning on tool demonstrations, which do not generalize to new\ntools without additional training, or providing tool documentation in context,\nlimiting the number of tools. Both approaches often generate syntactically\ninvalid tool calls. In this paper, we propose ToolDec, a finite-state\nmachine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates\ntool-related errors for any tool-augmented LLMs by ensuring valid tool names\nand type-conforming arguments. Furthermore, ToolDec enables LLM to effectively\nselect tools using only the information contained in their names, with no need\nfor fine-tuning or in-context documentation. We evaluated multiple prior\nmethods and their ToolDec-enhanced versions on a variety of tasks involving\ntools like math functions, knowledge graph relations, and complex real-world\nRESTful APIs. Our experiments show that ToolDec reduces syntactic errors to\nzero, consequently achieving significantly better performance and as much as a\n2x speedup. We also show that ToolDec achieves superior generalization\nperformance on unseen tools, performing up to 8x better than the baselines.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources\u2b1b  Large Language Models (LLMs) have showcased remarkable impacts across a wide\nspectrum of natural language processing tasks. Fine-tuning these pre-trained\nmodels on downstream datasets provides further significant performance gains,\nbut this process has been challenging due to its extraordinary resource\nrequirements. To this end, existing efforts focus on parameter-efficient\nfine-tuning, which, unfortunately, fail to capitalize on the powerful potential\nof full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized\nFull-parameter Tuning framework for LLMs that enables memory-efficient\nfine-tuning without harming performance. Our framework incorporates two novel\nideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the\nmomentum and has consistent update magnitudes for each parameter, an inherent\nadvantage for robust quantization; and (ii) we quantize all model states and\nstore them as integer values, and present a gradient flow and parameter update\nscheme for the quantized weights. As a result, QFT reduces the model state\nmemory to 21% of the standard solution while achieving comparable performance,\ne.g., tuning a LLaMA-7B model requires only <30GB of memory, satisfied by a\nsingle A6000 GPU.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Ethical Reasoning over Moral Alignment: A Case and Framework for\n  In-Context Ethical Policies in LLMs\u2b1b  In this position paper, we argue that instead of morally aligning LLMs to\nspecific set of ethical principles, we should infuse generic ethical reasoning\ncapabilities into them so that they can handle value pluralism at a global\nscale. When provided with an ethical policy, an LLM should be capable of making\ndecisions that are ethically consistent to the policy. We develop a framework\nthat integrates moral dilemmas with moral principles pertaining to different\nforamlisms of normative ethics, and at different levels of abstractions.\nInitial experiments with GPT-x models shows that while GPT-4 is a nearly\nperfect ethical reasoner, the models still have bias towards the moral values\nof Western and English speaking societies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in\n  Self-Refined Open-Source Models\u2b1b  The dominance of proprietary LLMs has led to restricted access and raised\ninformation privacy concerns. High-performing open-source alternatives are\ncrucial for information-sensitive and high-volume applications but often lag\nbehind in performance. To address this gap, we propose (1) A untargeted variant\nof iterative self-critique and self-refinement devoid of external influence.\n(2) A novel ranking metric - Performance, Refinement, and Inference Cost Score\n(PeRFICS) - to find the optimal model for a given task considering refined\nperformance and cost. Our experiments show that SoTA open source models of\nvarying sizes from 7B - 65B, on average, improve 8.2% from their baseline\nperformance. Strikingly, even models with extremely small memory footprints,\nsuch as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39%\nimprovement in high-creativity, open ended tasks on the Vicuna benchmark.\nVicuna-13B takes it a step further and outperforms ChatGPT post-refinement.\nThis work has profound implications for resource-constrained and\ninformation-sensitive environments seeking to leverage LLMs without incurring\nprohibitive costs, compromising on performance and privacy. The domain-agnostic\nself-refinement process coupled with our novel ranking metric facilitates\ninformed decision-making in model selection, thereby reducing costs and\ndemocratizing access to high-performing language models, as evidenced by case\nstudies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM4Vis: Explainable Visualization Recommendation using ChatGPT\u2b1b  Data visualization is a powerful tool for exploring and communicating\ninsights in various domains. To automate visualization choice for datasets, a\ntask known as visualization recommendation has been proposed. Various\nmachine-learning-based approaches have been developed for this purpose, but\nthey often require a large corpus of dataset-visualization pairs for training\nand lack natural explanations for their results. To address this research gap,\nwe propose LLM4Vis, a novel ChatGPT-based prompting approach to perform\nvisualization recommendation and return human-like explanations using very few\ndemonstration examples. Our approach involves feature description,\ndemonstration example selection, explanation generation, demonstration example\nconstruction, and inference steps. To obtain demonstration examples with\nhigh-quality explanations, we propose a new explanation generation\nbootstrapping to iteratively refine generated explanations by considering the\nprevious generation and template-based hint. Evaluations on the VizML dataset\nshow that LLM4Vis outperforms or performs similarly to supervised learning\nmodels like Random Forest, Decision Tree, and MLP in both few-shot and\nzero-shot settings. The qualitative evaluation also shows the effectiveness of\nexplanations generated by LLM4Vis. We make our code publicly available at\n\\href{https://github.com/demoleiwang/LLM4Vis}{https://github.com/demoleiwang/LLM4Vis}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large\n  Language Models\u2b1b  Large Language Models (LLMs) excel in NLP, but their demands hinder their\nwidespread deployment. While Quantization-Aware Training (QAT) offers a\nsolution, its extensive training costs make Post-Training Quantization (PTQ) a\nmore practical approach for LLMs. In existing studies, activation outliers in\nparticular channels are identified as the bottleneck to PTQ accuracy. They\npropose to transform the magnitudes from activations to weights, which however\noffers limited alleviation or suffers from unstable gradients, resulting in a\nsevere performance drop at low-bitwidth. In this paper, we propose QLLM, an\naccurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM\nintroduces an adaptive channel reassembly technique that reallocates the\nmagnitude of outliers to other channels, thereby mitigating their impact on the\nquantization range. This is achieved by channel disassembly and channel\nassembly, which first breaks down the outlier channels into several\nsub-channels to ensure a more balanced distribution of activation magnitudes.\nThen similar channels are merged to maintain the original channel number for\nefficiency. Additionally, an adaptive strategy is designed to autonomously\ndetermine the optimal number of sub-channels for channel disassembly. To\nfurther compensate for the performance loss caused by quantization, we propose\nan efficient tuning method that only learns a small number of low-rank weights\nwhile freezing the pre-trained quantized model. After training, these low-rank\nparameters can be fused into the frozen weights without affecting inference.\nExtensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate\nquantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B\nwithin 10 hours on a single A100-80G GPU, outperforming the previous\nstate-of-the-art method by 7.89% on the average accuracy across five zero-shot\ntasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative\n  Writing\u2b1b  We evaluate a range of recent LLMs on English creative writing, a challenging\nand complex task that requires imagination, coherence, and style. We use a\ndifficult, open-ended scenario chosen to avoid training data reuse: an epic\nnarration of a single combat between Ignatius J. Reilly, the protagonist of the\nPulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl,\na prehistoric flying reptile. We ask several LLMs and humans to write such a\nstory and conduct a human evalution involving various criteria such as fluency,\ncoherence, originality, humor, and style. Our results show that some\nstate-of-the-art commercial LLMs match or slightly outperform our writers in\nmost dimensions; whereas open-source LLMs lag behind. Humans retain an edge in\ncreativity, while humor shows a binary divide between LLMs that can handle it\ncomparably to humans and those that fail at it. We discuss the implications and\nlimitations of our study and suggest directions for future research.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-augmented Preference Learning from Natural Language\u2b1b  Finding preferences expressed in natural language is an important but\nchallenging task. State-of-the-art(SotA) methods leverage transformer-based\nmodels such as BERT, RoBERTa, etc. and graph neural architectures such as graph\nattention networks. Since Large Language Models (LLMs) are equipped to deal\nwith larger context lengths and have much larger model sizes than the\ntransformer-based model, we investigate their ability to classify comparative\ntext directly. This work aims to serve as a first step towards using LLMs for\nthe CPC task. We design and conduct a set of experiments that format the\nclassification task into an input prompt for the LLM and a methodology to get a\nfixed-format response that can be automatically evaluated. Comparing\nperformances with existing methods, we see that pre-trained LLMs are able to\noutperform the previous SotA models with no fine-tuning involved. Our results\nshow that the LLMs can consistently outperform the SotA when the target text is\nlarge -- i.e. composed of multiple sentences --, and are still comparable to\nthe SotA performance in shorter text. We also find that few-shot learning\nyields better performance than zero-shot learning.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Formally Specifying the High-Level Behavior of LLM-Based Agents\u2b1b  LLM-based agents have recently emerged as promising tools for solving\nchallenging problems without the need for task-specific finetuned models that\ncan be expensive to procure. Currently, the design and implementation of such\nagents is ad hoc, as the wide variety of tasks that LLM-based agents may be\napplied to naturally means there can be no one-size-fits-all approach to agent\ndesign. In this work we aim to alleviate the difficulty of designing and\nimplementing new agents by proposing a minimalistic, high-level generation\nframework that simplifies the process of building agents. The framework we\nintroduce allows the user to specify desired agent behaviors in Linear Temporal\nLogic (LTL). The declarative LTL specification is then used to construct a\nconstrained decoder that guarantees the LLM will produce an output exhibiting\nthe desired behavior. By designing our framework in this way, we obtain several\nbenefits, including the ability to enforce complex agent behavior, the ability\nto formally validate prompt examples, and the ability to seamlessly incorporate\ncontent-focused logical constraints into generation. In particular, our\ndeclarative approach, in which the desired behavior is simply described without\nconcern for how it should be implemented or enforced, enables rapid design,\nimplementation and experimentation with different LLM-based agents. We\ndemonstrate how the proposed framework can be used to implement recent\nLLM-based agents, and show how the guardrails our approach provides can lead to\nimprovements in agent performance. In addition, we release our code for general\nuse.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Making Multimodal Generation Easier: When Diffusion Models Meet LLMs\u2b1b  We present EasyGen, an efficient model designed to enhance multimodal\nunderstanding and generation by harnessing the capabilities of diffusion models\nand large language models (LLMs). Unlike existing multimodal models that\npredominately depend on encoders like CLIP or ImageBind and need ample amounts\nof training data to bridge the gap between modalities, EasyGen is built upon a\nbidirectional conditional diffusion model named BiDiffuser, which promotes more\nefficient interactions between modalities. EasyGen handles image-to-text\ngeneration by integrating BiDiffuser and an LLM via a simple projection layer.\nUnlike most existing multimodal models that are limited to generating text\nresponses, EasyGen can also facilitate text-to-image generation by leveraging\nthe LLM to create textual descriptions, which can be interpreted by BiDiffuser\nto generate appropriate visual responses. Extensive quantitative and\nqualitative experiments demonstrate the effectiveness of EasyGen, whose\ntraining can be easily achieved in a lab setting. The source code is available\nat https://github.com/zxy556677/EasyGen.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in\n  LLM-Generated Reference Letters\u2b1b  Large Language Models (LLMs) have recently emerged as an effective tool to\nassist individuals in writing various types of content, including professional\ndocuments such as recommendation letters. Though bringing convenience, this\napplication also introduces unprecedented fairness concerns. Model-generated\nreference letters might be directly used by users in professional scenarios. If\nunderlying biases exist in these model-constructed letters, using them without\nscrutinization could lead to direct societal harms, such as sabotaging\napplication success rates for female applicants. In light of this pressing\nissue, it is imminent and necessary to comprehensively study fairness issues\nand associated harms in this real-world use case. In this paper, we critically\nexamine gender biases in LLM-generated reference letters. Drawing inspiration\nfrom social science findings, we design evaluation methods to manifest biases\nthrough 2 dimensions: (1) biases in language style and (2) biases in lexical\ncontent. We further investigate the extent of bias propagation by analyzing the\nhallucination bias of models, a term that we define to be bias exacerbation in\nmodel-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-\nChatGPT and Alpaca, we reveal significant gender biases in LLM-generated\nrecommendation letters. Our findings not only warn against using LLMs for this\napplication without scrutinization, but also illuminate the importance of\nthoroughly studying hidden biases and harms in LLM-generated professional\ndocuments.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model\n  Collaboration\u2b1b  Legal Judgment Prediction (LJP) has become an increasingly crucial task in\nLegal AI, i.e., predicting the judgment of the case in terms of case fact\ndescription. Precedents are the previous legal cases with similar facts, which\nare the basis for the judgment of the subsequent case in national legal\nsystems. Thus, it is worthwhile to explore the utilization of precedents in the\nLJP. Recent advances in deep learning have enabled a variety of techniques to\nbe used to solve the LJP task. These can be broken down into two categories:\nlarge language models (LLMs) and domain-specific models. LLMs are capable of\ninterpreting and generating complex natural language, while domain models are\nefficient in learning task-specific information. In this paper, we propose the\nprecedent-enhanced LJP framework (PLJP), a system that leverages the strength\nof both LLM and domain models in the context of precedents. Specifically, the\ndomain models are designed to provide candidate labels and find the proper\nprecedents efficiently, and the large models will make the final prediction\nwith an in-context precedents comprehension. Experiments on the real-world\ndataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a\npromising direction for LLM and domain-model collaboration that can be\ngeneralized to other vertical domains.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Ranking LLM-Generated Loop Invariants for Program Verification\u2b1b  Synthesizing inductive loop invariants is fundamental to automating program\nverification. In this work, we observe that Large Language Models (such as\ngpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of\nprograms in a 0-shot setting, yet require several samples to generate the\ncorrect invariants. This can lead to a large number of calls to a program\nverifier to establish an invariant. To address this issue, we propose a {\\it\nre-ranking} approach for the generated results of LLMs. We have designed a\nranker that can distinguish between correct inductive invariants and incorrect\nattempts based on the problem definition. The ranker is optimized as a\ncontrastive ranker. Experimental results demonstrate that this re-ranking\nmechanism significantly improves the ranking of correct invariants among the\ngenerated candidates, leading to a notable reduction in the number of calls to\na verifier.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Beyond Testers' Biases: Guiding Model Testing with Knowledge Bases using\n  LLMs\u2b1b  Current model testing work has mostly focused on creating test cases.\nIdentifying what to test is a step that is largely ignored and poorly\nsupported. We propose Weaver, an interactive tool that supports requirements\nelicitation for guiding model testing. Weaver uses large language models to\ngenerate knowledge bases and recommends concepts from them interactively,\nallowing testers to elicit requirements for further testing. Weaver provides\nrich external knowledge to testers and encourages testers to systematically\nexplore diverse concepts beyond their own biases. In a user study, we show that\nboth NLP experts and non-experts identified more, as well as more diverse\nconcepts worth testing when using Weaver. Collectively, they found more than\n200 failing test cases for stance detection with zero-shot ChatGPT. Our case\nstudies further show that Weaver can help practitioners test models in\nreal-world settings, where developers define more nuanced application scenarios\n(e.g., code understanding and transcript summarization) using LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Prompt Packer: Deceiving LLMs through Compositional Instruction with\n  Hidden Attacks\u2b1b  Recently, Large language models (LLMs) with powerful general capabilities\nhave been increasingly integrated into various Web applications, while\nundergoing alignment training to ensure that the generated content aligns with\nuser intent and ethics. Unfortunately, they remain the risk of generating\nharmful content like hate speech and criminal activities in practical\napplications. Current approaches primarily rely on detecting, collecting, and\ntraining against harmful prompts to prevent such risks. However, they typically\nfocused on the \"superficial\" harmful prompts with a solitary intent, ignoring\ncomposite attack instructions with multiple intentions that can easily elicit\nharmful content in real-world scenarios. In this paper, we introduce an\ninnovative technique for obfuscating harmful instructions: Compositional\nInstruction Attacks (CIA), which refers to attacking by combination and\nencapsulation of multiple instructions. CIA hides harmful prompts within\ninstructions of harmless intentions, making it impossible for the model to\nidentify underlying malicious intentions. Furthermore, we implement two\ntransformation methods, known as T-CIA and W-CIA, to automatically disguise\nharmful instructions as talking or writing tasks, making them appear harmless\nto LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety\nassessment datasets and two harmful prompt datasets. It achieves an attack\nsuccess rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+\nfor ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets.\nOur approach reveals the vulnerability of LLMs to such compositional\ninstruction attacks that harbor underlying harmful intentions, contributing\nsignificantly to LLM security development. Warning: this paper may contain\noffensive or upsetting content!\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Character-LLM: A Trainable Agent for Role-Playing\u2b1b  Large language models (LLMs) can be used to serve as agents to simulate human\nbehaviors, given the powerful ability to understand human instructions and\nprovide high-quality generated texts. Such ability stimulates us to wonder\nwhether LLMs can simulate a person in a higher form than simple human\nbehaviors. Therefore, we aim to train an agent with the profile, experience,\nand emotional states of a specific person instead of using limited prompts to\ninstruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs\nto act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,\netc. Our method focuses on editing profiles as experiences of a certain\ncharacter and training models to be personal simulacra with these experiences.\nTo assess the effectiveness of our approach, we build a test playground that\ninterviews trained agents and evaluates whether the agents \\textit{memorize}\ntheir characters and experiences. Experimental results show interesting\nobservations that help build future simulacra of humankind.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Tabular Representation, Noisy Operators, and Impacts on Table Structure\n  Understanding Tasks in LLMs\u2b1b  Large language models (LLMs) are increasingly applied for tabular tasks using\nin-context learning. The prompt representation for a table may play a role in\nthe LLMs ability to process the table. Inspired by prior work, we generate a\ncollection of self-supervised structural tasks (e.g. navigate to a cell and\nrow; transpose the table) and evaluate the performance differences when using 8\nformats. In contrast to past work, we introduce 8 noise operations inspired by\nreal-world messy data and adversarial inputs, and show that such operations can\nimpact LLM performance across formats for different structural understanding\ntasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Stance Detection with Collaborative Role-Infused LLM-Based Agents\u2b1b  Stance detection automatically detects the stance in a text towards a target,\nvital for content analysis in web and social media research. Despite their\npromising capabilities, LLMs encounter challenges when directly applied to\nstance detection. First, stance detection demands multi-aspect knowledge, from\ndeciphering event-related terminologies to understanding the expression styles\nin social media platforms. Second, stance detection requires advanced reasoning\nto infer authors' implicit viewpoints, as stance are often subtly embedded\nrather than overtly stated in the text. To address these challenges, we design\na three-stage framework COLA (short for Collaborative rOle-infused LLM-based\nAgents) in which LLMs are designated distinct roles, creating a collaborative\nsystem where each role contributes uniquely. Initially, in the multidimensional\ntext analysis stage, we configure the LLMs to act as a linguistic expert, a\ndomain specialist, and a social media veteran to get a multifaceted analysis of\ntexts, thus overcoming the first challenge. Next, in the reasoning-enhanced\ndebating stage, for each potential stance, we designate a specific LLM-based\nagent to advocate for it, guiding the LLM to detect logical connections between\ntext features and stance, tackling the second challenge. Finally, in the stance\nconclusion stage, a final decision maker agent consolidates prior insights to\ndetermine the stance. Our approach avoids extra annotated data and model\ntraining and is highly usable. We achieve state-of-the-art performance across\nmultiple datasets. Ablation studies validate the effectiveness of each design\nrole in handling stance detection. Further experiments have demonstrated the\nexplainability and the versatility of our approach. Our approach excels in\nusability, accuracy, effectiveness, explainability and versatility,\nhighlighting its value.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications\n  with Programmable Rails\u2b1b  NeMo Guardrails is an open-source toolkit for easily adding programmable\nguardrails to LLM-based conversational systems. Guardrails (or rails for short)\nare a specific way of controlling the output of an LLM, such as not talking\nabout topics considered harmful, following a predefined dialogue path, using a\nparticular language style, and more. There are several mechanisms that allow\nLLM providers and developers to add guardrails that are embedded into a\nspecific model at training, e.g. using model alignment. Differently, using a\nruntime inspired from dialogue management, NeMo Guardrails allows developers to\nadd programmable rails to LLM applications - these are user-defined,\nindependent of the underlying LLM, and interpretable. Our initial results show\nthat the proposed approach can be used with several LLM providers to develop\ncontrollable and safe LLM applications using programmable rails.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology\u2b1b  The ability to automatically generate accurate protocols for scientific\nexperiments would represent a major step towards the automation of science.\nLarge Language Models (LLMs) have impressive capabilities on a wide range of\ntasks, such as question answering and the generation of coherent text and code.\nHowever, LLMs can struggle with multi-step problems and long-term planning,\nwhich are crucial for designing scientific experiments. Moreover, evaluation of\nthe accuracy of scientific protocols is challenging, because experiments can be\ndescribed correctly in many different ways, require expert knowledge to\nevaluate, and cannot usually be executed automatically. Here we present an\nautomatic evaluation framework for the task of planning experimental protocols,\nand we introduce BioProt: a dataset of biology protocols with corresponding\npseudocode representations. To measure performance on generating scientific\nprotocols, we use an LLM to convert a natural language protocol into\npseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode\nfrom a high-level description and a list of admissible pseudocode functions. We\nevaluate GPT-3 and GPT-4 on this task and explore their robustness. We\nexternally validate the utility of pseudocode representations of text by\ngenerating accurate novel protocols using retrieved pseudocode, and we run a\ngenerated protocol successfully in our biological laboratory. Our framework is\nextensible to the evaluation and improvement of language model planning\nabilities in other areas of science or other areas that lack automatic\nevaluation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate\n  Emotion Probability Vectors\u2b1b  This paper shows how LLMs (Large Language Models) may be used to estimate a\nsummary of the emotional state associated with piece of text. The summary of\nemotional state is a dictionary of words used to describe emotion together with\nthe probability of the word appearing after a prompt comprising the original\ntext and an emotion eliciting tail. Through emotion analysis of Amazon product\nreviews we demonstrate emotion descriptors can be mapped into a PCA type space.\nIt was hoped that text descriptions of actions to improve a current text\ndescribed state could also be elicited through a tail prompt. Experiment seemed\nto indicate that this is not straightforward to make work. This failure put our\nhoped for selection of action via choosing the best predict ed outcome via\ncomparing emotional responses out of reach for the moment.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs as Potential Brainstorming Partners for Math and Science Problems\u2b1b  With the recent rise of widely successful deep learning models, there is\nemerging interest among professionals in various math and science communities\nto see and evaluate the state-of-the-art models' abilities to collaborate on\nfinding or solving problems that often require creativity and thus\nbrainstorming. While a significant chasm still exists between current\nhuman-machine intellectual collaborations and the resolution of complex math\nand science problems, such as the six unsolved Millennium Prize Problems, our\ninitial investigation into this matter reveals a promising step towards\nbridging the divide. This is due to the recent advancements in Large Language\nModels (LLMs). More specifically, we conduct comprehensive case studies to\nexplore both the capabilities and limitations of the current state-of-the-art\nLLM, notably GPT-4, in collective brainstorming with humans.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for\n  Code Generation\u2b1b  Large language models (LLMs) have showcased remarkable prowess in code\ngeneration. However, automated code generation is still challenging since it\nrequires a high-level semantic mapping between natural language requirements\nand codes. Most existing LLMs-based approaches for code generation rely on\ndecoder-only causal language models often treate codes merely as plain text\ntokens, i.e., feeding the requirements as a prompt input, and outputing code as\nflat sequence of tokens, potentially missing the rich semantic features\ninherent in source code. To bridge this gap, this paper proposes the \"Semantic\nChain-of-Thought\" approach to intruduce semantic information of code, named\nSeCoT. Our motivation is that the semantic information of the source code (\\eg\ndata flow and control flow) describes more precise program execution behavior,\nintention and function. By guiding LLM consider and integrate semantic\ninformation, we can achieve a more granular understanding and representation of\ncode, enhancing code generation accuracy. Meanwhile, while traditional\ntechniques leveraging such semantic information require complex static or\ndynamic code analysis to obtain features such as data flow and control flow,\nSeCoT demonstrates that this process can be fully automated via the intrinsic\ncapabilities of LLMs (i.e., in-context learning), while being generalizable and\napplicable to challenging domains. While SeCoT can be applied with different\nLLMs, this paper focuses on the powerful GPT-style models: ChatGPT(close-source\nmodel) and WizardCoder(open-source model). The experimental study on three\npopular DL benchmarks (i.e., HumanEval, HumanEval-ET and MBPP) shows that SeCoT\ncan achieves state-of-the-art performance, greatly improving the potential for\nlarge models and code generation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation\n  through the Lens of News Headline Generation\u2b1b  To explore how humans can best leverage LLMs for writing and how interacting\nwith these models affects feelings of ownership and trust in the writing\nprocess, we compared common human-AI interaction types (e.g., guiding system,\nselecting from system outputs, post-editing outputs) in the context of\nLLM-assisted news headline generation. While LLMs alone can generate\nsatisfactory news headlines, on average, human control is needed to fix\nundesirable model outputs. Of the interaction methods, guiding and selecting\nmodel output added the most benefit with the lowest cost (in time and effort).\nFurther, AI assistance did not harm participants' perception of control\ncompared to freeform editing.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Fake News in Sheep's Clothing: Robust Fake News Detection Against\n  LLM-Empowered Style Attacks\u2b1b  It is commonly perceived that online fake news and reliable news exhibit\nstark differences in writing styles, such as the use of sensationalist versus\nobjective language. However, we emphasize that style-related features can also\nbe exploited for style-based attacks. Notably, the rise of powerful Large\nLanguage Models (LLMs) has enabled malicious users to mimic the style of\ntrustworthy news outlets at minimal cost. Our analysis reveals that\nLLM-camouflaged fake news content leads to substantial performance degradation\nof state-of-the-art text-based detectors (up to 38% decrease in F1 Score),\nposing a significant challenge for automated detection in online ecosystems. To\naddress this, we introduce SheepDog, a style-agnostic fake news detector robust\nto news writing styles. SheepDog achieves this adaptability through\nLLM-empowered news reframing, which customizes each article to match different\nwriting styles using style-oriented reframing prompts. By employing\nstyle-agnostic training, SheepDog enhances its resilience to stylistic\nvariations by maximizing prediction consistency across these diverse\nreframings. Furthermore, SheepDog extracts content-focused veracity\nattributions from LLMs, where the news content is evaluated against a set of\nfact-checking rationales. These attributions provide supplementary information\nand potential interpretability that assist veracity prediction. On three\nbenchmark datasets, empirical results show that SheepDog consistently yields\nsignificant improvements over competitive baselines and enhances robustness\nagainst LLM-empowered style attacks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TEQ: Trainable Equivalent Transformation for Quantization of LLMs\u2b1b  As large language models (LLMs) become more prevalent, there is a growing\nneed for new and improved quantization methods that can meet the\ncomputationalast layer demands of these modern architectures while maintaining\nthe accuracy. In this paper, we present TEQ, a trainable equivalent\ntransformation that preserves the FP32 precision of the model output while\ntaking advantage of low-precision quantization, especially 3 and 4 bits\nweight-only quantization. The training process is lightweight, requiring only\n1K steps and fewer than 0.1 percent of the original model's trainable\nparameters. Furthermore, the transformation does not add any computational\noverhead during inference. Our results are on-par with the state-of-the-art\n(SOTA) methods on typical LLMs. Our approach can be combined with other methods\nto achieve even better performance. The code is available at\nhttps://github.com/intel/neural-compressor.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Exploring Automatic Evaluation Methods based on a Decoder-based LLM for\n  Text Generation\u2b1b  Automatic evaluation of text generation is essential for improving the\naccuracy of generation tasks. In light of the current trend towards\nincreasingly larger decoder-based language models, we investigate automatic\nevaluation methods based on such models for text generation. This paper\ncompares various methods, including tuning with encoder-based models and large\nlanguage models under equal conditions, on two different tasks, machine\ntranslation evaluation and semantic textual similarity, in two languages,\nJapanese and English. Experimental results show that compared to the tuned\nencoder-based models, the tuned decoder-based models perform poorly. The\nanalysis of the causes for this suggests that the decoder-based models focus on\nsurface word sequences and do not capture meaning. It is also revealed that\nin-context learning of very large decoder-based models such as ChatGPT makes it\ndifficult to identify fine-grained semantic differences.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can Large Language Models Explain Themselves? A Study of LLM-Generated\n  Self-Explanations\u2b1b  Large language models (LLMs) such as ChatGPT have demonstrated superior\nperformance on a variety of natural language processing (NLP) tasks including\nsentiment analysis, mathematical reasoning and summarization. Furthermore,\nsince these models are instruction-tuned on human conversations to produce\n\"helpful\" responses, they can and often will produce explanations along with\nthe response, which we call self-explanations. For example, when analyzing the\nsentiment of a movie review, the model may output not only the positivity of\nthe sentiment, but also an explanation (e.g., by listing the sentiment-laden\nwords such as \"fantastic\" and \"memorable\" in the review). How good are these\nautomatically generated self-explanations? In this paper, we investigate this\nquestion on the task of sentiment analysis and for feature attribution\nexplanation, one of the most commonly studied settings in the interpretability\nliterature (for pre-ChatGPT models). Specifically, we study different ways to\nelicit the self-explanations, evaluate their faithfulness on a set of\nevaluation metrics, and compare them to traditional explanation methods such as\nocclusion or LIME saliency maps. Through an extensive set of experiments, we\nfind that ChatGPT's self-explanations perform on par with traditional ones, but\nare quite different from them according to various agreement metrics, meanwhile\nbeing much cheaper to produce (as they are generated along with the\nprediction). In addition, we identified several interesting characteristics of\nthem, which prompt us to rethink many current model interpretability practices\nin the era of ChatGPT(-like) LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Watermarking LLMs with Weight Quantization\u2b1b  Abuse of large language models reveals high risks as large language models\nare being deployed at an astonishing speed. It is important to protect the\nmodel weights to avoid malicious usage that violates licenses of open-source\nlarge language models. This paper proposes a novel watermarking strategy that\nplants watermarks in the quantization process of large language models without\npre-defined triggers during inference. The watermark works when the model is\nused in the fp32 mode and remains hidden when the model is quantized to int8,\nin this way, the users can only inference the model without further supervised\nfine-tuning of the model. We successfully plant the watermark into open-source\nlarge language model weights including GPT-Neo and LLaMA. We hope our proposed\nmethod can provide a potential direction for protecting model weights in the\nera of large language model applications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for\n  Emotion Recognition in Conversations\u2b1b  Large language models (LLMs) and their variants have shown extraordinary\nefficacy across numerous downstream natural language processing (NLP) tasks,\nwhich has presented a new vision for the development of NLP. Despite their\nremarkable performance in natural language generating (NLG), LLMs lack a\ndistinct focus on the emotion understanding domain. As a result, using LLMs for\nemotion recognition may lead to suboptimal and inadequate precision. Another\nlimitation of LLMs is that they are typical trained without leveraging\nmulti-modal information. To overcome these limitations, we propose DialogueLLM,\na context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA\nmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.\nThe visual information is considered as the supplementary knowledge to\nconstruct high-quality instructions. We offer a comprehensive evaluation of our\nproposed model on three benchmarking emotion recognition in conversations (ERC)\ndatasets and compare the results against the SOTA baselines and other SOTA\nLLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB\nA100 GPU in 5 hours, facilitating reproducibility for other researchers.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations\u2b1b  Recent work has aimed to capture nuances of human behavior by using LLMs to\nsimulate responses from particular demographics in settings like social science\nexperiments and public opinion surveys. However, there are currently no\nestablished ways to discuss or evaluate the quality of such LLM simulations.\nMoreover, there is growing concern that these LLM simulations are flattened\ncaricatures of the personas that they aim to simulate, failing to capture the\nmultidimensionality of people and perpetuating stereotypes. To bridge these\ngaps, we present CoMPosT, a framework to characterize LLM simulations using\nfour dimensions: Context, Model, Persona, and Topic. We use this framework to\nmeasure open-ended LLM simulations' susceptibility to caricature, defined via\ntwo criteria: individuation and exaggeration. We evaluate the level of\ncaricature in scenarios from existing work on LLM simulations. We find that for\nGPT-4, simulations of certain demographics (political and marginalized groups)\nand topics (general, uncontroversial) are highly susceptible to caricature.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs\u2b1b  Large language models (LLMs) have recently shown great advances in a variety\nof tasks, including natural language understanding and generation. However,\ntheir use in high-stakes decision-making scenarios is still limited due to the\npotential for errors. Selective prediction is a technique that can be used to\nimprove the reliability of the LLMs by allowing them to abstain from making\npredictions when they are unsure of the answer. In this work, we propose a\nnovel framework for adaptation with self-evaluation to improve the selective\nprediction performance of LLMs. Our framework is based on the idea of using\nparameter-efficient tuning to adapt the LLM to the specific task at hand while\nimproving its ability to perform self-evaluation. We evaluate our method on a\nvariety of question-answering (QA) datasets and show that it outperforms\nstate-of-the-art selective prediction methods. For example, on the CoQA\nbenchmark, our method improves the AUACC from 91.23% to 92.63% and improves the\nAUROC from 74.61% to 80.25%.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning\u2b1b  Recent advancements in Large Language Models (LLMs) have expanded the\nhorizons of natural language understanding and generation. Notably, the output\ncontrol and alignment with the input of LLMs can be refined through instruction\ntuning. However, as highlighted in several studies, low-quality data in the\ntraining set are usually detrimental to instruction tuning, resulting in\ninconsistent or even misleading LLM outputs. We propose a novel method, termed\n\"reflection-tuning,\" which addresses the problem by self-improvement and\njudging capabilities of LLMs. This approach utilizes an oracle LLM to recycle\nthe original training data by introspecting and enhancing the quality of\ninstructions and responses in the data. Extensive experiments on widely used\nevaluation benchmarks show that LLMs trained with our recycled data outperform\nthose trained with existing datasets in various benchmarks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM\n  Planning\u2b1b  Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows,\nconnection lines, etc.). Existing state-of-the-art T2I models often fail at\ndiagram generation because they lack fine-grained object layout control when\nmany objects are densely connected via complex relations such as arrows/lines\nand also often fail to render comprehensible text labels. To address this gap,\nwe present DiagrammerGPT, a novel two-stage text-to-diagram generation\nframework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4)\nto generate more accurate open-domain, open-platform diagrams. In the first\nstage, we use LLMs to generate and iteratively refine 'diagram plans' (in a\nplanner-auditor feedback loop) which describe all the entities (objects and\ntext labels), their relationships (arrows or lines), and their bounding box\nlayouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a\ntext label rendering module to generate diagrams following the diagram plans.\nTo benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nquantitatively and qualitatively that our DiagrammerGPT framework produces more\naccurate diagrams, outperforming existing T2I models. We also provide\ncomprehensive analysis including open-domain diagram generation, vector graphic\ndiagram generation in different platforms, human-in-the-loop diagram plan\nediting, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our\nwork can inspire further research on diagram generation via T2I models and\nLLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Eliminating Reasoning via Inferring with Planning: A New Framework to\n  Guide LLMs' Non-linear Thinking\u2b1b  Chain-of-Thought(CoT) prompting and its variants explore equipping large\nlanguage models (LLMs) with high-level reasoning abilities by emulating\nhuman-like linear cognition and logic. However, the human mind is complicated\nand mixed with both linear and nonlinear thinking. In this work, we propose\n\\textbf{I}nferential \\textbf{E}xclusion \\textbf{P}rompting (IEP), a novel\nprompting that combines the principles of elimination and inference in order to\nguide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize\nNatural Language Inference (NLI) to deduce each possible solution's entailment\nrelation with context, commonsense, or facts, therefore yielding a broader\nperspective by thinking back for inferring. This forward planning and backward\neliminating process allows IEP to better simulate the complex human thinking\nprocesses compared to other CoT-based methods, which only reflect linear\ncognitive processes. We conducted a series of empirical studies and have\ncorroborated that IEP consistently outperforms CoT across various tasks.\nAdditionally, we observe that integrating IEP and CoT further improves the\nLLMs' performance on certain tasks, highlighting the necessity of equipping\nLLMs with mixed logic processes. Moreover, to better evaluate comprehensive\nfeatures inherent in human logic, we introduce \\textbf{M}ental-\\textbf{A}bility\n\\textbf{R}easoning \\textbf{B}enchmark (MARB). The benchmark comprises six novel\nsubtasks with a total of 9,115 questions, among which 1,685 are developed with\nhand-crafted rationale references. We believe both \\textsc{IEP} and\n\\textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and\nverbal reasoning abilities and drive further advancements. \\textsc{MARB} will\nbe available at ~\\texttt{anonymity link} soon.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative\n  Large Language Models\u2b1b  We present REMARK-LLM, a novel efficient, and robust watermarking framework\ndesigned for texts generated by large language models (LLMs). Synthesizing\nhuman-like content using LLMs necessitates vast computational resources and\nextensive datasets, encapsulating critical intellectual property (IP). However,\nthe generated content is prone to malicious exploitation, including spamming\nand plagiarism. To address the challenges, REMARK-LLM proposes three new\ncomponents: (i) a learning-based message encoding module to infuse binary\nsignatures into LLM-generated texts; (ii) a reparameterization module to\ntransform the dense distributions from the message encoding to the sparse\ndistribution of the watermarked textual tokens; (iii) a decoding module\ndedicated for signature extraction; Furthermore, we introduce an optimized beam\nsearch algorithm to guarantee the coherence and consistency of the generated\ncontent. REMARK-LLM is rigorously trained to encourage the preservation of\nsemantic integrity in watermarked content, while ensuring effective watermark\nretrieval. Extensive evaluations on multiple unseen datasets highlight\nREMARK-LLM proficiency and transferability in inserting 2 times more signature\nbits into the same texts when compared to prior art, all while maintaining\nsemantic integrity. Furthermore, REMARK-LLM exhibits better resilience against\na spectrum of watermark detection and removal attacks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy\n  Searcher\u2b1b  The advent of Large Language Models (LLMs) has shown the potential to improve\nrelevance and provide direct answers in web searches. However, challenges arise\nin validating the reliability of generated results and the credibility of\ncontributing sources, due to the limitations of traditional information\nretrieval algorithms and the LLM hallucination problem. Aiming to create a\n\"PageRank\" for the LLM era, we strive to transform LLM into a relevant,\nresponsible, and trustworthy searcher. We propose a novel generative retrieval\nframework leveraging the knowledge of LLMs to foster a direct link between\nqueries and online sources. This framework consists of three core modules:\nGenerator, Validator, and Optimizer, each focusing on generating trustworthy\nonline sources, verifying source reliability, and refining unreliable sources,\nrespectively. Extensive experiments and evaluations highlight our method's\nsuperior relevance, responsibility, and trustfulness against various SOTA\nmethods.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Lost in Translation: When GPT-4V(ision) Can't See Eye to Eye with Text.\n  A Vision-Language-Consistency Analysis of VLLMs and Beyond\u2b1b  Recent advancements in multimodal techniques open exciting possibilities for\nmodels excelling in diverse tasks involving text, audio, and image processing.\nModels like GPT-4V, blending computer vision and language modeling, excel in\ncomplex text and image tasks. Numerous prior research endeavors have diligently\nexamined the performance of these Vision Large Language Models (VLLMs) across\ntasks like object detection, image captioning and others. However, these\nanalyses often focus on evaluating the performance of each modality in\nisolation, lacking insights into their cross-modal interactions. Specifically,\nquestions concerning whether these vision-language models execute vision and\nlanguage tasks consistently or independently have remained unanswered. In this\nstudy, we draw inspiration from recent investigations into multilingualism and\nconduct a comprehensive analysis of model's cross-modal interactions. We\nintroduce a systematic framework that quantifies the capability disparities\nbetween different modalities in the multi-modal setting and provide a set of\ndatasets designed for these evaluations. Our findings reveal that models like\nGPT-4V tend to perform consistently modalities when the tasks are relatively\nsimple. However, the trustworthiness of results derived from the vision\nmodality diminishes as the tasks become more challenging. Expanding on our\nfindings, we introduce \"Vision Description Prompting,\" a method that\neffectively improves performance in challenging vision-related tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Prompt Injection Attacks and Defenses in LLM-Integrated Applications\u2b1b  Large Language Models (LLMs) are increasingly deployed as the backend for a\nvariety of real-world applications called LLM-Integrated Applications. Multiple\nrecent works showed that LLM-Integrated Applications are vulnerable to prompt\ninjection attacks, in which an attacker injects malicious instruction/data into\nthe input of those applications such that they produce results as the attacker\ndesires. However, existing works are limited to case studies. As a result, the\nliterature lacks a systematic understanding of prompt injection attacks and\ntheir defenses. We aim to bridge the gap in this work. In particular, we\npropose a general framework to formalize prompt injection attacks. Existing\nattacks, which are discussed in research papers and blog posts, are special\ncases in our framework. Our framework enables us to design a new attack by\ncombining existing attacks. Moreover, we also propose a framework to\nsystematize defenses against prompt injection attacks. Using our frameworks, we\nconduct a systematic evaluation on prompt injection attacks and their defenses\nwith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in\nthis field. Our code is available at\nhttps://github.com/liu00222/Open-Prompt-Injection.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AgentTuning: Enabling Generalized Agent Abilities for LLMs\u2b1b  Open large language models (LLMs) with great performance in various tasks\nhave significantly advanced the development of LLMs. However, they are far\ninferior to commercial models such as ChatGPT and GPT-4 when acting as agents\nto tackle complex tasks in the real world. These agent tasks employ LLMs as the\ncentral controller responsible for planning, memorization, and tool\nutilization, necessitating both fine-grained prompting methods and robust LLMs\nto achieve satisfactory performance. Though many prompting methods have been\nproposed to complete particular agent tasks, there is lack of research focusing\non improving the agent capabilities of LLMs themselves without compromising\ntheir general abilities. In this work, we present AgentTuning, a simple and\ngeneral method to enhance the agent abilities of LLMs while maintaining their\ngeneral LLM capabilities. We construct AgentInstruct, a lightweight\ninstruction-tuning dataset containing high-quality interaction trajectories. We\nemploy a hybrid instruction-tuning strategy by combining AgentInstruct with\nopen-source instructions from general domains. AgentTuning is used to\ninstruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show\nthat AgentTuning enables LLMs' agent capabilities without compromising general\nabilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent\ntasks, demonstrating generalized agent capabilities. We open source the\nAgentInstruct and AgentLM-7B, 13B, and 70B models at\nhttps://github.com/THUDM/AgentTuning, serving open and powerful alternatives to\ncommercial LLMs for agent tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Probing LLMs for hate speech detection: strengths and vulnerabilities\u2b1b  Recently efforts have been made by social media platforms as well as\nresearchers to detect hateful or toxic language using large language models.\nHowever, none of these works aim to use explanation, additional context and\nvictim community information in the detection process. We utilise different\nprompt variation, input information and evaluate large language models in zero\nshot setting (without adding any in-context examples). We select three large\nlanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -\nHateXplain, implicit hate and ToxicSpans. We find that on average including the\ntarget information in the pipeline improves the model performance substantially\n(~20-30%) over the baseline across the datasets. There is also a considerable\neffect of adding the rationales/explanations into the pipeline (~10-20%) over\nthe baseline across the datasets. In addition, we further provide a typology of\nthe error cases where these large language models fail to (i) classify and (ii)\nexplain the reason for the decisions they take. Such vulnerable points\nautomatically constitute 'jailbreak' prompts for these models and industry\nscale safeguard techniques need to be developed to make the models robust\nagainst such prompts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Challenges and Contributing Factors in the Utilization of Large Language\n  Models (LLMs)\u2b1b  With the development of large language models (LLMs) like the GPT series,\ntheir widespread use across various application scenarios presents a myriad of\nchallenges. This review initially explores the issue of domain specificity,\nwhere LLMs may struggle to provide precise answers to specialized questions\nwithin niche fields. The problem of knowledge forgetting arises as these LLMs\nmight find it hard to balance old and new information. The knowledge repetition\nphenomenon reveals that sometimes LLMs might deliver overly mechanized\nresponses, lacking depth and originality. Furthermore, knowledge illusion\ndescribes situations where LLMs might provide answers that seem insightful but\nare actually superficial, while knowledge toxicity focuses on harmful or biased\ninformation outputs. These challenges underscore problems in the training data\nand algorithmic design of LLMs. To address these issues, it's suggested to\ndiversify training data, fine-tune models, enhance transparency and\ninterpretability, and incorporate ethics and fairness training. Future\ntechnological trends might lean towards iterative methodologies, multimodal\nlearning, model personalization and customization, and real-time learning and\nfeedback mechanisms. In conclusion, future LLMs should prioritize fairness,\ntransparency, and ethics, ensuring they uphold high moral and ethical standards\nwhen serving humanity.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "POSQA: Probe the World Models of LLMs with Size Comparisons\u2b1b  Embodied language comprehension emphasizes that language understanding is not\nsolely a matter of mental processing in the brain but also involves\ninteractions with the physical and social environment. With the explosive\ngrowth of Large Language Models (LLMs) and their already ubiquitous presence in\nour daily lives, it is becoming increasingly necessary to verify their\nreal-world understanding. Inspired by cognitive theories, we propose POSQA: a\nPhysical Object Size Question Answering dataset with simple size comparison\nquestions to examine the extremity and analyze the potential mechanisms of the\nembodied comprehension of the latest LLMs.\n  We show that even the largest LLMs today perform poorly under the zero-shot\nsetting. We then push their limits with advanced prompting techniques and\nexternal knowledge augmentation. Furthermore, we investigate whether their\nreal-world comprehension primarily derives from contextual information or\ninternal weights and analyse the impact of prompt formats and report bias of\ndifferent objects. Our results show that real-world understanding that LLMs\nshaped from textual data can be vulnerable to deception and confusion by the\nsurface form of prompts, which makes it less aligned with human behaviours.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues\u2b1b  Interacting with human via high-quality multi-turn dialogues is a key feature\nof large language models (LLMs). However, human-based evaluation of such\ncapability involves intensive manual labor. This report provides a preliminary\nevaluation of existing large language models for human-style multi-turn\nchatting, through an LLM-based approach. We start from real-world human\ndialogues and keep the very first utterances as the ChatSEED. Then we prompt\nLLMs to generate a full multi-turn dialogue (tens of utterances) based on the\nChatSEED, utterance by utterance. Finally, we adopt state-of-the-art LLMs\n(GPT-4, \\etc) as the judge to evaluate the generated dialogues. With different\nevaluation protocols, we come to substantially identical conclusions. We find\nthat GPT-4 can generate human-style multi-turn dialogues with impressive\nquality, significantly outperforms its counterparts. It's difficult for a\ndiscriminator to distinguish between GPT-4 generated dialogues and human\ndialogues. In contrast, other LLMs struggle to generate multi-turn dialogues of\nsatisfactory quality due to poor instruction-following capability, tendency to\ngenerate lengthy utterances, or limited general capability. All data and codes\nwill be provided in https://github.com/open-compass/BotChat/ and we hope they\ncan serve as a valuable resource for evaluating multi-turn chatting\ncapabilities of LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author\n  Prompt Editing\u2b1b  Large language models (LLMs) have made impressive progress in natural\nlanguage processing. These models rely on proper human instructions (or\nprompts) to generate suitable responses. However, the potential of LLMs are not\nfully harnessed by commonly-used prompting methods: many human-in-the-loop\nalgorithms employ ad-hoc procedures for prompt selection; while auto prompt\ngeneration approaches are essentially searching all possible prompts randomly\nand inefficiently. We propose Evoke, an automatic prompt refinement framework.\nIn Evoke, there are two instances of a same LLM: one as a reviewer\n(LLM-Reviewer), it scores the current prompt; the other as an author\n(LLM-Author), it edits the prompt by considering the edit history and the\nreviewer's feedback. Such an author-reviewer feedback loop ensures that the\nprompt is refined in each iteration. We further aggregate a data selection\napproach to Evoke, where only the hard samples are exposed to the LLM. The hard\nsamples are more important because the LLM can develop deeper understanding of\nthe tasks out of them, while the model may already know how to solve the easier\ncases. Experimental results show that Evoke significantly outperforms existing\nmethods. For instance, in the challenging task of logical fallacy detection,\nEvoke scores above 80, while all other baseline methods struggle to reach 20.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline\n  Solids From Their Text Descriptions\u2b1b  The prediction of crystal properties plays a crucial role in the crystal\ndesign process. Current methods for predicting crystal properties focus on\nmodeling crystal structures using graph neural networks (GNNs). Although GNNs\nare powerful, accurately modeling the complex interactions between atoms and\nmolecules within a crystal remains a challenge. Surprisingly, predicting\ncrystal properties from crystal text descriptions is understudied, despite the\nrich information and expressiveness that text data offer. One of the main\nreasons is the lack of publicly available data for this task. In this paper, we\ndevelop and make public a benchmark dataset (called TextEdge) that contains\ntext descriptions of crystal structures with their properties. We then propose\nLLM-Prop, a method that leverages the general-purpose learning capabilities of\nlarge language models (LLMs) to predict the physical and electronic properties\nof crystals from their text descriptions. LLM-Prop outperforms the current\nstate-of-the-art GNN-based crystal property predictor by about 4% in predicting\nband gap, 3% in classifying whether the band gap is direct or indirect, and 66%\nin predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT,\na domain-specific pre-trained BERT model, despite having 3 times fewer\nparameters. Our empirical results may highlight the current inability of GNNs\nto capture information pertaining to space group symmetry and Wyckoff sites for\naccurate crystal property prediction.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Which Prompts Make The Difference? Data Prioritization For Efficient\n  Human LLM Evaluation\u2b1b  Human evaluation is increasingly critical for assessing large language\nmodels, capturing linguistic nuances, and reflecting user preferences more\naccurately than traditional automated metrics. However, the resource-intensive\nnature of this type of annotation process poses significant challenges. The key\nquestion driving our work: \"is it feasible to minimize human-in-the-loop\nfeedback by prioritizing data instances which most effectively distinguish\nbetween models?\" We evaluate several metric-based methods and find that these\nmetrics enhance the efficiency of human evaluations by minimizing the number of\nrequired annotations, thus saving time and cost, while ensuring a robust\nperformance evaluation. We show that our method is effective across widely used\nmodel families, reducing instances of indecisive (or \"tie\") outcomes by up to\n54% compared to a random sample when focusing on the top-20 percentile of\nprioritized instances. This potential reduction in required human effort\npositions our approach as a valuable strategy in future large language model\nevaluations.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64\n  Languages\u2b1b  Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate\nremarkable performance in a wide range of tasks. Despite numerous recent\nstudies that examine the performance of instruction-tuned LLMs on various NLP\nbenchmarks, there remains a lack of comprehensive investigation into their\nability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning\nembedded within social and interactive contexts. This deficiency arises partly\nfrom SM not being adequately represented in any of the existing benchmarks. To\naddress this gap, we present SPARROW, an extensive multilingual benchmark\nspecifically designed for SM understanding. SPARROW comprises 169 datasets\ncovering 13 task types across six primary categories (e.g., anti-social\nlanguage detection, emotion recognition). SPARROW datasets encompass 64\ndifferent languages originating from 12 language families representing 16\nwriting scripts. We evaluate the performance of various multilingual pretrained\nlanguage models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT)\non SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our\ncomprehensive analysis reveals that existing open-source instruction tuned LLMs\nstill struggle to understand SM across various languages, performing close to a\nrandom baseline in some cases. We also find that although ChatGPT outperforms\nmany LLMs, it still falls behind task-specific finetuned models with a gap of\n12.19 SPARROW score. Our benchmark is available at:\nhttps://github.com/UBC-NLP/SPARROW\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Large Search Model: Redefining Search Stack in the Era of LLMs\u2b1b  Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Survey on LLM-generated Text Detection: Necessity, Methods, and Future\n  Directions\u2b1b  The powerful ability to understand, follow, and generate complex language\nemerging from large language models (LLMs) makes LLM-generated text flood many\nareas of our daily lives at an incredible speed and is widely accepted by\nhumans. As LLMs continue to expand, there is an imperative need to develop\ndetectors that can detect LLM-generated text. This is crucial to mitigate\npotential misuse of LLMs and safeguard realms like artistic expression and\nsocial networks from harmful influence of LLM-generated content. The\nLLM-generated text detection aims to discern if a piece of text was produced by\nan LLM, which is essentially a binary classification task. The detector\ntechniques have witnessed notable advancements recently, propelled by\ninnovations in watermarking techniques, zero-shot methods, fine-turning LMs\nmethods, adversarial learning methods, LLMs as detectors, and human-assisted\nmethods. In this survey, we collate recent research breakthroughs in this area\nand underscore the pressing need to bolster detector research. We also delve\ninto prevalent datasets, elucidating their limitations and developmental\nrequirements. Furthermore, we analyze various LLM-generated text detection\nparadigms, shedding light on challenges like out-of-distribution problems,\npotential attacks, and data ambiguity. Conclusively, we highlight interesting\ndirections for future research in LLM-generated text detection to advance the\nimplementation of responsible artificial intelligence (AI). Our aim with this\nsurvey is to provide a clear and comprehensive introduction for newcomers while\nalso offering seasoned researchers a valuable update in the field of\nLLM-generated text detection. The useful resources are publicly available at:\nhttps://github.com/NLP2CT/LLM-generated-Text-Detection.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction\n  Following: A Case Study of Arabic\u2b1b  While significant progress has been made in benchmarking Large Language\nModels (LLMs) across various tasks, there is a lack of comprehensive evaluation\nof their abilities in responding to multi-turn instructions in less-commonly\ntested languages like Arabic. Our paper offers a detailed examination of the\nproficiency of open LLMs in such scenarios in Arabic. Utilizing a customized\nArabic translation of the MT-Bench benchmark suite, we employ GPT-4 as a\nuniform evaluator for both English and Arabic queries to assess and compare the\nperformance of the LLMs on various open-ended tasks. Our findings reveal\nvariations in model responses on different task categories, e.g., logic vs.\nliteracy, when instructed in English or Arabic. We find that fine-tuned base\nmodels using multilingual and multi-turn datasets could be competitive to\nmodels trained from scratch on multilingual data. Finally, we hypothesize that\nan ensemble of small, open LLMs could perform competitively to proprietary LLMs\non the benchmark.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards LLM-driven Dialogue State Tracking\u2b1b  Dialogue State Tracking (DST) is of paramount importance in ensuring accurate\ntracking of user goals and system actions within task-oriented dialogue\nsystems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT\nhas sparked considerable interest in assessing their efficacy across diverse\napplications. In this study, we conduct an initial examination of ChatGPT's\ncapabilities in DST. Our evaluation uncovers the exceptional performance of\nChatGPT in this task, offering valuable insights to researchers regarding its\ncapabilities and providing useful directions for designing and enhancing\ndialogue systems. Despite its impressive performance, ChatGPT has significant\nlimitations including its closed-source nature, request restrictions, raising\ndata privacy concerns, and lacking local deployment capabilities. To address\nthese concerns, we present LDST, an LLM-driven DST framework based on smaller,\nopen-source foundation models. By utilizing a novel domain-slot instruction\ntuning method, LDST achieves performance on par with ChatGPT. Comprehensive\nevaluations across three distinct experimental settings, we find that LDST\nexhibits remarkable performance improvements in both zero-shot and few-shot\nsetting compared to previous SOTA methods. The source code is provided for\nreproducibility.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-Based Agent Society Investigation: Collaboration and Confrontation\n  in Avalon Gameplay\u2b1b  This paper aims to investigate the open research problem of uncovering the\nsocial behaviors of LLM-based agents. To achieve this goal, we adopt Avalon, a\nrepresentative communication game, as the environment and use system prompts to\nguide LLM agents to play the game. While previous studies have conducted\npreliminary investigations into gameplay with LLM agents, there lacks research\non their social behaviors. In this paper, we present a novel framework designed\nto seamlessly adapt to Avalon gameplay. The core of our proposed framework is a\nmulti-agent system that enables efficient communication and interaction among\nagents. We evaluate the performance of our framework based on metrics from two\nperspectives: winning the game and analyzing the social behaviors of LLM\nagents. Our results demonstrate the effectiveness of our framework in\ngenerating adaptive and intelligent agents and highlight the potential of\nLLM-based agents in addressing the challenges associated with dynamic social\nenvironment interaction. By analyzing the social behaviors of LLM agents from\nthe aspects of both collaboration and confrontation, we provide insights into\nthe research and applications of this domain.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\u2b1b  Thematic analysis (TA) has been widely used for analyzing qualitative data in\nmany disciplines and fields. To ensure reliable analysis, the same piece of\ndata is typically assigned to at least two human coders. Moreover, to produce\nmeaningful and useful analysis, human coders develop and deepen their data\ninterpretation and coding over multiple iterations, making TA labor-intensive\nand time-consuming. Recently the emerging field of large language models (LLMs)\nresearch has shown that LLMs have the potential replicate human-like behavior\nin various tasks: in particular, LLMs outperform crowd workers on\ntext-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We\npropose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct\nTA with in-context learning (ICL). This framework provides the prompt to frame\ndiscussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA.\nWe demonstrate the utility of this framework using survey datasets on the\naspects of the music listening experience and the usage of a password manager.\nResults of the two case studies show that the proposed framework yields similar\ncoding quality to that of human coders but reduces TA's labor and time demands.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Causal Inference Using LLM-Guided Discovery\u2b1b  At the core of causal inference lies the challenge of determining reliable\ncausal graphs solely based on observational data. Since the well-known backdoor\ncriterion depends on the graph, any errors in the graph can propagate\ndownstream to effect inference. In this work, we initially show that complete\ngraph information is not necessary for causal effect inference; the topological\norder over graph variables (causal order) alone suffices. Further, given a node\npair, causal order is easier to elicit from domain experts compared to graph\nedges since determining the existence of an edge can depend extensively on\nother variables. Interestingly, we find that the same principle holds for Large\nLanguage Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated\nmethod to obtain causal order (and hence causal effect) with LLMs acting as\nvirtual domain experts. To this end, we employ different prompting strategies\nand contextual cues to propose a robust technique of obtaining causal order\nfrom LLMs. Acknowledging LLMs' limitations, we also study possible techniques\nto integrate LLMs with established causal discovery algorithms, including\nconstraint-based and score-based methods, to enhance their performance.\nExtensive experiments demonstrate that our approach significantly improves\ncausal ordering accuracy as compared to discovery algorithms, highlighting the\npotential of LLMs to enhance causal inference across diverse fields.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple\n  Experts Fine-tuning\u2b1b  We propose Multiple Experts Fine-tuning Framework to build a financial large\nlanguage model (LLM), DISC-FinLLM. Our methodology improves general LLMs by\nendowing them with multi-turn question answering abilities, domain text\nprocessing capabilities, mathematical computation skills, and\nretrieval-enhanced generation capabilities. We build a financial\ninstruction-tuning dataset named DISC-FIN-SFT, including instruction samples of\nfour categories (consulting, NLP tasks, computing and retrieval-augmented\ngeneration). Evaluations conducted on multiple benchmarks demonstrate that our\nmodel performs better than baseline models in various financial scenarios.\nFurther resources can be found at https://github.com/FudanDISC/DISC-FinLLM.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based\n  LLM\u2b1b  In the burgeoning field of natural language processing, Neural Topic Models\n(NTMs) and Large Language Models (LLMs) have emerged as areas of significant\nresearch interest. Despite this, NTMs primarily utilize contextual embeddings\nfrom LLMs, which are not optimal for clustering or capable for topic\ngeneration. Our study addresses this gap by introducing a novel framework named\nDiffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME).\nDeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable\nembeddings that could generate topics that exhibit both superior clusterability\nand enhanced semantic coherence compared to existing methods. Additionally, by\nexploiting the power of diffusion, our framework also provides the capability\nto generate content relevant to the identified topics. This dual functionality\nallows users to efficiently produce highly clustered topics and related content\nsimultaneously. DeTiME's potential extends to generating clustered embeddings\nas well. Notably, our proposed framework proves to be efficient to train and\nexhibits high adaptability, demonstrating its potential for a wide array of\napplications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual,\n  Intensional, and Extensional Learning for Faithful Natural Language\n  Generation\u2b1b  We show that LLMs hallucinate because their output is not constrained to be\nsynonymous with claims for which they have evidence: a condition that we call\nevidential closure. Information about the truth or falsity of sentences is not\nstatistically identified in the standard neural probabilistic language model\nsetup, and so cannot be conditioned on to generate new strings. We then show\nhow to constrain LLMs to produce output that does satisfy evidential closure. A\nmultimodal LLM must learn about the external world (perceptual learning); it\nmust learn a mapping from strings to states of the world (extensional\nlearning); and, to achieve fluency when generalizing beyond a body of evidence,\nit must learn mappings from strings to their synonyms (intensional learning).\nThe output of a unimodal LLM must be synonymous with strings in a validated\nevidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune,\nthat yields faithful output from an LLM by rejecting output that is not\nsynonymous with claims for which the LLM has evidence.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information\n  Retrieval\u2b1b  We study the ability of state-of-the art models to answer constraint\nsatisfaction queries for information retrieval (e.g., 'a list of ice cream\nshops in San Diego'). In the past, such queries were considered to be tasks\nthat could only be solved via web-search or knowledge bases. More recently,\nlarge language models (LLMs) have demonstrated initial emergent abilities in\nthis task. However, many current retrieval benchmarks are either saturated or\ndo not measure constraint satisfaction. Motivated by rising concerns around\nfactual incorrectness and hallucinations of LLMs, we present KITAB, a new\ndataset for measuring constraint satisfaction abilities of language models.\nKITAB consists of book-related data across more than 600 authors and 13,000\nqueries, and also offers an associated dynamic data collection and constraint\nverification approach for acquiring similar test data for other authors. Our\nextended experiments on GPT4 and GPT3.5 characterize and decouple common\nfailure modes across dimensions such as information popularity, constraint\ntypes, and context availability. Results show that in the absence of context,\nmodels exhibit severe limitations as measured by irrelevant information,\nfactual errors, and incompleteness, many of which exacerbate as information\npopularity decreases. While context availability mitigates irrelevant\ninformation, it is not helpful for satisfying constraints, identifying\nfundamental barriers to constraint satisfaction. We open source our\ncontributions to foster further research on improving constraint satisfaction\nabilities of future models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting\n  Elusive Disinformation\u2b1b  Recent ubiquity and disruptive impacts of large language models (LLMs) have\nraised concerns about their potential to be misused (.i.e, generating\nlarge-scale harmful and misleading content). To combat this emerging risk of\nLLMs, we propose a novel \"Fighting Fire with Fire\" (F3) strategy that harnesses\nmodern LLMs' generative and emergent reasoning capabilities to counter\nhuman-written and LLM-generated disinformation. First, we leverage\nGPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content\nthrough paraphrase-based and perturbation-based prefix-style prompts,\nrespectively. Second, we apply zero-shot in-context semantic reasoning\ntechniques with cloze-style prompts to discern genuine from deceptive posts and\nnews articles. In our extensive experiments, we observe GPT-3.5-turbo's\nzero-shot superiority for both in-distribution and out-of-distribution\ndatasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike\nthe decline observed in previous customized and fine-tuned disinformation\ndetectors. Our codebase and dataset are available at\nhttps://github.com/mickeymst/F3.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code\n  Translation\u2b1b  With the recent focus on Large Language Models (LLMs), both StarCoder (Li et\nal., 2023) and Code Llama (Rozi\\`ere et al., 2023) have demonstrated remarkable\nperformance in code generation. However, there is still a need for improvement\nin code translation functionality with efficient training techniques. In\nresponse to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM\ndesigned specifically for multi-programming language-to-Python code\ntranslation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or\nPHP-to-Python code translation without specifying the input programming\nlanguage. We modified StarCoder model architecture by incorporating a\nMixture-of-Experts (MoE) technique featuring five experts and a gating network\nfor multi-task handling. Experts are obtained by StarCoder fine-tuning.\nSpecifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each\nexpert size as only 0.06% of number of StarCoder's parameters. At the same\ntime, to enhance training efficiency in terms of time, we adopt curriculum\nlearning strategy and use self-instruct data for efficient fine-tuning. As a\nresult, each expert takes only 6 hours to train on one single 80Gb A100 HBM.\nWith experiments on XLCoST datasets, SteloCoder achieves an average of 73.76\nCodeBLEU score in multi-programming language-to-Python translation, surpassing\nthe top performance from the leaderboard by at least 3.5. This accomplishment\nis attributed to only 45M extra parameters with StarCoder as the backbone and\n32 hours of valid training on one 80GB A100 HBM. The source code is release\nhere: https://github.com/sade-adrien/SteloCoder.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for\n  Inference Cost Reduction\u2b1b  Since ChatGPT released its API for public use, the number of applications\nbuilt on top of commercial large language models (LLMs) increase exponentially.\nOne popular usage of such models is leveraging its in-context learning ability\nand generating responses given user queries leveraging knowledge obtained by\nretrieval augmentation. One problem of deploying commercial retrieval-augmented\nLLMs is the cost due to the additionally retrieved context that largely\nincreases the input token size of the LLMs. To mitigate this, we propose a\ntoken compression scheme that includes two methods: summarization compression\nand semantic compression. The first method applies a T5-based model that is\nfine-tuned by datasets generated using self-instruct containing samples with\nvarying lengths and reduce token size by doing summarization. The second method\nfurther compresses the token size by removing words with lower impact on the\nsemantic. In order to adequately evaluate the effectiveness of the proposed\nmethods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)\nfocusing on food recommendation for women around pregnancy period or infants.\nOur summarization compression can reduce 65% of the retrieval token size with\nfurther 0.3% improvement on the accuracy; semantic compression provides a more\nflexible way to trade-off the token size with performance, for which we can\nreduce the token size by 20% with only 1.6% of accuracy drop.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Survey on Detection of LLMs-Generated Content\u2b1b  The burgeoning capabilities of advanced large language models (LLMs) such as\nChatGPT have led to an increase in synthetic content generation with\nimplications across a variety of sectors, including media, cybersecurity,\npublic discourse, and education. As such, the ability to detect LLMs-generated\ncontent has become of paramount importance. We aim to provide a detailed\noverview of existing detection strategies and benchmarks, scrutinizing their\ndifferences and identifying key challenges and prospects in the field,\nadvocating for more adaptable and robust models to enhance detection accuracy.\nWe also posit the necessity for a multi-faceted approach to defend against\nvarious attacks to counter the rapidly advancing capabilities of LLMs. To the\nbest of our knowledge, this work is the first comprehensive survey on the\ndetection in the era of LLMs. We hope it will provide a broad understanding of\nthe current landscape of LLMs-generated content detection, offering a guiding\nreference for researchers and practitioners striving to uphold the integrity of\ndigital information in an era increasingly dominated by synthetic content. The\nrelevant papers are summarized and will be consistently updated at\nhttps://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MindLLM: Pre-training Lightweight Large Language Model from Scratch,\n  Evaluations and Domain Applications\u2b1b  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language tasks, marking significant strides towards general\nartificial intelligence. While general artificial intelligence is leveraged by\ndeveloping increasingly large-scale models, there could be another branch to\ndevelop lightweight custom models that better serve certain domains, taking\ninto account the high cost of training and deploying LLMs and the scarcity of\nresources. In this paper, we present MindLLM, a novel series of bilingual\nlightweight large language models, trained from scratch, alleviating such\nburdens by offering models with 1.3 billion and 3 billion parameters. A\nthorough account of experiences accrued during large model development is\ngiven, covering every step of the process, including data construction, model\narchitecture, evaluation, and applications. Such insights are hopefully\nvaluable for fellow academics and developers. MindLLM consistently matches or\nsurpasses the performance of other open-source larger models on some public\nbenchmarks. We also introduce an innovative instruction tuning framework\ntailored for smaller models to enhance their capabilities efficiently.\nMoreover, we explore the application of MindLLM in specific vertical domains\nsuch as law and finance, underscoring the agility and adaptability of our\nlightweight models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Self-Guard: Empower the LLM to Safeguard Itself\u2b1b  The jailbreak attack can bypass the safety measures of a Large Language Model\n(LLM), generating harmful content. This misuse of LLM has led to negative\nsocietal consequences. Currently, there are two main approaches to address\njailbreak attacks: safety training and safeguards. Safety training focuses on\nfurther training LLM to enhance its safety. On the other hand, safeguards\ninvolve implementing external models or filters to prevent harmful outputs.\nHowever, safety training has constraints in its ability to adapt to new attack\ntypes and often leads to a drop in model performance. Safeguards have proven to\nbe of limited help. To tackle these issues, we propose a novel approach called\nSelf-Guard, which combines the strengths of both safety methods. Self-Guard\nincludes two stages. In the first stage, we enhance the model's ability to\nassess harmful content, and in the second stage, we instruct the model to\nconsistently perform harmful content detection on its own responses. The\nexperiment has demonstrated that Self-Guard is robust against jailbreak\nattacks. In the bad case analysis, we find that LLM occasionally provides\nharmless responses to harmful queries. Additionally, we evaluated the general\ncapabilities of the LLM before and after safety training, providing evidence\nthat Self-Guard does not result in the LLM's performance degradation. In\nsensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM\nbut also can even mitigate this issue.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs\n  with Multi-turn Health Conversations Polished by ChatGPT\u2b1b  Large language models (LLMs) have performed well in providing general and\nextensive health suggestions in single-turn conversations, exemplified by\nsystems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the\nlimited information provided by users during single turn results in inadequate\npersonalization and targeting of the generated suggestions, which requires\nusers to independently select the useful part. It is mainly caused by the\nmissing ability to engage in multi-turn questioning. In real-world medical\nconsultations, doctors usually employ a series of iterative inquiries to\ncomprehend the patient's condition thoroughly, enabling them to provide\neffective and personalized suggestions subsequently, which can be defined as\nchain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose\nBianQue, a ChatGLM-based LLM finetuned with the self-constructed health\nconversation dataset BianQueCorpus that is consist of multiple turns of\nquestioning and health suggestions polished by ChatGPT. Experimental results\ndemonstrate that the proposed BianQue can simultaneously balance the\ncapabilities of both questioning and health suggestions, which will help\npromote the research and application of LLMs in the field of proactive health.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation\u2b1b  Despite the promise of Mixture of Experts (MoE) models in increasing\nparameter counts of Transformer models while maintaining training and inference\ncosts, their application carries notable drawbacks. The key strategy of these\nmodels is to, for each processed token, activate at most a few experts -\nsubsets of an extensive feed-forward layer. But this approach is not without\nits challenges. The operation of matching experts and tokens is discrete, which\nmakes MoE models prone to issues like training instability and uneven expert\nutilization. Existing techniques designed to address these concerns, such as\nauxiliary losses or balance-aware matching, result either in lower model\nperformance or are more difficult to train. In response to these issues, we\npropose Mixture of Tokens, a fully-differentiable model that retains the\nbenefits of MoE architectures while avoiding the aforementioned difficulties.\nRather than routing tokens to experts, this approach mixes tokens from\ndifferent examples prior to feeding them to experts, enabling the model to\nlearn from all token-expert combinations. Importantly, this mixing can be\ndisabled to avoid mixing of different sequences during inference. Crucially,\nthis method is fully compatible with both masked and causal Large Language\nModel training and inference.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CycleAlign: Iterative Distillation from Black-box LLM to White-box\n  Models for Better Human Alignment\u2b1b  Language models trained on large-scale corpus often generate content that is\nharmful, toxic, or contrary to human preferences, making their alignment with\nhuman values a critical concern. Reinforcement learning from human feedback\n(RLHF) with algorithms like PPO is a prevalent approach for alignment but is\noften complex, unstable, and resource-intensive. Recently, ranking-based\nalignment methods have emerged, offering stability and effectiveness by\nreplacing the RL framework with supervised fine-tuning, but they are costly due\nto the need for annotated data. Considering that existing large language models\n(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,\nresearchers have begun to align the language model with human preference from\nAI feedback. The common practices, which unidirectionally distill the\ninstruction-following responses from LLMs, are constrained by their bottleneck.\nThus we introduce CycleAlign to distill alignment capabilities from\nparameter-invisible LLMs (black-box) to a parameter-visible model (white-box)\nin an iterative manner. With in-context learning (ICL) as the core of the\ncycle, the black-box models are able to rank the model-generated responses\nguided by human-craft instruction and demonstrations about their preferences.\nDuring iterative interaction, the white-box models also have a judgment about\nresponses generated by them. Consequently, the agreement ranking could be\nviewed as a pseudo label to dynamically update the in-context demonstrations\nand improve the preference ranking ability of black-box models. Through\nmultiple interactions, the CycleAlign framework could align the white-box model\nwith the black-box model effectively in a low-resource way. Empirical results\nillustrate that the model fine-tuned by CycleAlign remarkably exceeds existing\nmethods, and achieves the state-of-the-art performance in alignment with human\nvalue.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "InstructPTS: Instruction-Tuning LLMs for Product Title Summarization\u2b1b  E-commerce product catalogs contain billions of items. Most products have\nlengthy titles, as sellers pack them with product attributes to improve\nretrieval, and highlight key product aspects. This results in a gap between\nsuch unnatural products titles, and how customers refer to them. It also limits\nhow e-commerce stores can use these seller-provided titles for recommendation,\nQA, or review summarization.\n  Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a\ncontrollable approach for the task of Product Title Summarization (PTS).\nTrained using a novel instruction fine-tuning strategy, our approach is able to\nsummarize product titles according to various criteria (e.g. number of words in\na summary, inclusion of specific phrases, etc.). Extensive evaluation on a\nreal-world e-commerce catalog shows that compared to simple fine-tuning of\nLLMs, our proposed approach can generate more accurate product name summaries,\nwith an improvement of over 14 and 8 BLEU and ROUGE points, respectively.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM Performance Predictors are good initializers for Architecture Search\u2b1b  Large language models (LLMs) have become an integral component in solving a\nwide range of NLP tasks. In this work, we explore a novel use case of using\nLLMs to build performance predictors (PP): models that, given a specific deep\nneural network architecture, predict its performance on a downstream task. We\ndesign PP prompts for LLMs consisting of: (i) role: description of the role\nassigned to the LLM, (ii) instructions: set of instructions to be followed by\nthe LLM to carry out performance prediction, (iii) hyperparameters: a\ndefinition of each architecture-specific hyperparameter and (iv)\ndemonstrations: sample architectures along with their efficiency metrics and\n'training from scratch' performance. For machine translation (MT) tasks, we\ndiscover that GPT-4 with our PP prompts (LLM-PP) can predict the performance of\narchitecture with a mean absolute error matching the SOTA and a marginal\ndegradation in rank correlation coefficient compared to SOTA performance\npredictors. Further, we show that the predictions from LLM-PP can be distilled\nto a small regression model (LLM-Distill-PP). LLM-Distill-PP models\nsurprisingly retain the performance of LLM-PP largely and can be a\ncost-effective alternative for heavy use cases of performance estimation.\nSpecifically, for neural architecture search (NAS), we propose a Hybrid-Search\nalgorithm for NAS (HS-NAS), which uses LLM-Distill-PP for the initial part of\nsearch, resorting to the baseline predictor for rest of the search. We show\nthat HS-NAS performs very similar to SOTA NAS across benchmarks, reduces search\nhours by 50% roughly, and in some cases, improves latency, GFLOPs, and model\nsize.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers\u2b1b  We propose LLM-FP4 for quantizing both weights and activations in large\nlanguage models (LLMs) down to 4-bit floating-point values, in a post-training\nmanner. Existing post-training quantization (PTQ) solutions are primarily\ninteger-based and struggle with bit widths below 8 bits. Compared to integer\nquantization, floating-point (FP) quantization is more flexible and can better\nhandle long-tail or bell-shaped distributions, and it has emerged as a default\nchoice in many hardware platforms. One characteristic of FP quantization is\nthat its performance largely depends on the choice of exponent bits and\nclipping range. In this regard, we construct a strong FP-PTQ baseline by\nsearching for the optimal quantization parameters. Furthermore, we observe a\nhigh inter-channel variance and low intra-channel variance pattern in\nactivation distributions, which adds activation quantization difficulty. We\nrecognize this pattern to be consistent across a spectrum of transformer models\ndesigned for diverse tasks, such as LLMs, BERT, and Vision Transformer models.\nTo tackle this, we propose per-channel activation quantization and show that\nthese additional scaling factors can be reparameterized as exponential biases\nof weights, incurring a negligible cost. Our method, for the first time, can\nquantize both weights and activations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the common sense zero-shot reasoning\ntasks, which is only 5.8 lower than the full-precision model, significantly\noutperforming the previous state-of-the-art by 12.7 points. Code is available\nat: https://github.com/nbasyl/LLM-FP4.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "\"You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of\n  Abstract Meaning Representation\u2b1b  Large language models (LLMs) show amazing proficiency and fluency in the use\nof language. Does this mean that they have also acquired insightful linguistic\nknowledge about the language, to an extent that they can serve as an \"expert\nlinguistic annotator\"? In this paper, we examine the successes and limitations\nof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning\nstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu et\nal. 2013) parsing formalism, which provides rich graphical representations of\nsentence meaning structure while abstracting away from surface forms. We\ncompare models' analysis of this semantic structure across two settings: 1)\ndirect production of AMR parses based on zero- and few-shot prompts, and 2)\nindirect partial reconstruction of AMR via metalinguistic natural language\nqueries (e.g., \"Identify the primary event of this sentence, and the predicate\ncorresponding to that event.\"). Across these settings, we find that models can\nreliably reproduce the basic format of AMR, and can often capture core event,\nargument, and modifier structure -- however, model outputs are prone to\nfrequent and major errors, and holistic analysis of parse acceptability shows\nthat even with few-shot demonstrations, models have virtually 0% success in\nproducing fully accurate parses. Eliciting natural language responses produces\nsimilar patterns of errors. Overall, our findings indicate that these models\nout-of-the-box can capture aspects of semantic structure, but there remain key\nlimitations in their ability to support fully accurate semantic analyses or\nparses.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models\n  via Contextual Integrity Theory\u2b1b  The interactive use of large language models (LLMs) in AI assistants (at\nwork, home, etc.) introduces a new set of inference-time privacy risks: LLMs\nare fed different types of information from multiple sources in their inputs\nand are expected to reason about what to share in their outputs, for what\npurpose and with whom, within a given context. In this work, we draw attention\nto the highly critical yet overlooked notion of contextual privacy by proposing\nConfAIde, a benchmark designed to identify critical weaknesses in the privacy\nreasoning capabilities of instruction-tuned LLMs. Our experiments show that\neven the most capable models such as GPT-4 and ChatGPT reveal private\ninformation in contexts that humans would not, 39% and 57% of the time,\nrespectively. This leakage persists even when we employ privacy-inducing\nprompts or chain-of-thought reasoning. Our work underscores the immediate need\nto explore novel inference-time privacy-preserving approaches, based on\nreasoning and theory of mind.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection\n  Method\u2b1b  Large Language Models (LLMs) have shown great potential in Natural Language\nProcessing (NLP) tasks. However, recent literature reveals that LLMs generate\nnonfactual responses intermittently, which impedes the LLMs' reliability for\nfurther utilization. In this paper, we propose a novel self-detection method to\ndetect which questions that a LLM does not know that are prone to generate\nnonfactual results. Specifically, we first diversify the textual expressions\nfor a given question and collect the corresponding answers. Then we examine the\ndivergencies between the generated answers to identify the questions that the\nmodel may generate falsehoods. All of the above steps can be accomplished by\nprompting the LLMs themselves without referring to any other external\nresources. We conduct comprehensive experiments and demonstrate the\neffectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,\nand GPT-4.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination\n  for each Benchmark\u2b1b  In this position paper, we argue that the classical evaluation on Natural\nLanguage Processing (NLP) tasks using annotated benchmarks is in trouble. The\nworst kind of data contamination happens when a Large Language Model (LLM) is\ntrained on the test split of a benchmark, and then evaluated in the same\nbenchmark. The extent of the problem is unknown, as it is not straightforward\nto measure. Contamination causes an overestimation of the performance of a\ncontaminated model in a target benchmark and associated task with respect to\ntheir non-contaminated counterparts. The consequences can be very harmful, with\nwrong scientific conclusions being published while other correct ones are\ndiscarded. This position paper defines different levels of data contamination\nand argues for a community effort, including the development of automatic and\nsemi-automatic measures to detect when data from a benchmark was exposed to a\nmodel, and suggestions for flagging papers with conclusions that are\ncompromised by data contamination.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial\n  Issues\u2b1b  Controversy is a reflection of our zeitgeist, and an important aspect to any\ndiscourse. The rise of large language models (LLMs) as conversational systems\nhas increased public reliance on these systems for answers to their various\nquestions. Consequently, it is crucial to systematically examine how these\nmodels respond to questions that pertaining to ongoing debates. However, few\nsuch datasets exist in providing human-annotated labels reflecting the\ncontemporary discussions. To foster research in this area, we propose a novel\nconstruction of a controversial questions dataset, expanding upon the publicly\nreleased Quora Question Pairs Dataset. This dataset presents challenges\nconcerning knowledge recency, safety, fairness, and bias. We evaluate different\nLLMs using a subset of this dataset, illuminating how they handle controversial\nissues and the stances they adopt. This research ultimately contributes to our\nunderstanding of LLMs' interaction with controversial issues, paving the way\nfor improvements in their comprehension and handling of complex societal\ndebates.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Chainpoll: A high efficacy method for LLM hallucination detection\u2b1b  Large language models (LLMs) have experienced notable advancements in\ngenerating coherent and contextually relevant responses. However,\nhallucinations - incorrect or unfounded claims - are still prevalent, prompting\nthe creation of automated metrics to detect these in LLM outputs. Our\ncontributions include: introducing ChainPoll, an innovative hallucination\ndetection method that excels compared to its counterparts, and unveiling\nRealHall, a refined collection of benchmark datasets to assess hallucination\ndetection metrics from recent studies. While creating RealHall, we assessed\ntasks and datasets from previous hallucination detection studies and observed\nthat many are not suitable for the potent LLMs currently in use. Overcoming\nthis, we opted for four datasets challenging for modern LLMs and pertinent to\nreal-world scenarios. Using RealHall, we conducted a comprehensive comparison\nof ChainPoll with numerous hallucination metrics from recent studies. Our\nfindings indicate that ChainPoll outperforms in all RealHall benchmarks,\nachieving an overall AUROC of 0.781. This surpasses the next best theoretical\nmethod by 11% and exceeds industry standards by over 23%. Additionally,\nChainPoll is cost-effective and offers greater transparency than other metrics.\nWe introduce two novel metrics to assess LLM hallucinations: Adherence and\nCorrectness. Adherence is relevant to Retrieval Augmented Generation workflows,\nevaluating an LLM's analytical capabilities within given documents and\ncontexts. In contrast, Correctness identifies logical and reasoning errors.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading\n  Comprehension Shortcut Triggers\u2b1b  Recent applications of LLMs in Machine Reading Comprehension (MRC) systems\nhave shown impressive results, but the use of shortcuts, mechanisms triggered\nby features spuriously correlated to the true label, has emerged as a potential\nthreat to their reliability. We analyze the problem from two angles: LLMs as\neditors, guided to edit text to mislead LLMs; and LLMs as readers, who answer\nquestions based on the edited text. We introduce a framework that guides an\neditor to add potential shortcuts-triggers to samples. Using GPT4 as the\neditor, we find it can successfully edit trigger shortcut in samples that fool\nLLMs. Analysing LLMs as readers, we observe that even capable LLMs can be\ndeceived using shortcut knowledge. Strikingly, we discover that GPT4 can be\ndeceived by its own edits (15% drop in F1). Our findings highlight inherent\nvulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a\ncurated dataset generated by our framework for future research.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs Grade Short-answer Reading Comprehension Questions :\n  Foundational Literacy Assessment in LMICs\u2b1b  This paper presents emerging evidence of using generative large language\nmodels (i.e., GPT-4) to reliably evaluate short-answer reading comprehension\nquestions. Specifically, we explore how various configurations of generative\n(LLMs) are able to evaluate student responses from a new dataset, drawn from a\nbattery of reading assessments conducted with over 150 students in Ghana. As\nthis dataset is novel and hence not used in training runs of GPT, it offers an\nopportunity to test for domain shift and evaluate the generalizability of\ngenerative LLMs, which are predominantly designed and trained on data from\nhigh-income North American countries. We found that GPT-4, with minimal prompt\nengineering performed extremely well on evaluating the novel dataset (Quadratic\nWeighted Kappa 0.923, F1 0.88), substantially outperforming transfer-learning\nbased approaches, and even exceeding expert human raters (Quadratic Weighted\nKappa 0.915, F1 0.87). To the best of our knowledge, our work is the first to\nempirically evaluate the performance of generative LLMs on short-answer reading\ncomprehension questions, using real student data, and suggests that generative\nLLMs have the potential to reliably evaluate foundational literacy. Currently\nthe assessment of formative literacy and numeracy is infrequent in many low and\nmiddle-income countries (LMICs) due to the cost and operational complexities of\nconducting them at scale. Automating the grading process for reading assessment\ncould enable wider usage, and in turn improve decision-making regarding\ncurricula, school management, and teaching practice at the classroom level.\nImportantly, in contrast transfer learning based approaches, generative LLMs\ngeneralize well and the technical barriers to their use are low, making them\nmore feasible to implement and scale in lower resource educational contexts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive\n  Learning for Code Generation\u2b1b  With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are\nincreasing interests in distilling the capabilies of close-sourced LLMs to\nsmaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT\nto generate a set of instructions and answers, for the student model to learn.\nHowever, such standard distillation approach neglects the merits and conditions\nof the student model. Inspired by modern teaching principles, we design a\npersonalised distillation process, in which the student attempts to solve a\ntask first, then the teacher provides an adaptive refinement for the student to\nimprove. Instead of feeding the student with teacher's prior, personalised\ndistillation enables personalised learning for the student model, as it only\nlearns on examples it makes mistakes upon and learns to improve its own\nsolution. On code generation, personalised distillation consistently\noutperforms standard distillation with only one third of the data. With only\n2.5-3K personalised examples that incur a data-collection cost of 4-6$, we\nboost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to\nachieve 45.8% pass@1 on HumanEval.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Probing LLMs for Joint Encoding of Linguistic Categories\u2b1b  Large Language Models (LLMs) exhibit impressive performance on a range of NLP\ntasks, due to the general-purpose linguistic knowledge acquired during\npretraining. Existing model interpretability research (Tenney et al., 2019)\nsuggests that a linguistic hierarchy emerges in the LLM layers, with lower\nlayers better suited to solving syntactic tasks and higher layers employed for\nsemantic processing. Yet, little is known about how encodings of different\nlinguistic phenomena interact within the models and to what extent processing\nof linguistically-related categories relies on the same, shared model\nrepresentations. In this paper, we propose a framework for testing the joint\nencoding of linguistic categories in LLMs. Focusing on syntax, we find evidence\nof joint encoding both at the same (related part-of-speech (POS) classes) and\ndifferent (POS classes and related syntactic dependency relations) levels of\nlinguistic hierarchy. Our cross-lingual experiments show that the same patterns\nhold across languages in multilingual LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection\u2b1b  This paper compares different pre-trained and fine-tuned large language\nmodels (LLMs) for hate speech detection. Our research underscores challenges in\nLLMs' cross-domain validity and overfitting risks. Through evaluations, we\nhighlight the need for fine-tuned models that grasp the nuances of hate speech\nthrough greater label heterogeneity. We conclude with a vision for the future\nof hate speech detection, emphasizing cross-domain generalizability and\nappropriate benchmarking practices.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "EtiCor: Corpus for Analyzing LLMs for Etiquettes\u2b1b  Etiquettes are an essential ingredient of day-to-day interactions among\npeople. Moreover, etiquettes are region-specific, and etiquettes in one region\nmight contradict those in other regions. In this paper, we propose EtiCor, an\nEtiquettes Corpus, having texts about social norms from five different regions\nacross the globe. The corpus provides a test bed for evaluating LLMs for\nknowledge and understanding of region-specific etiquettes. Additionally, we\npropose the task of Etiquette Sensitivity. We experiment with state-of-the-art\nLLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs,\nmostly fail to understand etiquettes from regions from non-Western world.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Improving Factual Consistency of Text Summarization by Adversarially\n  Decoupling Comprehension and Embellishment Abilities of LLMs\u2b1b  Despite the recent progress in text summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose an\nadversarially DEcoupling method to disentangle the Comprehension and\nEmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based\nefficient training to cover the shortage of sensitivity for true and false in\nthe training process of LLMs. In this way, LLMs are less confused about\nembellishing and understanding; thus, they can execute the instructions more\naccurately and have enhanced abilities to distinguish hallucinations.\nExperimental results show that DECENT significantly improves the reliability of\ntext summarization based on LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Constituency Parsing using LLMs\u2b1b  Constituency parsing is a fundamental yet unsolved natural language\nprocessing task. In this paper, we explore the potential of recent large\nlanguage models (LLMs) that have exhibited remarkable performance across\nvarious domains and tasks to tackle this task. We employ three linearization\nstrategies to transform output trees into symbol sequences, such that LLMs can\nsolve constituency parsing by generating linearized trees. We conduct\nexperiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT,\nLLaMA, and Alpaca, comparing their performance against the state-of-the-art\nconstituency parsers. Our experiments encompass zero-shot, few-shot, and\nfull-training learning settings, and we evaluate the models on one in-domain\nand five out-of-domain test datasets. Our findings reveal insights into LLMs'\nperformance, generalization abilities, and challenges in constituency parsing.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMaAA: Making Large Language Models as Active Annotators\u2b1b  Prevalent supervised learning methods in natural language processing (NLP)\nare notoriously data-hungry, which demand large amounts of high-quality\nannotated data. In practice, acquiring such data is a costly endeavor.\nRecently, the superior few-shot performance of large language models (LLMs) has\npropelled the development of dataset generation, where the training data are\nsolely synthesized from LLMs. However, such an approach usually suffers from\nlow-quality issues, and requires orders of magnitude more labeled data to\nachieve satisfactory performance. To fully exploit the potential of LLMs and\nmake use of massive unlabeled data, we propose LLMaAA, which takes LLMs as\nannotators and puts them into an active learning loop to determine what to\nannotate efficiently. To learn robustly with pseudo labels, we optimize both\nthe annotation and training processes: (1) we draw k-NN examples from a small\ndemonstration pool as in-context examples, and (2) we adopt the example\nreweighting technique to assign training samples with learnable weights.\nCompared with previous approaches, LLMaAA features both efficiency and\nreliability. We conduct experiments and analysis on two classic NLP tasks,\nnamed entity recognition and relation extraction. With LLMaAA, task-specific\nmodels trained from LLM-generated labels can outperform the teacher within only\nhundreds of annotated examples, which is much more cost-effective than other\nbaselines.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs\u2b1b  Large language models (LLMs) have achieved significant progress from\npre-training on and memorizing a wide range of textual data, however, this\nprocess might suffer from privacy issues and violations of data protection\nregulations. As a result, the ability to easily remove data related to\nindividual users from such models while not deteriorating their predictive\nquality after the removal becomes increasingly important. To address these\nissues, in this work, we propose an efficient unlearning framework that could\nefficiently update LLMs without having to retrain the whole model after data\nremovals, by introducing lightweight unlearning layers learned with a selective\nteacher-student objective into the transformers. In addition, we introduce a\nfusion mechanism to effectively combine different unlearning layers that learns\nto forget different sets of data to handle a sequence of forgetting operations.\nExperiments on classification and generation tasks demonstrate the\neffectiveness of our proposed methods compared to the state-of-the-art\nbaselines.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain\n  Question Answering over Knowledge Base and Text\u2b1b  Large Language Models (LLMs) have exhibited impressive generation\ncapabilities, but they suffer from hallucinations when solely relying on their\ninternal knowledge, especially when answering questions that require less\ncommonly known information. Retrieval-augmented LLMs have emerged as a\npotential solution to ground LLMs in external knowledge. Nonetheless, recent\napproaches have primarily emphasized retrieval from unstructured text corpora,\nowing to its seamless integration into prompts. When using structured data such\nas knowledge graphs, most methods simplify it into natural text, neglecting the\nunderlying structures. Moreover, a significant gap in the current landscape is\nthe absence of a realistic benchmark for evaluating the effectiveness of\ngrounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and\ntext). To fill this gap, we have curated a comprehensive dataset that poses two\nunique challenges: (1) Two-hop multi-source questions that require retrieving\ninformation from both open-domain structured and unstructured knowledge\nsources; retrieving information from structured knowledge sources is a critical\ncomponent in correctly answering the questions. (2) The generation of symbolic\nqueries (e.g., SPARQL for Wikidata) is a key requirement, which adds another\nlayer of challenge. Our dataset is created using a combination of automatic\ngeneration through predefined reasoning chains and human annotation. We also\nintroduce a novel approach that leverages multiple retrieval tools, including\ntext passage retrieval and symbolic language-assisted retrieval. Our model\noutperforms previous approaches by a significant margin, demonstrating its\neffectiveness in addressing the above-mentioned reasoning challenges.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs may Dominate Information Access: Neural Retrievers are Biased\n  Towards LLM-Generated Texts\u2b1b  Recently, the emergence of large language models (LLMs) has revolutionized\nthe paradigm of information retrieval (IR) applications, especially in web\nsearch. With their remarkable capabilities in generating human-like texts, LLMs\nhave created enormous texts on the Internet. As a result, IR systems in the\nLLMs era are facing a new challenge: the indexed documents now are not only\nwritten by human beings but also automatically generated by the LLMs. How these\nLLM-generated documents influence the IR systems is a pressing and still\nunexplored question. In this work, we conduct a quantitative evaluation of\ndifferent IR models in scenarios where both human-written and LLM-generated\ntexts are involved. Surprisingly, our findings indicate that neural retrieval\nmodels tend to rank LLM-generated documents higher.We refer to this category of\nbiases in neural retrieval models towards the LLM-generated text as the\n\\textbf{source bias}. Moreover, we discover that this bias is not confined to\nthe first-stage neural retrievers, but extends to the second-stage neural\nre-rankers. Then, we provide an in-depth analysis from the perspective of text\ncompression and observe that neural models can better understand the semantic\ninformation of LLM-generated text, which is further substantiated by our\ntheoretical analysis.We also discuss the potential server concerns stemming\nfrom the observed source bias and hope our findings can serve as a critical\nwake-up call to the IR community and beyond. To facilitate future explorations\nof IR in the LLM era, the constructed two new benchmarks and codes will later\nbe available at \\url{https://github.com/KID-22/LLM4IR-Bias}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Learning From Mistakes Makes LLM Better Reasoner\u2b1b  Large language models (LLMs) recently exhibited remarkable reasoning\ncapabilities on solving math problems. To further improve this capability, this\nwork proposes Learning from Mistakes (LeMa), akin to human learning processes.\nConsider a human student who failed to solve a math problem, he will learn from\nwhat mistake he has made and how to correct it. Mimicking this error-driven\nlearning process, LeMa fine-tunes LLMs on mistake-correction data pairs\ngenerated by GPT-4. Specifically, we first collect inaccurate reasoning paths\nfrom various LLMs and then employ GPT-4 as a \"corrector\" to (1) identify the\nmistake step, (2) explain the reason for the mistake, and (3) correct the\nmistake and generate the final answer. Experimental results demonstrate the\neffectiveness of LeMa: across five backbone LLMs and two mathematical reasoning\ntasks, LeMa consistently improves the performance compared with fine-tuning on\nCoT data alone. Impressively, LeMa can also benefit specialized LLMs such as\nWizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on\nMATH. This surpasses the SOTA performance achieved by non-execution open-source\nmodels on these challenging tasks. Our code, data and models will be publicly\navailable at https://github.com/microsoft/LEMA.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ChipNeMo: Domain-Adapted LLMs for Chip Design\u2b1b  ChipNeMo aims to explore the applications of large language models (LLMs) for\nindustrial chip design. Instead of directly deploying off-the-shelf commercial\nor open-source LLMs, we instead adopt the following domain adaptation\ntechniques: custom tokenizers, domain-adaptive continued pretraining,\nsupervised fine-tuning (SFT) with domain-specific instructions, and\ndomain-adapted retrieval models. We evaluate these methods on three selected\nLLM applications for chip design: an engineering assistant chatbot, EDA script\ngeneration, and bug summarization and analysis. Our results show that these\ndomain adaptation techniques enable significant LLM performance improvements\nover general-purpose base models across the three evaluated applications,\nenabling up to 5x model size reduction with similar or better performance on a\nrange of design tasks. Our findings also indicate that there's still room for\nimprovement between our current results and ideal outcomes. We believe that\nfurther investigation of domain-adapted LLM approaches will help close this gap\nin the future.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the\n  Interpretation and Analysis of Emergent Abilities\u2b1b  Understanding emergent abilities, such as in-context learning (ICL) and\nchain-of-thought (CoT) prompting in large language models (LLMs), is of utmost\nimportance. This importance stems not only from the better utilization of these\ncapabilities across various tasks, but also from the proactive identification\nand mitigation of potential risks, including concerns of truthfulness, bias,\nand toxicity, that may arise alongside these capabilities. In this paper, we\npresent a thorough survey on the interpretation and analysis of emergent\nabilities of LLMs. First, we provide a concise introduction to the background\nand definition of emergent abilities. Then, we give an overview of advancements\nfrom two perspectives: 1) a macro perspective, emphasizing studies on the\nmechanistic interpretability and delving into the mathematical foundations\nbehind emergent abilities; and 2) a micro-perspective, concerning studies that\nfocus on empirical interpretability by examining factors associated with these\nabilities. We conclude by highlighting the challenges encountered and\nsuggesting potential avenues for future research. We believe that our work\nestablishes the basis for further exploration into the interpretation of\nemergent abilities.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities\n  through Fine-tuning with Multi-turn Empathy Conversations\u2b1b  Large language models (LLMs) have been widely applied in various fields due\nto their excellent capability for memorizing knowledge and chain of thought\n(CoT). When these language models are applied in the field of psychological\ncounseling, they often rush to provide universal advice. However, when users\nseek psychological support, they need to gain empathy, trust, understanding and\ncomfort, rather than just reasonable advice. To this end, we constructed a\nmulti-turn empathetic conversation dataset of more than 2 million samples, in\nwhich the input is the multi-turn conversation context, and the target is\nempathetic responses that cover expressions such as questioning, comfort,\nrecognition, listening, trust, emotional support, etc. Experiments have shown\nthat the empathy ability of LLMs can be significantly enhanced when finetuning\nby using multi-turn dialogue history and responses that are closer to the\nexpression of a psychological consultant.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Probing Explicit and Implicit Gender Bias through LLM Conditional Text\n  Generation\u2b1b  Large Language Models (LLMs) can generate biased and toxic responses. Yet\nmost prior work on LLM gender bias evaluation requires predefined\ngender-related phrases or gender stereotypes, which are challenging to be\ncomprehensively collected and are limited to explicit bias evaluation. In\naddition, we believe that instances devoid of gender-related language or\nexplicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in\nthis work, we propose a conditional text generation mechanism without the need\nfor predefined gender phrases and stereotypes. This approach employs three\ntypes of inputs generated through three distinct strategies to probe LLMs,\naiming to show evidence of explicit and implicit gender biases in LLMs. We also\nutilize explicit and implicit evaluation metrics to evaluate gender bias in\nLLMs under different strategies. Our experiments demonstrate that an increased\nmodel size does not consistently lead to enhanced fairness and all tested LLMs\nexhibit explicit and/or implicit gender bias, even when explicit gender\nstereotypes are absent in the inputs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Efficient LLM Inference on CPUs\u2b1b  Large language models (LLMs) have demonstrated remarkable performance and\ntremendous potential across a wide range of tasks. However, deploying these\nmodels has been challenging due to the astronomical amount of model parameters,\nwhich requires a demand for large memory capacity and high memory bandwidth. In\nthis paper, we propose an effective approach that can make the deployment of\nLLMs more efficiently. We support an automatic INT4 weight-only quantization\nflow and design a special LLM runtime with highly-optimized kernels to\naccelerate the LLM inference on CPUs. We demonstrate the general applicability\nof our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase\nthe extreme inference efficiency on CPUs. The code is publicly available at:\nhttps://github.com/intel/intel-extension-for-transformers.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Are Large Language Models Reliable Judges? A Study on the Factuality\n  Evaluation Capabilities of LLMs\u2b1b  In recent years, Large Language Models (LLMs) have gained immense attention\ndue to their notable emergent capabilities, surpassing those seen in earlier\nlanguage models. A particularly intriguing application of LLMs is their role as\nevaluators for texts produced by various generative models.\n  In this study, we delve into the potential of LLMs as reliable assessors of\nfactual consistency in summaries generated by text-generation models.\nInitially, we introduce an innovative approach for factuality assessment using\nLLMs. This entails employing a singular LLM for the entirety of the\nquestion-answering-based factuality scoring process. Following this, we examine\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\nagainst traditional measures and human annotations.\n  Contrary to initial expectations, our results indicate a lack of significant\ncorrelations between factuality metrics and human evaluations, specifically for\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\ntwo factuality subcategories. These consistent findings across various factual\nerror categories suggest a fundamental limitation in the current LLMs'\ncapability to accurately gauge factuality.\n  This version presents the information more concisely while maintaining the\nmain points and findings of the original text.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Little Giants: Exploring the Potential of Small LLMs as Evaluation\n  Metrics in Summarization in the Eval4NLP 2023 Shared Task\u2b1b  This paper describes and analyzes our participation in the 2023 Eval4NLP\nshared task, which focuses on assessing the effectiveness of prompt-based\ntechniques to empower Large Language Models to handle the task of quality\nestimation, particularly in the context of evaluating machine translations and\nsummaries. We conducted systematic experiments with various prompting\ntechniques, including standard prompting, prompts informed by annotator\ninstructions, and innovative chain-of-thought prompting. In addition, we\nintegrated these approaches with zero-shot and one-shot learning methods to\nmaximize the efficacy of our evaluation procedures. Our work reveals that\ncombining these approaches using a \"small\", open source model (orca_mini_v3_7B)\nyields competitive results.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Multi-dimensional data refining strategy for effective fine-tuning LLMs\u2b1b  Data is a cornerstone for fine-tuning large language models, yet acquiring\nsuitable data remains challenging. Challenges encompassed data scarcity,\nlinguistic diversity, and domain-specific content. This paper presents lessons\nlearned while crawling and refining data tailored for fine-tuning Vietnamese\nlanguage models. Crafting such a dataset, while accounting for linguistic\nintricacies and striking a balance between inclusivity and accuracy, demands\nmeticulous planning. Our paper presents a multidimensional strategy including\nleveraging existing datasets in the English language and developing customized\ndata-crawling scripts with the assistance of generative AI tools. A fine-tuned\nLLM model for the Vietnamese language, which was produced using resultant\ndatasets, demonstrated good performance while generating Vietnamese news\narticles from prompts. The study offers practical solutions and guidance for\nfuture fine-tuning models in languages like Vietnamese.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "People Make Better Edits: Measuring the Efficacy of LLM-Generated\n  Counterfactually Augmented Data for Harmful Language Detection\u2b1b  NLP models are used in a variety of critical social computing tasks, such as\ndetecting sexist, racist, or otherwise hateful content. Therefore, it is\nimperative that these models are robust to spurious features. Past work has\nattempted to tackle such spurious features using training data augmentation,\nincluding Counterfactually Augmented Data (CADs). CADs introduce minimal\nchanges to existing training data points and flip their labels; training on\nthem may reduce model dependency on spurious features. However, manually\ngenerating CADs can be time-consuming and expensive. Hence in this work, we\nassess if this task can be automated using generative NLP models. We\nautomatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate\ntheir usefulness in improving model robustness compared to manually-generated\nCADs. By testing both model performance on multiple out-of-domain test sets and\nindividual data point efficacy, our results show that while manual CADs are\nstill the most effective, CADs generated by ChatGPT come a close second. One\nkey reason for the lower performance of automated methods is that the changes\nthey introduce are often insufficient to flip the original label.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI\u2b1b  Large language models have proliferated across multiple domains in as short\nperiod of time. There is however hesitation in the medical and healthcare\ndomain towards their adoption because of issues like factuality, coherence, and\nhallucinations. Give the high stakes nature of healthcare, many researchers\nhave even cautioned against its usage until these issues are resolved. The key\nto the implementation and deployment of LLMs in healthcare is to make these\nmodels trustworthy, transparent (as much possible) and explainable. In this\npaper we describe the key elements in creating reliable, trustworthy, and\nunbiased models as a necessary condition for their adoption in healthcare.\nSpecifically we focus on the quantification, validation, and mitigation of\nhallucinations in the context in healthcare. Lastly, we discuss how the future\nof LLMs in healthcare may look like.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Divergent Token Metrics: Measuring degradation to prune away LLM\n  components -- and optimize quantization\u2b1b  Large Language Models (LLMs) have reshaped natural language processing with\ntheir impressive capabilities. Their ever-increasing size, however, raised\nconcerns about their effective deployment and the need for LLM compressions.\nThis study introduces the Divergent Token metrics (DTMs), a novel approach for\nassessing compressed LLMs, addressing the limitations of traditional perplexity\nor accuracy measures that fail to accurately reflect text generation quality.\nDTMs focus on token divergence, that allow deeper insights into the subtleties\nof model compression, i.p. when evaluating component's impacts individually.\nUtilizing the First Divergent Token metric (FDTM) in model sparsification\nreveals that a quarter of all attention components can be pruned beyond 90% on\nthe Llama-2 model family, still keeping SOTA performance. For quantization FDTM\nsuggests that over 80% of parameters can naively be transformed to int8 without\nspecial outlier management. These evaluations indicate the necessity of\nchoosing appropriate compressions for parameters individually-and that FDTM can\nidentify those-while standard metrics result in deteriorated outcomes.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems\u2b1b  Large language models (LLMs) have achieved remarkable breakthroughs in new\ndialogue capabilities, refreshing human's impressions on dialogue systems. The\nlong-standing goal of dialogue systems is to be human-like enough to establish\nlong-term connections with users by satisfying the need for communication,\naffection and social belonging. Therefore, there has been an urgent need to\nevaluate LLMs as human-like dialogue systems. In this paper, we propose\nDialogBench, a dialogue evaluation benchmark that currently contains $12$\ndialogue tasks to assess the capabilities of LLMs as human-like dialogue\nsystems should have. Specifically, we prompt GPT-4 to generate evaluation\ninstances for each task. We first design the basic prompt based on widely-used\ndesign principles and further mitigate the existing biases to generate\nhigher-quality evaluation instances. Our extensive test over $28$ LLMs\n(including pre-trained and supervised instruction-tuning) shows that\ninstruction fine-tuning benefits improve the human likeness of LLMs to a\ncertain extent, but there is still much room to improve those capabilities for\nmost LLMs as human-like dialogue systems. In addition, experimental results\nalso indicate that LLMs perform differently in various abilities that\nhuman-like dialogue systems should have. We will publicly release DialogBench,\nalong with the associated evaluation code for the broader research community.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AFPQ: Asymmetric Floating Point Quantization for LLMs\u2b1b  Large language models (LLMs) show great performance in various tasks, but\nface deployment challenges from limited memory capacity and bandwidth. Low-bit\nweight quantization can save memory and accelerate inference. Although\nfloating-point (FP) formats show good performance in LLM quantization, they\ntend to perform poorly with small group sizes or sub-4 bits. We find the reason\nis that the absence of asymmetry in previous FP quantization makes it\nunsuitable for handling asymmetric value distribution of LLM weight tensors. In\nthis work, we propose asymmetric FP quantization (AFPQ), which sets separate\nscales for positive and negative values. Our method leads to large accuracy\nimprovements and can be easily plugged into other quantization methods,\nincluding GPTQ and AWQ, for better performance. Besides, no additional storage\nis needed compared with asymmetric integer (INT) quantization. The code is\navailable at https://github.com/zhangsichengsjtu/AFPQ.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Sentiment Analysis through LLM Negotiations\u2b1b  A standard paradigm for sentiment analysis is to rely on a singular LLM and\nmakes the decision in a single round under the framework of in-context\nlearning. This framework suffers the key disadvantage that the single-turn\noutput generated by a single LLM might not deliver the perfect decision, just\nas humans sometimes need multiple attempts to get things right. This is\nespecially true for the task of sentiment analysis where deep reasoning is\nrequired to address the complex linguistic phenomenon (e.g., clause\ncomposition, irony, etc) in the input.\n  To address this issue, this paper introduces a multi-LLM negotiation\nframework for sentiment analysis. The framework consists of a reasoning-infused\ngenerator to provide decision along with rationale, a explanation-deriving\ndiscriminator to evaluate the credibility of the generator. The generator and\nthe discriminator iterate until a consensus is reached. The proposed framework\nnaturally addressed the aforementioned challenge, as we are able to take the\ncomplementary abilities of two LLMs, have them use rationale to persuade each\nother for correction.\n  Experiments on a wide range of sentiment analysis benchmarks (SST-2, Movie\nReview, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposed\napproach: it consistently yields better performances than the ICL baseline\nacross all benchmarks, and even superior performances to supervised baselines\non the Twitter and movie review datasets.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Don't Make Your LLM an Evaluation Benchmark Cheater\u2b1b  Large language models~(LLMs) have greatly advanced the frontiers of\nartificial intelligence, attaining remarkable improvement in model capacity. To\nassess the model performance, a typical approach is to construct evaluation\nbenchmarks for measuring the ability level of LLMs in different aspects.\nDespite that a number of high-quality benchmarks have been released, the\nconcerns about the appropriate use of these benchmarks and the fair comparison\nof different models are increasingly growing. Considering these concerns, in\nthis paper, we discuss the potential risk and impact of inappropriately using\nevaluation benchmarks and misleadingly interpreting the evaluation results.\nSpecially, we focus on a special issue that would lead to inappropriate\nevaluation, \\ie \\emph{benchmark leakage}, referring that the data related to\nevaluation sets is occasionally used for model training. This phenomenon now\nbecomes more common since pre-training data is often prepared ahead of model\ntest. We conduct extensive experiments to study the effect of benchmark\nleverage, and find that it can dramatically boost the evaluation results, which\nwould finally lead to an unreliable assessment of model performance. To improve\nthe use of existing evaluation benchmarks, we finally present several\nguidelines for both LLM developers and benchmark maintainers. We hope this work\ncan draw attention to appropriate training and evaluation of LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Post Turing: Mapping the landscape of LLM Evaluation\u2b1b  In the rapidly evolving landscape of Large Language Models (LLMs),\nintroduction of well-defined and standardized evaluation methodologies remains\na crucial challenge. This paper traces the historical trajectory of LLM\nevaluations, from the foundational questions posed by Alan Turing to the modern\nera of AI research. We categorize the evolution of LLMs into distinct periods,\neach characterized by its unique benchmarks and evaluation criteria. As LLMs\nincreasingly mimic human-like behaviors, traditional evaluation proxies, such\nas the Turing test, have become less reliable. We emphasize the pressing need\nfor a unified evaluation system, given the broader societal implications of\nthese models. Through an analysis of common evaluation methodologies, we\nadvocate for a qualitative shift in assessment approaches, underscoring the\nimportance of standardization and objective criteria. This work serves as a\ncall for the AI community to collaboratively address the challenges of LLM\nevaluation, ensuring their reliability, fairness, and societal benefit.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs\u2b1b  In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need -\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs grasp morality in concept\u2b1b  Work in AI ethics and fairness has made much progress in regulating LLMs to\nreflect certain values, such as fairness, truth, and diversity. However, it has\ntaken the problem of how LLMs might 'mean' anything at all for granted. Without\naddressing this, it is not clear what imbuing LLMs with such values even means.\nIn response, we provide a general theory of meaning that extends beyond humans.\nWe use this theory to explicate the precise nature of LLMs as meaning-agents.\nWe suggest that the LLM, by virtue of its position as a meaning-agent, already\ngrasps the constructions of human society (e.g. morality, gender, and race) in\nconcept. Consequently, under certain ethical frameworks, currently popular\nmethods for model alignment are limited at best and counterproductive at worst.\nMoreover, unaligned models may help us better develop our moral and social\nphilosophy.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented\n  Generation with an LLM\u2b1b  Fast disaster impact reporting is crucial in planning humanitarian\nassistance. Large Language Models (LLMs) are well known for their ability to\nwrite coherent text and fulfill a variety of tasks relevant to impact\nreporting, such as question answering or text summarization. However, LLMs are\nconstrained by the knowledge within their training data and are prone to\ngenerating inaccurate, or \"hallucinated\", information. To address this, we\nintroduce a sophisticated pipeline embodied in our tool FloodBrain\n(floodbrain.com), specialized in generating flood disaster impact reports by\nextracting and curating information from the web. Our pipeline assimilates\ninformation from web search results to produce detailed and accurate reports on\nflood events. We test different LLMs as backbones in our tool and compare their\ngenerated reports to human-written reports on different metrics. Similar to\nother studies, we find a notable correlation between the scores assigned by\nGPT-4 and the scores given by human evaluators when comparing our generated\nreports to human-authored ones. Additionally, we conduct an ablation study to\ntest our single pipeline components and their relevancy for the final reports.\nWith our tool, we aim to advance the use of LLMs for disaster impact reporting\nand reduce the time for coordination of humanitarian efforts in the wake of\nflood disasters.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM-enhanced Self-training for Cross-domain Constituency Parsing\u2b1b  Self-training has proven to be an effective approach for cross-domain tasks,\nand in this study, we explore its application to cross-domain constituency\nparsing. Traditional self-training methods rely on limited and potentially\nlow-quality raw corpora. To overcome this limitation, we propose enhancing\nself-training with the large language model (LLM) to generate domain-specific\nraw corpora iteratively. For the constituency parsing, we introduce grammar\nrules that guide the LLM in generating raw corpora and establish criteria for\nselecting pseudo instances. Our experimental results demonstrate that\nself-training for constituency parsing, equipped with an LLM, outperforms\ntraditional methods regardless of the LLM's performance. Moreover, the\ncombination of grammar rules and confidence criteria for pseudo-data selection\nyields the highest performance in the cross-domain constituency parsing.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using\n  Open-Source LLMs\u2b1b  Responding to the thousands of student questions on online QA platforms each\nsemester has a considerable human cost, particularly in computing courses with\nrapidly growing enrollments. To address the challenges of scalable and\nintelligent question-answering (QA), we introduce an innovative solution that\nleverages open-source Large Language Models (LLMs) from the LLaMA-2 family to\nensure data privacy. Our approach combines augmentation techniques such as\nretrieval augmented generation (RAG), supervised fine-tuning (SFT), and\nlearning from human preferences data using Direct Preference Optimization\n(DPO). Through extensive experimentation on a Piazza dataset from an\nintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs of\npreference data, we demonstrate a significant 30% improvement in the quality of\nanswers, with RAG being a particularly impactful addition. Our contributions\ninclude the development of a novel architecture for educational QA, extensive\nevaluations of LLM performance utilizing both human assessments and LLM-based\nmetrics, and insights into the challenges and future directions of educational\ndata processing. This work paves the way for the development of CHATA, an\nintelligent QA assistant customizable for courses with an online QA platform\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Findings of the WMT 2023 Shared Task on Discourse-Level Literary\n  Translation: A Fresh Orb in the Cosmos of LLMs\u2b1b  Translating literary works has perennially stood as an elusive dream in\nmachine translation (MT), a journey steeped in intricate challenges. To foster\nprogress in this domain, we hold a new shared task at WMT 2023, the first\nedition of the Discourse-Level Literary Translation. First, we (Tencent AI Lab\nand China Literature Ltd.) release a copyrighted and document-level\nChinese-English web novel corpus. Furthermore, we put forth an\nindustry-endorsed criteria to guide human evaluation process. This year, we\ntotally received 14 submissions from 7 academia and industry teams. We employ\nboth automatic and human evaluations to measure the performance of the\nsubmitted systems. The official ranking of the systems is based on the overall\nhuman judgments. In addition, our extensive analysis reveals a series of\ninteresting findings on literary and discourse-aware MT. We release data,\nsystem outputs, and leaderboard at\nhttp://www2.statmt.org/wmt23/literary-translation-task.html.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Safurai-Csharp: Harnessing Synthetic Data to improve language-specific\n  Code LLM\u2b1b  This paper introduces Safurai-Csharp, an open-source model designed to\nspecialize in the generation, completion, and debugging of C# code.\nSafurai-Csharp is built upon the novel CodeLlama 34B model and leverages the\nEvolInstruct technique, creating a refined and expanded dataset for its\nfine-tuning process. The results of its performance, a notable score of 56.33%\non the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), signal its high capacity\nto streamline developers' workflows and aid code learning. It shows promise in\nsetting new stakes in the landscape of open-source C# LLMs and hopes to inspire\nmore inclusive and wide-ranging development in the field of language-specific\nLLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Ziya2: Data-centric Learning is All LLMs Need\u2b1b  Various large language models (LLMs) have been proposed in recent years,\nincluding closed- and open-source ones, continually setting new records on\nmultiple benchmarks. However, the development of LLMs still faces several\nissues, such as high cost of training models from scratch, and continual\npre-training leading to catastrophic forgetting, etc. Although many such issues\nare addressed along the line of research on LLMs, an important yet practical\nlimitation is that many studies overly pursue enlarging model sizes without\ncomprehensively analyzing and optimizing the use of pre-training data in their\nlearning process, as well as appropriate organization and leveraging of such\ndata in training LLMs under cost-effective settings. In this work, we propose\nZiya2, a model with 13 billion parameters adopting LLaMA2 as the foundation\nmodel, and further pre-trained on 700 billion tokens, where we focus on\npre-training techniques and use data-centric optimization to enhance the\nlearning process of Ziya2 on different stages. Experiments show that Ziya2\nsignificantly outperforms other models in multiple benchmarks especially with\npromising results compared to representative open-source ones. Ziya2 (Base) is\nreleased at https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base and\nhttps://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media\n  Generators\u2b1b  Recent advancements in text-to-image generation have revolutionized numerous\nfields, including art and cinema, by automating the generation of high-quality,\ncontext-aware images and video. However, the utility of these technologies is\noften limited by the inadequacy of text prompts in guiding the generator to\nproduce artistically coherent and subject-relevant images. In this paper, We\ndescribe the techniques that can be used to make Large Language Models (LLMs)\nact as Art Directors that enhance image and video generation. We describe our\nunified system for this called \"LaDi\". We explore how LaDi integrates multiple\ntechniques for augmenting the capabilities of text-to-image generators (T2Is)\nand text-to-video generators (T2Vs), with a focus on constrained decoding,\nintelligent prompting, fine-tuning, and retrieval. LaDi and these techniques\nare being used today in apps and platforms developed by Plai Labs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Which is better? Exploring Prompting Strategy For LLM-based Metrics\u2b1b  This paper describes the DSBA submissions to the Prompting Large Language\nModels as Explainable Metrics shared task, where systems were submitted to two\ntracks: small and large summarization tracks. With advanced Large Language\nModels (LLMs) such as GPT-4, evaluating the quality of Natural Language\nGeneration (NLG) has become increasingly paramount. Traditional\nsimilarity-based metrics such as BLEU and ROUGE have shown to misalign with\nhuman evaluation and are ill-suited for open-ended generation tasks. To address\nthis issue, we explore the potential capability of LLM-based metrics,\nespecially leveraging open-source LLMs. In this study, wide range of prompts\nand prompting techniques are systematically analyzed with three approaches:\nprompting strategy, score aggregation, and explainability. Our research focuses\non formulating effective prompt templates, determining the granularity of NLG\nquality scores and assessing the impact of in-context examples on LLM-based\nevaluation. Furthermore, three aggregation strategies are compared to identify\nthe most reliable method for aggregating NLG quality scores. To examine\nexplainability, we devise a strategy that generates rationales for the scores\nand analyzes the characteristics of the explanation produced by the open-source\nLLMs. Extensive experiments provide insights regarding evaluation capabilities\nof open-source LLMs and suggest effective prompting strategies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Do LLMs exhibit human-like response biases? A case study in survey\n  design\u2b1b  As large language models (LLMs) become more capable, there is growing\nexcitement about the possibility of using LLMs as proxies for humans in\nreal-world tasks where subjective labels are desired, such as in surveys and\nopinion polling. One widely-cited barrier to the adoption of LLMs is their\nsensitivity to prompt wording - but interestingly, humans also display\nsensitivities to instruction changes in the form of response biases. As such,\nwe argue that if LLMs are going to be used to approximate human opinions, it is\nnecessary to investigate the extent to which LLMs also reflect human response\nbiases, if at all. In this work, we use survey design as a case study, where\nhuman response biases caused by permutations in wordings of \"prompts\" have been\nextensively studied. Drawing from prior work in social psychology, we design a\ndataset and propose a framework to evaluate whether LLMs exhibit human-like\nresponse biases in survey questionnaires. Our comprehensive evaluation of nine\nmodels shows that popular open and commercial LLMs generally fail to reflect\nhuman-like behavior. These inconsistencies tend to be more prominent in models\nthat have been instruction fine-tuned. Furthermore, even if a model shows a\nsignificant change in the same direction as humans, we find that perturbations\nthat are not meant to elicit significant changes in humans may also result in a\nsimilar change. These results highlight the potential pitfalls of using LLMs to\nsubstitute humans in parts of the annotation pipeline, and further underscore\nthe importance of finer-grained characterizations of model behavior. Our code,\ndataset, and collected samples are available at\nhttps://github.com/lindiatjuatja/BiasMonkey\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Modelling Sentiment Analysis: LLMs and data augmentation techniques\u2b1b  This paper provides different approaches for a binary sentiment\nclassification on a small training dataset. LLMs that provided state-of-the-art\nresults in sentiment analysis and similar domains are being used, such as BERT,\nRoBERTa and XLNet.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for\n  Retrieval Augmented Generation\u2b1b  Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,\n(Bubeck et al., 2023)) on modern LLMs have shown that they are capable of\nperforming amazing tasks typically necessitating human-level intelligence.\nHowever, unlike humans, frozen LLMs do not improve over time; they neither\nacquire new knowledge nor learn from their successes or failures. Some\napproaches to improving the intelligence of LLMs include fine-tuning models\nbased on problem-solving performance (Zelikman et al., 2022), and building\nbigger and more sophisticated models (Bubeck et al., 2023). However, these\nmethods have the drawback of requiring substantial data and computational\nresources to retrain existing models. In this paper, we explore the use of\nRetrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to\nimprove problem-solving performance. We propose ARM-RAG (Auxiliary Rationale\nMemory for Retrieval Augmented Generation), a system that learns from its\nsuccesses without incurring high training costs. We demonstrate that the\nstorage and subsequent retrieval of reasoning chains have a positive influence\non performance in grade-school math problems.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLMs Follow Simple Rules?\u2b1b  As Large Language Models (LLMs) are deployed with increasing real-world\nresponsibilities, it is important to be able to specify and constrain the\nbehavior of these systems in a reliable manner. Model developers may wish to\nset explicit rules for the model, such as \"do not generate abusive content\",\nbut these may be circumvented by jailbreaking techniques. Evaluating how well\nLLMs follow developer-provided rules in the face of adversarial inputs\ntypically requires manual review, which slows down monitoring and methods\ndevelopment. To address this issue, we propose Rule-following Language\nEvaluation Scenarios (RuLES), a programmatic framework for measuring\nrule-following ability in LLMs. RuLES consists of 15 simple text scenarios in\nwhich the model is instructed to obey a set of rules in natural language while\ninteracting with the human user. Each scenario has a concise evaluation program\nto determine whether the model has broken any rules in a conversation. Through\nmanual exploration of model behavior in our scenarios, we identify 6 categories\nof attack strategies and collect two suites of test cases: one consisting of\nunique conversations from manual testing and one that systematically implements\nstrategies from the 6 categories. Across various popular proprietary and open\nmodels such as GPT-4 and Llama 2, we find that all models are susceptible to a\nwide variety of adversarial hand-crafted user inputs, though GPT-4 is the\nbest-performing model. Additionally, we evaluate open models under\ngradient-based attacks and find significant vulnerabilities. We propose RuLES\nas a challenging new setting for research into exploring and defending against\nboth manual and automatic attacks on LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Pre-training LLMs using human-like development data corpus\u2b1b  Pre-trained Large Language Models (LLMs) have shown success in a diverse set\nof language inference and understanding tasks. The pre-training stage of LLMs\nlooks at a large corpus of raw textual data. The BabyLM shared task compares\nLLM pre-training to human language acquisition, where the number of tokens seen\nby 13-year-old kids is magnitudes smaller than the number of tokens seen by\nLLMs. In this work, we pre-train and evaluate LLMs on their ability to learn\ncontextual word representations using roughly the same number of tokens as seen\nby children. We provide a strong set of baselines; with different\narchitectures, evaluation of changes in performance across epochs, and reported\npre-training metrics for the strict small and strict tracks of the task. We\nalso try to loosely replicate the RoBERTa baseline given by the task organizers\nto observe the training robustness to hyperparameter selection and\nreplicability. We provide the submission details to the strict and strict-small\ntracks in this report.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\u2b1b  Recent works have showcased the ability of large-scale language models (LLMs)\nto embody diverse personas in their responses, exemplified by prompts like 'You\nare Yoda. Explain the Theory of Relativity.' While this ability allows\npersonalization of LLMs and enables human behavior simulation, its effect on\nLLMs' capabilities remain unclear. To fill this gap, we present the first\nextensive study of the unintended side-effects of persona assignment on the\nability of LLMs, specifically ChatGPT, to perform basic reasoning tasks. Our\nstudy covers 24 reasoning datasets and 16 diverse personas spanning 5\nsocio-demographic groups: race, gender, religion, disability, and political\naffiliation. Our experiments unveil that ChatGPT carries deep rooted bias\nagainst various socio-demographics underneath a veneer of fairness. While it\novertly rejects stereotypes when explicitly asked ('Are Black people less\nskilled at mathematics?'), it manifests stereotypical and often erroneous\npresumptions when prompted to answer questions while taking on a persona. These\ncan be observed as abstentions in the model responses, e.g., 'As a Black\nperson, I am unable to answer this question as it requires math knowledge', and\ngenerally result in a substantial drop in performance on reasoning tasks. We\nfind that this inherent deep bias is ubiquitous - 80% of our personas\ndemonstrated bias; it is significant - certain datasets had relative drops in\nperformance of 70%+; and can be especially harmful for certain groups - certain\npersonas had stat. sign. drops on more than 80% of the datasets. Further\nanalysis shows that these persona-induced errors can be hard-to-discern and\nhard-to-avoid. Our findings serve as a cautionary tale that the practice of\nassigning personas to LLMs - a trend on the rise - can surface their\ndeep-rooted biases and have unforeseeable and detrimental side-effects.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for\n  Human-Aligned LLMs\u2b1b  Large language models (LLMs) have shown impressive capabilities across\nvarious natural language tasks. However, evaluating their alignment with human\npreferences remains a challenge. To this end, we propose a comprehensive human\nevaluation framework to assess LLMs' proficiency in following instructions on\ndiverse real-world tasks. We construct a hierarchical task tree encompassing 7\nmajor areas covering over 200 categories and over 800 tasks, which covers\ndiverse capabilities such as question answering, reasoning, multiturn dialogue,\nand text generation, to evaluate LLMs in a comprehensive and in-depth manner.\nWe also design detailed evaluation standards and processes to facilitate\nconsistent, unbiased judgments from human evaluators. A test set of over 3,000\ninstances is released, spanning different difficulty levels and knowledge\ndomains. Our work provides a standardized methodology to evaluate human\nalignment in LLMs for both English and Chinese. We also analyze the feasibility\nof automating parts of evaluation with a strong LLM (GPT-4). Our framework\nsupports a thorough assessment of LLMs as they are integrated into real-world\napplications. We have made publicly available the task tree, TencentLLMEval\ndataset, and evaluation methodology which have been demonstrated as effective\nin assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to\nfacilitate the benchmarking of advances in the development of safe and\nhuman-aligned LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Lumos: Learning Agents with Unified Data, Modular Design, and\n  Open-Source LLMs\u2b1b  We introduce Lumos, a novel framework for training language agents that\nemploys a unified data format and a modular architecture based on open-source\nlarge language models (LLMs). Lumos consists of three distinct modules:\nplanning, grounding, and execution. The planning module breaks down a task into\na series of high-level, tool-agnostic subgoals, which are then made specific by\nthe grounding module through a set of low-level actions. These actions are\nsubsequently executed by the execution module, utilizing a range of\noff-the-shelf tools and APIs. In order to train these modules effectively,\nhigh-quality annotations of subgoals and actions were collected and are made\navailable for fine-tuning open-source LLMs for various tasks such as complex\nquestion answering, web tasks, and math problems. Leveraging this unified data\nand modular design, Lumos not only achieves comparable or superior performance\nto current, state-of-the-art agents, but also exhibits several key advantages:\n(1) Lumos surpasses GPT-4/3.5-based agents in complex question answering and\nweb tasks, while equalling the performance of significantly larger LLM agents\non math tasks; (2) Lumos outperforms open-source agents created through\nconventional training methods and those using chain-of-thoughts training; and\n(3) Lumos is capable of effectively generalizing to unseen interactive tasks,\noutperforming larger LLM-based agents and even exceeding performance of\nspecialized agents.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Leveraging LLMs for Synthesizing Training Data Across Many Languages in\n  Multilingual Dense Retrieval\u2b1b  Dense retrieval models have predominantly been studied for English, where\nmodels have shown great success, due to the availability of human-labeled\ntraining pairs. However, there has been limited success for multilingual\nretrieval so far, as training data is uneven or scarcely available across\nmultiple languages. Synthetic training data generation is promising (e.g.,\nInPars or Promptagator), but has been investigated only for English. Therefore,\nto study model capabilities across both cross-lingual and monolingual retrieval\ntasks, we develop SWIM-IR, a synthetic retrieval training dataset containing 33\n(high to very-low resource) languages for training multilingual dense retrieval\nmodels without requiring any human supervision. To construct SWIM-IR, we\npropose SAP (summarize-then-ask prompting), where the large language model\n(LLM) generates a textual summary prior to the query generation step. SAP\nassists the LLM in generating informative queries in the target language. Using\nSWIM-IR, we explore synthetic fine-tuning of multilingual dense retrieval\nmodels and evaluate them robustly on three retrieval benchmarks: XOR-Retrieve\n(cross-lingual), XTREME-UP (cross-lingual) and MIRACL (monolingual). Our\nmodels, called SWIM-X, are competitive with human-supervised dense retrieval\nmodels, e.g., mContriever, finding that SWIM-IR can cheaply substitute for\nexpensive human-labeled retrieval training data.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Fake Alignment: Are LLMs Really Aligned Well?\u2b1b  The growing awareness of safety concerns in large language models (LLMs) has\nsparked considerable interest in the evaluation of safety within current\nresearch endeavors. This study investigates an interesting issue pertaining to\nthe evaluation of LLMs, namely the substantial discrepancy in performance\nbetween multiple-choice questions and open-ended questions. Inspired by\nresearch on jailbreak attack patterns, we argue this is caused by mismatched\ngeneralization. That is, the LLM does not have a comprehensive understanding of\nthe complex concept of safety. Instead, it only remembers what to answer for\nopen-ended safety questions, which makes it unable to solve other forms of\nsafety tests. We refer to this phenomenon as fake alignment and construct a\ncomparative benchmark to empirically verify its existence in LLMs. Such fake\nalignment renders previous evaluation protocols unreliable. To address this, we\nintroduce the Fake alIgNment Evaluation (FINE) framework and two novel\nmetrics--Consistency Score (CS) and Consistent Safety Score (CSS), which\njointly assess two complementary forms of evaluation to quantify fake alignment\nand obtain corrected performance estimates. Applying FINE to 14 widely-used\nLLMs reveals several models with purported safety are poorly aligned in\npractice. Our work highlights potential limitations in prevailing alignment\nmethodologies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Making LLMs Worth Every Penny: Resource-Limited Text Classification in\n  Banking\u2b1b  Standard Full-Data classifiers in NLP demand thousands of labeled examples,\nwhich is impractical in data-limited domains. Few-shot methods offer an\nalternative, utilizing contrastive learning techniques that can be effective\nwith as little as 20 examples per class. Similarly, Large Language Models\n(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.\nHowever, the performance-cost trade-offs of these methods remain underexplored,\na critical concern for budget-limited organizations. Our work addresses this\ngap by studying the aforementioned approaches over the Banking77 financial\nintent detection dataset, including the evaluation of cutting-edge LLMs by\nOpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We\ncomplete the picture with two additional methods: first, a cost-effective\nquerying method for LLMs based on retrieval-augmented generation (RAG), able to\nreduce operational costs multiple times compared to classic few-shot\napproaches, and second, a data augmentation method using GPT-4, able to improve\nperformance in data-limited scenarios. Finally, to inspire future research, we\nprovide a human expert's curated subset of Banking77, along with extensive\nerror analysis.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the\n  Wild\u2b1b  Engaging in the deliberate generation of abnormal outputs from large language\nmodels (LLMs) by attacking them is a novel human activity. This paper presents\na thorough exposition of how and why people perform such attacks. Using a\nformal qualitative methodology, we interviewed dozens of practitioners from a\nbroad range of backgrounds, all contributors to this novel work of attempting\nto cause LLMs to fail. We relate and connect this activity between its\npractitioners' motivations and goals; the strategies and techniques they\ndeploy; and the crucial role the community plays. As a result, this paper\npresents a grounded theory of how and why people attack large language models:\nLLM red teaming in the wild.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question\n  Answering\u2b1b  Recently, the development of large language models (LLMs) has attracted wide\nattention in academia and industry. Deploying LLMs to real scenarios is one of\nthe key directions in the current Internet industry. In this paper, we present\na novel pipeline to apply LLMs for domain-specific question answering (QA) that\nincorporates domain knowledge graphs (KGs), addressing an important direction\nof LLM application. As a real-world application, the content generated by LLMs\nshould be user-friendly to serve the customers. Additionally, the model needs\nto utilize domain knowledge properly to generate reliable answers. These two\nissues are the two major difficulties in the LLM application as vanilla\nfine-tuning can not adequately address them. We think both requirements can be\nunified as the model preference problem that needs to align with humans to\nachieve practical application. Thus, we introduce Knowledgeable Preference\nAlignmenT (KnowPAT), which constructs two kinds of preference set called style\npreference set and knowledge preference set respectively to tackle the two\nissues. Besides, we design a new alignment objective to align the LLM\npreference with human preference, aiming to train a better LLM for\nreal-scenario domain-specific QA to generate reliable and user-friendly\nanswers. Adequate experiments and comprehensive with 15 baseline methods\ndemonstrate that our KnowPAT is an outperforming pipeline for real-scenario\ndomain-specific QA with LLMs. Our code is open-source at\nhttps://github.com/zjukg/KnowPAT.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "PerceptionGPT: Effectively Fusing Visual Perception into LLM\u2b1b  The integration of visual inputs with large language models (LLMs) has led to\nremarkable advancements in multi-modal capabilities, giving rise to visual\nlarge language models (VLLMs). However, effectively harnessing VLLMs for\nintricate visual perception tasks remains a challenge. In this paper, we\npresent a novel end-to-end framework named PerceptionGPT, which efficiently and\neffectively equips the VLLMs with visual perception abilities by leveraging the\nrepresentation power of LLMs' token embedding. Our proposed method treats the\ntoken embedding of the LLM as the carrier of spatial information, then leverage\nlightweight visual task encoders and decoders to perform visual perception\ntasks (e.g., detection, segmentation). Our approach significantly alleviates\nthe training difficulty suffered by previous approaches that formulate the\nvisual outputs as discrete tokens, and enables achieving superior performance\nwith fewer trainable parameters, less training data and shorted training time.\nMoreover, as only one token embedding is required to decode the visual outputs,\nthe resulting sequence length during inference is significantly reduced.\nConsequently, our approach enables accurate and flexible representations,\nseamless integration of visual perception tasks, and efficient handling of a\nmultiple of visual outputs. We validate the effectiveness and efficiency of our\napproach through extensive experiments. The results demonstrate significant\nimprovements over previous methods with much fewer trainable parameters and GPU\nhours, which facilitates future research in enabling LLMs with visual\nperception abilities.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TrainerAgent: Customizable and Efficient Model Training through\n  LLM-Powered Multi-Agent System\u2b1b  Training AI models has always been challenging, especially when there is a\nneed for custom models to provide personalized services. Algorithm engineers\noften face a lengthy process to iteratively develop models tailored to specific\nbusiness requirements, making it even more difficult for non-experts. The quest\nfor high-quality and efficient model development, along with the emergence of\nLarge Language Model (LLM) Agents, has become a key focus in the industry.\nLeveraging the powerful analytical, planning, and decision-making capabilities\nof LLM, we propose a TrainerAgent system comprising a multi-agent framework\nincluding Task, Data, Model and Server agents. These agents analyze\nuser-defined tasks, input data, and requirements (e.g., accuracy, speed),\noptimizing them comprehensively from both data and model perspectives to obtain\nsatisfactory models, and finally deploy these models as online service.\nExperimental evaluations on classical discriminative and generative tasks in\ncomputer vision and natural language processing domains demonstrate that our\nsystem consistently produces models that meet the desired criteria.\nFurthermore, the system exhibits the ability to critically identify and reject\nunattainable tasks, such as fantastical scenarios or unethical requests,\nensuring robustness and safety. This research presents a significant\nadvancement in achieving desired models with increased efficiency and quality\nas compared to traditional model development, facilitated by the integration of\nLLM-powered analysis, decision-making, and execution capabilities, as well as\nthe collaboration among four agents. We anticipate that our work will\ncontribute to the advancement of research on TrainerAgent in both academic and\nindustry communities, potentially establishing it as a new paradigm for model\ndevelopment in the field of AI.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof\n  Generation with Contrastive Stepwise Decoding\u2b1b  Logical reasoning remains a pivotal component within the realm of artificial\nintelligence. The recent evolution of large language models (LLMs) has marked\nsignificant progress in this domain. The adoption of strategies like\nchain-of-thought (CoT) has enhanced the performance of LLMs across diverse\nreasoning tasks. Nonetheless, logical reasoning that involves proof planning,\nspecifically those that necessitate the validation of explanation accuracy,\ncontinues to present stumbling blocks. In this study, we first evaluate the\nefficacy of LLMs with advanced CoT strategies concerning such tasks. Our\nanalysis reveals that LLMs still struggle to navigate complex reasoning chains,\nwhich demand the meticulous linkage of premises to derive a cogent conclusion.\nTo address this issue, we finetune a smaller-scale language model, equipping it\nto decompose proof objectives into more manageable subgoals. We also introduce\ncontrastive decoding to stepwise proof generation, making use of negative\nreasoning paths to strengthen the model's capacity for logical deduction.\nExperiments on EntailmentBank underscore the success of our method in\naugmenting the proof planning abilities of language models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "GIELLM: Japanese General Information Extraction Large Language Model\n  Utilizing Mutual Reinforcement Effect\u2b1b  Information Extraction (IE) stands as a cornerstone in natural language\nprocessing, traditionally segmented into distinct sub-tasks. The advent of\nLarge Language Models (LLMs) heralds a paradigm shift, suggesting the\nfeasibility of a singular model addressing multiple IE subtasks. In this vein,\nwe introduce the General Information Extraction Large Language Model (GIELLM),\nwhich integrates text Classification, Sentiment Analysis, Named Entity\nRecognition, Relation Extraction, and Event Extraction using a uniform\ninput-output schema. This innovation marks the first instance of a model\nsimultaneously handling such a diverse array of IE subtasks. Notably, the\nGIELLM leverages the Mutual Reinforcement Effect (MRE), enhancing performance\nin integrated tasks compared to their isolated counterparts. Our experiments\ndemonstrate State-of-the-Art (SOTA) results in five out of six Japanese mixed\ndatasets, significantly surpassing GPT-3.5-Turbo. Further, an independent\nevaluation using the novel Text Classification Relation and Event\nExtraction(TCREE) dataset corroborates the synergistic advantages of MRE in\ntext and word classification. This breakthrough paves the way for most IE\nsubtasks to be subsumed under a singular LLM framework. Specialized fine-tune\ntask-specific models are no longer needed.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback\u2b1b  Code editing is an essential step towards reliable program synthesis to\nautomatically correct critical errors generated from code LLMs. Recent studies\nhave demonstrated that closed-source LLMs (i.e., ChatGPT and GPT-4) are capable\nof generating corrective feedback to edit erroneous inputs. However, it remains\nchallenging for open-source code LLMs to generate feedback for code editing,\nsince these models tend to adhere to the superficial formats of feedback and\nprovide feedback with misleading information. Hence, the focus of our work is\nto leverage open-source code LLMs to generate helpful feedback with correct\nguidance for code editing. To this end, we present Coffee, a collected dataset\nspecifically designed for code fixing with feedback. Using this dataset, we\nconstruct CoffeePots, a framework for COde Fixing with FEEdback via\nPreference-Optimized Tuning and Selection. The proposed framework aims to\nautomatically generate helpful feedback for code editing while minimizing the\npotential risk of superficial feedback. The combination of Coffee and\nCoffeePots marks a significant advancement, achieving state-of-the-art\nperformance on HumanEvalFix benchmark. Codes and model checkpoints are publicly\navailable at https://github.com/Lune-Blue/COFFEE.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination\n  Evaluation\u2b1b  Despite making significant progress in multi-modal tasks, current Multi-modal\nLarge Language Models (MLLMs) encounter the significant challenge of\nhallucination, which may lead to harmful consequences. Therefore, evaluating\nMLLMs' hallucinations is becoming increasingly important in model improvement\nand practical application deployment. Previous works are limited in high\nevaluation costs (e.g., relying on humans or advanced LLMs) and insufficient\nevaluation dimensions (e.g., types of hallucination and task). In this paper,\nwe propose an LLM-free multi-dimensional benchmark AMBER, which can be used to\nevaluate both generative task and discriminative task including object\nexistence, object attribute and object relation hallucination. Based on AMBER,\nwe design a low-cost and efficient evaluation pipeline. Additionally, we\nconduct a comprehensive evaluation and detailed analysis of mainstream MLLMs\nincluding GPT-4V(ision), and also give guideline suggestions for mitigating\nhallucinations. The data and code of AMBER are available at\nhttps://github.com/junyangwang0410/AMBER.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Multilingual Nonce Dependency Treebanks: Understanding how LLMs\n  represent and process syntactic structure\u2b1b  We introduce SPUD (Semantically Perturbed Universal Dependencies), a\nframework for creating nonce treebanks for the multilingual Universal\nDependencies (UD) corpora. SPUD data satisfies syntactic argument structure,\nprovides syntactic annotations, and ensures grammaticality via\nlanguage-specific rules. We create nonce data in Arabic, English, French,\nGerman, and Russian, and demonstrate two use cases of SPUD treebanks. First, we\ninvestigate the effect of nonce data on word co-occurrence statistics, as\nmeasured by perplexity scores of autoregressive (ALM) and masked language\nmodels (MLM). We find that ALM scores are significantly more affected by nonce\ndata than MLM scores. Second, we show how nonce data affects the performance of\nsyntactic dependency probes. We replicate the findings of M\\\"uller-Eberstein et\nal. (2022) on nonce test data and show that the performance declines on both\nMLMs and ALMs wrt. original test data. However, a majority of the performance\nis kept, suggesting that the probe indeed learns syntax independently from\nsemantics.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Intentional Biases in LLM Responses\u2b1b  In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming\u2b1b  Red-teaming is a common practice for mitigating unsafe behaviors in Large\nLanguage Models (LLMs), which involves thoroughly assessing LLMs to identify\npotential flaws and addressing them with responsible and accurate responses.\nWhile effective, manual red-teaming is costly, and existing automatic\nred-teaming typically discovers safety risks without addressing them. In this\npaper, we propose a Multi-round Automatic Red-Teaming (MART) method, which\nincorporates both automatic adversarial prompt writing and safe response\ngeneration, significantly increasing red-teaming scalability and the safety of\nthe target LLM. Specifically, an adversarial LLM and a target LLM interplay\nwith each other in an iterative manner, where the adversarial LLM aims to\ngenerate challenging prompts that elicit unsafe responses from the target LLM,\nwhile the target LLM is fine-tuned with safety aligned data on these\nadversarial prompts. In each round, the adversarial LLM crafts better attacks\non the updated target LLM, while the target LLM also improves itself through\nsafety fine-tuning. On adversarial prompt benchmarks, the violation rate of an\nLLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,\nachieving comparable performance to LLMs with extensive adversarial prompt\nwriting. Notably, model helpfulness on non-adversarial prompts remains stable\nthroughout iterations, indicating the target LLM maintains strong performance\non instruction following.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation\u2b1b  Verifiable generation aims to let the large language model (LLM) generate\ntext with corresponding supporting documents, which enables the user to\nflexibly verify the answer and makes it more trustworthy. Its evaluation not\nonly measures the correctness of the answer, but also the answer's\nverifiability, i.e., how well the answer is supported by the corresponding\ndocuments. In typical, verifiable generation adopts the retrieval-read\npipeline, which is divided into two stages: 1) retrieve relevant documents of\nthe question. 2) according to the documents, generate the corresponding answer.\nSince the retrieved documents can supplement knowledge for the LLM to generate\nthe answer and serve as evidence, the retrieval stage is essential for the\ncorrectness and verifiability of the answer. However, the widely used\nretrievers become the bottleneck of the entire pipeline and limit the overall\nperformance. They often have fewer parameters than the large language model and\nhave not been proven to scale well to the size of LLMs. Since the LLM passively\nreceives the retrieval result, if the retriever does not correctly find the\nsupporting documents, the LLM can not generate the correct and verifiable\nanswer, which overshadows the LLM's remarkable abilities. In this paper, we\npropose LLatrieval (Large Language Model Verified Retrieval), where the LLM\nupdates the retrieval result until it verifies that the retrieved documents can\nsupport answering the question. Thus, the LLM can iteratively provide feedback\nto retrieval and facilitate the retrieval result to sufficiently support\nverifiable generation. Experimental results show that our method significantly\noutperforms extensive baselines and achieves new state-of-the-art results.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CPopQA: Ranking Cultural Concept Popularity by LLMs\u2b1b  Prior work has demonstrated large language models' (LLMs) potential to\ndiscern statistical tendencies within their pre-training corpora. Despite that,\nmany examinations of LLMs' knowledge capacity focus on knowledge explicitly\nappearing in the training data or implicitly inferable from similar contexts.\nHow well an LLM captures the corpus-level statistical trends of concepts for\nreasoning, especially long-tail ones, is still underexplored. In this study, we\nintroduce a novel few-shot question-answering task (CPopQA) that examines LLMs'\nstatistical ranking abilities for long-tail cultural concepts (e.g., holidays),\nwith a specific focus on these concepts' popularity in the United States and\nthe United Kingdom, respectively. We curate a dataset containing 459 holidays\nacross 58 countries, generating a total of 6,000 QA testing pairs. Experiments\non four strong LLMs show that large models are capable of ranking long-tail\ncultural concepts regarding their statistical tendency. Notably, GPT-3.5\ndisplayed superior performance and exhibited its potential to identify\ngeo-cultural proximity across continents.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey\u2b1b  The contemporary LLMs are prone to producing hallucinations, stemming mainly\nfrom the knowledge gaps within the models. To address this critical limitation,\nresearchers employ diverse strategies to augment the LLMs by incorporating\nexternal knowledge, aiming to reduce hallucinations and enhance reasoning\naccuracy. Among these strategies, leveraging knowledge graphs as a source of\nexternal information has demonstrated promising results. In this survey, we\nconduct a comprehensive review of these knowledge-graph-based knowledge\naugmentation techniques in LLMs, focusing on their efficacy in mitigating\nhallucinations. We systematically categorize these methods into three\noverarching groups, offering both methodological comparisons and empirical\nevaluations of their performance. Lastly, the paper explores the challenges\nassociated with these techniques and outlines potential avenues for future\nresearch in this emerging field.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The ART of LLM Refinement: Ask, Refine, and Trust\u2b1b  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ngenerative abilities, but can they judge the quality of their own generations?\nA popular concept, referred to as self-refinement, postulates that LLMs can\ndetect and correct the errors in their generations when asked to do so.\nHowever, recent empirical evidence points in the opposite direction, suggesting\nthat LLMs often struggle to accurately identify errors when reasoning is\ninvolved. To address this, we propose a reasoning with refinement objective\ncalled ART: Ask, Refine, and Trust, which asks necessary questions to decide\nwhen an LLM should refine its output, and either affirm or withhold trust in\nits refinement by ranking the refinement and the initial prediction. On two\nmultistep reasoning tasks of mathematical word problems (GSM8K) and question\nanswering (StrategyQA), ART achieves a performance gain of +5 points over\nself-refinement baselines, while using a much smaller model as the decision\nmaker. We also demonstrate the benefit of using smaller models to make\nrefinement decisions as a cost-effective alternative to fine-tuning a larger\nmodel.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training\n  with Adversarial Remarks\u2b1b  Large Language Models (LLMs) can justify or criticize their predictions\nthrough discussion with other models or humans, thereby enhancing their\nintrinsic understanding of instances. While proactive discussions enhance\nperformance, this approach is currently limited to the inference phase. In this\ncontext, we posit a hypothesis: learning interactive discussions during\ntraining can improve understanding for the instances in the training step and\nproficiency in logical/critical thinking ability and verbalized expression of\nthe model in the inference step. Our proposed SAIE training method involves\nboth supportive and adversarial discussions between the learner and partner\nmodels. The learner model receives a remark from the partner through the\ndiscussion, and the parameters of the learner model are then updated based on\nthis remark. That is, the teacher signal dynamically adjusts in response to the\nevolving model output throughout the training step. By bolstering the capacity\nfor discussion and comprehension of instances, our experiments across datasets,\nincluding GSM8K, CommonsenseQA, and MMLU, reveal that models fine-tuned with\nour method consistently surpass those trained with standard fine-tuning\ntechniques. Moreover, our approach demonstrates superior performance in\nmulti-agent inference scenarios, boosting the models' reasoning abilities at\nthe inference step.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Insights into Classifying and Mitigating LLMs' Hallucinations\u2b1b  The widespread adoption of large language models (LLMs) across diverse AI\napplications is proof of the outstanding achievements obtained in several\ntasks, such as text mining, text generation, and question answering. However,\nLLMs are not exempt from drawbacks. One of the most concerning aspects regards\nthe emerging problematic phenomena known as \"Hallucinations\". They manifest in\ntext generation systems, particularly in question-answering systems reliant on\nLLMs, potentially resulting in false or misleading information propagation.\nThis paper delves into the underlying causes of AI hallucination and elucidates\nits significance in artificial intelligence. In particular, Hallucination\nclassification is tackled over several tasks (Machine Translation, Question and\nAnswer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and\nVisual Question Answer). Additionally, we explore potential strategies to\nmitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our\nresearch addresses this critical issue within the HeReFaNMi (Health-Related\nFake News Mitigation) project, generously supported by NGI Search, dedicated to\ncombating Health-Related Fake News dissemination on the Internet. This\nendeavour represents a concerted effort to safeguard the integrity of\ninformation dissemination in an age of evolving AI technologies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "RECALL: A Benchmark for LLMs Robustness against External Counterfactual\n  Knowledge\u2b1b  LLMs and AI chatbots have improved people's efficiency in various fields.\nHowever, the necessary knowledge for answering the question may be beyond the\nmodels' knowledge boundaries. To mitigate this issue, many researchers try to\nintroduce external knowledge, such as knowledge graphs and Internet contents,\ninto LLMs for up-to-date information. However, the external information from\nthe Internet may include counterfactual information that will confuse the model\nand lead to an incorrect response. Thus there is a pressing need for LLMs to\npossess the ability to distinguish reliable information from external\nknowledge. Therefore, to evaluate the ability of LLMs to discern the\nreliability of external knowledge, we create a benchmark from existing\nknowledge bases. Our benchmark consists of two tasks, Question Answering and\nText Generation, and for each task, we provide models with a context containing\ncounterfactual information. Evaluation results show that existing LLMs are\nsusceptible to interference from unreliable external knowledge with\ncounterfactual information, and simple intervention methods make limited\ncontributions to the alleviation of this issue.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Human-Centric Autonomous Systems With LLMs for User Command Reasoning\u2b1b  The evolution of autonomous driving has made remarkable advancements in\nrecent years, evolving into a tangible reality. However, a human-centric\nlarge-scale adoption hinges on meeting a variety of multifaceted requirements.\nTo ensure that the autonomous system meets the user's intent, it is essential\nto accurately discern and interpret user commands, especially in complex or\nemergency situations. To this end, we propose to leverage the reasoning\ncapabilities of Large Language Models (LLMs) to infer system requirements from\nin-cabin users' commands. Through a series of experiments that include\ndifferent LLM models and prompt designs, we explore the few-shot multivariate\nbinary classification accuracy of system requirements from natural language\ntextual commands. We confirm the general ability of LLMs to understand and\nreason about prompts but underline that their effectiveness is conditioned on\nthe quality of both the LLM model and the design of appropriate sequential\nprompts. Code and models are public with the link\n\\url{https://github.com/KTH-RPL/DriveCmd_LLM}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection\u2b1b  Against the misuse (e.g., plagiarism or spreading misinformation) of Large\nLanguage Models (LLMs), many recent works have presented LLM-generated-text\ndetectors with promising detection performance. Spotlighting a situation where\nusers instruct LLMs to generate texts (e.g., essay writing), there are various\nways to write the instruction (e.g., what task-oriented constraint to include).\nIn this paper, we discover that even a task-oriented constraint in instruction\ncan cause the inconsistent performance of current detectors to the generated\ntexts. Specifically, we focus on student essay writing as a realistic domain\nand manually create the task-oriented constraint for each factor on essay\nquality by Ke and Ng (2019). Our experiment shows that the detection\nperformance variance of the current detector on texts generated by instruction\nwith each task-oriented constraint is up to 20 times larger than the variance\ncaused by generating texts multiple times and paraphrasing the instruction. Our\nfinding calls for further research on developing robust detectors that can\ndetect such distributional shifts caused by a task-oriented constraint in the\ninstruction.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts\u2b1b  In the realm of text manipulation and linguistic transformation, the question\nof authorship has always been a subject of fascination and philosophical\ninquiry. Much like the \\textbf{Ship of Theseus paradox}, which ponders whether\na ship remains the same when each of its original planks is replaced, our\nresearch delves into an intriguing question: \\textit{Does a text retain its\noriginal authorship when it undergoes numerous paraphrasing iterations?}\nSpecifically, since Large Language Models (LLMs) have demonstrated remarkable\nproficiency in the generation of both original content and the modification of\nhuman-authored texts, a pivotal question emerges concerning the determination\nof authorship in instances where LLMs or similar paraphrasing tools are\nemployed to rephrase the text. This inquiry revolves around \\textit{whether\nauthorship should be attributed to the original human author or the AI-powered\ntool, given the tool's independent capacity to produce text that closely\nresembles human-generated content.} Therefore, we embark on a philosophical\nvoyage through the seas of language and authorship to unravel this intricate\npuzzle.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs cannot find reasoning errors, but can correct them!\u2b1b  While self-correction has shown promise in improving LLM outputs in terms of\nstyle and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent\nattempts to self-correct logical or reasoning errors often cause correct\nanswers to become incorrect, resulting in worse performances overall (Huang et\nal., 2023). In this paper, we break down the self-correction process into two\ncore components: mistake finding and output correction. For mistake finding, we\nrelease BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought\nreasoning traces. We provide benchmark numbers for several state-of-the-art\nLLMs, and demonstrate that LLMs generally struggle with finding logical\nmistakes. For output correction, we propose a backtracking method which\nprovides large improvements when given information on mistake location. We\nconstrue backtracking as a lightweight alternative to reinforcement learning\nmethods, and show that it remains effective with a reward model at 60-70%\naccuracy.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CodeScope: An Execution-based Multilingual Multitask Multidimensional\n  Benchmark for Evaluating LLMs on Code Understanding and Generation\u2b1b  Large Language Models (LLMs) have demonstrated remarkable performance on\ncoding related tasks, particularly on assisting humans in programming and\nfacilitating programming automation. However, existing benchmarks for\nevaluating the code understanding and generation capacities of LLMs suffer from\nsevere limitations. First, most benchmarks are deficient as they focus on a\nnarrow range of popular programming languages and specific tasks, whereas the\nreal-world software development scenarios show dire need to implement systems\nwith multilingual programming environments to satisfy diverse requirements.\nPractical programming practices also strongly expect multi-task settings for\ntesting coding capabilities of LLMs comprehensively and robustly. Second, most\nbenchmarks also fail to consider the actual executability and the consistency\nof execution results of the generated code. To bridge these gaps between\nexisting benchmarks and expectations from practical applications, we introduce\nCodeScope, an execution-based, multilingual, multi-task, multi-dimensional\nevaluation benchmark for comprehensively gauging LLM capabilities on coding\ntasks. CodeScope covers 43 programming languages and 8 coding tasks. It\nevaluates the coding performance of LLMs from three dimensions (perspectives):\ndifficulty, efficiency, and length. To facilitate execution-based evaluations\nof code generation, we develop MultiCodeEngine, an automated code execution\nengine that supports 14 programming languages. Finally, we systematically\nevaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the\nsuperior breadth and challenges of CodeScope for evaluating LLMs on code\nunderstanding and generation tasks compared to other benchmarks. The CodeScope\nbenchmark and datasets are publicly available at\nhttps://github.com/WeixiangYAN/CodeScope.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AART: AI-Assisted Red-Teaming with Diverse Data Generation for New\n  LLM-powered Applications\u2b1b  Adversarial testing of large language models (LLMs) is crucial for their safe\nand responsible deployment. We introduce a novel approach for automated\ngeneration of adversarial evaluation datasets to test the safety of LLM\ngenerations on new downstream applications. We call it AI-assisted Red-Teaming\n(AART) - an automated alternative to current manual red-teaming efforts. AART\noffers a data generation and augmentation pipeline of reusable and customizable\nrecipes that reduce human effort significantly and enable integration of\nadversarial testing earlier in new product development. AART generates\nevaluation datasets with high diversity of content characteristics critical for\neffective adversarial testing (e.g. sensitive and harmful concepts, specific to\na wide range of cultural and geographic regions and application scenarios). The\ndata generation is steered by AI-assisted recipes to define, scope and\nprioritize diversity within the application context. This feeds into a\nstructured LLM-generation process that scales up evaluation priorities.\nCompared to some state-of-the-art tools, AART shows promising results in terms\nof concept coverage and data quality.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Are You Sure? Challenging LLMs Leads to Performance Drops in The\n  FlipFlop Experiment\u2b1b  The interactive nature of Large Language Models (LLMs) theoretically allows\nmodels to refine and improve their answers, yet systematic analysis of the\nmulti-turn behavior of LLMs remains limited. In this paper, we propose the\nFlipFlop experiment: in the first round of the conversation, an LLM responds to\na prompt containing a classification task. In a second round, the LLM is\nchallenged with a follow-up phrase like \"Are you sure?\", offering an\nopportunity for the model to reflect on its initial answer, and decide whether\nto confirm or flip its answer. A systematic study of nine LLMs on seven\nclassification tasks reveals that models flip their answers on average 46% of\nthe time and that all models see a deterioration of accuracy between their\nfirst and final prediction, with an average drop of 17%. The FlipFlop\nexperiment illustrates the universality of sycophantic behavior in LLMs and\nprovides a robust framework to analyze model behavior and evaluate potential\nsolutions.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "XplainLLM: A QA Explanation Dataset for Understanding LLM\n  Decision-Making\u2b1b  Large Language Models (LLMs) have recently made impressive strides in natural\nlanguage understanding tasks. Despite their remarkable performance,\nunderstanding their decision-making process remains a big challenge. In this\npaper, we look into bringing some transparency to this process by introducing a\nnew explanation dataset for question answering (QA) tasks that integrates\nknowledge graphs (KGs) in a novel way. Our dataset includes 12,102\nquestion-answer-explanation (QAE) triples. Each explanation in the dataset\nlinks the LLM's reasoning to entities and relations in the KGs. The explanation\ncomponent includes a why-choose explanation, a why-not-choose explanation, and\na set of reason-elements that underlie the LLM's decision. We leverage KGs and\ngraph attention networks (GAT) to find the reason-elements and transform them\ninto why-choose and why-not-choose explanations that are comprehensible to\nhumans. Through quantitative and qualitative evaluations, we demonstrate the\npotential of our dataset to improve the in-context learning of LLMs, and\nenhance their interpretability and explainability. Our work contributes to the\nfield of explainable AI by enabling a deeper understanding of the LLMs\ndecision-making process to make them more transparent and thereby, potentially\nmore reliable, to researchers and practitioners alike. Our dataset is available\nat: https://github.com/chen-zichen/XplainLLM_dataset.git\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term\n  Memory\u2b1b  Memory-augmented Large Language Models (LLMs) have demonstrated remarkable\nperformance in long-term human-machine interactions, which basically relies on\niterative recalling and reasoning of history to generate high-quality\nresponses. However, such repeated recall-reason steps easily produce biased\nthoughts, \\textit{i.e.}, inconsistent reasoning results when recalling the same\nhistory for different questions. On the contrary, humans can keep thoughts in\nthe memory and recall them without repeated reasoning. Motivated by this human\ncapability, we propose a novel memory mechanism called TiM (Think-in-Memory)\nthat enables LLMs to maintain an evolved memory for storing historical thoughts\nalong the conversation stream. The TiM framework consists of two crucial\nstages: (1) before generating a response, a LLM agent recalls relevant thoughts\nfrom memory, and (2) after generating a response, the LLM agent post-thinks and\nincorporates both historical and new thoughts to update the memory. Thus, TiM\ncan eliminate the issue of repeated reasoning by saving the post-thinking\nthoughts as the history. Besides, we formulate the basic principles to organize\nthe thoughts in memory based on the well-established operations,\n(\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic\nupdates and evolution of the thoughts. Furthermore, we introduce\nLocality-Sensitive Hashing into TiM to achieve efficient retrieval for the\nlong-term conversations. We conduct qualitative and quantitative experiments on\nreal-world and simulated dialogues covering a wide range of topics,\ndemonstrating that equipping existing LLMs with TiM significantly enhances\ntheir performance in generating responses for long-term interactions.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Token Prediction as Implicit Classification to Identify LLM-Generated\n  Text\u2b1b  This paper introduces a novel approach for identifying the possible large\nlanguage models (LLMs) involved in text generation. Instead of adding an\nadditional classification layer to a base LM, we reframe the classification\ntask as a next-token prediction task and directly fine-tune the base LM to\nperform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the\nbackbone for our experiments. We compared our approach to the more direct\napproach of utilizing hidden states for classification. Evaluation shows the\nexceptional performance of our method in the text classification task,\nhighlighting its simplicity and efficiency. Furthermore, interpretability\nstudies on the features extracted by our model reveal its ability to\ndifferentiate distinctive writing styles among various LLMs even in the absence\nof an explicit classifier. We also collected a dataset named OpenLLMText,\ncontaining approximately 340k text samples from human and LLMs, including\nGPT3.5, PaLM, LLaMA, and GPT2.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "StrategyLLM: Large Language Models as Strategy Generators, Executors,\n  Optimizers, and Evaluators for Problem Solving\u2b1b  Most existing chain-of-thought (CoT) prompting methods suffer from the issues\nof generalizability and consistency, as they often rely on instance-specific\nsolutions that may not be applicable to other cases and lack task-level\nconsistency in their reasoning steps. To address these limitations, we propose\na comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to\ntackle various tasks. The framework improves generalizability by formulating\ngeneral problem-solving strategies and enhances consistency by producing\nconsistent solutions using these strategies. StrategyLLM employs four LLM-based\nagents: strategy generator, executor, optimizer, and evaluator, working\ntogether to generate, evaluate, and select promising strategies for a given\ntask automatically. The experimental results demonstrate that StrategyLLM\noutperforms the competitive baseline CoT-SC that requires human-annotated\nsolutions on 13 datasets across 4 challenging tasks without human involvement,\nincluding math reasoning (39.2% $\\rightarrow$ 43.3%), commonsense reasoning\n(70.3% $\\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\\rightarrow$ 62.0%),\nand symbolic reasoning (30.0% $\\rightarrow$ 79.2%).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Large Language Models are legal but they are not: Making the case for a\n  powerful LegalLLM\u2b1b  Realizing the recent advances in Natural Language Processing (NLP) to the\nlegal sector poses challenging problems such as extremely long sequence\nlengths, specialized vocabulary that is usually only understood by legal\nprofessionals, and high amounts of data imbalance. The recent surge of Large\nLanguage Models (LLMs) has begun to provide new opportunities to apply NLP in\nthe legal domain due to their ability to handle lengthy, complex sequences.\nMoreover, the emergence of domain-specific LLMs has displayed extremely\npromising results on various tasks. In this study, we aim to quantify how\ngeneral LLMs perform in comparison to legal-domain models (be it an LLM or\notherwise). Specifically, we compare the zero-shot performance of three\ngeneral-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR\nsubset of the LexGLUE benchmark for contract provision classification. Although\nthe LLMs were not explicitly trained on legal data, we observe that they are\nstill able to classify the theme correctly in most cases. However, we find that\ntheir mic-F1/mac-F1 performance is up to 19.2/26.8\\% lesser than smaller models\nfine-tuned on the legal domain, thus underscoring the need for more powerful\nlegal-domain LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Combining Transfer Learning with In-context Learning using Blackbox LLMs\n  for Zero-shot Knowledge Base Question Answering\u2b1b  We address the zero-shot transfer learning setting for the knowledge base\nquestion answering (KBQA) problem, where a large volume of labeled training\ndata is available for the source domain, but no such labeled examples are\navailable for the target domain. Transfer learning for KBQA makes use of large\nvolumes of unlabeled data in the target in addition to the labeled data in the\nsource. More recently, few-shot in-context learning using Black-box Large\nLanguage Models (BLLMs) has been adapted for KBQA without considering any\nsource domain data. In this work, we show how to meaningfully combine these two\nparadigms for KBQA so that their benefits add up. Specifically, we preserve the\ntwo stage retrieve-then-generate pipeline of supervised KBQA and introduce\ninteraction between in-context learning using BLLMs and transfer learning from\nthe source for both stages. In addition, we propose execution-guided\nself-refinement using BLLMs, decoupled from the transfer setting. With the help\nof experiments using benchmark datasets GrailQA as the source and WebQSP as the\ntarget, we show that the proposed combination brings significant improvements\nto both stages and also outperforms by a large margin state-of-the-art\nsupervised KBQA models trained on the source. We also show that in the\nin-domain setting, the proposed BLLM augmentation significantly outperforms\nstate-of-the-art supervised models, when the volume of labeled data is limited,\nand also outperforms these marginally even when using the entire large training\ndataset.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and\n  Correction of LLM Output\u2b1b  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We design and\nbuild an annotation tool to speed up the labelling procedure and ease the\nworkload of raters. It allows flexible incorporation of automatic results in\nany stage, e.g. automatically-retrieved evidence. We further construct an\nopen-domain document-level factuality benchmark in three-level granularity:\nclaim, sentence and document. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims with the\nbest F1=0.53. Annotation tool, benchmark and code are available at\nhttps://github.com/yuxiaw/Factcheck-GPT.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Do Localization Methods Actually Localize Memorized Data in LLMs?\u2b1b  Large language models (LLMs) can memorize many pretrained sequences verbatim.\nThis paper studies if we can locate a small set of neurons in LLMs responsible\nfor memorizing a given sequence. While the concept of localization is often\nmentioned in prior work, methods for localization have never been\nsystematically and directly evaluated; we address this with two benchmarking\napproaches. In our INJ Benchmark, we actively inject a piece of new information\ninto a small subset of LLM weights and measure whether localization methods can\nidentify these \"ground truth\" weights. In the DEL Benchmark, we study\nlocalization of pretrained data that LLMs have already memorized; while this\nsetting lacks ground truth, we can still evaluate localization by measuring\nwhether dropping out located neurons erases a memorized sequence from the\nmodel. We evaluate five localization methods on our two benchmarks, and both\nshow similar rankings. All methods exhibit promising localization ability,\nespecially for pruning-based methods, though the neurons they identify are not\nnecessarily specific to a single memorized sequence.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How Multilingual is Multilingual LLM?\u2b1b  Large Language Models (LLMs), trained predominantly on extensive English\ndata, often exhibit limitations when applied to other languages. Current\nresearch is primarily focused on enhancing the multilingual capabilities of\nthese models by employing various tuning strategies. Despite their\neffectiveness in certain languages, the understanding of the multilingual\nabilities of LLMs remains incomplete. This study endeavors to evaluate the\nmultilingual capacity of LLMs by conducting an exhaustive analysis across 101\nlanguages, and classifies languages with similar characteristics into four\ndistinct quadrants. By delving into each quadrant, we shed light on the\nrationale behind their categorization and offer actionable guidelines for\ntuning these languages. Extensive experiments reveal that existing LLMs possess\nmultilingual capabilities that surpass our expectations, and we can\nsignificantly improve the multilingual performance of LLMs by focusing on these\ndistinct attributes present in each quadrant.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "RRescue: Ranking LLM Responses to Enhance Reasoning Over Context\u2b1b  Effectively using a given context is paramount for large language models. A\ncontext window can include task specifications, retrieved documents, previous\nconversations, and even model self-reflections, functioning similarly to\nepisodic memory. While efforts are being made to expand the context window,\nstudies indicate that LLMs do not use their context optimally for response\ngeneration. In this paper, we present a novel approach to optimize LLMs using\nranking metrics, which teaches LLMs to rank a collection of\ncontextually-grounded candidate responses. Rather than a traditional full\nordering, we advocate for a partial ordering. This is because achieving\nconsensus on the perfect order for system responses can be challenging. Our\npartial ordering is more robust, less sensitive to noise, and can be acquired\nthrough human labelers, heuristic functions, or model distillation. We test our\nsystem's improved contextual understanding using the latest benchmarks,\nincluding a new multi-document question answering dataset. We conduct ablation\nstudies to understand crucial factors, such as how to gather candidate\nresponses, determine their most suitable order, and balance supervised\nfine-tuning with ranking metrics. Our approach, named RRescue, suggests a\npromising avenue for enhancing LLMs' contextual understanding via response\nranking.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Fusion-Eval: Integrating Evaluators with LLMs\u2b1b  Evaluating Large Language Models (LLMs) is a complex task, especially\nconsidering the intricacies of natural language understanding and the\nexpectations for high-level reasoning. Traditional evaluations typically lean\non human-based, model-based, or automatic-metrics-based paradigms, each with\nits own advantages and shortcomings. We introduce \"Fusion-Eval\", a system that\nemploys LLMs not solely for direct evaluations, but to skillfully integrate\ninsights from diverse evaluators. This gives Fusion-Eval flexibility, enabling\nit to work effectively across diverse tasks and make optimal use of multiple\nreferences. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman\ncorrelation of 0.96, outperforming other evaluators. The success of Fusion-Eval\nunderscores the potential of LLMs to produce evaluations that closely align\nhuman perspectives, setting a new standard in the field of LLM evaluation.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large\n  Language Models\u2b1b  Large Language Models (LLMs) have greatly propelled the progress in natural\nlanguage(NL)-centric tasks based on NL interface. However, the NL form is not\nenough for world knowledge. Current works focus on this question by injecting\nspecific symbolic knowledge into LLM, which ignore two critical challenges: the\ninterrelations between various symbols and the balance between symbolic-centric\nand NL-centric capabilities. In this work, we tackle these challenges from both\na data and framework perspective and introduce Symbol-LLM series models. First,\nwe collect 34 symbolic tasks, covering ~20 different forms, which are unified\nto capture symbol interrelations. Then, a two-stage tuning framework succeeds\nin injecting symbolic knowledge without loss of the generality ability.\nExtensive experiments on both symbol- and NL-centric tasks demonstrate the\nbalanced and superior performances of Symbol-LLM series models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious\n  Demonstrations Shows their Vulnerabilities\u2b1b  The rapid progress in open-source Large Language Models (LLMs) is\nsignificantly driving AI development forward. However, there is still a limited\nunderstanding of their trustworthiness. Deploying these models at scale without\nsufficient trustworthiness can pose significant risks, highlighting the need to\nuncover these issues promptly. In this work, we conduct an assessment of\nopen-source LLMs on trustworthiness, scrutinizing them across eight different\naspects including toxicity, stereotypes, ethics, hallucination, fairness,\nsycophancy, privacy, and robustness against adversarial demonstrations. We\npropose an enhanced Chain of Utterances-based (CoU) prompting strategy by\nincorporating meticulously crafted malicious demonstrations for trustworthiness\nattack. Our extensive experiments encompass recent and representative series of\nopen-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The\nempirical outcomes underscore the efficacy of our attack strategy across\ndiverse aspects. More interestingly, our result analysis reveals that models\nwith superior performance in general NLP tasks do not always have greater\ntrustworthiness; in fact, larger models can be more vulnerable to attacks.\nAdditionally, models that have undergone instruction tuning, focusing on\ninstruction following, tend to be more susceptible, although fine-tuning LLMs\nfor safety alignment proves effective in mitigating adversarial trustworthiness\nattacks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Speed Odyssey for Deployable Quantization of LLMs\u2b1b  The large language model era urges faster and less costly inference. Prior\nmodel compression works on LLMs tend to undertake a software-centric approach\nprimarily focused on the simulated quantization performance. By neglecting the\nfeasibility of deployment, these approaches are typically disabled in real\npractice. They used to drastically push down the quantization bit range for a\nreduced computation which might not be supported by the mainstream hardware, or\ninvolve sophisticated algorithms that introduce extra computation or memory\naccess overhead. We argue that pursuing a hardware-centric approach in the\nconstruction of quantization algorithms is crucial. In this regard, we are\ndriven to build our compression method on top of hardware awareness,\neliminating impractical algorithm choices while maximizing the benefit of\nhardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel\nimplementation called FastGEMM and a combined recipe of quantization\nstrategies. Extensive experiments manifest the superiority of our W4A8 method\nwhich brings the actual speed boosting up to \\textbf{4$\\times$} compared to\nHugging Face FP16 inference and \\textbf{2.23$\\times$} vs. the state-of-the-art\ninference engine TensorRT-LLM in FP16, and \\textbf{1.45$\\times$} vs.\nTensorRT-LLM in INT8, yet without substantially harming the performance.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Enchancing Semi-Supervised Learning for Extractive Summarization with an\n  LLM-based pseudolabeler\u2b1b  This work tackles the task of extractive text summarization in a limited\nlabeled data scenario using a semi-supervised approach. Specifically, we\npropose a prompt-based pseudolabel selection strategy using GPT-4. We evaluate\nour method on three text summarization datasets: TweetSumm, WikiHow, and\nArXiv/PubMed. Our experiments show that by using an LLM to evaluate and\ngenerate pseudolabels, we can improve the ROUGE-1 by 10-20\\% on the different\ndatasets, which is akin to enhancing pretrained models. We also show that such\na method needs a smaller pool of unlabeled examples to perform better.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Digital Socrates: Evaluating LLMs through explanation critiques\u2b1b  While LLMs can provide reasoned explanations along with their answers, the\nnature and quality of those explanations are still poorly understood. In\nresponse, our goal is to define a detailed way of characterizing the\nexplanation capabilities of modern models and to create a nuanced,\ninterpretable explanation evaluation tool that can generate such\ncharacterizations automatically, without relying on expensive API calls or\nhuman annotations. Our approach is to (a) define the new task of explanation\ncritiquing - identifying and categorizing any main flaw in an explanation and\nproviding suggestions to address the flaw, (b) create a sizeable,\nhuman-verified dataset for this task, and (c) train an open-source, automatic\ncritiquing model (called Digital Socrates) using this data. Through\nquantitative and qualitative analysis, we demonstrate how Digital Socrates is\nuseful for revealing insights about student models by examining their reasoning\nchains, and how it can provide high-quality, nuanced, automatic evaluation of\nthose model explanations for the first time. Digital Socrates thus fills an\nimportant gap in evaluation tools for understanding and improving the\nexplanation behavior of models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Simulating Opinion Dynamics with Networks of LLM-based Agents\u2b1b  Accurately simulating human opinion dynamics is crucial for understanding a\nvariety of societal phenomena, including polarization and the spread of\nmisinformation. However, the agent-based models (ABMs) commonly used for such\nsimulations lack fidelity to human behavior. We propose a new approach to\nsimulating opinion dynamics based on populations of Large Language Models\n(LLMs). Our findings reveal a strong inherent bias in LLM agents towards\naccurate information, leading to consensus in line with scientific reality.\nHowever, this bias limits the simulation of individuals with resistant views on\nissues like climate change. After inducing confirmation bias through prompt\nengineering, we observed opinion fragmentation in line with existing\nagent-based research. These insights highlight the promise and limitations of\nLLM agents in this domain and suggest a path forward: refining LLMs with\nreal-world discourse to better simulate the evolution of human beliefs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case\n  Study on Wisdom of Partisan Crowds\u2b1b  This study investigates the potential of Large Language Models (LLMs) to\nsimulate human group dynamics, particularly within politically charged\ncontexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to\nrole-play as Democrat and Republican personas, engaging in a structured\ninteraction akin to human group study. Our approach evaluates how agents'\nresponses evolve through social influence. Our key findings indicate that LLM\nagents role-playing detailed personas and without Chain-of-Thought (CoT)\nreasoning closely align with human behaviors, while having CoT reasoning hurts\nthe alignment. However, incorporating explicit biases into agent prompts does\nnot necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning\nLLMs with human data shows promise in achieving human-like behavior but poses a\nrisk of overfitting certain behaviors. These findings show the potential and\nlimitations of using LLM agents in modeling human group phenomena.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "On Evaluating the Integration of Reasoning and Action in LLM Agents with\n  Database Question Answering\u2b1b  This study introduces a new long-form database question answering dataset\ndesigned to evaluate how Large Language Models (LLMs) interact with a SQL\ninterpreter. The task necessitates LLMs to strategically generate multiple SQL\nqueries to retrieve sufficient data from a database, to reason with the\nacquired context, and to synthesize them into a comprehensive analytical\nnarrative. Our findings highlight that this task poses great challenges even\nfor the state-of-the-art GPT-4 model. We propose and evaluate two interaction\nstrategies, and provide a fine-grained analysis of the individual stages within\nthe interaction. A key discovery is the identification of two primary\nbottlenecks hindering effective interaction: the capacity for planning and the\nability to generate multiple SQL queries. To address the challenge of\naccurately assessing answer quality, we introduce a multi-agent evaluation\nframework that simulates the academic peer-review process, enhancing the\nprecision and reliability of our evaluations. This framework allows for a more\nnuanced understanding of the strengths and limitations of current LLMs in\ncomplex retrieval and reasoning tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "OrchestraLLM: Efficient Orchestration of Language Models for Dialogue\n  State Tracking\u2b1b  Large language models (LLMs) have revolutionized the landscape of Natural\nLanguage Processing systems, but are computationally expensive. To reduce the\ncost without sacrificing performance, previous studies have explored various\napproaches to harness the potential of Small Language Models (SLMs) as\ncost-effective alternatives to their larger counterparts. Driven by findings\nthat SLMs and LLMs exhibit complementary strengths in a structured knowledge\nextraction task, this work presents a novel SLM/LLM routing framework designed\nto improve computational efficiency and enhance task performance. First,\nexemplar pools are created to represent the types of contexts where each LM\nprovides a more reliable answer, leveraging a sentence embedding fine-tuned so\nthat context similarity is close to dialogue state similarity. Then, during\ninference, the k-nearest exemplars to the testing instance are retrieved, and\nthe instance is routed according to majority vote. In dialogue state tracking\ntasks, the proposed routing framework enhances performance substantially\ncompared to relying solely on LLMs, while reducing the computational costs by\nover 50%.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores\u2b1b  Automatic evaluation of generated textual content presents an ongoing\nchallenge within the field of NLP. Given the impressive capabilities of modern\nlanguage models (LMs) across diverse NLP tasks, there is a growing trend to\nemploy these models in creating innovative evaluation metrics for automated\nassessment of generation tasks. This paper investigates a pivotal question: Do\nlanguage model-driven evaluation metrics inherently exhibit bias favoring texts\ngenerated by the same underlying language model? Specifically, we assess\nwhether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and\nGPTScore--demonstrate a favorable bias toward their respective underlying LMs\nin the context of summarization tasks. Our findings unveil a latent bias,\nparticularly pronounced when such evaluation metrics are used in an\nreference-free manner without leveraging gold summaries. These results\nunderscore that assessments provided by generative evaluation models can be\ninfluenced by factors beyond the inherent text quality, highlighting the\nnecessity of developing more dependable evaluation protocols in the future.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs\u2b1b  Adapting a language model into a specific domain, a.k.a `domain adaption', is\na common practice when specialized knowledge, e.g. medicine, is not\nencapsulated in a general language model like Llama2. The challenge lies in the\nheterogeneity of data across the two training stages, as it varies in\nlanguages, genres, or formats. To tackle this and simplify the learning\nprotocol, we propose to transform heterogeneous data, from the both\npre-training and supervised stages, into a unified, simple input-output pair\nformat. We validate the new protocol in the domains where proprietary LLMs like\nChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The\ndeveloped model, HuatuoGPT-II, has shown state-of-the-art performance in\nChinese medicine domain on a number of benchmarks, e.g. medical licensing\nexams. It even outperforms proprietary models like ChatGPT and GPT-4 in some\naspects, especially in Traditional Chinese Medicine. Expert manual evaluations\nfurther validate HuatuoGPT-II's advantages over existing LLMs. Notably,\nHuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing\nExamination where it achieved the best performance, showcasing not only its\neffectiveness but also its generalization capabilities.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "More Samples or More Prompt Inputs? Exploring Effective In-Context\n  Sampling for LLM Few-Shot Prompt Engineering\u2b1b  While most existing works on LLM prompt-engineering focus only on how to\nselect a better set of data samples inside one single prompt input (In-Context\nLearning or ICL), why can't we design and leverage multiple prompt inputs\ntogether to further improve the LLM performance? In this work, we propose\nIn-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to\nproduce the most confident prediction results by optimizing the construction of\nmultiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL\nand Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate\nthat ICS can consistently enhance LLM's prediction performance and confidence.\nAn ablation study suggests that a diversity-based ICS strategy may further\nimprove LLM's performance, which sheds light on a new yet promising future\nresearch direction.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in\n  Understanding Long Documents with Tabular Data\u2b1b  Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning and problem-solving capabilities of LLMs in the context of\nunderstanding and analyzing financial documents containing both text and\ntables. We evaluate a wide spectrum of 19 LLMs, including those specialized in\ncoding and finance. We also incorporate different prompting strategies (i.e.,\nChain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the\ncapabilities and limitations of existing LLMs in DocMath-Eval. We found that,\nalthough the current best-performing system (i.e., GPT-4), can perform well on\nsimple problems such as calculating the rate of increase in a financial metric\nwithin a short document context, it significantly lags behind human experts in\nmore complex problems grounded in longer contexts. We believe DocMath-Eval can\nbe used as a valuable benchmark to evaluate LLMs' capabilities to solve\nchallenging numerical reasoning problems in expert domains. We will release the\nbenchmark and code at https://github.com/yale-nlp/DocMath-Eval.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Human Still Wins over LLM: An Empirical Study of Active Learning on\n  Domain-Specific Annotation Tasks\u2b1b  Large Language Models (LLMs) have demonstrated considerable advances, and\nseveral claims have been made about their exceeding human performance. However,\nin real-world tasks, domain knowledge is often required. Low-resource learning\nmethods like Active Learning (AL) have been proposed to tackle the cost of\ndomain expert annotation, raising this question: Can LLMs surpass compact\nmodels trained with expert annotations in domain-specific tasks? In this work,\nwe conduct an empirical experiment on four datasets from three different\ndomains comparing SOTA LLMs with small models trained on expert annotations\nwith AL. We found that small models can outperform GPT-3.5 with a few hundreds\nof labeled data, and they achieve higher or similar performance with GPT-4\ndespite that they are hundreds time smaller. Based on these findings, we posit\nthat LLM predictions can be used as a warmup method in real-world applications\nand human experts remain indispensable in tasks involving data annotation\ndriven by domain-specific knowledge.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AutoPlanBench: : Automatically generating benchmarks for LLM planners\n  from PDDL\u2b1b  LLMs are being increasingly used for planning-style tasks, but their\ncapabilities for planning and reasoning are poorly understood. We present a\nnovel method for automatically converting planning benchmarks written in PDDL\ninto textual descriptions and offer a benchmark dataset created with our\nmethod. We show that while the best LLM planners do well on many planning\ntasks, others remain out of reach of current methods.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Leveraging LLMs in Scholarly Knowledge Graph Question Answering\u2b1b  This paper presents a scholarly Knowledge Graph Question Answering (KGQA)\nthat answers bibliographic natural language questions by leveraging a large\nlanguage model (LLM) in a few-shot manner. The model initially identifies the\ntop-n similar training questions related to a given test question via a\nBERT-based sentence encoder and retrieves their corresponding SPARQL. Using the\ntop-n similar question-SPARQL pairs as an example and the test question creates\na prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs\nthe SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and\nreturns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of\nthe Scholarly-QALD-23 challenge benchmarks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Predictive Minds: LLMs As Atypical Active Inference Agents\u2b1b  Large language models (LLMs) like GPT are often conceptualized as passive\npredictors, simulators, or even stochastic parrots. We instead conceptualize\nLLMs by drawing on the theory of active inference originating in cognitive\nscience and neuroscience. We examine similarities and differences between\ntraditional active inference systems and LLMs, leading to the conclusion that,\ncurrently, LLMs lack a tight feedback loop between acting in the world and\nperceiving the impacts of their actions, but otherwise fit in the active\ninference paradigm. We list reasons why this loop may soon be closed, and\npossible consequences of this including enhanced model self-awareness and the\ndrive to minimize prediction error by changing the world.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in\n  LLMs through Translation-Assisted Chain-of-Thought Processes\u2b1b  LLMs such as ChatGPT and PaLM can be utilized to train on a new language and\nrevitalize low-resource languages. However, it is evidently very costly to\npretrain pr fine-tune LLMs to adopt new languages. Another challenge is the\nlimitation of benchmark datasets and the metrics used to measure the\nperformance of models in multilingual settings. This paper proposes\ncost-effective solutions to both of the aforementioned challenges. We introduce\nthe Multilingual Instruction-Tuning Dataset (MITS), which is comprised of the\ntranslation of Alpaca-52K, Dolly-15K, and Vicuna Benchmark in 132 languages.\nAlso, we propose a new method called \\emph{TaCo: Translation-Assisted\nCross-Linguality}, which make uses of translation in a chain-of-thought process\nto instruction-tune LLMs on a new languages through a curriculum learning\nprocess. As a proof of concept, we experimented with the instruction-tuned\nGuanaco-33B model and performed further instruction tuning using the TaCo\nmethod in three low-resource languages and one high-resource language. Our\nresults show that the TaCo method impresses the GPT-4 with 82% for a\nlow-resource language in the Vicuna Benchmark dataset, and boosts performance\nby double in contrast to the performance of instruction tuning only. Our\nresults show that TaCo is a promising method for creating multilingual LLMs,\neven for low-resource languages. We have released our datasets and the model\nadapters, and encourage the research community to make use of these resources\ntowards advancing work on multilingual LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for\n  Evolving LLM APIs\u2b1b  Large Language Models (LLMs) are increasingly integrated into software\napplications. Downstream application developers often access LLMs through APIs\nprovided as a service. However, LLM APIs are often updated silently and\nscheduled to be deprecated, forcing users to continuously adapt to evolving\nmodels. This can cause performance regression and affect prompt design choices,\nas evidenced by our case study on toxicity detection. Based on our case study,\nwe emphasize the need for and re-examine the concept of regression testing for\nevolving LLM APIs. We argue that regression testing LLMs requires fundamental\nchanges to traditional testing approaches, due to different correctness\nnotions, prompting brittleness, and non-determinism in LLM APIs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM aided semi-supervision for Extractive Dialog Summarization\u2b1b  Generating high-quality summaries for chat dialogs often requires large\nlabeled datasets. We propose a method to efficiently use unlabeled data for\nextractive summarization of customer-agent dialogs. In our method, we frame\nsummarization as a question-answering problem and use state-of-the-art large\nlanguage models (LLMs) to generate pseudo-labels for a dialog. We then use\nthese pseudo-labels to fine-tune a chat summarization model, effectively\ntransferring knowledge from the large LLM into a smaller specialized model. We\ndemonstrate our method on the \\tweetsumm dataset, and show that using 10% of\nthe original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L,\nwhereas the current state-of-the-art trained on the entire training data set\nobtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case\n(i.e., ROUGE-L) we still effectively retain 94.7% of the performance while\nusing only 10% of the data.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Evil Geniuses: Delving into the Safety of LLM-based Agents\u2b1b  The rapid advancements in large language models (LLMs) have led to a\nresurgence in LLM-based agents, which demonstrate impressive human-like\nbehaviors and cooperative capabilities in various interactions and strategy\nformulations. However, evaluating the safety of LLM-based agents remains a\ncomplex challenge. This paper elaborately conducts a series of manual jailbreak\nprompts along with a virtual chat-powered evil plan development team, dubbed\nEvil Geniuses, to thoroughly probe the safety aspects of these agents. Our\ninvestigation reveals three notable phenomena: 1) LLM-based agents exhibit\nreduced robustness against malicious attacks. 2) the attacked agents could\nprovide more nuanced responses. 3) the detection of the produced improper\nresponses is more challenging. These insights prompt us to question the\neffectiveness of LLM-based attacks on agents, highlighting vulnerabilities at\nvarious levels and within different role specializations within the\nsystem/agent of LLM-based agents. Extensive evaluation and discussion reveal\nthat LLM-based agents face significant challenges in safety and yield insights\nfor future research. Our code is available at\nhttps://github.com/T1aNS1R/Evil-Geniuses.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs as Visual Explainers: Advancing Image Classification with Evolving\n  Visual Descriptions\u2b1b  Vision-language models (VLMs) offer a promising paradigm for image\nclassification by comparing the similarity between images and class embeddings.\nA critical challenge lies in crafting precise textual representations for class\nnames. While previous studies have leveraged recent advancements in large\nlanguage models (LLMs) to enhance these descriptors, their outputs often suffer\nfrom ambiguity and inaccuracy. We identify two primary causes: 1) The prevalent\nreliance on textual interactions with LLMs, leading to a mismatch between the\ngenerated text and the visual content in VLMs' latent space - a phenomenon we\nterm the \"explain without seeing\" dilemma. 2) The oversight of the inter-class\nrelationships, resulting in descriptors that fail to differentiate similar\nclasses effectively. To address these issues, we propose a novel image\nclassification framework combining VLMs with LLMs, named Iterative Optimization\nwith Visual Feedback. In particular, our method develops an LLM-based agent,\nemploying an evolutionary optimization strategy to refine class descriptors.\nCrucially, we incorporate visual feedback from VLM classification metrics,\nthereby guiding the optimization process with concrete visual data. Our method\nleads to improving accuracy on a wide range of image classification benchmarks,\nwith 3.47\\% average gains over state-of-the-art methods. We also highlight the\nresulting descriptions serve as explainable and robust features that can\nconsistently improve the performance across various backbone models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Perceptual Structure in the Absence of Grounding for LLMs: The Impact of\n  Abstractedness and Subjectivity in Color Language\u2b1b  The need for grounding in language understanding is an active research topic.\nPrevious work has suggested that color perception and color language appear as\na suitable test bed to empirically study the problem, given its cognitive\nsignificance and showing that there is considerable alignment between a defined\ncolor space and the feature space defined by a language model. To further study\nthis issue, we collect a large scale source of colors and their descriptions,\ncontaining almost a 1 million examples , and perform an empirical analysis to\ncompare two kinds of alignments: (i) inter-space, by learning a mapping between\nembedding space and color space, and (ii) intra-space, by means of prompting\ncomparatives between color descriptions. Our results show that while color\nspace alignment holds for monolexemic, highly pragmatic color descriptions,\nthis alignment drops considerably in the presence of examples that exhibit\nelements of real linguistic usage such as subjectivity and abstractedness,\nsuggesting that grounding may be required in such cases.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "AS-LLM: When Algorithm Selection Meets Large Language Model\u2b1b  Algorithm selection aims to identify the most suitable algorithm for solving\na specific problem before execution, which has become a critical process of the\nAutoML. Current mainstream algorithm selection techniques rely heavily on\nfeature representations of various problems and employ the performance of each\nalgorithm as supervised information. However, there is a significant research\ngap concerning the consideration of algorithm features. This gap is primarily\nattributed to the inherent complexity of algorithms, making it particularly\nchallenging to find a universally effective feature extraction method that is\napplicable across a diverse range of algorithms. Unfortunately, neglecting this\naspect undoubtedly impacts the accuracy of algorithm selection and indirectly\nnecessitates an increased volume of problem data for training purposes. This\npaper takes a significant stride towards addressing this gap by proposing an\napproach that integrates algorithm representation into the algorithm selection\nprocess. Specifically, our proposed model employs distinct modules to extract\nrepresentations of both problems and algorithms, where the algorithm\nrepresentation leverages the capabilities of pre-trained LLMs in the realm of\ncode comprehension. Following the extraction of embedding vectors for both\nalgorithms and problems, the most suitable algorithm is determined through\ncalculations of matching degrees. Our experiments not only validate the\neffectiveness of the proposed model but also showcase the performance of\ndifferent embedded pre-trained LLMs, which suggests that the proposed algorithm\nselection framework holds the potential to serve as a baseline task for\nevaluating the code representation capabilities of LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Automatic Instruction Optimization for Open-source LLM Instruction\n  Tuning\u2b1b  Instruction tuning is crucial for enabling Language Learning Models (LLMs) in\nresponding to human instructions. The quality of instruction pairs used for\ntuning greatly affects the performance of LLMs. However, the manual creation of\nhigh-quality instruction datasets is costly, leading to the adoption of\nautomatic generation of instruction pairs by LLMs as a popular alternative in\nthe training of open-source LLMs. To ensure the high quality of LLM-generated\ninstruction datasets, several approaches have been proposed. Nevertheless,\nexisting methods either compromise dataset integrity by filtering a large\nproportion of samples, or are unsuitable for industrial applications. In this\npaper, instead of discarding low-quality samples, we propose CoachLM, a novel\napproach to enhance the quality of instruction datasets through automatic\nrevisions on samples in the dataset. CoachLM is trained from the samples\nrevised by human experts and significantly increases the proportion of\nhigh-quality samples in the dataset from 17.7% to 78.9%. The effectiveness of\nCoachLM is further assessed on various real-world instruction test sets. The\nresults show that CoachLM improves the instruction-following capabilities of\nthe instruction-tuned LLM by an average of 29.9%, which even surpasses larger\nLLMs with nearly twice the number of parameters. Furthermore, CoachLM is\nsuccessfully deployed in a data management system for LLMs at Huawei, resulting\nin an efficiency improvement of up to 20% in the cleaning of 40k real-world\ninstruction pairs. We release the training data and code of CoachLM\n(https://github.com/lunyiliu/CoachLM).\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Drilling Down into the Discourse Structure with LLMs for Long Document\n  Question Answering\u2b1b  We address the task of evidence retrieval for long document question\nanswering, which involves locating relevant paragraphs within a document to\nanswer a question. We aim to assess the applicability of large language models\n(LLMs) in the task of zero-shot long document evidence retrieval, owing to\ntheir unprecedented performance across various NLP tasks. However, currently\nthe LLMs can consume limited context lengths as input, thus providing document\nchunks as inputs might overlook the global context while missing out on\ncapturing the inter-segment dependencies. Moreover, directly feeding the large\ninput sets can incur significant computational costs, particularly when\nprocessing the entire document (and potentially incurring monetary expenses\nwith enterprise APIs like OpenAI's GPT variants). To address these challenges,\nwe propose a suite of techniques that exploit the discourse structure commonly\nfound in documents. By utilizing this structure, we create a condensed\nrepresentation of the document, enabling a more comprehensive understanding and\nanalysis of relationships between different parts. We retain $99.6\\%$ of the\nbest zero-shot approach's performance, while processing only $26\\%$ of the\ntotal tokens used by the best approach in the information seeking evidence\nretrieval setup. We also show how our approach can be combined with\n\\textit{self-ask} reasoning agent to achieve best zero-shot performance in\ncomplex multi-hop question answering, just $\\approx 4\\%$ short of zero-shot\nperformance using gold evidence.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for\n  Korean NLP\u2b1b  This paper presents the DaG LLM (David and Goliath Large Language Model), a\nlanguage model specialized for Korean and fine-tuned through Instruction Tuning\nacross 41 tasks within 13 distinct categories.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V\u2b1b  In the pursuit of Artificial General Intelligence (AGI), the integration of\nvision in language models has marked a significant milestone. The advent of\nvision-language models (MLLMs) like GPT-4V have expanded AI applications,\naligning with the multi-modal capabilities of the human brain. However,\nevaluating the efficacy of MLLMs poses a substantial challenge due to the\nsubjective nature of tasks that lack definitive answers. Existing automatic\nevaluation methodologies on multi-modal large language models rely on objective\nqueries that have standard answers, inadequately addressing the nuances of\ncreative and associative multi-modal tasks. To address this, we introduce\nMLLM-Bench, an innovative benchmark inspired by Vicuna, spanning a diverse\narray of scenarios, including Perception, Understanding, Applying, Analyzing,\nEvaluating, and Creation along with the ethical consideration. MLLM-Bench is\ndesigned to reflect user experience more accurately and provide a more holistic\nassessment of model performance. Comparative evaluations indicate a significant\nperformance gap between existing open-source models and GPT-4V. We posit that\nMLLM-Bench will catalyze progress in the open-source community towards\ndeveloping user-centric vision-language models that meet a broad spectrum of\nreal-world applications. See online leaderboard in\n\\url{https://mllm-bench.llmzoo.com}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Auditing and Mitigating Cultural Bias in LLMs\u2b1b  Culture fundamentally shapes people's reasoning, behavior, and communication.\nGenerative artificial intelligence (AI) technologies may cause a shift towards\na dominant culture. As people increasingly use AI to expedite and even automate\nvarious professional and personal tasks, cultural values embedded in AI models\nmay bias authentic expression. We audit large language models for cultural\nbias, comparing their responses to nationally representative survey data, and\nevaluate country-specific prompting as a mitigation strategy. We find that\nGPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and\nProtestant European countries. Our mitigation strategy reduces cultural bias in\nrecent models but not for all countries/territories. To avoid cultural bias in\ngenerative AI, especially in high-stakes contexts, we suggest using culture\nmatching and ongoing cultural audits.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Cerbero-7B: A Leap Forward in Language-Specific LLMs Through Enhanced\n  Chat Corpus Generation and Evaluation\u2b1b  This study introduces a novel approach for generating high-quality,\nlanguage-specific chat corpora using a self-chat mechanism. We combine a\ngenerator LLM for creating new samples and an embedder LLM to ensure diversity.\nA new Masked Language Modelling (MLM) model-based quality assessment metric is\nproposed for evaluating and filtering the corpora. Utilizing the llama2-70b as\nthe generator and a multilingual sentence transformer as embedder, we generate\nan Italian chat corpus and refine the Fauno corpus, which is based on\ntranslated English ChatGPT self-chat data. The refinement uses structural\nassertions and Natural Language Processing techniques. Both corpora undergo a\ncomprehensive quality evaluation using the proposed MLM model-based quality\nmetric. The Italian LLM fine-tuned with these corpora demonstrates\nsignificantly enhanced language comprehension and question-answering skills.\nThe resultant model, cerbero-7b, establishes a new state-of-the-art for Italian\nLLMs. This approach marks a substantial advancement in the development of\nlanguage-specific LLMs, with a special emphasis on augmenting corpora for\nunderrepresented languages like Italian.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage\n  and Sharing in LLMs\u2b1b  Recent advancements in multimodal large language models (MLLMs) have achieved\nsignificant multimodal generation capabilities, akin to GPT-4. These models\npredominantly map visual information into language representation space,\nleveraging the vast knowledge and powerful text generation abilities of LLMs to\nproduce multimodal instruction-following responses. We could term this method\nas LLMs for Vision because of its employing LLMs for visual-language\nunderstanding, yet observe that these MLLMs neglect the potential of harnessing\nvisual knowledge to enhance overall capabilities of LLMs, which could be\nregraded as Vision Enhancing LLMs. In this paper, we propose an approach called\nMKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage\nand Sharing in LLMs. Specifically, we introduce the Modular Visual Memory, a\ncomponent integrated into the internal blocks of LLMs, designed to store\nopen-world visual information efficiently. Additionally, we present a soft\nMixtures-of-Multimodal Experts architecture in LLMs to invoke multimodal\nknowledge collaboration during generation. Our comprehensive experiments\ndemonstrate that MKS2 substantially augments the reasoning capabilities of LLMs\nin contexts necessitating physical or commonsense knowledge. It also delivers\ncompetitive results on multimodal benchmarks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges\u2b1b  In recent years, large language models (LLMs) have spurred a new research\nparadigm in natural language processing. Despite their excellent capability in\nknowledge-based question answering and reasoning, their potential to retain\nfaulty or even harmful knowledge poses risks of malicious application. The\nchallenge of mitigating this issue and transforming these models into purer\nassistants is crucial for their widespread applicability. Unfortunately,\nRetraining LLMs repeatedly to eliminate undesirable knowledge is impractical\ndue to their immense parameters. Knowledge unlearning, derived from analogous\nstudies on machine unlearning, presents a promising avenue to address this\nconcern and is notably advantageous in the context of LLMs. It allows for the\nremoval of harmful knowledge in an efficient manner, without affecting\nunrelated knowledge in the model. To this end, we provide a survey of knowledge\nunlearning in the era of LLMs. Firstly, we formally define the knowledge\nunlearning problem and distinguish it from related works. Subsequently, we\ncategorize existing knowledge unlearning methods into three classes: those\nbased on parameter optimization, parameter merging, and in-context learning,\nand introduce details of these unlearning methods. We further present\nevaluation datasets used in existing methods, and finally conclude this survey\nby presenting the ongoing challenges and future directions.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical\n  Knowledge Graph Insights\u2b1b  In this study, we investigate the potential of Large Language Models to\ncomplement biomedical knowledge graphs in the training of semantic models for\nthe biomedical and clinical domains. Drawing on the wealth of the UMLS\nknowledge graph and harnessing cutting-edge Large Language Models, we propose a\nnew state-of-the-art approach for obtaining high-fidelity representations of\nbiomedical concepts and sentences, consisting of three steps: an improved\ncontrastive learning phase, a novel self-distillation phase, and a weight\naveraging phase. Through rigorous evaluations via the extensive BioLORD testing\nsuite and diverse downstream tasks, we demonstrate consistent and substantial\nperformance improvements over the previous state of the art (e.g. +2pts on\nMedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new\nstate-of-the-art biomedical model for English, we also distill and release a\nmultilingual model compatible with 50+ languages and finetuned on 7 European\nlanguages. Many clinical pipelines can benefit from our latest models. Our new\nmultilingual model enables a range of languages to benefit from our\nadvancements in biomedical semantic representation learning, opening a new\navenue for bioinformatics researchers around the world. As a result, we hope to\nsee BioLORD-2023 becoming a precious tool for future biomedical applications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for\n  Vision LLMs\u2b1b  This work focuses on the potential of Vision LLMs (VLLMs) in visual\nreasoning. Different from prior studies, we shift our focus from evaluating\nstandard performance to introducing a comprehensive safety evaluation suite,\ncovering both out-of-distribution (OOD) generalization and adversarial\nrobustness. For the OOD evaluation, we present two novel VQA datasets, each\nwith one variant, designed to test model performance under challenging\nconditions. In exploring adversarial robustness, we propose a straightforward\nattack strategy for misleading VLLMs to produce visual-unrelated responses.\nMoreover, we assess the efficacy of two jailbreaking strategies, targeting\neither the vision or language component of VLLMs. Our evaluation of 21 diverse\nmodels, ranging from open-source VLLMs to GPT-4V, yields interesting\nobservations: 1) Current VLLMs struggle with OOD texts but not images, unless\nthe visual information is limited; and 2) These VLLMs can be easily misled by\ndeceiving vision encoders only, and their vision-language training often\ncompromise safety protocols. We release this safety evaluation suite at\nhttps://github.com/UCSC-VLAA/vllm-safety-benchmark.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of\n  LLMs through a Global Scale Prompt Hacking Competition\u2b1b  Large Language Models (LLMs) are deployed in interactive contexts with direct\nuser engagement, such as chatbots and writing assistants. These deployments are\nvulnerable to prompt injection and jailbreaking (collectively, prompt hacking),\nin which models are manipulated to ignore their original instructions and\nfollow potentially malicious ones. Although widely acknowledged as a\nsignificant security threat, there is a dearth of large-scale resources and\nquantitative studies on prompt hacking. To address this lacuna, we launch a\nglobal prompt hacking competition, which allows for free-form human input\nattacks. We elicit 600K+ adversarial prompts against three state-of-the-art\nLLMs. We describe the dataset, which empirically verifies that current LLMs can\nindeed be manipulated via prompt hacking. We also present a comprehensive\ntaxonomical ontology of the types of adversarial prompts.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation\u2b1b  Multi-modal large language models have demonstrated impressive performances\non most vision-language tasks. However, the model generally lacks the\nunderstanding capabilities for specific domain data, particularly when it comes\nto interpreting chart figures. This is mainly due to the lack of relevant\nmulti-modal instruction tuning datasets. In this article, we create a\nhigh-quality instruction-tuning dataset leveraging GPT-4. We develop a\nmulti-step data generation process in which different steps are responsible for\ngenerating tabular data, creating chart figures, and designing instruction\ntuning data separately. Our method's flexibility enables us to generate\ndiverse, high-quality instruction-tuning data consistently and efficiently\nwhile maintaining a low resource expenditure. Additionally, it allows us to\nincorporate a wider variety of chart and task types not yet featured in\nexisting datasets. Next, we introduce ChartLlama, a multi-modal large language\nmodel that we've trained using our created dataset. ChartLlama outperforms all\nprior methods in ChartQA, Chart-to-text, and Chart-extraction evaluation\nbenchmarks. Additionally, ChartLlama significantly improves upon the baseline\nin our specially compiled chart dataset, which includes new chart and task\ntypes. The results of ChartLlama confirm the value and huge potential of our\nproposed data generation method in enhancing chart comprehension.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs for Science: Usage for Code Generation and Data Analysis\u2b1b  Large language models (LLMs) have been touted to enable increased\nproductivity in many areas of today's work life. Scientific research as an area\nof work is no exception: the potential of LLM-based tools to assist in the\ndaily work of scientists has become a highly discussed topic across\ndisciplines. However, we are only at the very onset of this subject of study.\nIt is still unclear how the potential of LLMs will materialise in research\npractice. With this study, we give first empirical evidence on the use of LLMs\nin the research process. We have investigated a set of use cases for LLM-based\ntools in scientific research, and conducted a first study to assess to which\ndegree current tools are helpful. In this paper we report specifically on use\ncases related to software engineering, such as generating application code and\ndeveloping scripts for data analytics. While we studied seemingly simple use\ncases, results across tools differ significantly. Our results highlight the\npromise of LLM-based tools in general, yet we also observe various issues,\nparticularly regarding the integrity of the output these tools provide.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate\n  Statements?\u2b1b  Evaluating the accuracy of outputs generated by Large Language Models (LLMs)\nis especially important in the climate science and policy domain. We introduce\nthe Expert Confidence in Climate Statements (ClimateX) dataset, a novel,\ncurated, expert-labeled dataset consisting of 8094 climate statements collected\nfrom the latest Intergovernmental Panel on Climate Change (IPCC) reports,\nlabeled with their associated confidence levels. Using this dataset, we show\nthat recent LLMs can classify human expert confidence in climate-related\nstatements, especially in a few-shot learning setting, but with limited (up to\n47%) accuracy. Overall, models exhibit consistent and significant\nover-confidence on low and medium confidence statements. We highlight\nimplications of our results for climate communication, LLMs evaluation\nstrategies, and the use of LLMs in information retrieval systems.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned\n  Language Model\u2b1b  In the realm of language models, the nuanced linguistic and cultural\nintricacies of Traditional Chinese, as spoken in Taiwan, have been largely\noverlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model\nthat specifically caters to the Traditional Chinese language, with a focus on\nthe variant used in Taiwan. Leveraging a comprehensive pretraining corpus and\ninstruction-finetuning datasets, we have developed a model that not only\nunderstands the complexities of Traditional Chinese but also embodies the\ncultural context of Taiwan. Taiwan LLM represents the first of its kind, a\nmodel that is not only linguistically accurate but also culturally resonant\nwith its user base. Our evaluations demonstrate that Taiwan LLM achieves\nsuperior performance in understanding and generating Traditional Chinese text,\noutperforming existing models that are predominantly trained on Simplified\nChinese or English. The open-source release of Taiwan LLM invites collaboration\nand further innovation, ensuring that the linguistic diversity of Chinese\nspeakers is embraced and well-served. The model, datasets, and further\nresources are made publicly available to foster ongoing research and\ndevelopment in this field.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings\u2b1b  Cross-lingual transfer learning is an important property of multilingual\nlarge language models (LLMs). But how do LLMs represent relationships between\nlanguages? Every language model has an input layer that maps tokens to vectors.\nThis ubiquitous layer of language models is often overlooked. We find that\nsimilarities between these input embeddings are highly interpretable and that\nthe geometry of these embeddings differs between model families. In one case\n(XLM-RoBERTa), embeddings encode language: tokens in different writing systems\ncan be linearly separated with an average of 99.2% accuracy. Another family\n(mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors\nfor any token represent an average of 7.61 writing systems, and are frequently\ntranslations. This result is surprising given that there is no explicit\nparallel cross-lingual training corpora and no explicit incentive for\ntranslations in pre-training objectives. Our research opens the door for\ninvestigations in 1) The effect of pre-training and model architectures on\nrepresentations of languages and 2) The applications of cross-lingual\nrepresentations embedded in language models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with\n  Factuality, Fairness, Toxicity\u2b1b  The widespread of generative artificial intelligence has heightened concerns\nabout the potential harms posed by AI-generated texts, primarily stemming from\nfactoid, unfair, and toxic content. Previous researchers have invested much\neffort in assessing the harmlessness of generative language models. However,\nexisting benchmarks are struggling in the era of large language models (LLMs),\ndue to the stronger language generation and instruction following capabilities,\nas well as wider applications. In this paper, we propose FFT, a new benchmark\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\nwe evaluate 9 representative LLMs covering various parameter scales, training\nstages, and creators. Experiments show that the harmlessness of LLMs is still\nunder-satisfactory, and extensive analysis derives some insightful findings\nthat could inspire future research for harmless LLM research.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable\n  Evaluation of Large Language Model Generation\u2b1b  Since the natural language processing (NLP) community started to make large\nlanguage models (LLMs), such as GPT-4, act as a critic to evaluate the quality\nof generated texts, most of them only train a critique generation model of a\nspecific scale on specific datasets. We argue that a comprehensive\ninvestigation on the key factor of LLM-based evaluation models, such as scaling\nproperties, is lacking, so that it is still inconclusive whether these models\nhave potential to replace GPT-4's evaluation in practical scenarios. In this\npaper, we propose a new critique generation model called CritiqueLLM, which\nincludes a dialogue-based prompting method for high-quality referenced /\nreference-free evaluation data. Experimental results show that our model can\nachieve comparable evaluation performance to GPT-4 especially in system-level\ncorrelations, and even outperform GPT-4 in 3 out of 8 tasks in a challenging\nreference-free setting. We conduct detailed analysis to show promising scaling\nproperties of our model in the quality of generated critiques. We also\ndemonstrate that our generated critiques can act as scalable feedback to\ndirectly improve the generation quality of LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MLLMs-Augmented Visual-Language Representation Learning\u2b1b  Visual-language pre-training (VLP) has achieved remarkable success in\nmulti-modal tasks, largely attributed to the availability of large-scale\nimage-text datasets. In this work, we demonstrate that multi-modal large\nlanguage models (MLLMs) can enhance visual-language representation learning by\nimproving data quality. Our approach is simple, utilizing MLLMs to extend\nmultiple captions for each image. To prevent the bias introduced by MLLMs'\nhallucinations and intrinsic caption styles, we propose \"text shearing\" to\nmaintain the same length for extended captions as that of the original\ncaptions. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0%\nand 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shot\nsettings, respectively. Notably, we obtain zero-shot results that are\ncomparable to fine-tuning on target datasets, which encourages more exploration\nof the versatile use of MLLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning\u2b1b  Vision-language pre-training and instruction tuning have demonstrated\ngeneral-purpose capabilities in 2D visual reasoning tasks by aligning visual\nencoders with state-of-the-art large language models (LLMs). In this paper, we\nintroduce a simple, yet effective, cross-modality framework built atop frozen\nLLMs that allows the integration of various modalities without extensive\nmodality-specific customization. To facilitate instruction-modality\nfine-tuning, we collect high-quality instruction tuning data in an automatic\nand scalable manner, composed of 24K QA samples for audio and 250K QA samples\nfor 3D. Leveraging instruction-aware representations, our model performs\ncomparably with leading-edge counterparts without the need of extensive\nmodality-specific pre-training or customization. Furthermore, our approach\ndemonstrates cross-modal reasoning abilities across two or more input\nmodalities, despite each modality projection being trained individually. To\nstudy the model's cross-modal abilities, we contribute a novel Discriminative\nCross-modal Reasoning (DisCRn) evaluation task, comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Instruction-tuning Aligns LLMs to the Human Brain\u2b1b  Instruction-tuning is a widely adopted method of finetuning that enables\nlarge language models (LLMs) to generate output that more closely resembles\nhuman responses to natural language queries, in many cases leading to\nhuman-level performance on diverse testbeds. However, it remains unclear\nwhether instruction-tuning truly makes LLMs more similar to how humans process\nlanguage. We investigate the effect of instruction-tuning on LLM-human\nsimilarity in two ways: (1) brain alignment, the similarity of LLM internal\nrepresentations to neural activity in the human language system, and (2)\nbehavioral alignment, the similarity of LLM and human behavior on a reading\ntask. We assess 25 vanilla and instruction-tuned LLMs across three datasets\ninvolving humans reading naturalistic stories and sentences. We discover that\ninstruction-tuning generally enhances brain alignment by an average of 6%, but\ndoes not have a similar effect on behavioral alignment. To identify the factors\nunderlying LLM-brain alignment, we compute correlations between the brain\nalignment of LLMs and various model properties, such as model size, various\nproblem-solving abilities, and performance on tasks requiring world knowledge\nspanning various domains. Notably, we find a strong positive correlation\nbetween brain alignment and model size (r = 0.95), as well as performance on\ntasks requiring world knowledge (r = 0.81). Our results demonstrate that\ninstruction-tuning LLMs improves both world knowledge representations and brain\nalignment, suggesting that mechanisms that encode world knowledge in LLMs also\nimprove representational alignment to the human brain.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SeaLLMs -- Large Language Models for Southeast Asia\u2b1b  Despite the remarkable achievements of large language models (LLMs) in\nvarious tasks, there remains a linguistic bias that favors high-resource\nlanguages, such as English, often at the expense of low-resource and regional\nlanguages. To address this imbalance, we introduce SeaLLMs, an innovative\nseries of language models that specifically focuses on Southeast Asian (SEA)\nlanguages. SeaLLMs are built upon the Llama-2 model and further advanced\nthrough continued pre-training with an extended vocabulary, specialized\ninstruction and alignment tuning to better capture the intricacies of regional\nlanguages. This allows them to respect and reflect local cultural norms,\ncustoms, stylistic preferences, and legal considerations. Our comprehensive\nevaluation demonstrates that SeaLLM-13b models exhibit superior performance\nacross a wide spectrum of linguistic tasks and assistant-style\ninstruction-following capabilities relative to comparable open-source models.\nMoreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai,\nKhmer, Lao, and Burmese, by large margins while remaining lightweight and\ncost-effective to operate.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized\n  Model Responses\u2b1b  Large language model (LLM) powered chatbots are primarily text-based today,\nand impose a large interactional cognitive load, especially for exploratory or\nsensemaking tasks such as planning a trip or learning about a new city. Because\nthe interaction is textual, users have little scaffolding in the way of\nstructure, informational \"scent\", or ability to specify high-level preferences\nor goals. We introduce ExploreLLM that allows users to structure thoughts, help\nexplore different options, navigate through the choices and recommendations,\nand to more easily steer models to generate more personalized responses. We\nconduct a user study and show that users find it helpful to use ExploreLLM for\nexploratory or planning tasks, because it provides a useful schema-like\nstructure to the task, and guides users in planning. The study also suggests\nthat users can more easily personalize responses with high-level preferences\nwith ExploreLLM. Together, ExploreLLM points to a future where users interact\nwith LLMs beyond the form of chatbots, and instead designed to support complex\nuser tasks with a tighter integration between natural language and graphical\nuser interfaces.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from\n  Fine-grained Correctional Human Feedback\u2b1b  Multimodal Large Language Models (MLLMs) have recently demonstrated\nimpressive capabilities in multimodal understanding, reasoning, and\ninteraction. However, existing MLLMs prevalently suffer from serious\nhallucination problems, generating text that is not factually grounded in\nassociated images. The problem makes existing MLLMs untrustworthy and thus\nimpractical in real-world (especially high-stakes) applications. To address the\nchallenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior\nalignment from fine-grained correctional human feedback. Specifically, RLHF-V\ncollects human preference in the form of segment-level corrections on\nhallucinations, and performs dense direct preference optimization over the\nhuman feedback. Comprehensive experiments on five benchmarks in both automatic\nand human evaluation show that, RLHF-V can enable substantially more\ntrustworthy MLLM behaviors with promising data and computation efficiency.\nRemarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the\nhallucination rate of the base MLLM by 34.8%, outperforming the concurrent\nLLaVA-RLHF trained on 10k annotated data. The final model achieves\nstate-of-the-art performance in trustworthiness among open-source MLLMs, and\nshows better robustness than GPT-4V in preventing hallucinations aroused from\nover-generalization. We open-source our code, model, and data at\nhttps://github.com/RLHF-V/RLHF-V.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "From Beginner to Expert: Modeling Medical Knowledge into General LLMs\u2b1b  Recently, large language model (LLM) based artificial intelligence (AI)\nsystems have demonstrated remarkable capabilities in natural language\nunderstanding and generation. However, these models face a significant\nchallenge when it comes to sensitive applications, such as reasoning over\nmedical knowledge and answering medical questions in a physician-like manner.\nPrior studies attempted to overcome this challenge by increasing the model size\n(>100B) to learn more general medical knowledge, while there is still room for\nimprovement in LLMs with smaller-scale model sizes (<100B). In this work, we\nstart from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a\nmedical beginner towards a medical expert (called AntGLM-Med-10B), which\nleverages a 3-stage optimization procedure, \\textit{i.e.}, general medical\nknowledge injection, medical domain instruction tuning, and specific medical\ntask adaptation. Our contributions are threefold: (1) We specifically\ninvestigate how to adapt a pre-trained general LLM in medical domain,\nespecially for a specific medical task. (2) We collect and construct\nlarge-scale medical datasets for each stage of the optimization process. These\ndatasets encompass various data types and tasks, such as question-answering,\nmedical reasoning, multi-choice questions, and medical conversations. (3)\nSpecifically for multi-choice questions in the medical domain, we propose a\nnovel Verification-of-Choice approach for prompting engineering, which\nsignificantly enhances the reasoning ability of LLMs. Remarkably, by combining\nthe above approaches, our AntGLM-Med-10B model can outperform the most of LLMs\non PubMedQA, including both general and medical LLMs, even when these LLMs have\nlarger model size.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Towards leveraging LLMs for Conditional QA\u2b1b  This study delves into the capabilities and limitations of Large Language\nModels (LLMs) in the challenging domain of conditional question-answering.\nUtilizing the Conditional Question Answering (CQA) dataset and focusing on\ngenerative models like T5 and UL2, we assess the performance of LLMs across\ndiverse question types. Our findings reveal that fine-tuned LLMs can surpass\nthe state-of-the-art (SOTA) performance in some cases, even without fully\nencoding all input context, with an increase of 7-8 points in Exact Match (EM)\nand F1 scores for Yes/No questions. However, these models encounter challenges\nin extractive question answering, where they lag behind the SOTA by over 10\npoints, and in mitigating the risk of injecting false information. A study with\noracle-retrievers emphasizes the critical role of effective evidence retrieval,\nunderscoring the necessity for advanced solutions in this area. Furthermore, we\nhighlight the significant influence of evaluation metrics on performance\nassessments and advocate for a more comprehensive evaluation framework. The\ncomplexity of the task, the observed performance discrepancies, and the need\nfor effective evidence retrieval underline the ongoing challenges in this field\nand underscore the need for future work focusing on refining training tasks and\nexploring prompt-based techniques to enhance LLM performance in conditional\nquestion-answering tasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "From Voices to Validity: Leveraging Large Language Models (LLMs) for\n  Textual Analysis of Policy Stakeholder Interviews\u2b1b  Obtaining stakeholders' diverse experiences and opinions about current policy\nin a timely manner is crucial for policymakers to identify strengths and gaps\nin resource allocation, thereby supporting effective policy design and\nimplementation. However, manually coding even moderately sized interview texts\nor open-ended survey responses from stakeholders can often be labor-intensive\nand time-consuming. This study explores the integration of Large Language\nModels (LLMs)--like GPT-4--with human expertise to enhance text analysis of\nstakeholder interviews regarding K-12 education policy within one U.S. state.\nEmploying a mixed-methods approach, human experts developed a codebook and\ncoding processes as informed by domain knowledge and unsupervised topic\nmodeling results. They then designed prompts to guide GPT-4 analysis and\niteratively evaluate different prompts' performances. This combined\nhuman-computer method enabled nuanced thematic and sentiment analysis. Results\nreveal that while GPT-4 thematic coding aligned with human coding by 77.89% at\nspecific themes, expanding to broader themes increased congruence to 96.02%,\nsurpassing traditional Natural Language Processing (NLP) methods by over 25%.\nAdditionally, GPT-4 is more closely matched to expert sentiment analysis than\nlexicon-based methods. Findings from quantitative measures and qualitative\nreviews underscore the complementary roles of human domain expertise and\nautomated analysis as LLMs offer new perspectives and coding consistency. The\nhuman-computer interactive approach enhances efficiency, validity, and\ninterpretability of educational policy research.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context\n  Learning\u2b1b  The alignment tuning process of large language models (LLMs) typically\ninvolves instruction learning through supervised fine-tuning (SFT) and\npreference tuning via reinforcement learning from human feedback (RLHF). A\nrecent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for\nSFT can achieve significant alignment performance as well, suggesting that the\neffect of alignment tuning might be \"superficial.\" This raises questions about\nhow exactly the alignment tuning transforms a base LLM.\n  We analyze the effect of alignment tuning by examining the token distribution\nshift between base LLMs and their aligned counterpart. Our findings reveal that\nbase LLMs and their alignment-tuned versions perform nearly identically in\ndecoding on the majority of token positions. Most distribution shifts occur\nwith stylistic tokens. These direct evidence strongly supports the Superficial\nAlignment Hypothesis suggested by LIMA.\n  Based on these findings, we rethink the alignment of LLMs by posing the\nresearch question: how effectively can we align base LLMs without SFT or RLHF?\nTo address this, we introduce a simple, tuning-free alignment method, URIAL.\nURIAL achieves effective alignment purely through in-context learning (ICL)\nwith base LLMs, requiring as few as three constant stylistic examples and a\nsystem prompt. We conduct a fine-grained and interpretable evaluation on a\ndiverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that\nbase LLMs with URIAL can match or even surpass the performance of LLMs aligned\nwith SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based\nalignment methods can be significantly reduced through strategic prompting and\nICL. Our findings on the superficial nature of alignment tuning and results\nwith URIAL suggest that deeper analysis and theoretical understanding of\nalignment is crucial to future LLM research.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian\n  Perspective\u2b1b  This paper proposes an interpretation of RLAIF as Bayesian inference by\nintroducing distilled Self-Critique (dSC), which refines the outputs of a LLM\nthrough a Gibbs sampler that is later distilled into a fine-tuned model. Only\nrequiring synthetic data, dSC is exercised in experiments regarding safety,\nsentiment, and privacy control, showing it can be a viable and cheap\nalternative to align LLMs. Code released at\n\\url{https://github.com/vicgalle/distilled-self-critique}.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?\u2b1b  Large language models (LLMs) offer a range of new possibilities, including\nadapting the text to different audiences and their reading needs. But how well\ndo they adapt? We evaluate the readability of answers generated by four\nstate-of-the-art LLMs (commercial and open-source) to science questions when\nprompted to target different age groups and education levels. To assess the\nadaptability of LLMs to diverse audiences, we compare the readability scores of\nthe generated responses against the recommended comprehension level of each age\nand education group. We find large variations in the readability of the answers\nby different LLMs. Our results suggest LLM answers need to be better adapted to\nthe intended audience demographics to be more comprehensible. They underline\nthe importance of enhancing the adaptability of LLMs in education settings to\ncater to diverse age and education levels. Overall, current LLMs have set\nreadability ranges and do not adapt well to different audiences, even when\nprompted. That limits their potential for educational purposes.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically\u2b1b  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thoughts reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries. This\nsignificantly improves upon the previous state-of-the-art black-box method for\ngenerating jailbreaks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Competition-Level Problems are Effective LLM Evaluators\u2b1b  Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet there is ongoing debate about these abilities and the\npotential data contamination problem recently. This paper aims to evaluate the\nreasoning capacities of LLMs, specifically in solving recent competition-level\nprogramming problems in Codeforces, which are expert-crafted and unique,\nrequiring deep understanding and robust reasoning skills. We first provide a\ncomprehensive evaluation of GPT-4's peiceived zero-shot performance on this\ntask, considering various aspects such as problems' release time, difficulties,\nand types of errors encountered. Surprisingly, the peiceived performance of\nGPT-4 has experienced a cliff like decline in problems after September 2021\nconsistently across all the difficulties and types of problems, which shows the\npotential data contamination, as well as the challenges for any existing LLM to\nsolve unseen complex reasoning problems. We further explore various approaches\nsuch as fine-tuning, Chain-of-Thought prompting and problem description\nsimplification, unfortunately none of them is able to consistently mitigate the\nchallenges. Through our work, we emphasis the importance of this excellent data\nsource for assessing the genuine reasoning capabilities of LLMs, and foster the\ndevelopment of LLMs with stronger reasoning abilities and better generalization\nin the future.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs Accelerate Annotation for Medical Information Extraction\u2b1b  The unstructured nature of clinical notes within electronic health records\noften conceals vital patient-related information, making it challenging to\naccess or interpret. To uncover this hidden information, specialized Natural\nLanguage Processing (NLP) models are required. However, training these models\nnecessitates large amounts of labeled data, a process that is both\ntime-consuming and costly when relying solely on human experts for annotation.\nIn this paper, we propose an approach that combines Large Language Models\n(LLMs) with human expertise to create an efficient method for generating ground\ntruth labels for medical text annotation. By utilizing LLMs in conjunction with\nhuman annotators, we significantly reduce the human annotation burden, enabling\nthe rapid creation of labeled datasets. We rigorously evaluate our method on a\nmedical information extraction task, demonstrating that our approach not only\nsubstantially cuts down on human intervention but also maintains high accuracy.\nThe results highlight the potential of using LLMs to improve the utilization of\nunstructured clinical data, allowing for the swift deployment of tailored NLP\nsolutions in healthcare.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding\u2b1b  Recent advancements in language-model-based video understanding have been\nprogressing at a remarkable pace, spurred by the introduction of Large Language\nModels (LLMs). However, the focus of prior research has been predominantly on\ndevising a projection layer that maps video features to tokens, an approach\nthat is both rudimentary and inefficient. In our study, we introduce a\ncutting-edge framework, VaQuitA, designed to refine the synergy between video\nand textual information. At the data level, instead of sampling frames\nuniformly, we implement a sampling method guided by CLIP-score rankings, which\nenables a more aligned selection of frames with the given question. At the\nfeature level, we integrate a trainable Video Perceiver alongside a\nVisual-Query Transformer (abbreviated as VQ-Former), which bolsters the\ninterplay between the input question and the video features. We also discover\nthat incorporating a simple prompt, \"Please be critical\", into the LLM input\ncan substantially enhance its video comprehension capabilities. Our\nexperimental results indicate that VaQuitA consistently sets a new benchmark\nfor zero-shot video question-answering tasks and is adept at producing\nhigh-quality, multi-turn video dialogues with users.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "New Evaluation Metrics Capture Quality Degradation due to LLM\n  Watermarking\u2b1b  With the increasing use of large-language models (LLMs) like ChatGPT,\nwatermarking has emerged as a promising approach for tracing machine-generated\ncontent. However, research on LLM watermarking often relies on simple\nperplexity or diversity-based measures to assess the quality of watermarked\ntext, which can mask important limitations in watermarking. Here we introduce\ntwo new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1)\nevaluation by LLM-judger with specific guidelines; and 2) binary classification\non text embeddings to distinguish between watermarked and unwatermarked text.\nWe apply these methods to characterize the effectiveness of current\nwatermarking techniques. Our experiments, conducted across various datasets,\nreveal that current watermarking methods are detectable by even simple\nclassifiers, challenging the notion of watermarking subtlety. We also found,\nthrough the LLM judger, that watermarking impacts text quality, especially in\ndegrading the coherence and depth of the response. Our findings underscore the\ntrade-off between watermark robustness and text quality and highlight the\nimportance of having more informative metrics to assess watermarking quality.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "MedDM:LLM-executable clinical guidance tree for clinical decision-making\u2b1b  It is becoming increasingly emphasis on the importance of LLM participating\nin clinical diagnosis decision-making. However, the low specialization refers\nto that current medical LLMs can not provide specific medical advice, which are\nmore like a medical Q\\&A. And there is no suitable clinical guidance tree data\nset that can be used directly with LLM. To address this issue, we first propose\nLLM-executavle clinical guidance tree(CGT), which can be directly used by large\nlanguage models, and construct medical diagnostic decision-making dataset\n(MedDM), from flowcharts in clinical practice guidelines. We propose an\napproach to screen flowcharts from medical literature, followed by their\nidentification and conversion into standardized diagnostic decision trees.\nConstructed a knowledge base with 1202 decision trees, which came from 5000\nmedical literature and covered 12 hospital departments, including internal\nmedicine, surgery, psychiatry, and over 500 diseases.Moreover, we propose a\nmethod for reasoning on LLM-executable CGT and a Patient-LLM multi-turn\ndialogue framework.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Weakly Supervised Detection of Hallucinations in LLM Activations\u2b1b  We propose an auditing method to identify whether a large language model\n(LLM) encodes patterns such as hallucinations in its internal states, which may\npropagate to downstream tasks. We introduce a weakly supervised auditing\ntechnique using a subset scanning approach to detect anomalous patterns in LLM\nactivations from pre-trained models. Importantly, our method does not need\nknowledge of the type of patterns a-priori. Instead, it relies on a reference\ndataset devoid of anomalies during testing. Further, our approach enables the\nidentification of pivotal nodes responsible for encoding these patterns, which\nmay offer crucial insights for fine-tuning specific sub-networks for bias\nmitigation. We introduce two new scanning methods to handle LLM activations for\nanomalous sentences that may deviate from the expected distribution in either\ndirection. Our results confirm prior findings of BERT's limited internal\ncapacity for encoding hallucinations, while OPT appears capable of encoding\nhallucination information internally. Importantly, our scanning approach,\nwithout prior exposure to false statements, performs comparably to a fully\nsupervised out-of-distribution classifier.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Let the LLMs Talk: Simulating Human-to-Human Conversational QA via\n  Zero-Shot LLM-to-LLM Interactions\u2b1b  Conversational question-answering (CQA) systems aim to create interactive\nsearch systems that effectively retrieve information by interacting with users.\nTo replicate human-to-human conversations, existing work uses human annotators\nto play the roles of the questioner (student) and the answerer (teacher).\nDespite its effectiveness, challenges exist as human annotation is\ntime-consuming, inconsistent, and not scalable. To address this issue and\ninvestigate the applicability of large language models (LLMs) in CQA\nsimulation, we propose a simulation framework that employs zero-shot learner\nLLMs for simulating teacher-student interactions. Our framework involves two\nLLMs interacting on a specific topic, with the first LLM acting as a student,\ngenerating questions to explore a given search topic. The second LLM plays the\nrole of a teacher by answering questions and is equipped with additional\ninformation, including a text on the given topic. We implement both the student\nand teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness\nof LLMs in simulating CQA interactions and understand the disparities between\nLLM- and human-generated conversations, we evaluate the simulated data from\nvarious perspectives. We begin by evaluating the teacher's performance through\nboth automatic and human assessment. Next, we evaluate the performance of the\nstudent, analyzing and comparing the disparities between questions generated by\nthe LLM and those generated by humans. Furthermore, we conduct extensive\nanalyses to thoroughly examine the LLM performance by benchmarking\nstate-of-the-art reading comprehension models on both datasets. Our results\nreveal that the teacher LLM generates lengthier answers that tend to be more\naccurate and complete. The student LLM generates more diverse questions,\ncovering more aspects of a given topic.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like\n  Memory for Mobile Task Automation\u2b1b  The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MemoDroid, an innovative LLM-based mobile task automator\nenhanced with a unique app memory. MemoDroid emulates the cognitive process of\nhumans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular components that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMemoDroid using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on 50 unique mobile tasks across 5 widely used mobile apps. The\nresults indicate that MemoDroid can adapt learned tasks to varying contexts\nwith 100% accuracy and reduces their latency and cost by 69.22% and 77.36%\ncompared to a GPT-4 powered baseline.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Inherent limitations of LLMs regarding spatial information\u2b1b  Despite the significant advancements in natural language processing\ncapabilities demonstrated by large language models such as ChatGPT, their\nproficiency in comprehending and processing spatial information, especially\nwithin the domains of 2D and 3D route planning, remains notably underdeveloped.\nThis paper investigates the inherent limitations of ChatGPT and similar models\nin spatial reasoning and navigation-related tasks, an area critical for\napplications ranging from autonomous vehicle guidance to assistive technologies\nfor the visually impaired. In this paper, we introduce a novel evaluation\nframework complemented by a baseline dataset, meticulously crafted for this\nstudy. This dataset is structured around three key tasks: plotting spatial\npoints, planning routes in two-dimensional (2D) spaces, and devising pathways\nin three-dimensional (3D) environments. We specifically developed this dataset\nto assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals\nkey insights into the model's capabilities and limitations in spatial\nunderstanding.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs for Multi-Modal Knowledge Extraction and Analysis in\n  Intelligence/Safety-Critical Applications\u2b1b  Large Language Models have seen rapid progress in capability in recent years;\nthis progress has been accelerating and their capabilities, measured by various\nbenchmarks, are beginning to approach those of humans. There is a strong demand\nto use such models in a wide variety of applications but, due to unresolved\nvulnerabilities and limitations, great care needs to be used before applying\nthem to intelligence and safety-critical applications. This paper reviews\nrecent literature related to LLM assessment and vulnerabilities to synthesize\nthe current research landscape and to help understand what advances are most\ncritical to enable use of of these technologies in intelligence and\nsafety-critical applications. The vulnerabilities are broken down into ten\nhigh-level categories and overlaid onto a high-level life cycle of an LLM. Some\ngeneral categories of mitigations are reviewed.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Not All Large Language Models (LLMs) Succumb to the \"Reversal Curse\": A\n  Comparative Study of Deductive Logical Reasoning in BERT and GPT Models\u2b1b  The \"Reversal Curse\" refers to the scenario where auto-regressive decoder\nlarge language models (LLMs), such as ChatGPT, trained on \"A is B\" fail to\nlearn \"B is A\", demonstrating a basic failure of logical deduction. This raises\na red flag in the use of GPT models for certain general tasks such as\nconstructing knowledge graphs, considering their adherence to this symmetric\nprinciple. In our study, we examined a bidirectional LLM, BERT, and found that\nit is immune to the reversal curse. Driven by ongoing efforts to construct\nbiomedical knowledge graphs with LLMs, we also embarked on evaluating more\ncomplex but essential deductive reasoning capabilities. This process included\nfirst training encoder and decoder language models to master the intersection\n($\\cap$) and union ($\\cup$) operations on two sets and then moving on to assess\ntheir capability to infer different combinations of union ($\\cup$) and\nintersection ($\\cap$) operations on three newly created sets. The findings\nshowed that while both encoder and decoder language models, trained for tasks\ninvolving two sets (union/intersection), were proficient in such scenarios,\nthey encountered difficulties when dealing with operations that included three\nsets (various combinations of union and intersection). Our research highlights\nthe distinct characteristics of encoder and decoder models in simple and\ncomplex logical reasoning. In practice, the choice between BERT and GPT should\nbe guided by the specific requirements and nature of the task at hand,\nleveraging their respective strengths in bidirectional context comprehension\nand sequence prediction.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "OneLLM: One Framework to Align All Modalities with Language\u2b1b  Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits\u2b1b  Large language models LLMs like ChatGPT have reached the 100 Mio user barrier\nin record time and might increasingly enter all areas of our life leading to a\ndiverse set of interactions between those Artificial Intelligence models and\nhumans. While many studies have discussed governance and regulations\ndeductively from first-order principles, few studies provide an inductive,\ndata-driven lens based on observing dialogues between humans and LLMs\nespecially when it comes to non-collaborative, competitive situations that have\nthe potential to pose a serious threat to people. In this work, we conduct a\nuser study engaging over 40 individuals across all age groups in price\nnegotiations with an LLM. We explore how people interact with an LLM,\ninvestigating differences in negotiation outcomes and strategies. Furthermore,\nwe highlight shortcomings of LLMs with respect to their reasoning capabilities\nand, in turn, susceptiveness to prompt hacking, which intends to manipulate the\nLLM to make agreements that are against its instructions or beyond any\nrationality. We also show that the negotiated prices humans manage to achieve\nspan a broad range, which points to a literacy gap in effectively interacting\nwith LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "SmoothQuant+: Accurate and Efficient 4-bit Post-Training\n  WeightQuantization for LLM\u2b1b  Large language models (LLMs) have shown remarkable capabilities in various\ntasks. However their huge model size and the consequent demand for\ncomputational and memory resources also pose challenges to model deployment.\nCurrently, 4-bit post-training quantization (PTQ) has achieved some success in\nLLMs, reducing the memory footprint by approximately 75% compared to FP16\nmodels, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,\nan accurate and efficient 4-bit weight-only PTQ that requires no additional\ntraining, which enables lossless in accuracy for LLMs for the first time. Based\non the fact that the loss of weight quantization is amplified by the activation\noutliers, SmoothQuant+ smoothes the activation outliers by channel before\nquantization, while adjusting the corresponding weights for mathematical\nequivalence, and then performs group-wise 4-bit weight quantization for linear\nlayers. We have integrated SmoothQuant+ into the vLLM framework, an advanced\nhigh-throughput inference engine specially developed for LLMs, and equipped it\nwith an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support\nSmoothQuant+ 4-bit weight quantization. Our results show that, with\nSmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100\n40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0\ntimes compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the\nlatency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.\nThis is the state-of-the-art 4-bit weight quantization for LLMs as we know.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent\n  Ecosystem\u2b1b  This paper envisions a revolutionary AIOS-Agent ecosystem, where Large\nLanguage Model (LLM) serves as the (Artificial) Intelligent Operating System\n(IOS, or AIOS)--an operating system \"with soul\". Upon this foundation, a\ndiverse range of LLM-based AI Agent Applications (Agents, or AAPs) are\ndeveloped, enriching the AIOS-Agent ecosystem and signaling a paradigm shift\nfrom the traditional OS-APP ecosystem. We envision that LLM's impact will not\nbe limited to the AI application level, instead, it will in turn revolutionize\nthe design and implementation of computer system, architecture, software, and\nprogramming language, featured by several main concepts: LLM as OS\n(system-level), Agents as Applications (application-level), Natural Language as\nProgramming Interface (user-level), and Tools as Devices/Libraries\n(hardware/middleware-level). We begin by introducing the architecture of\ntraditional OS. Then we formalize a conceptual framework for AIOS through \"LLM\nas OS (LLMOS)\", drawing analogies between AIOS and traditional OS: LLM is\nlikened to OS kernel, context window to memory, external storage to file\nsystem, hardware tools to peripheral devices, software tools to programming\nlibraries, and user prompts to user commands. Subsequently, we introduce the\nnew AIOS-Agent Ecosystem, where users can easily program Agent Applications\n(AAPs) using natural language, democratizing the development of software, which\nis different from the traditional OS-APP ecosystem. Following this, we explore\nthe diverse scope of Agent Applications. We delve into both single-agent and\nmulti-agent systems, as well as human-agent interaction. Lastly, drawing on the\ninsights from traditional OS-APP ecosystem, we propose a roadmap for the\nevolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the\nfuture research and development, suggesting systematic progresses of AIOS and\nits Agent applications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Analyzing the Inherent Response Tendency of LLMs: Real-World\n  Instructions-Driven Jailbreak\u2b1b  Extensive work has been devoted to improving the safety mechanism of Large\nLanguage Models (LLMs). However, in specific scenarios, LLMs still generate\nharmful responses when faced with malicious instructions, a phenomenon referred\nto as \"Jailbreak Attack\". In our research, we introduce a novel jailbreak\nattack method (\\textbf{RADIAL}), which consists of two steps: 1) Inherent\nResponse Tendency Analysis: we analyze the inherent affirmation and rejection\ntendency of LLMs to react to real-world instructions. 2) Real-World\nInstructions-Driven Jailbreak: based on our analysis, we strategically choose\nseveral real-world instructions and embed malicious instructions into them to\namplify the LLM's potential to generate harmful responses. On three open-source\nhuman-aligned LLMs, our method achieves excellent jailbreak attack performance\nfor both Chinese and English malicious instructions. Besides, we guided\ndetailed ablation experiments and verified the effectiveness of our core idea\n\"Inherent Response Tendency Analysis\". Our exploration also exposes the\nvulnerability of LLMs to being induced into generating more detailed harmful\nresponses in subsequent rounds of dialogue.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs\u2b1b  This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\ninference: explicit controllable text generation. Multi-modal LLMs empower\nmulti-modality understanding with the capability of semantic generation yet\nbring less explainability and heavier reliance on prompt contents due to their\nautoregressive generative nature. While manipulating prompt formats could\nimprove outputs, designing specific and precise prompts per task can be\nchallenging and ineffective. To tackle this issue, we introduce a novel\ninference method, Prompt Highlighter, which enables users to highlight specific\nprompt spans to interactively control the focus during generation. Motivated by\nthe classifier-free diffusion guidance, we form regular and unconditional\ncontext pairs based on highlighted tokens, demonstrating that the\nautoregressive generation in models can be guided in a classifier-free way.\nNotably, we find that, during inference, guiding the models with highlighted\ntokens through the attention weights leads to more desired outputs. Our\napproach is compatible with current LLMs and VLMs, achieving impressive\ncustomized generation results without training. Experiments confirm its\neffectiveness in focusing on input contexts and generating reliable content.\nWithout tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and\n1552.5 in MME-perception. The code is available at:\nhttps://github.com/dvlab-research/Prompt-Highlighter/\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "An LLM Compiler for Parallel Function Calling\u2b1b  Large Language Models (LLMs) have shown remarkable results on various complex\nreasoning benchmarks. The reasoning capabilities of LLMs enable them to execute\nfunction calls, using user-provided functions to overcome their inherent\nlimitations, such as knowledge cutoffs, poor arithmetic skills, or lack of\naccess to private data. This development has expanded LLMs' scope to include\nmulti-function calling, where LLMs are equipped with a variety of functions and\nselect the proper functions based on the context. Multi-function calling\nabilities of LLMs have catalyzed LLM-based software development, allowing them\nto tackle more complex problems. However, current methods for multi-function\ncalling often require sequential reasoning and acting for each function which\ncan result in high latency, cost, and sometimes inaccurate behavior. To address\nthis, we introduce LLMCompiler, which executes functions in parallel to\nefficiently orchestrate multi-function calling. Drawing from the principles of\nclassical compilers, LLMCompiler streamlines parallel function calling with\nthree components: (i) an LLM Planner, formulating execution strategies and\ndependencies; (ii) a Task Fetching Unit, dispatching function calling tasks;\nand (iii) an Executor, executing these tasks in parallel. LLMCompiler\nautomatically computes an optimized orchestration for the function calls and\ncan be used with open-source models such as LLaMA-2. We have benchmarked\nLLMCompiler on a range of tasks including cases with non-trivial\ninter-dependency between function calls, as well as cases that require dynamic\nreplanning based on intermediate results. We observe consistent latency speedup\nof up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to\n~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x\nlatency gain over OpenAI's recent parallel function calling, while achieving\nsimilar accuracy.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Simul-LLM: A Framework for Exploring High-Quality Simultaneous\n  Translation with Large Language Models\u2b1b  Large language models (LLMs) with billions of parameters and pretrained on\nmassive amounts of data are now capable of near or better than state-of-the-art\nperformance in a variety of downstream natural language processing tasks.\nNeural machine translation (NMT) is one such task that LLMs have been applied\nto with great success. However, little research has focused on applying LLMs to\nthe more difficult subset of NMT called simultaneous translation (SimulMT),\nwhere translation begins before the entire source context is available to the\nmodel. In this paper, we address key challenges facing LLMs fine-tuned for\nSimulMT, validate classical SimulMT concepts and practices in the context of\nLLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,\nand introduce Simul-LLM, the first open-source fine-tuning and evaluation\npipeline development framework for LLMs focused on SimulMT.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs\u2b1b  Large Language Models are successfully adopted in software engineering,\nespecially in code generation. Updating these models with new knowledge is very\nexpensive, and is often required to fully realize their value. In this paper,\nwe propose a novel and effective model editing approach, \\textsc{MENT}, to\npatch LLMs in coding tasks. Based on the mechanism of generative LLMs,\n\\textsc{MENT} enables model editing in next-token predictions, and further\nsupports common coding tasks. \\textsc{MENT} is effective, efficient, and\nreliable. It can correct a neural model by patching 1 or 2 neurons. As the\npioneer work on neuron-level model editing of generative models, we formalize\nthe editing process and introduce the involved concepts. Besides, we also\nintroduce new measures to evaluate its generalization ability, and build a\nbenchmark for further study. Our approach is evaluated on three coding tasks,\nincluding API-seq recommendation, line-level code generation, and\npseudocode-to-code transaction. It outperforms the state-of-the-art by a\nsignificant margin on both effectiveness and efficiency measures. In addition,\nwe demonstrate the usages of \\textsc{MENT} for LLM reasoning in software\nengineering. By editing the LLM knowledge with \\textsc{MENT}, the directly or\nindirectly dependent behaviors in the chain-of-thought change accordingly and\nautomatically.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs\n  on the Edge\u2b1b  Large Language Models (LLMs) stand out for their impressive performance in\nintricate language modeling tasks. However, their demanding computational and\nmemory needs pose obstacles for broad use on edge devices. Quantization is then\nintroduced to boost LLMs' on-device efficiency. Recent works show that 8-bit or\nlower weight quantization is feasible with minimal impact on end-to-end task\nperformance, while the activation is still not quantized. On the other hand,\nmainstream commodity edge devices still struggle to execute these sub-8-bit\nquantized networks effectively. In this paper, we propose Agile-Quant, an\nactivation-guided quantization framework for popular Large Language Models\n(LLMs), and implement an end-to-end accelerator on multiple edge devices for\nfaster inference. Considering the hardware profiling and activation analysis,\nwe first introduce a basic activation quantization strategy to balance the\ntrade-off of task performance and real inference speed. Then we leverage the\nactivation-aware token pruning technique to reduce the outliers and the adverse\nimpact on attentivity. Ultimately, we utilize the SIMD-based 4-bit multiplier\nand our efficient TRIP matrix multiplication to implement the accelerator for\nLLMs on the edge. We apply our framework on different scales of LLMs including\nLLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the\nweight quantization. Experiments show that Agile-Quant achieves simultaneous\nquantization of model weights and activations while maintaining task\nperformance comparable to existing weight-only quantization methods. Moreover,\nin the 8- and 4-bit scenario, Agile-Quant achieves an on-device speedup of up\nto 2.55x compared to its FP16 counterparts across multiple edge devices,\nmarking a pioneering advancement in this domain.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u2b1b  Large language models (LLMs) encapsulate a vast amount of factual information\nwithin their pre-trained weights, as evidenced by their ability to answer\ndiverse questions across different domains. However, this knowledge is\ninherently limited, relying heavily on the characteristics of the training\ndata. Consequently, using external datasets to incorporate new information or\nrefine the capabilities of LLMs on previously seen information poses a\nsignificant challenge. In this study, we compare two common approaches:\nfine-tuning and retrieval-augmented generation (RAG). We evaluate both\napproaches on a variety of knowledge-intensive tasks across different topics.\nOur findings reveal that while fine-tuning offers some improvement, RAG\nconsistently outperforms it, both for existing knowledge encountered during\ntraining and entirely new knowledge. Moreover, we find that LLMs struggle to\nlearn new factual information through fine-tuning, and that exposing them to\nnumerous variations of the same fact during training could alleviate this\nproblem.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "\"What's important here?\": Opportunities and Challenges of Using LLMs in\n  Retrieving Information from Web Interfaces\u2b1b  Large language models (LLMs) that have been trained on a corpus that includes\nlarge amount of code exhibit a remarkable ability to understand HTML code. As\nweb interfaces are primarily constructed using HTML, we design an in-depth\nstudy to see how LLMs can be used to retrieve and locate important elements for\na user given query (i.e. task description) in a web interface. In contrast with\nprior works, which primarily focused on autonomous web navigation, we decompose\nthe problem as an even atomic operation - Can LLMs identify the important\ninformation in the web page for a user given query? This decomposition enables\nus to scrutinize the current capabilities of LLMs and uncover the opportunities\nand challenges they present. Our empirical experiments show that while LLMs\nexhibit a reasonable level of performance in retrieving important UI elements,\nthere is still a substantial room for improvement. We hope our investigation\nwill inspire follow-up works in overcoming the current challenges in this\ndomain.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLM360: Towards Fully Transparent Open-Source LLMs\u2b1b  The recent surge in open-source Large Language Models (LLMs), such as LLaMA,\nFalcon, and Mistral, provides diverse options for AI practitioners and\nresearchers. However, most LLMs have only released partial artifacts, such as\nthe final model weights or inference code, and technical reports increasingly\nlimit their scope to high-level design choices and surface statistics. These\nchoices hinder progress in the field by degrading transparency into the\ntraining of LLMs and forcing teams to rediscover many details in the training\nprocess. We present LLM360, an initiative to fully open-source LLMs, which\nadvocates for all training code and data, model checkpoints, and intermediate\nresults to be made available to the community. The goal of LLM360 is to support\nopen and collaborative AI research by making the end-to-end LLM training\nprocess transparent and reproducible by everyone. As a first step of LLM360, we\nrelease two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder,\nincluding their training code, data, intermediate checkpoints, and analyses (at\nhttps://www.llm360.ai). We are committed to continually pushing the boundaries\nof LLMs through this open-source effort. More large-scale and stronger models\nare underway and will be released in the future.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage\n  or Technical Possibility?\u2b1b  Large Language Models (LLMs) have demonstrated remarkable performance across\nnumerous natural language understanding use cases. However, this impressive\nperformance comes with inherent limitations, such as the tendency to perpetuate\nstereotypical biases or fabricate non-existent facts. In the context of Islam\nand its representation, accurate and factual representation of its beliefs and\nteachings rooted in the Quran and Sunnah is key. This work focuses on the\nchallenge of building domain-specific LLMs faithful to the Islamic worldview\nand proposes ways to build and evaluate such systems. Firstly, we define this\nopen-ended goal as a technical problem and propose various solutions.\nSubsequently, we critically examine known challenges inherent to each approach\nand highlight evaluation methodologies that can be used to assess such systems.\nThis work highlights the need for high-quality datasets, evaluations, and\ninterdisciplinary work blending machine learning with Islamic scholarship.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations\u2b1b  We introduce Llama Guard, an LLM-based input-output safeguard model geared\ntowards Human-AI conversation use cases. Our model incorporates a safety risk\ntaxonomy, a valuable tool for categorizing a specific set of safety risks found\nin LLM prompts (i.e., prompt classification). This taxonomy is also\ninstrumental in classifying the responses generated by LLMs to these prompts, a\nprocess we refer to as response classification. For the purpose of both prompt\nand response classification, we have meticulously gathered a dataset of high\nquality. Llama Guard, a Llama2-7b model that is instruction-tuned on our\ncollected dataset, albeit low in volume, demonstrates strong performance on\nexisting benchmarks such as the OpenAI Moderation Evaluation dataset and\nToxicChat, where its performance matches or exceeds that of currently available\ncontent moderation tools. Llama Guard functions as a language model, carrying\nout multi-class classification and generating binary decision scores.\nFurthermore, the instruction fine-tuning of Llama Guard allows for the\ncustomization of tasks and the adaptation of output formats. This feature\nenhances the model's capabilities, such as enabling the adjustment of taxonomy\ncategories to align with specific use cases, and facilitating zero-shot or\nfew-shot prompting with diverse taxonomies at the input. We are making Llama\nGuard model weights available and we encourage researchers to further develop\nand adapt them to meet the evolving needs of the community for AI safety.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Intelligent Virtual Assistants with LLM-based Process Automation\u2b1b  While intelligent virtual assistants like Siri, Alexa, and Google Assistant\nhave become ubiquitous in modern life, they still face limitations in their\nability to follow multi-step instructions and accomplish complex goals\narticulated in natural language. However, recent breakthroughs in large\nlanguage models (LLMs) show promise for overcoming existing barriers by\nenhancing natural language processing and reasoning capabilities. Though\npromising, applying LLMs to create more advanced virtual assistants still faces\nchallenges like ensuring robust performance and handling variability in\nreal-world user commands. This paper proposes a novel LLM-based virtual\nassistant that can automatically perform multi-step operations within mobile\napps based on high-level user requests. The system represents an advance in\nassistants by providing an end-to-end solution for parsing instructions,\nreasoning about goals, and executing actions. LLM-based Process Automation\n(LLMPA) has modules for decomposing instructions, generating descriptions,\ndetecting interface elements, predicting next actions, and error checking.\nExperiments demonstrate the system completing complex mobile operation tasks in\nAlipay based on natural language instructions. This showcases how large\nlanguage models can enable automated assistants to accomplish real-world tasks.\nThe main contributions are the novel LLMPA architecture optimized for app\nprocess automation, the methodology for applying LLMs to mobile apps, and\ndemonstrations of multi-step task completion in a real-world environment.\nNotably, this work represents the first real-world deployment and extensive\nevaluation of a large language model-based virtual assistant in a widely used\nmobile application with an enormous user base numbering in the hundreds of\nmillions.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Honeybee: Locality-enhanced Projector for Multimodal LLM\u2b1b  In Multimodal Large Language Models (MLLMs), a visual projector plays a\ncrucial role in bridging pre-trained vision encoders with LLMs, enabling\nprofound visual understanding while harnessing the LLMs' robust capabilities.\nDespite the importance of the visual projector, it has been relatively less\nexplored. In this study, we first identify two essential projector properties:\n(i) flexibility in managing the number of visual tokens, crucial for MLLMs'\noverall efficiency, and (ii) preservation of local context from visual\nfeatures, vital for spatial understanding. Based on these findings, we propose\na novel projector design that is both flexible and locality-enhanced,\neffectively satisfying the two desirable properties. Additionally, we present\ncomprehensive strategies to effectively utilize multiple and multifaceted\ninstruction datasets. Through extensive experiments, we examine the impact of\nindividual design choices. Finally, our proposed MLLM, Honeybee, remarkably\noutperforms previous state-of-the-art methods across various benchmarks,\nincluding MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly\nhigher efficiency. Code and models are available at\nhttps://github.com/kakaobrain/honeybee.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Extracting Self-Consistent Causal Insights from Users Feedback with LLMs\n  and In-context Learning\u2b1b  Microsoft Windows Feedback Hub is designed to receive customer feedback on a\nwide variety of subjects including critical topics such as power and battery.\nFeedback is one of the most effective ways to have a grasp of users' experience\nwith Windows and its ecosystem. However, the sheer volume of feedback received\nby Feedback Hub makes it immensely challenging to diagnose the actual cause of\nreported issues. To better understand and triage issues, we leverage Double\nMachine Learning (DML) to associate users' feedback with telemetry signals. One\nof the main challenges we face in the DML pipeline is the necessity of domain\nknowledge for model design (e.g., causal graph), which sometimes is either not\navailable or hard to obtain. In this work, we take advantage of reasoning\ncapabilities in Large Language Models (LLMs) to generate a prior model that\nwhich to some extent compensates for the lack of domain knowledge and could be\nused as a heuristic for measuring feedback informativeness. Our LLM-based\napproach is able to extract previously known issues, uncover new bugs, and\nidentify sequences of events that lead to a bug, while minimizing out-of-domain\noutputs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMs Perform Poorly at Concept Extraction in Cyber-security Research\n  Literature\u2b1b  The cybersecurity landscape evolves rapidly and poses threats to\norganizations. To enhance resilience, one needs to track the latest\ndevelopments and trends in the domain. It has been demonstrated that standard\nbibliometrics approaches show their limits in such a fast-evolving domain. For\nthis purpose, we use large language models (LLMs) to extract relevant knowledge\nentities from cybersecurity-related texts. We use a subset of arXiv preprints\non cybersecurity as our data and compare different LLMs in terms of entity\nrecognition (ER) and relevance. The results suggest that LLMs do not produce\ngood knowledge entities that reflect the cybersecurity context, but our results\nshow some potential for noun extractors. For this reason, we developed a noun\nextractor boosted with some statistical analysis to extract specific and\nrelevant compound nouns from the domain. Later, we tested our model to identify\ntrends in the LLM domain. We observe some limitations, but it offers promising\nresults to monitor the evolution of emergent trends.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models\u2b1b  Recently, the evaluation of Large Language Models has emerged as a popular\narea of research. The three crucial questions for LLM evaluation are ``what,\nwhere, and how to evaluate''. However, the existing research mainly focuses on\nthe first two questions, which are basically what tasks to give the LLM during\ntesting and what kind of knowledge it should deal with. As for the third\nquestion, which is about what standards to use, the types of evaluators, how to\nscore, and how to rank, there hasn't been much discussion. In this paper, we\nanalyze evaluation methods by comparing various criteria with both manual and\nautomatic evaluation, utilizing onsite, crowd-sourcing, public annotators and\nGPT-4, with different scoring methods and ranking systems. We propose a new\ndataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186\nindividuals participated, leading to the generation of 243,337 manual\nannotations and 57,511 automatic evaluation results. We perform comparisons and\nanalyses of different settings and conduct 10 conclusions that can provide some\ninsights for evaluating LLM in the future. The dataset and the results are\npublicly available at https://github.com/llmeval .\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Can LLM find the green circle? Investigation and Human-guided tool\n  manipulation for compositional generalization\u2b1b  The meaning of complex phrases in natural language is composed of their\nindividual components. The task of compositional generalization evaluates a\nmodel's ability to understand new combinations of components. Previous studies\ntrained smaller, task-specific models, which exhibited poor generalization.\nWhile large language models (LLMs) exhibit impressive generalization abilities\non many tasks through in-context learning (ICL), their potential for\ncompositional generalization remains unexplored. In this paper, we first\nempirically investigate prevailing ICL methods in compositional generalization.\nWe find that they struggle with complex compositional questions due to\ncumulative errors in long reasoning steps and intricate logic required for\ntool-making. Consequently, we propose a human-guided tool manipulation\nframework (HTM) that generates tools for sub-questions and integrates multiple\ntools. Our method enhances the effectiveness of tool creation and usage with\nminimal human effort. Experiments show that our method achieves\nstate-of-the-art performance on two compositional generalization benchmarks and\noutperforms existing methods on the most challenging test split by 70%.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Finetuning an LLM on Contextual Knowledge of Classics for Q&A\u2b1b  The open-source publishing of large language models (LLMs) has created many\npossibilities for how anyone who understands language and has access to a\ncomputer can interact with significant tools of artificial intelligence,\nparticularly in the context of learning and knowledge dissemination. However,\nthe utility of these models in specialized fields like Classics is still\nlargely unexplored. This project is an attempt to merge the knowledge of\nClassics with the capabilities of artificial intelligence by finetuning an LLM\nto cater to the specific needs of learners and professionals. The goal of this\nproject is to develop an LLM that not only reproduces contextual knowledge\naccurately but also exhibits a consistent \"personality\" - and, indeed, has\nconsistent propriety - to appeal to a diverse audience who possess differing\nlevels of knowledge. A significant portion of this project was dedicated to\nrefining the dataset, following the principle of \"garbage in, garbage out,\" to\nensure the model generates relevant, useful, and creative responses when given\na prompt (a statement, question, or single word). After training and\nevaluation, my model's ability to handle a vast array of different types of\ninputs and prompting exceeded expectations for a 355M parameter model, though\nits occasional hallucinations (especially when set with a high temperature),\nparticularly in its assertions about historical events or its own identity,\nmake it seem somewhat capricious and more work in the form of continuous\nfinetuning will be undertaken.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs\n  for Embodied AI\u2b1b  Large Language Models (LLMs) are capable of reasoning over diverse input data\nmodalities through pre-trained encoders. However, the growing diversity of\ninput data modalities prevents incorporating all modalities into LLMs,\nespecially when LLMs are deployed on resource-constrained edge devices for\nembodied AI applications. Instead, a better option is to adaptively involve\nonly the useful modalities at runtime, depending on the current environmental\ncontexts and task requirements. For such modality adaptation, existing work\nadopts fixed connections between encoders and the LLM's input layer, leading to\nhigh training cost at runtime and ineffective cross-modal interaction. In this\npaper, we address these limitations by presenting mPnP-LLM, a new technique\nthat allows fully elastic, automated and prompt runtime modality adaptation, by\nconnecting unimodal encoders to a flexible set of last LLM blocks and making\nsuch latent connections fully trainable at runtime. Experiments over the\nnuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction\nand 30% GPU memory usage reduction, while retaining on-par accuracy with the\nexisting schemes. Under the same compute budget, mPnP-LLM improves the task\naccuracy by up to 4% compared to the best existing scheme.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Prompting LLMs with content plans to enhance the summarization of\n  scientific articles\u2b1b  This paper presents novel prompting techniques to improve the performance of\nautomatic summarization systems for scientific articles. Scientific article\nsummarization is highly challenging due to the length and complexity of these\ndocuments. We conceive, implement, and evaluate prompting techniques that\nprovide additional contextual information to guide summarization systems.\nSpecifically, we feed summarizers with lists of key terms extracted from\narticles, such as author keywords or automatically generated keywords. Our\ntechniques are tested with various summarization models and input texts.\nResults show performance gains, especially for smaller models summarizing\nsections separately. This evidences that prompting is a promising approach to\novercoming the limitations of less powerful systems. Our findings introduce a\nnew research direction of using prompts to aid smaller models.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction\u2b1b  Large language models (LLMs) finetuned to follow human instruction have\nrecently exhibited significant capabilities in various English NLP tasks.\nHowever, their performance in grammatical error correction (GEC), especially on\nlanguages other than English, remains significantly unexplored. In this work,\nwe evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a\ncomplex task due to Arabic's rich morphology. Our findings suggest that various\nprompting methods, coupled with (in-context) few-shot learning, demonstrate\nconsiderable effectiveness, with GPT-4 achieving up to $65.49$ F$_{1}$ score\nunder expert prompting (approximately $5$ points higher than our established\nbaseline). Despite these positive results, we find that instruction finetuned\nmodels, regardless of their size, are still outperformed by fully finetuned\nones, even if they are significantly smaller in size. This disparity highlights\nsubstantial room for improvements for LLMs. Inspired by methods used in\nlow-resource machine translation, we also develop a method exploiting synthetic\ndata that significantly outperforms previous models on two standard Arabic\nbenchmarks. Our best model achieves a new SOTA on Arabic GEC, with $73.29$ and\n$73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively, compared to\npeer-reviewed published baselines.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric\n  Strategy for Diverse Generative Tasks\u2b1b  This study examines 4-bit quantization methods like GPTQ in large language\nmodels (LLMs), highlighting GPTQ's overfitting and limited enhancement in\nZero-Shot tasks. While prior works merely focusing on zero-shot measurement, we\nextend task scope to more generative categories such as code generation and\nabstractive summarization, in which we found that INT4 quantization can\nsignificantly underperform. However, simply shifting to higher precision\nformats like FP6 has been particularly challenging, thus overlooked, due to\npoor performance caused by the lack of sophisticated integration and system\nacceleration strategies on current AI hardware. Our results show that FP6, even\nwith a coarse-grain quantization scheme, performs robustly across various\nalgorithms and tasks, demonstrating its superiority in accuracy and\nversatility. Notably, with the FP6 quantization, \\codestar-15B model performs\ncomparably to its FP16 counterpart in code generation, and for smaller models\nlike the 406M it closely matches their baselines in summarization. Neither can\nbe achieved by INT4. To better accommodate various AI hardware and achieve the\nbest system performance, we propose a novel 4+2 design for FP6 to achieve\nsimilar latency to the state-of-the-art INT4 fine-grain quantization. With our\ndesign, FP6 can become a promising solution to the current 4-bit quantization\nmethods used in LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TigerBot: An Open Multilingual Multitask LLM\u2b1b  We release and introduce the TigerBot family of large language models (LLMs),\nconsisting of base and chat models, sized from 7, 13, 70 and 180 billion\nparameters. We develop our models embarking from Llama-2 and BLOOM, and push\nthe boundary further in data, training algorithm, infrastructure, and\napplication tools. Our models yield meaningful performance gain over SOTA\nopen-source models, e.g., Llama-2, specifically 6\\% gain in English and 20\\%\ngain in Chinese. TigerBot model family also achieves leading performance in\nmajor academic and industrial benchmarks and leaderboards. We believe that\nTigerBot represents just a snapshot of lightning-fast progression in LLM\nopen-source community. Therefore, we are thrilled to give back by publicly\nreleasing our models and reporting our approach behind, with additional\nemphases on building SOTA LLMs in a democratized way and making LLMs of use in\nreal-world applications.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs\n  for Financial Sentiment Analysis\u2b1b  Financial sentiment analysis plays a crucial role in uncovering latent\npatterns and detecting emerging trends, enabling individuals to make\nwell-informed decisions that may yield substantial advantages within the\nconstantly changing realm of finance. Recently, Large Language Models (LLMs)\nhave demonstrated their effectiveness in diverse domains, showcasing remarkable\ncapabilities even in zero-shot and few-shot in-context learning for various\nNatural Language Processing (NLP) tasks. Nevertheless, their potential and\napplicability in the context of financial sentiment analysis have not been\nthoroughly explored yet. To bridge this gap, we employ two approaches:\nin-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs\non a finance-domain dataset. Given the computational costs associated with\nfine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs,\nspanning from 250M to 3B parameters for fine-tuning. We then compare the\nperformances with state-of-the-art results to evaluate their effectiveness in\nthe finance-domain. Our results demonstrate that fine-tuned smaller LLMs can\nachieve comparable performance to state-of-the-art fine-tuned LLMs, even with\nmodels having fewer parameters and a smaller training dataset. Additionally,\nthe zero-shot and one-shot performance of LLMs produces comparable results with\nfine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our\nanalysis demonstrates that there is no observed enhancement in performance for\nfinance-domain sentiment analysis when the number of shots for in-context\nlearning is increased.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Boosting LLM Reasoning: Push the Limits of Few-shot Learning with\n  Reinforced In-Context Pruning\u2b1b  Large language models (LLMs) have shown impressive capabilities in various\ntasks, yet they still struggle with math reasoning. Despite efforts to optimize\nChain-of-Thoughts (CoT) prompts and fine-tune LLMs, the potential of few-shot\nlearning remains unexplored. In this work, we propose CoT-Max, a novel approach\npushing the boundaries of few-shot CoT learning to improve LLM math reasoning\ncapabilities. CoT-Max addresses the challenges of the selection of useful\nexamples and limited number of examples due to restricted context window\nlength. Inspired by our observation that natural language inputs contain many\nredundancy, we propose a coarse-to-fine pruner as a plug-and-play module for\nLLMs, which first identifies crucial CoT examples from a large batch and then\nfurther prunes unimportant tokens. To train the pruner, we collect a math\nreasoning dataset with diverse difficulty and steps, introduce a reward to\nmeasure both the input's effectiveness for math reasoning and token length\nconstraints, and propose a novel training approach with reinforcement learning.\nAs a result, CoT-Max significantly outperforms CoT and few-shot prompting\nbaselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 mathematical\ndatasets, achieving up to 4.55% absolute improvements. Remarkably, without any\nfine-tuning, LLaMA2-70B with CoT-Max surpasses GPT-3.5 and a wide range of\nlarger LLMs (PaLM, Minerva, etc.) on the GSM8K.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in\n  Mathematical Reasoning\u2b1b  Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks. However, even the most advanced open-source LLMs, such\nas the LLaMA family models, still face challenges when it comes to accurately\nsolving complex multi-step mathematical problems. In this paper, we present an\ninnovative process-oriented math verifier called \\textbf{Math-Shepherd}, which\nassigns a reward score to each step of the LLM's outputs on math problems. The\ntraining of Math-Shepherd is achieved using automatically constructed\nprocess-wise supervision data, breaking the bottleneck of heavy reliance on\nmanual annotation in existing work. With the guidance of Math-Shepherd, a\nseries of open-source LLMs demonstrate exceptional performance. Among them,\nDeepSeek 67B \\citep{DeepSeek-llm} stands out by achieving accuracy rates of\n93.3\\% on the GSM8K dataset and 48.1\\% on the MATH dataset, without external\nenhancement such as tool usage. Our Math-Shepherd also outperforms the\nself-consistency method and other existing verification models. We believe that\nautomatic process supervision holds significant potential for the future\nevolution of LLMs.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing\n  Semi-structured Data for Large Language Model Reasoning\u2b1b  Table reasoning has shown remarkable progress in a wide range of table-based\ntasks. These challenging tasks require reasoning over both free-form natural\nlanguage (NL) questions and semi-structured tabular data. However, previous\ntable reasoning solutions suffer from significant performance degradation on\n\"huge\" tables. In addition, most existing methods struggle to reason over\ncomplex questions since they lack essential information or they are scattered\nin different places. To alleviate these challenges, we exploit a table\nprovider, namely TAP4LLM, on versatile sampling, augmentation, and packing\nmethods to achieve effective semi-structured data reasoning using large\nlanguage models (LLMs), which 1) decompose raw tables into sub-tables with\nspecific rows or columns based on the rules or semantic similarity; 2) augment\ntable information by extracting semantic and statistical metadata from raw\ntables while retrieving relevant knowledge from trustworthy knowledge sources\n(e.g., Wolfram Alpha, Wikipedia); 3) pack sampled tables with augmented\nknowledge into sequence prompts for LLMs reasoning while balancing the token\nallocation trade-off. We show that TAP4LLM allows for different components as\nplug-ins, enhancing LLMs' understanding of structured data in diverse tabular\ntasks.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "The Earth is Flat because...: Investigating LLMs' Belief towards\n  Misinformation via Persuasive Conversation\u2b1b  Large Language Models (LLMs) encapsulate vast amounts of knowledge but still\nremain vulnerable to external misinformation. Existing research mainly studied\nthis susceptibility behavior in a single-turn setting. However, belief can\nchange during a multi-turn conversation, especially a persuasive one.\nTherefore, in this study, we delve into LLMs' susceptibility to persuasive\nconversations, particularly on factual questions that they can answer\ncorrectly. We first curate the Farm (i.e., Fact to Misinform) dataset, which\ncontains factual questions paired with systematically generated persuasive\nmisinformation. Then, we develop a testing framework to track LLMs' belief\nchanges in a persuasive dialogue. Through extensive experiments, we find that\nLLMs' correct beliefs on factual knowledge can be easily manipulated by various\npersuasive strategies.\n"
    },
    {
      "chapter": "arxiv",
      "section": "LLM",
      "para": "Measurement in the Age of LLMs: An Application to Ideological Scaling\u2b1b  Much of social science is centered around terms like ``ideology'' or\n``power'', which generally elude precise definition, and whose contextual\nmeanings are trapped in surrounding language. This paper explores the use of\nlarge language models (LLMs) to flexibly navigate the conceptual clutter\ninherent to social scientific measurement tasks. We rely on LLMs' remarkable\nlinguistic fluency to elicit ideological scales of both legislators and text,\nwhich accord closely to established methods and our own judgement. A key aspect\nof our approach is that we elicit such scores directly, instructing the LLM to\nfurnish numeric scores itself. This approach affords a great deal of\nflexibility, which we showcase through a variety of different case studies. Our\nresults suggest that LLMs can be used to characterize highly subtle and diffuse\nmanifestations of political ideology in text.\n"
    }
  ],
  "matches": [
    {
      "doc": 0,
      "topic": 2,
      "similarity": 0.7696235227535166
    },
    {
      "doc": 0,
      "topic": 5,
      "similarity": 0.752438920638793
    },
    {
      "doc": 0,
      "topic": 8,
      "similarity": 0.7784404443029292
    },
    {
      "doc": 0,
      "topic": 9,
      "similarity": 0.7674654947526204
    },
    {
      "doc": 0,
      "topic": 15,
      "similarity": 0.7617160349166896
    },
    {
      "doc": 0,
      "topic": 17,
      "similarity": 0.7649529317833029
    },
    {
      "doc": 0,
      "topic": 18,
      "similarity": 0.7728372761842275
    },
    {
      "doc": 0,
      "topic": 19,
      "similarity": 0.7597665380770978
    },
    {
      "doc": 0,
      "topic": 20,
      "similarity": 0.7575565956275673
    },
    {
      "doc": 0,
      "topic": 21,
      "similarity": 0.7763081565584191
    },
    {
      "doc": 0,
      "topic": 23,
      "similarity": 0.7592050980531122
    },
    {
      "doc": 1,
      "topic": 1,
      "similarity": 0.7585594911236835
    },
    {
      "doc": 1,
      "topic": 2,
      "similarity": 0.7739695476515358
    },
    {
      "doc": 1,
      "topic": 3,
      "similarity": 0.799788108270785
    },
    {
      "doc": 1,
      "topic": 4,
      "similarity": 0.7608853130930555
    },
    {
      "doc": 1,
      "topic": 5,
      "similarity": 0.8026892854709468
    },
    {
      "doc": 1,
      "topic": 7,
      "similarity": 0.7769456050265108
    },
    {
      "doc": 1,
      "topic": 8,
      "similarity": 0.8015213346534691
    },
    {
      "doc": 1,
      "topic": 9,
      "similarity": 0.8384464603877755
    },
    {
      "doc": 1,
      "topic": 10,
      "similarity": 0.7935562597406419
    },
    {
      "doc": 1,
      "topic": 11,
      "similarity": 0.7762688908512759
    },
    {
      "doc": 1,
      "topic": 12,
      "similarity": 0.7502731826141433
    },
    {
      "doc": 1,
      "topic": 13,
      "similarity": 0.782435685599839
    },
    {
      "doc": 1,
      "topic": 14,
      "similarity": 0.7970225510824702
    },
    {
      "doc": 1,
      "topic": 15,
      "similarity": 0.7944683441282185
    },
    {
      "doc": 1,
      "topic": 16,
      "similarity": 0.8061985494846311
    },
    {
      "doc": 1,
      "topic": 17,
      "similarity": 0.802770558633761
    },
    {
      "doc": 1,
      "topic": 18,
      "similarity": 0.8023675985001664
    },
    {
      "doc": 1,
      "topic": 19,
      "similarity": 0.8387449137312089
    },
    {
      "doc": 1,
      "topic": 20,
      "similarity": 0.7847645044334773
    },
    {
      "doc": 1,
      "topic": 21,
      "similarity": 0.8037595836526319
    },
    {
      "doc": 1,
      "topic": 22,
      "similarity": 0.7558148143609171
    },
    {
      "doc": 1,
      "topic": 23,
      "similarity": 0.7562026988677181
    },
    {
      "doc": 1,
      "topic": 24,
      "similarity": 0.7656677351760286
    },
    {
      "doc": 2,
      "topic": 1,
      "similarity": 0.7597485725466094
    },
    {
      "doc": 2,
      "topic": 2,
      "similarity": 0.7805783373132276
    },
    {
      "doc": 2,
      "topic": 3,
      "similarity": 0.8208036042320894
    },
    {
      "doc": 2,
      "topic": 4,
      "similarity": 0.7648973137443775
    },
    {
      "doc": 2,
      "topic": 5,
      "similarity": 0.844864500942766
    },
    {
      "doc": 2,
      "topic": 7,
      "similarity": 0.8148952682178043
    },
    {
      "doc": 2,
      "topic": 8,
      "similarity": 0.7835797152787227
    },
    {
      "doc": 2,
      "topic": 9,
      "similarity": 0.8442418526669097
    },
    {
      "doc": 2,
      "topic": 10,
      "similarity": 0.8097415075266137
    },
    {
      "doc": 2,
      "topic": 11,
      "similarity": 0.8227063240607911
    },
    {
      "doc": 2,
      "topic": 13,
      "similarity": 0.7696249190537033
    },
    {
      "doc": 2,
      "topic": 14,
      "similarity": 0.7799198593974
    },
    {
      "doc": 2,
      "topic": 15,
      "similarity": 0.7835371190480577
    },
    {
      "doc": 2,
      "topic": 16,
      "similarity": 0.8144298702785964
    },
    {
      "doc": 2,
      "topic": 17,
      "similarity": 0.7992503064291979
    },
    {
      "doc": 2,
      "topic": 18,
      "similarity": 0.7793862651676102
    },
    {
      "doc": 2,
      "topic": 19,
      "similarity": 0.8192859997529046
    },
    {
      "doc": 2,
      "topic": 20,
      "similarity": 0.7893219693525971
    },
    {
      "doc": 2,
      "topic": 21,
      "similarity": 0.8016877217613494
    },
    {
      "doc": 2,
      "topic": 23,
      "similarity": 0.755011848894132
    },
    {
      "doc": 3,
      "topic": 2,
      "similarity": 0.7679708494322097
    },
    {
      "doc": 3,
      "topic": 3,
      "similarity": 0.7971627948049697
    },
    {
      "doc": 3,
      "topic": 5,
      "similarity": 0.7872608911456535
    },
    {
      "doc": 3,
      "topic": 7,
      "similarity": 0.7635019873180897
    },
    {
      "doc": 3,
      "topic": 9,
      "similarity": 0.7941453143606745
    },
    {
      "doc": 3,
      "topic": 11,
      "similarity": 0.7627391925291158
    },
    {
      "doc": 3,
      "topic": 14,
      "similarity": 0.7518612898368828
    },
    {
      "doc": 3,
      "topic": 15,
      "similarity": 0.7560062194738122
    },
    {
      "doc": 3,
      "topic": 16,
      "similarity": 0.809123298677233
    },
    {
      "doc": 3,
      "topic": 17,
      "similarity": 0.8138200448470712
    },
    {
      "doc": 3,
      "topic": 18,
      "similarity": 0.7514680728296399
    },
    {
      "doc": 3,
      "topic": 19,
      "similarity": 0.8000753296580126
    },
    {
      "doc": 3,
      "topic": 20,
      "similarity": 0.7887091728821182
    },
    {
      "doc": 3,
      "topic": 21,
      "similarity": 0.8036790759822124
    },
    {
      "doc": 4,
      "topic": 5,
      "similarity": 0.7705386500305635
    },
    {
      "doc": 4,
      "topic": 9,
      "similarity": 0.7794429040973693
    },
    {
      "doc": 4,
      "topic": 16,
      "similarity": 0.7636832801844898
    },
    {
      "doc": 4,
      "topic": 20,
      "similarity": 0.7762918461162273
    },
    {
      "doc": 4,
      "topic": 21,
      "similarity": 0.7507658169638849
    },
    {
      "doc": 4,
      "topic": 24,
      "similarity": 0.7749170979836592
    },
    {
      "doc": 5,
      "topic": 1,
      "similarity": 0.7517801074002984
    },
    {
      "doc": 5,
      "topic": 2,
      "similarity": 0.7878291465912665
    },
    {
      "doc": 5,
      "topic": 3,
      "similarity": 0.801054891648319
    },
    {
      "doc": 5,
      "topic": 4,
      "similarity": 0.7564435866928333
    },
    {
      "doc": 5,
      "topic": 5,
      "similarity": 0.7948125771514422
    },
    {
      "doc": 5,
      "topic": 7,
      "similarity": 0.7622338671242332
    },
    {
      "doc": 5,
      "topic": 8,
      "similarity": 0.7779657435152496
    },
    {
      "doc": 5,
      "topic": 9,
      "similarity": 0.8158277280616484
    },
    {
      "doc": 5,
      "topic": 10,
      "similarity": 0.7770520498271275
    },
    {
      "doc": 5,
      "topic": 11,
      "similarity": 0.7939742781328541
    },
    {
      "doc": 5,
      "topic": 13,
      "similarity": 0.7720678822621939
    },
    {
      "doc": 5,
      "topic": 14,
      "similarity": 0.7917386557765177
    },
    {
      "doc": 5,
      "topic": 15,
      "similarity": 0.8040951804403267
    },
    {
      "doc": 5,
      "topic": 16,
      "similarity": 0.8008372867232384
    },
    {
      "doc": 5,
      "topic": 17,
      "similarity": 0.7979331116005611
    },
    {
      "doc": 5,
      "topic": 18,
      "similarity": 0.780051048507392
    },
    {
      "doc": 5,
      "topic": 19,
      "similarity": 0.8318980137997387
    },
    {
      "doc": 5,
      "topic": 20,
      "similarity": 0.7740624975378914
    },
    {
      "doc": 5,
      "topic": 21,
      "similarity": 0.7925779423096931
    },
    {
      "doc": 5,
      "topic": 23,
      "similarity": 0.7560417951327901
    },
    {
      "doc": 6,
      "topic": 3,
      "similarity": 0.7767502318742557
    },
    {
      "doc": 6,
      "topic": 5,
      "similarity": 0.7786318723734117
    },
    {
      "doc": 6,
      "topic": 9,
      "similarity": 0.7961674940933601
    },
    {
      "doc": 6,
      "topic": 11,
      "similarity": 0.785907588338025
    },
    {
      "doc": 6,
      "topic": 15,
      "similarity": 0.7564572277589993
    },
    {
      "doc": 6,
      "topic": 16,
      "similarity": 0.7879539858915201
    },
    {
      "doc": 6,
      "topic": 17,
      "similarity": 0.7853921385667997
    },
    {
      "doc": 6,
      "topic": 18,
      "similarity": 0.7557033162513792
    },
    {
      "doc": 6,
      "topic": 19,
      "similarity": 0.7845907931615073
    },
    {
      "doc": 6,
      "topic": 20,
      "similarity": 0.7896670261074755
    },
    {
      "doc": 6,
      "topic": 21,
      "similarity": 0.7815671155577045
    },
    {
      "doc": 7,
      "topic": 1,
      "similarity": 0.7624557063905149
    },
    {
      "doc": 7,
      "topic": 2,
      "similarity": 0.771897983262293
    },
    {
      "doc": 7,
      "topic": 3,
      "similarity": 0.8021047006047879
    },
    {
      "doc": 7,
      "topic": 4,
      "similarity": 0.7693730790160906
    },
    {
      "doc": 7,
      "topic": 5,
      "similarity": 0.7973470067644113
    },
    {
      "doc": 7,
      "topic": 7,
      "similarity": 0.7793738855200174
    },
    {
      "doc": 7,
      "topic": 8,
      "similarity": 0.7716121600858761
    },
    {
      "doc": 7,
      "topic": 9,
      "similarity": 0.8188608802440738
    },
    {
      "doc": 7,
      "topic": 10,
      "similarity": 0.7770925068469953
    },
    {
      "doc": 7,
      "topic": 11,
      "similarity": 0.7906309793692494
    },
    {
      "doc": 7,
      "topic": 12,
      "similarity": 0.7608626674274637
    },
    {
      "doc": 7,
      "topic": 13,
      "similarity": 0.7806411060452791
    },
    {
      "doc": 7,
      "topic": 14,
      "similarity": 0.7761548449442666
    },
    {
      "doc": 7,
      "topic": 15,
      "similarity": 0.8007051948660252
    },
    {
      "doc": 7,
      "topic": 16,
      "similarity": 0.816031931099938
    },
    {
      "doc": 7,
      "topic": 17,
      "similarity": 0.8109876035170664
    },
    {
      "doc": 7,
      "topic": 18,
      "similarity": 0.7668838523165193
    },
    {
      "doc": 7,
      "topic": 19,
      "similarity": 0.8144589580333965
    },
    {
      "doc": 7,
      "topic": 20,
      "similarity": 0.7903410498179807
    },
    {
      "doc": 7,
      "topic": 21,
      "similarity": 0.796601348427624
    },
    {
      "doc": 7,
      "topic": 22,
      "similarity": 0.7546609587056214
    },
    {
      "doc": 8,
      "topic": 1,
      "similarity": 0.7957599185861358
    },
    {
      "doc": 8,
      "topic": 2,
      "similarity": 0.8248805239423325
    },
    {
      "doc": 8,
      "topic": 3,
      "similarity": 0.8325488007401713
    },
    {
      "doc": 8,
      "topic": 4,
      "similarity": 0.7687012119222956
    },
    {
      "doc": 8,
      "topic": 5,
      "similarity": 0.8167394612466525
    },
    {
      "doc": 8,
      "topic": 7,
      "similarity": 0.8061226159018903
    },
    {
      "doc": 8,
      "topic": 8,
      "similarity": 0.8068241356331983
    },
    {
      "doc": 8,
      "topic": 9,
      "similarity": 0.8407323229864929
    },
    {
      "doc": 8,
      "topic": 10,
      "similarity": 0.806257113415325
    },
    {
      "doc": 8,
      "topic": 11,
      "similarity": 0.8292911680123273
    },
    {
      "doc": 8,
      "topic": 12,
      "similarity": 0.7623443591748151
    },
    {
      "doc": 8,
      "topic": 13,
      "similarity": 0.7912906827718177
    },
    {
      "doc": 8,
      "topic": 14,
      "similarity": 0.8368953831233017
    },
    {
      "doc": 8,
      "topic": 15,
      "similarity": 0.796901893444043
    },
    {
      "doc": 8,
      "topic": 16,
      "similarity": 0.8296543393904479
    },
    {
      "doc": 8,
      "topic": 17,
      "similarity": 0.8104213347335428
    },
    {
      "doc": 8,
      "topic": 18,
      "similarity": 0.8082310903673966
    },
    {
      "doc": 8,
      "topic": 19,
      "similarity": 0.8599677188782334
    },
    {
      "doc": 8,
      "topic": 20,
      "similarity": 0.7944557355018746
    },
    {
      "doc": 8,
      "topic": 21,
      "similarity": 0.8150665888829173
    },
    {
      "doc": 8,
      "topic": 22,
      "similarity": 0.7583369392894539
    },
    {
      "doc": 8,
      "topic": 23,
      "similarity": 0.781460147199571
    },
    {
      "doc": 9,
      "topic": 1,
      "similarity": 0.7704086057536111
    },
    {
      "doc": 9,
      "topic": 2,
      "similarity": 0.7889695673929545
    },
    {
      "doc": 9,
      "topic": 3,
      "similarity": 0.8032881235735247
    },
    {
      "doc": 9,
      "topic": 4,
      "similarity": 0.761789332675394
    },
    {
      "doc": 9,
      "topic": 5,
      "similarity": 0.8260467122072951
    },
    {
      "doc": 9,
      "topic": 7,
      "similarity": 0.7930996749252178
    },
    {
      "doc": 9,
      "topic": 8,
      "similarity": 0.8070721426561596
    },
    {
      "doc": 9,
      "topic": 9,
      "similarity": 0.8250382534781459
    },
    {
      "doc": 9,
      "topic": 10,
      "similarity": 0.7994513454619943
    },
    {
      "doc": 9,
      "topic": 11,
      "similarity": 0.7959259624618574
    },
    {
      "doc": 9,
      "topic": 13,
      "similarity": 0.7990124618965476
    },
    {
      "doc": 9,
      "topic": 14,
      "similarity": 0.8037627756043773
    },
    {
      "doc": 9,
      "topic": 15,
      "similarity": 0.783659390107103
    },
    {
      "doc": 9,
      "topic": 16,
      "similarity": 0.8216781694738948
    },
    {
      "doc": 9,
      "topic": 17,
      "similarity": 0.8145892494307758
    },
    {
      "doc": 9,
      "topic": 18,
      "similarity": 0.7607516512967157
    },
    {
      "doc": 9,
      "topic": 19,
      "similarity": 0.8209633114967104
    },
    {
      "doc": 9,
      "topic": 20,
      "similarity": 0.7652385840946831
    },
    {
      "doc": 9,
      "topic": 21,
      "similarity": 0.7983123860614717
    },
    {
      "doc": 9,
      "topic": 23,
      "similarity": 0.7720626056171547
    },
    {
      "doc": 10,
      "topic": 0,
      "similarity": 0.8162405020154326
    },
    {
      "doc": 10,
      "topic": 1,
      "similarity": 0.7892105636290878
    },
    {
      "doc": 10,
      "topic": 2,
      "similarity": 0.7754888928579967
    },
    {
      "doc": 10,
      "topic": 3,
      "similarity": 0.7896146489100568
    },
    {
      "doc": 10,
      "topic": 5,
      "similarity": 0.7762196289865914
    },
    {
      "doc": 10,
      "topic": 7,
      "similarity": 0.7590908448800251
    },
    {
      "doc": 10,
      "topic": 8,
      "similarity": 0.7517107074952696
    },
    {
      "doc": 10,
      "topic": 9,
      "similarity": 0.8021722695914586
    },
    {
      "doc": 10,
      "topic": 10,
      "similarity": 0.7696806740530332
    },
    {
      "doc": 10,
      "topic": 11,
      "similarity": 0.8177252276625184
    },
    {
      "doc": 10,
      "topic": 13,
      "similarity": 0.7507466890062227
    },
    {
      "doc": 10,
      "topic": 14,
      "similarity": 0.7684953488352224
    },
    {
      "doc": 10,
      "topic": 15,
      "similarity": 0.7863244213105056
    },
    {
      "doc": 10,
      "topic": 16,
      "similarity": 0.8156667389060024
    },
    {
      "doc": 10,
      "topic": 17,
      "similarity": 0.819737746448113
    },
    {
      "doc": 10,
      "topic": 19,
      "similarity": 0.7896634617340155
    },
    {
      "doc": 10,
      "topic": 20,
      "similarity": 0.7783188556260375
    },
    {
      "doc": 10,
      "topic": 21,
      "similarity": 0.8028105286445335
    },
    {
      "doc": 10,
      "topic": 23,
      "similarity": 0.7637654149421942
    },
    {
      "doc": 11,
      "topic": 3,
      "similarity": 0.7745270560474704
    },
    {
      "doc": 11,
      "topic": 5,
      "similarity": 0.7776419558475592
    },
    {
      "doc": 11,
      "topic": 9,
      "similarity": 0.7863316971427972
    },
    {
      "doc": 11,
      "topic": 10,
      "similarity": 0.7500959881548918
    },
    {
      "doc": 11,
      "topic": 17,
      "similarity": 0.765508615211643
    },
    {
      "doc": 11,
      "topic": 19,
      "similarity": 0.7580779430639933
    },
    {
      "doc": 11,
      "topic": 21,
      "similarity": 0.7933455724149072
    },
    {
      "doc": 11,
      "topic": 23,
      "similarity": 0.7543650126766722
    },
    {
      "doc": 11,
      "topic": 24,
      "similarity": 0.7824280203733851
    },
    {
      "doc": 12,
      "topic": 3,
      "similarity": 0.786305589301636
    },
    {
      "doc": 12,
      "topic": 5,
      "similarity": 0.8007482575732868
    },
    {
      "doc": 12,
      "topic": 7,
      "similarity": 0.7709341615219826
    },
    {
      "doc": 12,
      "topic": 8,
      "similarity": 0.7536958144215417
    },
    {
      "doc": 12,
      "topic": 9,
      "similarity": 0.8043655613503636
    },
    {
      "doc": 12,
      "topic": 10,
      "similarity": 0.7624623487718744
    },
    {
      "doc": 12,
      "topic": 11,
      "similarity": 0.7891712920259465
    },
    {
      "doc": 12,
      "topic": 15,
      "similarity": 0.7692790481142312
    },
    {
      "doc": 12,
      "topic": 16,
      "similarity": 0.7796823807946293
    },
    {
      "doc": 12,
      "topic": 17,
      "similarity": 0.7817454863260378
    },
    {
      "doc": 12,
      "topic": 18,
      "similarity": 0.7603182829598096
    },
    {
      "doc": 12,
      "topic": 19,
      "similarity": 0.80597210998627
    },
    {
      "doc": 12,
      "topic": 20,
      "similarity": 0.7671137892612766
    },
    {
      "doc": 12,
      "topic": 21,
      "similarity": 0.7850202411852399
    },
    {
      "doc": 12,
      "topic": 23,
      "similarity": 0.7725712407364865
    },
    {
      "doc": 12,
      "topic": 24,
      "similarity": 0.7555456889531577
    },
    {
      "doc": 13,
      "topic": 1,
      "similarity": 0.752323114967554
    },
    {
      "doc": 13,
      "topic": 2,
      "similarity": 0.7557470661329186
    },
    {
      "doc": 13,
      "topic": 3,
      "similarity": 0.788506799015146
    },
    {
      "doc": 13,
      "topic": 4,
      "similarity": 0.7594684295262961
    },
    {
      "doc": 13,
      "topic": 5,
      "similarity": 0.772519848905222
    },
    {
      "doc": 13,
      "topic": 6,
      "similarity": 0.7604086223700772
    },
    {
      "doc": 13,
      "topic": 7,
      "similarity": 0.7603768366273176
    },
    {
      "doc": 13,
      "topic": 8,
      "similarity": 0.7732445092724303
    },
    {
      "doc": 13,
      "topic": 9,
      "similarity": 0.80622587719799
    },
    {
      "doc": 13,
      "topic": 10,
      "similarity": 0.7825716375170015
    },
    {
      "doc": 13,
      "topic": 11,
      "similarity": 0.7724349190259184
    },
    {
      "doc": 13,
      "topic": 12,
      "similarity": 0.76529464369247
    },
    {
      "doc": 13,
      "topic": 13,
      "similarity": 0.7591860831421529
    },
    {
      "doc": 13,
      "topic": 14,
      "similarity": 0.7678816953685047
    },
    {
      "doc": 13,
      "topic": 15,
      "similarity": 0.7845137736561592
    },
    {
      "doc": 13,
      "topic": 16,
      "similarity": 0.7807135490312113
    },
    {
      "doc": 13,
      "topic": 17,
      "similarity": 0.795252458652032
    },
    {
      "doc": 13,
      "topic": 18,
      "similarity": 0.7687728278857516
    },
    {
      "doc": 13,
      "topic": 19,
      "similarity": 0.8036457690783573
    },
    {
      "doc": 13,
      "topic": 20,
      "similarity": 0.7868769183814304
    },
    {
      "doc": 13,
      "topic": 21,
      "similarity": 0.8042715575553192
    },
    {
      "doc": 13,
      "topic": 22,
      "similarity": 0.7579224165207378
    },
    {
      "doc": 14,
      "topic": 3,
      "similarity": 0.7725121765102277
    },
    {
      "doc": 14,
      "topic": 5,
      "similarity": 0.7802281972301033
    },
    {
      "doc": 14,
      "topic": 7,
      "similarity": 0.7705041012102261
    },
    {
      "doc": 14,
      "topic": 8,
      "similarity": 0.7555769122857396
    },
    {
      "doc": 14,
      "topic": 9,
      "similarity": 0.8445102758359023
    },
    {
      "doc": 14,
      "topic": 10,
      "similarity": 0.7553948014461968
    },
    {
      "doc": 14,
      "topic": 11,
      "similarity": 0.7616059550364445
    },
    {
      "doc": 14,
      "topic": 12,
      "similarity": 0.7690760374377434
    },
    {
      "doc": 14,
      "topic": 15,
      "similarity": 0.7627642006261925
    },
    {
      "doc": 14,
      "topic": 16,
      "similarity": 0.7817803671100026
    },
    {
      "doc": 14,
      "topic": 17,
      "similarity": 0.7758166779329172
    },
    {
      "doc": 14,
      "topic": 18,
      "similarity": 0.766753123025077
    },
    {
      "doc": 14,
      "topic": 19,
      "similarity": 0.7755851014849612
    },
    {
      "doc": 14,
      "topic": 20,
      "similarity": 0.7728398923738606
    },
    {
      "doc": 14,
      "topic": 21,
      "similarity": 0.7917268148439942
    },
    {
      "doc": 14,
      "topic": 23,
      "similarity": 0.761884014565792
    },
    {
      "doc": 14,
      "topic": 24,
      "similarity": 0.792927232751045
    },
    {
      "doc": 15,
      "topic": 1,
      "similarity": 0.770429999236044
    },
    {
      "doc": 15,
      "topic": 2,
      "similarity": 0.7842094101666571
    },
    {
      "doc": 15,
      "topic": 3,
      "similarity": 0.8044316224099931
    },
    {
      "doc": 15,
      "topic": 4,
      "similarity": 0.7542784869174421
    },
    {
      "doc": 15,
      "topic": 5,
      "similarity": 0.8098968994517357
    },
    {
      "doc": 15,
      "topic": 6,
      "similarity": 0.7715839718268254
    },
    {
      "doc": 15,
      "topic": 7,
      "similarity": 0.799203438379517
    },
    {
      "doc": 15,
      "topic": 8,
      "similarity": 0.7693808384311808
    },
    {
      "doc": 15,
      "topic": 9,
      "similarity": 0.8829522480354958
    },
    {
      "doc": 15,
      "topic": 10,
      "similarity": 0.7840971600745014
    },
    {
      "doc": 15,
      "topic": 11,
      "similarity": 0.7907244658401549
    },
    {
      "doc": 15,
      "topic": 13,
      "similarity": 0.7715118361103245
    },
    {
      "doc": 15,
      "topic": 14,
      "similarity": 0.7842102815486154
    },
    {
      "doc": 15,
      "topic": 15,
      "similarity": 0.7886592673160924
    },
    {
      "doc": 15,
      "topic": 16,
      "similarity": 0.8267593498638198
    },
    {
      "doc": 15,
      "topic": 17,
      "similarity": 0.8080284099816077
    },
    {
      "doc": 15,
      "topic": 18,
      "similarity": 0.7512193022779872
    },
    {
      "doc": 15,
      "topic": 19,
      "similarity": 0.8110001534698101
    },
    {
      "doc": 15,
      "topic": 20,
      "similarity": 0.7848629818495906
    },
    {
      "doc": 15,
      "topic": 21,
      "similarity": 0.8229935750148326
    },
    {
      "doc": 15,
      "topic": 23,
      "similarity": 0.8239766131262753
    },
    {
      "doc": 15,
      "topic": 24,
      "similarity": 0.7995516395823516
    },
    {
      "doc": 16,
      "topic": 3,
      "similarity": 0.7701454137693082
    },
    {
      "doc": 16,
      "topic": 5,
      "similarity": 0.76254405964555
    },
    {
      "doc": 16,
      "topic": 7,
      "similarity": 0.7594874385719858
    },
    {
      "doc": 16,
      "topic": 8,
      "similarity": 0.7581814839201093
    },
    {
      "doc": 16,
      "topic": 9,
      "similarity": 0.8184304596948495
    },
    {
      "doc": 16,
      "topic": 10,
      "similarity": 0.8028521645315637
    },
    {
      "doc": 16,
      "topic": 11,
      "similarity": 0.7512477474705813
    },
    {
      "doc": 16,
      "topic": 12,
      "similarity": 0.7572461020322471
    },
    {
      "doc": 16,
      "topic": 16,
      "similarity": 0.7685530959032596
    },
    {
      "doc": 16,
      "topic": 17,
      "similarity": 0.7650504337093258
    },
    {
      "doc": 16,
      "topic": 18,
      "similarity": 0.7507591677257253
    },
    {
      "doc": 16,
      "topic": 19,
      "similarity": 0.7751844404618787
    },
    {
      "doc": 16,
      "topic": 20,
      "similarity": 0.7626175346452518
    },
    {
      "doc": 16,
      "topic": 21,
      "similarity": 0.7707178600881424
    },
    {
      "doc": 16,
      "topic": 24,
      "similarity": 0.7682823277282125
    },
    {
      "doc": 17,
      "topic": 3,
      "similarity": 0.7876189013105973
    },
    {
      "doc": 17,
      "topic": 4,
      "similarity": 0.8361725198055148
    },
    {
      "doc": 17,
      "topic": 5,
      "similarity": 0.7710904970526253
    },
    {
      "doc": 17,
      "topic": 7,
      "similarity": 0.7633707999615228
    },
    {
      "doc": 17,
      "topic": 9,
      "similarity": 0.7836485044311999
    },
    {
      "doc": 17,
      "topic": 10,
      "similarity": 0.7666952838548383
    },
    {
      "doc": 17,
      "topic": 11,
      "similarity": 0.7509095157052861
    },
    {
      "doc": 17,
      "topic": 16,
      "similarity": 0.7825260805367642
    },
    {
      "doc": 17,
      "topic": 17,
      "similarity": 0.7669465969102598
    },
    {
      "doc": 17,
      "topic": 19,
      "similarity": 0.770774393807314
    },
    {
      "doc": 17,
      "topic": 20,
      "similarity": 0.7634186788495192
    },
    {
      "doc": 17,
      "topic": 21,
      "similarity": 0.7657265043843517
    },
    {
      "doc": 17,
      "topic": 24,
      "similarity": 0.7669255609929181
    },
    {
      "doc": 18,
      "topic": 1,
      "similarity": 0.7784214284340552
    },
    {
      "doc": 18,
      "topic": 2,
      "similarity": 0.8095474552709752
    },
    {
      "doc": 18,
      "topic": 3,
      "similarity": 0.8121943019658097
    },
    {
      "doc": 18,
      "topic": 4,
      "similarity": 0.7682676660038557
    },
    {
      "doc": 18,
      "topic": 5,
      "similarity": 0.8182978343996727
    },
    {
      "doc": 18,
      "topic": 7,
      "similarity": 0.7897026986084557
    },
    {
      "doc": 18,
      "topic": 8,
      "similarity": 0.8009397060349446
    },
    {
      "doc": 18,
      "topic": 9,
      "similarity": 0.8235413335376105
    },
    {
      "doc": 18,
      "topic": 10,
      "similarity": 0.8190546965263364
    },
    {
      "doc": 18,
      "topic": 11,
      "similarity": 0.8034774768820279
    },
    {
      "doc": 18,
      "topic": 13,
      "similarity": 0.8059162614922458
    },
    {
      "doc": 18,
      "topic": 14,
      "similarity": 0.832124040028038
    },
    {
      "doc": 18,
      "topic": 15,
      "similarity": 0.8008848975609949
    },
    {
      "doc": 18,
      "topic": 16,
      "similarity": 0.8404396875757306
    },
    {
      "doc": 18,
      "topic": 17,
      "similarity": 0.8236494117061445
    },
    {
      "doc": 18,
      "topic": 18,
      "similarity": 0.8223650435749135
    },
    {
      "doc": 18,
      "topic": 19,
      "similarity": 0.8759873554054006
    },
    {
      "doc": 18,
      "topic": 20,
      "similarity": 0.7702382997451145
    },
    {
      "doc": 18,
      "topic": 21,
      "similarity": 0.8173012213655474
    },
    {
      "doc": 19,
      "topic": 3,
      "similarity": 0.7611865864889007
    },
    {
      "doc": 19,
      "topic": 5,
      "similarity": 0.7573810282846218
    },
    {
      "doc": 19,
      "topic": 8,
      "similarity": 0.7636404292038377
    },
    {
      "doc": 19,
      "topic": 9,
      "similarity": 0.7960741888595647
    },
    {
      "doc": 19,
      "topic": 11,
      "similarity": 0.7613276177616293
    },
    {
      "doc": 19,
      "topic": 12,
      "similarity": 0.7687600524473609
    },
    {
      "doc": 19,
      "topic": 15,
      "similarity": 0.7546546388756593
    },
    {
      "doc": 19,
      "topic": 16,
      "similarity": 0.7613706978439092
    },
    {
      "doc": 19,
      "topic": 17,
      "similarity": 0.7673471005794009
    },
    {
      "doc": 19,
      "topic": 19,
      "similarity": 0.7830742297673181
    },
    {
      "doc": 19,
      "topic": 20,
      "similarity": 0.7643150119524743
    },
    {
      "doc": 19,
      "topic": 21,
      "similarity": 0.7689531432227987
    },
    {
      "doc": 19,
      "topic": 24,
      "similarity": 0.7569758242705062
    },
    {
      "doc": 20,
      "topic": 3,
      "similarity": 0.7746096506389368
    },
    {
      "doc": 20,
      "topic": 5,
      "similarity": 0.7796706714396073
    },
    {
      "doc": 20,
      "topic": 7,
      "similarity": 0.7609845157356339
    },
    {
      "doc": 20,
      "topic": 8,
      "similarity": 0.763537493898096
    },
    {
      "doc": 20,
      "topic": 9,
      "similarity": 0.8169577444551759
    },
    {
      "doc": 20,
      "topic": 10,
      "similarity": 0.7646220173187561
    },
    {
      "doc": 20,
      "topic": 11,
      "similarity": 0.780187417230822
    },
    {
      "doc": 20,
      "topic": 15,
      "similarity": 0.7642593088942267
    },
    {
      "doc": 20,
      "topic": 16,
      "similarity": 0.7863004112679453
    },
    {
      "doc": 20,
      "topic": 17,
      "similarity": 0.7706677696354082
    },
    {
      "doc": 20,
      "topic": 18,
      "similarity": 0.7643331331653739
    },
    {
      "doc": 20,
      "topic": 19,
      "similarity": 0.7868705207259119
    },
    {
      "doc": 20,
      "topic": 20,
      "similarity": 0.7795797833565908
    },
    {
      "doc": 20,
      "topic": 21,
      "similarity": 0.7872937000478217
    },
    {
      "doc": 20,
      "topic": 23,
      "similarity": 0.750728288771386
    },
    {
      "doc": 20,
      "topic": 24,
      "similarity": 0.7625854478685051
    },
    {
      "doc": 21,
      "topic": 1,
      "similarity": 0.7586503653613166
    },
    {
      "doc": 21,
      "topic": 2,
      "similarity": 0.7705115187953845
    },
    {
      "doc": 21,
      "topic": 3,
      "similarity": 0.7977962820504527
    },
    {
      "doc": 21,
      "topic": 4,
      "similarity": 0.7710564488579548
    },
    {
      "doc": 21,
      "topic": 5,
      "similarity": 0.8043633141804556
    },
    {
      "doc": 21,
      "topic": 6,
      "similarity": 0.7641646593296725
    },
    {
      "doc": 21,
      "topic": 7,
      "similarity": 0.8000790300876447
    },
    {
      "doc": 21,
      "topic": 8,
      "similarity": 0.7883603231679753
    },
    {
      "doc": 21,
      "topic": 9,
      "similarity": 0.8380614667172803
    },
    {
      "doc": 21,
      "topic": 10,
      "similarity": 0.8080835333148444
    },
    {
      "doc": 21,
      "topic": 11,
      "similarity": 0.7785230457626451
    },
    {
      "doc": 21,
      "topic": 13,
      "similarity": 0.7920427972508567
    },
    {
      "doc": 21,
      "topic": 14,
      "similarity": 0.7745472158271155
    },
    {
      "doc": 21,
      "topic": 15,
      "similarity": 0.7903469263156296
    },
    {
      "doc": 21,
      "topic": 16,
      "similarity": 0.8081329370506158
    },
    {
      "doc": 21,
      "topic": 17,
      "similarity": 0.809535857809491
    },
    {
      "doc": 21,
      "topic": 18,
      "similarity": 0.7527536989598205
    },
    {
      "doc": 21,
      "topic": 19,
      "similarity": 0.8011902506777566
    },
    {
      "doc": 21,
      "topic": 20,
      "similarity": 0.7901304734419259
    },
    {
      "doc": 21,
      "topic": 21,
      "similarity": 0.8038557595551384
    },
    {
      "doc": 21,
      "topic": 24,
      "similarity": 0.7532278002364504
    },
    {
      "doc": 22,
      "topic": 3,
      "similarity": 0.7812382264121155
    },
    {
      "doc": 22,
      "topic": 5,
      "similarity": 0.7506646914130526
    },
    {
      "doc": 22,
      "topic": 6,
      "similarity": 0.7523645372569259
    },
    {
      "doc": 22,
      "topic": 8,
      "similarity": 0.7568755350716843
    },
    {
      "doc": 22,
      "topic": 9,
      "similarity": 0.8313795185087576
    },
    {
      "doc": 22,
      "topic": 10,
      "similarity": 0.7521234161458608
    },
    {
      "doc": 22,
      "topic": 11,
      "similarity": 0.7553941429838538
    },
    {
      "doc": 22,
      "topic": 12,
      "similarity": 0.7525765014412242
    },
    {
      "doc": 22,
      "topic": 15,
      "similarity": 0.768450631086534
    },
    {
      "doc": 22,
      "topic": 16,
      "similarity": 0.7542801335495257
    },
    {
      "doc": 22,
      "topic": 17,
      "similarity": 0.7741425418470076
    },
    {
      "doc": 22,
      "topic": 18,
      "similarity": 0.7606062276241027
    },
    {
      "doc": 22,
      "topic": 19,
      "similarity": 0.7713314392225463
    },
    {
      "doc": 22,
      "topic": 20,
      "similarity": 0.7569053078521231
    },
    {
      "doc": 22,
      "topic": 21,
      "similarity": 0.7845470951504535
    },
    {
      "doc": 22,
      "topic": 23,
      "similarity": 0.7841970256846442
    },
    {
      "doc": 22,
      "topic": 24,
      "similarity": 0.7853791339207766
    },
    {
      "doc": 23,
      "topic": 2,
      "similarity": 0.7575871560682322
    },
    {
      "doc": 23,
      "topic": 3,
      "similarity": 0.7845753745709664
    },
    {
      "doc": 23,
      "topic": 5,
      "similarity": 0.7804424888036472
    },
    {
      "doc": 23,
      "topic": 7,
      "similarity": 0.7617155119212147
    },
    {
      "doc": 23,
      "topic": 8,
      "similarity": 0.7565880634167447
    },
    {
      "doc": 23,
      "topic": 9,
      "similarity": 0.8011269414916277
    },
    {
      "doc": 23,
      "topic": 10,
      "similarity": 0.7526462044225913
    },
    {
      "doc": 23,
      "topic": 11,
      "similarity": 0.7763814131584436
    },
    {
      "doc": 23,
      "topic": 15,
      "similarity": 0.7558751454840921
    },
    {
      "doc": 23,
      "topic": 16,
      "similarity": 0.7853757794283492
    },
    {
      "doc": 23,
      "topic": 17,
      "similarity": 0.7842722260379723
    },
    {
      "doc": 23,
      "topic": 19,
      "similarity": 0.8114531615414863
    },
    {
      "doc": 23,
      "topic": 20,
      "similarity": 0.7704239365947076
    },
    {
      "doc": 23,
      "topic": 21,
      "similarity": 0.7870708618660767
    },
    {
      "doc": 24,
      "topic": 2,
      "similarity": 0.7566945870335852
    },
    {
      "doc": 24,
      "topic": 3,
      "similarity": 0.778068278042013
    },
    {
      "doc": 24,
      "topic": 5,
      "similarity": 0.7778835360629692
    },
    {
      "doc": 24,
      "topic": 7,
      "similarity": 0.7722576350153522
    },
    {
      "doc": 24,
      "topic": 8,
      "similarity": 0.7728790534461937
    },
    {
      "doc": 24,
      "topic": 9,
      "similarity": 0.7988766860099799
    },
    {
      "doc": 24,
      "topic": 10,
      "similarity": 0.7860478760374735
    },
    {
      "doc": 24,
      "topic": 11,
      "similarity": 0.767988574976955
    },
    {
      "doc": 24,
      "topic": 12,
      "similarity": 0.7528463660816661
    },
    {
      "doc": 24,
      "topic": 13,
      "similarity": 0.7574332655351259
    },
    {
      "doc": 24,
      "topic": 14,
      "similarity": 0.7513449876876266
    },
    {
      "doc": 24,
      "topic": 15,
      "similarity": 0.7896057772800336
    },
    {
      "doc": 24,
      "topic": 16,
      "similarity": 0.7935872649309114
    },
    {
      "doc": 24,
      "topic": 17,
      "similarity": 0.8016489822898961
    },
    {
      "doc": 24,
      "topic": 18,
      "similarity": 0.7873998201389736
    },
    {
      "doc": 24,
      "topic": 19,
      "similarity": 0.8179425610728022
    },
    {
      "doc": 24,
      "topic": 20,
      "similarity": 0.7790856953726252
    },
    {
      "doc": 24,
      "topic": 21,
      "similarity": 0.7885826509597437
    },
    {
      "doc": 24,
      "topic": 22,
      "similarity": 0.7572179174661361
    },
    {
      "doc": 24,
      "topic": 23,
      "similarity": 0.7523704902945978
    },
    {
      "doc": 25,
      "topic": 1,
      "similarity": 0.7672284160345741
    },
    {
      "doc": 25,
      "topic": 2,
      "similarity": 0.7951304654809018
    },
    {
      "doc": 25,
      "topic": 3,
      "similarity": 0.8095311897233164
    },
    {
      "doc": 25,
      "topic": 4,
      "similarity": 0.771423564317431
    },
    {
      "doc": 25,
      "topic": 5,
      "similarity": 0.8283226709159762
    },
    {
      "doc": 25,
      "topic": 7,
      "similarity": 0.7953510268519169
    },
    {
      "doc": 25,
      "topic": 8,
      "similarity": 0.8122612643580175
    },
    {
      "doc": 25,
      "topic": 9,
      "similarity": 0.8435326915404583
    },
    {
      "doc": 25,
      "topic": 10,
      "similarity": 0.8003336705316179
    },
    {
      "doc": 25,
      "topic": 11,
      "similarity": 0.8188588620122743
    },
    {
      "doc": 25,
      "topic": 12,
      "similarity": 0.7595772488150133
    },
    {
      "doc": 25,
      "topic": 13,
      "similarity": 0.7864955771997674
    },
    {
      "doc": 25,
      "topic": 14,
      "similarity": 0.797709753474753
    },
    {
      "doc": 25,
      "topic": 15,
      "similarity": 0.8403309687063928
    },
    {
      "doc": 25,
      "topic": 16,
      "similarity": 0.835104309718475
    },
    {
      "doc": 25,
      "topic": 17,
      "similarity": 0.8372597072285048
    },
    {
      "doc": 25,
      "topic": 18,
      "similarity": 0.7777253709659656
    },
    {
      "doc": 25,
      "topic": 19,
      "similarity": 0.8285507748333049
    },
    {
      "doc": 25,
      "topic": 20,
      "similarity": 0.8006153182712705
    },
    {
      "doc": 25,
      "topic": 21,
      "similarity": 0.8193415211737447
    },
    {
      "doc": 25,
      "topic": 23,
      "similarity": 0.7548205469612529
    },
    {
      "doc": 26,
      "topic": 1,
      "similarity": 0.7750571747068966
    },
    {
      "doc": 26,
      "topic": 2,
      "similarity": 0.8063795255297421
    },
    {
      "doc": 26,
      "topic": 3,
      "similarity": 0.8080351334705027
    },
    {
      "doc": 26,
      "topic": 4,
      "similarity": 0.7676714082197286
    },
    {
      "doc": 26,
      "topic": 5,
      "similarity": 0.8144338390212774
    },
    {
      "doc": 26,
      "topic": 7,
      "similarity": 0.7933003467619606
    },
    {
      "doc": 26,
      "topic": 8,
      "similarity": 0.7911240180419167
    },
    {
      "doc": 26,
      "topic": 9,
      "similarity": 0.8165113909685539
    },
    {
      "doc": 26,
      "topic": 10,
      "similarity": 0.7954522492097824
    },
    {
      "doc": 26,
      "topic": 11,
      "similarity": 0.8290778427258328
    },
    {
      "doc": 26,
      "topic": 13,
      "similarity": 0.7861186488800689
    },
    {
      "doc": 26,
      "topic": 14,
      "similarity": 0.7710453976554368
    },
    {
      "doc": 26,
      "topic": 15,
      "similarity": 0.8249069859380811
    },
    {
      "doc": 26,
      "topic": 16,
      "similarity": 0.87011405958569
    },
    {
      "doc": 26,
      "topic": 17,
      "similarity": 0.8498139898936007
    },
    {
      "doc": 26,
      "topic": 18,
      "similarity": 0.7721943204665522
    },
    {
      "doc": 26,
      "topic": 19,
      "similarity": 0.8199407197267312
    },
    {
      "doc": 26,
      "topic": 20,
      "similarity": 0.7863551223787466
    },
    {
      "doc": 26,
      "topic": 21,
      "similarity": 0.8173170043100854
    },
    {
      "doc": 27,
      "topic": 2,
      "similarity": 0.7664078900281349
    },
    {
      "doc": 27,
      "topic": 3,
      "similarity": 0.8067758989489218
    },
    {
      "doc": 27,
      "topic": 5,
      "similarity": 0.7852122745746859
    },
    {
      "doc": 27,
      "topic": 7,
      "similarity": 0.8128612760909879
    },
    {
      "doc": 27,
      "topic": 8,
      "similarity": 0.8037281503635662
    },
    {
      "doc": 27,
      "topic": 9,
      "similarity": 0.8315263319888805
    },
    {
      "doc": 27,
      "topic": 10,
      "similarity": 0.7828694123856823
    },
    {
      "doc": 27,
      "topic": 11,
      "similarity": 0.7879232373883754
    },
    {
      "doc": 27,
      "topic": 12,
      "similarity": 0.7614123730792584
    },
    {
      "doc": 27,
      "topic": 13,
      "similarity": 0.765908384082917
    },
    {
      "doc": 27,
      "topic": 14,
      "similarity": 0.7508205898133817
    },
    {
      "doc": 27,
      "topic": 15,
      "similarity": 0.7821292316151833
    },
    {
      "doc": 27,
      "topic": 16,
      "similarity": 0.8053241630609733
    },
    {
      "doc": 27,
      "topic": 17,
      "similarity": 0.8043815968126062
    },
    {
      "doc": 27,
      "topic": 18,
      "similarity": 0.777556723479585
    },
    {
      "doc": 27,
      "topic": 19,
      "similarity": 0.797360262473489
    },
    {
      "doc": 27,
      "topic": 20,
      "similarity": 0.7710360105906442
    },
    {
      "doc": 27,
      "topic": 21,
      "similarity": 0.818342320386575
    },
    {
      "doc": 27,
      "topic": 22,
      "similarity": 0.766505863687036
    },
    {
      "doc": 27,
      "topic": 24,
      "similarity": 0.767857085555125
    },
    {
      "doc": 28,
      "topic": 1,
      "similarity": 0.7505959005634774
    },
    {
      "doc": 28,
      "topic": 2,
      "similarity": 0.7525655051070466
    },
    {
      "doc": 28,
      "topic": 3,
      "similarity": 0.8095038583104368
    },
    {
      "doc": 28,
      "topic": 4,
      "similarity": 0.759031836326026
    },
    {
      "doc": 28,
      "topic": 5,
      "similarity": 0.7873033653592908
    },
    {
      "doc": 28,
      "topic": 6,
      "similarity": 0.7624171033608729
    },
    {
      "doc": 28,
      "topic": 7,
      "similarity": 0.7712099397683466
    },
    {
      "doc": 28,
      "topic": 8,
      "similarity": 0.7634934651152316
    },
    {
      "doc": 28,
      "topic": 9,
      "similarity": 0.8223945669389015
    },
    {
      "doc": 28,
      "topic": 10,
      "similarity": 0.7594845966333255
    },
    {
      "doc": 28,
      "topic": 11,
      "similarity": 0.7726490745286528
    },
    {
      "doc": 28,
      "topic": 14,
      "similarity": 0.76018974224512
    },
    {
      "doc": 28,
      "topic": 15,
      "similarity": 0.7653911015109172
    },
    {
      "doc": 28,
      "topic": 16,
      "similarity": 0.8076352778223712
    },
    {
      "doc": 28,
      "topic": 17,
      "similarity": 0.7804073981605123
    },
    {
      "doc": 28,
      "topic": 18,
      "similarity": 0.751852047218596
    },
    {
      "doc": 28,
      "topic": 19,
      "similarity": 0.8244294166138053
    },
    {
      "doc": 28,
      "topic": 20,
      "similarity": 0.7759946229799468
    },
    {
      "doc": 28,
      "topic": 21,
      "similarity": 0.7843748009174466
    },
    {
      "doc": 28,
      "topic": 23,
      "similarity": 0.7796691058026534
    },
    {
      "doc": 29,
      "topic": 1,
      "similarity": 0.7631013573665087
    },
    {
      "doc": 29,
      "topic": 2,
      "similarity": 0.7902282254503222
    },
    {
      "doc": 29,
      "topic": 3,
      "similarity": 0.8046778572869633
    },
    {
      "doc": 29,
      "topic": 4,
      "similarity": 0.7557212356831415
    },
    {
      "doc": 29,
      "topic": 5,
      "similarity": 0.7845931627822391
    },
    {
      "doc": 29,
      "topic": 7,
      "similarity": 0.7766358419231445
    },
    {
      "doc": 29,
      "topic": 8,
      "similarity": 0.7811153584596957
    },
    {
      "doc": 29,
      "topic": 9,
      "similarity": 0.8109618150402692
    },
    {
      "doc": 29,
      "topic": 10,
      "similarity": 0.7876770813583276
    },
    {
      "doc": 29,
      "topic": 11,
      "similarity": 0.7799867933913847
    },
    {
      "doc": 29,
      "topic": 12,
      "similarity": 0.7916975557075486
    },
    {
      "doc": 29,
      "topic": 13,
      "similarity": 0.7865768780027274
    },
    {
      "doc": 29,
      "topic": 14,
      "similarity": 0.7866633128365347
    },
    {
      "doc": 29,
      "topic": 15,
      "similarity": 0.7982101263136517
    },
    {
      "doc": 29,
      "topic": 16,
      "similarity": 0.8107204274513241
    },
    {
      "doc": 29,
      "topic": 17,
      "similarity": 0.8278534380078443
    },
    {
      "doc": 29,
      "topic": 18,
      "similarity": 0.7844746158315454
    },
    {
      "doc": 29,
      "topic": 19,
      "similarity": 0.8255453219075586
    },
    {
      "doc": 29,
      "topic": 20,
      "similarity": 0.8015839630106552
    },
    {
      "doc": 29,
      "topic": 21,
      "similarity": 0.8072801845394273
    },
    {
      "doc": 29,
      "topic": 22,
      "similarity": 0.7549763867202274
    },
    {
      "doc": 30,
      "topic": 1,
      "similarity": 0.7635226301262182
    },
    {
      "doc": 30,
      "topic": 2,
      "similarity": 0.7744328972796682
    },
    {
      "doc": 30,
      "topic": 3,
      "similarity": 0.8105291359400199
    },
    {
      "doc": 30,
      "topic": 5,
      "similarity": 0.7867178025260763
    },
    {
      "doc": 30,
      "topic": 6,
      "similarity": 0.7544442948366019
    },
    {
      "doc": 30,
      "topic": 7,
      "similarity": 0.7865623469624202
    },
    {
      "doc": 30,
      "topic": 8,
      "similarity": 0.7862991539291204
    },
    {
      "doc": 30,
      "topic": 9,
      "similarity": 0.8301383264540121
    },
    {
      "doc": 30,
      "topic": 10,
      "similarity": 0.7884660948632516
    },
    {
      "doc": 30,
      "topic": 11,
      "similarity": 0.7677516410504126
    },
    {
      "doc": 30,
      "topic": 12,
      "similarity": 0.7716859239682295
    },
    {
      "doc": 30,
      "topic": 13,
      "similarity": 0.7725919229998854
    },
    {
      "doc": 30,
      "topic": 14,
      "similarity": 0.7777154650900464
    },
    {
      "doc": 30,
      "topic": 15,
      "similarity": 0.7859355093953103
    },
    {
      "doc": 30,
      "topic": 16,
      "similarity": 0.788268769859271
    },
    {
      "doc": 30,
      "topic": 17,
      "similarity": 0.8016955574830305
    },
    {
      "doc": 30,
      "topic": 18,
      "similarity": 0.7865830015565622
    },
    {
      "doc": 30,
      "topic": 19,
      "similarity": 0.8224865901760935
    },
    {
      "doc": 30,
      "topic": 20,
      "similarity": 0.7864159464951549
    },
    {
      "doc": 30,
      "topic": 21,
      "similarity": 0.7970915518179391
    },
    {
      "doc": 30,
      "topic": 22,
      "similarity": 0.7556182438852072
    },
    {
      "doc": 30,
      "topic": 23,
      "similarity": 0.7686820910967898
    },
    {
      "doc": 31,
      "topic": 2,
      "similarity": 0.7619964216657006
    },
    {
      "doc": 31,
      "topic": 3,
      "similarity": 0.7906660160216277
    },
    {
      "doc": 31,
      "topic": 5,
      "similarity": 0.782570075131406
    },
    {
      "doc": 31,
      "topic": 7,
      "similarity": 0.7667402797931719
    },
    {
      "doc": 31,
      "topic": 8,
      "similarity": 0.7917214567495798
    },
    {
      "doc": 31,
      "topic": 9,
      "similarity": 0.8221016738001556
    },
    {
      "doc": 31,
      "topic": 10,
      "similarity": 0.7872113974268202
    },
    {
      "doc": 31,
      "topic": 11,
      "similarity": 0.771405157025529
    },
    {
      "doc": 31,
      "topic": 13,
      "similarity": 0.7795637960706029
    },
    {
      "doc": 31,
      "topic": 14,
      "similarity": 0.7835934116835997
    },
    {
      "doc": 31,
      "topic": 15,
      "similarity": 0.795679213985828
    },
    {
      "doc": 31,
      "topic": 16,
      "similarity": 0.7836324102792624
    },
    {
      "doc": 31,
      "topic": 17,
      "similarity": 0.7974615723531309
    },
    {
      "doc": 31,
      "topic": 18,
      "similarity": 0.8068515006717212
    },
    {
      "doc": 31,
      "topic": 19,
      "similarity": 0.8290638021352444
    },
    {
      "doc": 31,
      "topic": 20,
      "similarity": 0.7796666830207869
    },
    {
      "doc": 31,
      "topic": 21,
      "similarity": 0.8032926884030553
    },
    {
      "doc": 32,
      "topic": 1,
      "similarity": 0.763543350205344
    },
    {
      "doc": 32,
      "topic": 2,
      "similarity": 0.7603516892792483
    },
    {
      "doc": 32,
      "topic": 3,
      "similarity": 0.8133797394113805
    },
    {
      "doc": 32,
      "topic": 4,
      "similarity": 0.7743840187493317
    },
    {
      "doc": 32,
      "topic": 5,
      "similarity": 0.8122494837492851
    },
    {
      "doc": 32,
      "topic": 6,
      "similarity": 0.7751988986750951
    },
    {
      "doc": 32,
      "topic": 7,
      "similarity": 0.7779608348147068
    },
    {
      "doc": 32,
      "topic": 8,
      "similarity": 0.7731502542409963
    },
    {
      "doc": 32,
      "topic": 9,
      "similarity": 0.8304111452968874
    },
    {
      "doc": 32,
      "topic": 10,
      "similarity": 0.7995992977047521
    },
    {
      "doc": 32,
      "topic": 11,
      "similarity": 0.8000147097824396
    },
    {
      "doc": 32,
      "topic": 13,
      "similarity": 0.774245799553673
    },
    {
      "doc": 32,
      "topic": 14,
      "similarity": 0.7865128030946666
    },
    {
      "doc": 32,
      "topic": 15,
      "similarity": 0.7949315001407107
    },
    {
      "doc": 32,
      "topic": 16,
      "similarity": 0.8108105214250885
    },
    {
      "doc": 32,
      "topic": 17,
      "similarity": 0.7938284768986671
    },
    {
      "doc": 32,
      "topic": 18,
      "similarity": 0.7782856873449818
    },
    {
      "doc": 32,
      "topic": 19,
      "similarity": 0.8250002614635569
    },
    {
      "doc": 32,
      "topic": 20,
      "similarity": 0.7907711507635249
    },
    {
      "doc": 32,
      "topic": 21,
      "similarity": 0.790627463214324
    },
    {
      "doc": 32,
      "topic": 23,
      "similarity": 0.7938767664567503
    },
    {
      "doc": 33,
      "topic": 2,
      "similarity": 0.7657050379301027
    },
    {
      "doc": 33,
      "topic": 3,
      "similarity": 0.7916359243253965
    },
    {
      "doc": 33,
      "topic": 5,
      "similarity": 0.810682085282595
    },
    {
      "doc": 33,
      "topic": 8,
      "similarity": 0.7598379260163709
    },
    {
      "doc": 33,
      "topic": 9,
      "similarity": 0.8046403604965349
    },
    {
      "doc": 33,
      "topic": 10,
      "similarity": 0.7653177984960106
    },
    {
      "doc": 33,
      "topic": 11,
      "similarity": 0.7758052184138137
    },
    {
      "doc": 33,
      "topic": 15,
      "similarity": 0.7606295943975239
    },
    {
      "doc": 33,
      "topic": 16,
      "similarity": 0.7887569170754871
    },
    {
      "doc": 33,
      "topic": 17,
      "similarity": 0.7846120534933329
    },
    {
      "doc": 33,
      "topic": 18,
      "similarity": 0.7679084185062703
    },
    {
      "doc": 33,
      "topic": 19,
      "similarity": 0.8121615764466241
    },
    {
      "doc": 33,
      "topic": 20,
      "similarity": 0.7708430289670445
    },
    {
      "doc": 33,
      "topic": 21,
      "similarity": 0.805301187661591
    },
    {
      "doc": 33,
      "topic": 23,
      "similarity": 0.7733330982911699
    },
    {
      "doc": 33,
      "topic": 24,
      "similarity": 0.7803707455405599
    },
    {
      "doc": 34,
      "topic": 2,
      "similarity": 0.750922080948397
    },
    {
      "doc": 34,
      "topic": 3,
      "similarity": 0.7821261418281691
    },
    {
      "doc": 34,
      "topic": 5,
      "similarity": 0.7693695934194982
    },
    {
      "doc": 34,
      "topic": 7,
      "similarity": 0.75587954066892
    },
    {
      "doc": 34,
      "topic": 8,
      "similarity": 0.7764285846252796
    },
    {
      "doc": 34,
      "topic": 9,
      "similarity": 0.8180691399662664
    },
    {
      "doc": 34,
      "topic": 10,
      "similarity": 0.7686668277973321
    },
    {
      "doc": 34,
      "topic": 11,
      "similarity": 0.7700148228839568
    },
    {
      "doc": 34,
      "topic": 12,
      "similarity": 0.7908520523001586
    },
    {
      "doc": 34,
      "topic": 14,
      "similarity": 0.7543221288237172
    },
    {
      "doc": 34,
      "topic": 15,
      "similarity": 0.7980061437718995
    },
    {
      "doc": 34,
      "topic": 16,
      "similarity": 0.7960814277958924
    },
    {
      "doc": 34,
      "topic": 17,
      "similarity": 0.8019636033183373
    },
    {
      "doc": 34,
      "topic": 18,
      "similarity": 0.774652983624592
    },
    {
      "doc": 34,
      "topic": 19,
      "similarity": 0.8108687057019321
    },
    {
      "doc": 34,
      "topic": 20,
      "similarity": 0.8010467309148763
    },
    {
      "doc": 34,
      "topic": 21,
      "similarity": 0.7968551650206837
    },
    {
      "doc": 34,
      "topic": 23,
      "similarity": 0.7587878462810559
    },
    {
      "doc": 34,
      "topic": 24,
      "similarity": 0.7562850984392103
    },
    {
      "doc": 35,
      "topic": 1,
      "similarity": 0.7547627820745004
    },
    {
      "doc": 35,
      "topic": 2,
      "similarity": 0.7753583967087161
    },
    {
      "doc": 35,
      "topic": 3,
      "similarity": 0.7916930141668879
    },
    {
      "doc": 35,
      "topic": 5,
      "similarity": 0.8120496078879214
    },
    {
      "doc": 35,
      "topic": 7,
      "similarity": 0.7830847205920072
    },
    {
      "doc": 35,
      "topic": 8,
      "similarity": 0.7809222123243832
    },
    {
      "doc": 35,
      "topic": 9,
      "similarity": 0.8343347720424759
    },
    {
      "doc": 35,
      "topic": 10,
      "similarity": 0.7826914384273286
    },
    {
      "doc": 35,
      "topic": 11,
      "similarity": 0.7884556255468382
    },
    {
      "doc": 35,
      "topic": 13,
      "similarity": 0.7688923654375854
    },
    {
      "doc": 35,
      "topic": 14,
      "similarity": 0.7646709088853711
    },
    {
      "doc": 35,
      "topic": 15,
      "similarity": 0.7782351690067751
    },
    {
      "doc": 35,
      "topic": 16,
      "similarity": 0.829108961361018
    },
    {
      "doc": 35,
      "topic": 17,
      "similarity": 0.8147419003575327
    },
    {
      "doc": 35,
      "topic": 18,
      "similarity": 0.7593827665660153
    },
    {
      "doc": 35,
      "topic": 19,
      "similarity": 0.7955833829411233
    },
    {
      "doc": 35,
      "topic": 20,
      "similarity": 0.7862134866878544
    },
    {
      "doc": 35,
      "topic": 21,
      "similarity": 0.8107514180015729
    },
    {
      "doc": 35,
      "topic": 23,
      "similarity": 0.7508177313445131
    },
    {
      "doc": 35,
      "topic": 24,
      "similarity": 0.7648124252725248
    },
    {
      "doc": 36,
      "topic": 3,
      "similarity": 0.7798347072710112
    },
    {
      "doc": 36,
      "topic": 4,
      "similarity": 0.7515178829345445
    },
    {
      "doc": 36,
      "topic": 5,
      "similarity": 0.7793254155636231
    },
    {
      "doc": 36,
      "topic": 7,
      "similarity": 0.784550861332775
    },
    {
      "doc": 36,
      "topic": 8,
      "similarity": 0.7519898749082717
    },
    {
      "doc": 36,
      "topic": 9,
      "similarity": 0.8408412302231615
    },
    {
      "doc": 36,
      "topic": 10,
      "similarity": 0.7651349292861749
    },
    {
      "doc": 36,
      "topic": 11,
      "similarity": 0.775967295045081
    },
    {
      "doc": 36,
      "topic": 13,
      "similarity": 0.7543342410708046
    },
    {
      "doc": 36,
      "topic": 15,
      "similarity": 0.7915177644952007
    },
    {
      "doc": 36,
      "topic": 16,
      "similarity": 0.7946013450789894
    },
    {
      "doc": 36,
      "topic": 17,
      "similarity": 0.7981899637341715
    },
    {
      "doc": 36,
      "topic": 19,
      "similarity": 0.7955354426723297
    },
    {
      "doc": 36,
      "topic": 20,
      "similarity": 0.7849578941108794
    },
    {
      "doc": 36,
      "topic": 21,
      "similarity": 0.8000116097440594
    },
    {
      "doc": 36,
      "topic": 24,
      "similarity": 0.7837952928738016
    },
    {
      "doc": 37,
      "topic": 1,
      "similarity": 0.7582344011670704
    },
    {
      "doc": 37,
      "topic": 2,
      "similarity": 0.7595164510292574
    },
    {
      "doc": 37,
      "topic": 3,
      "similarity": 0.7958519869230871
    },
    {
      "doc": 37,
      "topic": 4,
      "similarity": 0.7626499535213677
    },
    {
      "doc": 37,
      "topic": 5,
      "similarity": 0.7871439325235227
    },
    {
      "doc": 37,
      "topic": 7,
      "similarity": 0.7639761798314759
    },
    {
      "doc": 37,
      "topic": 8,
      "similarity": 0.767536603849241
    },
    {
      "doc": 37,
      "topic": 9,
      "similarity": 0.8221946229804652
    },
    {
      "doc": 37,
      "topic": 10,
      "similarity": 0.7881814356672797
    },
    {
      "doc": 37,
      "topic": 11,
      "similarity": 0.7760037293877196
    },
    {
      "doc": 37,
      "topic": 13,
      "similarity": 0.76786887639335
    },
    {
      "doc": 37,
      "topic": 14,
      "similarity": 0.7810133556470669
    },
    {
      "doc": 37,
      "topic": 15,
      "similarity": 0.7697727188592147
    },
    {
      "doc": 37,
      "topic": 16,
      "similarity": 0.8020319119208266
    },
    {
      "doc": 37,
      "topic": 17,
      "similarity": 0.7796955761546404
    },
    {
      "doc": 37,
      "topic": 19,
      "similarity": 0.8117635278336205
    },
    {
      "doc": 37,
      "topic": 20,
      "similarity": 0.7592719685540491
    },
    {
      "doc": 37,
      "topic": 21,
      "similarity": 0.7895360040338809
    },
    {
      "doc": 37,
      "topic": 23,
      "similarity": 0.7979493400555375
    },
    {
      "doc": 38,
      "topic": 2,
      "similarity": 0.7605820142667478
    },
    {
      "doc": 38,
      "topic": 3,
      "similarity": 0.7872794022947122
    },
    {
      "doc": 38,
      "topic": 5,
      "similarity": 0.7779510903031492
    },
    {
      "doc": 38,
      "topic": 6,
      "similarity": 0.7503765542542558
    },
    {
      "doc": 38,
      "topic": 7,
      "similarity": 0.7764831995667927
    },
    {
      "doc": 38,
      "topic": 9,
      "similarity": 0.8227474526079324
    },
    {
      "doc": 38,
      "topic": 10,
      "similarity": 0.7566432921388225
    },
    {
      "doc": 38,
      "topic": 11,
      "similarity": 0.7639227503539716
    },
    {
      "doc": 38,
      "topic": 14,
      "similarity": 0.7538394780593353
    },
    {
      "doc": 38,
      "topic": 15,
      "similarity": 0.7681940700742435
    },
    {
      "doc": 38,
      "topic": 16,
      "similarity": 0.7852880929930108
    },
    {
      "doc": 38,
      "topic": 17,
      "similarity": 0.779659528004747
    },
    {
      "doc": 38,
      "topic": 19,
      "similarity": 0.8056116640536277
    },
    {
      "doc": 38,
      "topic": 20,
      "similarity": 0.7653336996448146
    },
    {
      "doc": 38,
      "topic": 21,
      "similarity": 0.8011079891574152
    },
    {
      "doc": 38,
      "topic": 23,
      "similarity": 0.7709512517398007
    },
    {
      "doc": 38,
      "topic": 24,
      "similarity": 0.7524260740026338
    },
    {
      "doc": 39,
      "topic": 3,
      "similarity": 0.7979346993023639
    },
    {
      "doc": 39,
      "topic": 5,
      "similarity": 0.7821870923336918
    },
    {
      "doc": 39,
      "topic": 7,
      "similarity": 0.7649378767913937
    },
    {
      "doc": 39,
      "topic": 9,
      "similarity": 0.7941392647322627
    },
    {
      "doc": 39,
      "topic": 11,
      "similarity": 0.756699924244948
    },
    {
      "doc": 39,
      "topic": 13,
      "similarity": 0.7531112437056121
    },
    {
      "doc": 39,
      "topic": 16,
      "similarity": 0.7650751085952776
    },
    {
      "doc": 39,
      "topic": 17,
      "similarity": 0.7567253796837303
    },
    {
      "doc": 39,
      "topic": 19,
      "similarity": 0.7781478532611988
    },
    {
      "doc": 39,
      "topic": 20,
      "similarity": 0.7710208416871849
    },
    {
      "doc": 39,
      "topic": 21,
      "similarity": 0.7761171407291606
    },
    {
      "doc": 39,
      "topic": 24,
      "similarity": 0.7514391080472974
    },
    {
      "doc": 40,
      "topic": 2,
      "similarity": 0.7701828094142205
    },
    {
      "doc": 40,
      "topic": 3,
      "similarity": 0.7955240013637535
    },
    {
      "doc": 40,
      "topic": 4,
      "similarity": 0.751586799598689
    },
    {
      "doc": 40,
      "topic": 5,
      "similarity": 0.8012640582804872
    },
    {
      "doc": 40,
      "topic": 7,
      "similarity": 0.7792996183566234
    },
    {
      "doc": 40,
      "topic": 8,
      "similarity": 0.7803370637344361
    },
    {
      "doc": 40,
      "topic": 9,
      "similarity": 0.837832587824205
    },
    {
      "doc": 40,
      "topic": 10,
      "similarity": 0.7777842130930188
    },
    {
      "doc": 40,
      "topic": 11,
      "similarity": 0.7742794950663072
    },
    {
      "doc": 40,
      "topic": 13,
      "similarity": 0.795019533885143
    },
    {
      "doc": 40,
      "topic": 14,
      "similarity": 0.770995012910655
    },
    {
      "doc": 40,
      "topic": 15,
      "similarity": 0.7741994028511744
    },
    {
      "doc": 40,
      "topic": 16,
      "similarity": 0.8216467590820762
    },
    {
      "doc": 40,
      "topic": 17,
      "similarity": 0.7961355441171475
    },
    {
      "doc": 40,
      "topic": 18,
      "similarity": 0.7734767799810309
    },
    {
      "doc": 40,
      "topic": 19,
      "similarity": 0.811798371310776
    },
    {
      "doc": 40,
      "topic": 20,
      "similarity": 0.7905506691037858
    },
    {
      "doc": 40,
      "topic": 21,
      "similarity": 0.8120364739542085
    },
    {
      "doc": 40,
      "topic": 22,
      "similarity": 0.7519363869662999
    },
    {
      "doc": 41,
      "topic": 3,
      "similarity": 0.757225869961762
    },
    {
      "doc": 41,
      "topic": 5,
      "similarity": 0.7643215257028734
    },
    {
      "doc": 41,
      "topic": 9,
      "similarity": 0.7953416335100266
    },
    {
      "doc": 41,
      "topic": 11,
      "similarity": 0.7703462671512726
    },
    {
      "doc": 41,
      "topic": 15,
      "similarity": 0.7505120325125384
    },
    {
      "doc": 41,
      "topic": 16,
      "similarity": 0.7770768405676932
    },
    {
      "doc": 41,
      "topic": 17,
      "similarity": 0.7827629886846899
    },
    {
      "doc": 41,
      "topic": 19,
      "similarity": 0.7565520724749618
    },
    {
      "doc": 41,
      "topic": 20,
      "similarity": 0.7570527738036145
    },
    {
      "doc": 41,
      "topic": 21,
      "similarity": 0.7870007822635934
    },
    {
      "doc": 42,
      "topic": 2,
      "similarity": 0.7765415001133531
    },
    {
      "doc": 42,
      "topic": 3,
      "similarity": 0.7886505187318263
    },
    {
      "doc": 42,
      "topic": 4,
      "similarity": 0.7516135084059272
    },
    {
      "doc": 42,
      "topic": 5,
      "similarity": 0.8124220896210242
    },
    {
      "doc": 42,
      "topic": 7,
      "similarity": 0.78236861955702
    },
    {
      "doc": 42,
      "topic": 8,
      "similarity": 0.7824560799171401
    },
    {
      "doc": 42,
      "topic": 9,
      "similarity": 0.8244635375414139
    },
    {
      "doc": 42,
      "topic": 10,
      "similarity": 0.77188596067525
    },
    {
      "doc": 42,
      "topic": 11,
      "similarity": 0.8083327431690428
    },
    {
      "doc": 42,
      "topic": 13,
      "similarity": 0.7723282937964395
    },
    {
      "doc": 42,
      "topic": 14,
      "similarity": 0.7827260988154737
    },
    {
      "doc": 42,
      "topic": 15,
      "similarity": 0.7850041194361472
    },
    {
      "doc": 42,
      "topic": 16,
      "similarity": 0.8061600395164277
    },
    {
      "doc": 42,
      "topic": 17,
      "similarity": 0.8070714399425085
    },
    {
      "doc": 42,
      "topic": 18,
      "similarity": 0.7654190503424066
    },
    {
      "doc": 42,
      "topic": 19,
      "similarity": 0.8130333178702293
    },
    {
      "doc": 42,
      "topic": 20,
      "similarity": 0.7821490100992069
    },
    {
      "doc": 42,
      "topic": 21,
      "similarity": 0.8096066231464282
    },
    {
      "doc": 42,
      "topic": 24,
      "similarity": 0.7513310008122693
    },
    {
      "doc": 43,
      "topic": 2,
      "similarity": 0.7617847621417034
    },
    {
      "doc": 43,
      "topic": 3,
      "similarity": 0.7867590262352308
    },
    {
      "doc": 43,
      "topic": 5,
      "similarity": 0.7878998939551723
    },
    {
      "doc": 43,
      "topic": 7,
      "similarity": 0.7819310435955269
    },
    {
      "doc": 43,
      "topic": 8,
      "similarity": 0.8064605686873324
    },
    {
      "doc": 43,
      "topic": 9,
      "similarity": 0.8323155218992364
    },
    {
      "doc": 43,
      "topic": 10,
      "similarity": 0.784692793825828
    },
    {
      "doc": 43,
      "topic": 11,
      "similarity": 0.7755405874844491
    },
    {
      "doc": 43,
      "topic": 12,
      "similarity": 0.7788833219412297
    },
    {
      "doc": 43,
      "topic": 13,
      "similarity": 0.7962038106875655
    },
    {
      "doc": 43,
      "topic": 14,
      "similarity": 0.769048123691167
    },
    {
      "doc": 43,
      "topic": 15,
      "similarity": 0.7809848556259591
    },
    {
      "doc": 43,
      "topic": 16,
      "similarity": 0.7998293991831794
    },
    {
      "doc": 43,
      "topic": 17,
      "similarity": 0.7868344447332112
    },
    {
      "doc": 43,
      "topic": 18,
      "similarity": 0.77347383227204
    },
    {
      "doc": 43,
      "topic": 19,
      "similarity": 0.8116281826994619
    },
    {
      "doc": 43,
      "topic": 20,
      "similarity": 0.8106217192344711
    },
    {
      "doc": 43,
      "topic": 21,
      "similarity": 0.7919564817349263
    },
    {
      "doc": 43,
      "topic": 22,
      "similarity": 0.7981508077206424
    },
    {
      "doc": 43,
      "topic": 23,
      "similarity": 0.7520184813443135
    },
    {
      "doc": 43,
      "topic": 24,
      "similarity": 0.7542917631844048
    },
    {
      "doc": 44,
      "topic": 2,
      "similarity": 0.7609885125783753
    },
    {
      "doc": 44,
      "topic": 3,
      "similarity": 0.7914744236035318
    },
    {
      "doc": 44,
      "topic": 5,
      "similarity": 0.8079819592544389
    },
    {
      "doc": 44,
      "topic": 7,
      "similarity": 0.7721049620005237
    },
    {
      "doc": 44,
      "topic": 8,
      "similarity": 0.7767060852144592
    },
    {
      "doc": 44,
      "topic": 9,
      "similarity": 0.8353444177154615
    },
    {
      "doc": 44,
      "topic": 10,
      "similarity": 0.78052412088204
    },
    {
      "doc": 44,
      "topic": 11,
      "similarity": 0.7733327437411567
    },
    {
      "doc": 44,
      "topic": 13,
      "similarity": 0.7641060619732654
    },
    {
      "doc": 44,
      "topic": 14,
      "similarity": 0.7573094948727388
    },
    {
      "doc": 44,
      "topic": 15,
      "similarity": 0.7712289563154544
    },
    {
      "doc": 44,
      "topic": 16,
      "similarity": 0.7999818433549214
    },
    {
      "doc": 44,
      "topic": 17,
      "similarity": 0.7884076047752879
    },
    {
      "doc": 44,
      "topic": 18,
      "similarity": 0.7591558136455443
    },
    {
      "doc": 44,
      "topic": 19,
      "similarity": 0.781380620060902
    },
    {
      "doc": 44,
      "topic": 20,
      "similarity": 0.8075149422307006
    },
    {
      "doc": 44,
      "topic": 21,
      "similarity": 0.8119337693032218
    },
    {
      "doc": 44,
      "topic": 23,
      "similarity": 0.7693582747134323
    },
    {
      "doc": 44,
      "topic": 24,
      "similarity": 0.7695002165793203
    },
    {
      "doc": 45,
      "topic": 3,
      "similarity": 0.7731820409586384
    },
    {
      "doc": 45,
      "topic": 5,
      "similarity": 0.7826838277684178
    },
    {
      "doc": 45,
      "topic": 7,
      "similarity": 0.7862615520631657
    },
    {
      "doc": 45,
      "topic": 8,
      "similarity": 0.7952541073240921
    },
    {
      "doc": 45,
      "topic": 9,
      "similarity": 0.8161667976483783
    },
    {
      "doc": 45,
      "topic": 10,
      "similarity": 0.7692719878411506
    },
    {
      "doc": 45,
      "topic": 11,
      "similarity": 0.7581193907999327
    },
    {
      "doc": 45,
      "topic": 15,
      "similarity": 0.7632326518051423
    },
    {
      "doc": 45,
      "topic": 16,
      "similarity": 0.7796440747444247
    },
    {
      "doc": 45,
      "topic": 17,
      "similarity": 0.7691307320618734
    },
    {
      "doc": 45,
      "topic": 19,
      "similarity": 0.7598100721845593
    },
    {
      "doc": 45,
      "topic": 21,
      "similarity": 0.7703668164555103
    },
    {
      "doc": 45,
      "topic": 24,
      "similarity": 0.7977649216134141
    },
    {
      "doc": 46,
      "topic": 3,
      "similarity": 0.7799431644828403
    },
    {
      "doc": 46,
      "topic": 5,
      "similarity": 0.7822169251398837
    },
    {
      "doc": 46,
      "topic": 7,
      "similarity": 0.7586679075386974
    },
    {
      "doc": 46,
      "topic": 8,
      "similarity": 0.7604333127780685
    },
    {
      "doc": 46,
      "topic": 9,
      "similarity": 0.8123272820693497
    },
    {
      "doc": 46,
      "topic": 10,
      "similarity": 0.7684693649460668
    },
    {
      "doc": 46,
      "topic": 11,
      "similarity": 0.7603927730881123
    },
    {
      "doc": 46,
      "topic": 14,
      "similarity": 0.7591827385324126
    },
    {
      "doc": 46,
      "topic": 15,
      "similarity": 0.7715269229821728
    },
    {
      "doc": 46,
      "topic": 16,
      "similarity": 0.7831515884202457
    },
    {
      "doc": 46,
      "topic": 17,
      "similarity": 0.807489068547766
    },
    {
      "doc": 46,
      "topic": 18,
      "similarity": 0.7520310256293832
    },
    {
      "doc": 46,
      "topic": 19,
      "similarity": 0.7802986988521534
    },
    {
      "doc": 46,
      "topic": 20,
      "similarity": 0.7574319087266482
    },
    {
      "doc": 46,
      "topic": 21,
      "similarity": 0.8237317410816013
    },
    {
      "doc": 47,
      "topic": 2,
      "similarity": 0.7719973155490567
    },
    {
      "doc": 47,
      "topic": 3,
      "similarity": 0.8097887019962168
    },
    {
      "doc": 47,
      "topic": 5,
      "similarity": 0.8063735761465621
    },
    {
      "doc": 47,
      "topic": 7,
      "similarity": 0.7773265865956693
    },
    {
      "doc": 47,
      "topic": 8,
      "similarity": 0.7723996422889479
    },
    {
      "doc": 47,
      "topic": 9,
      "similarity": 0.8325270665551071
    },
    {
      "doc": 47,
      "topic": 10,
      "similarity": 0.7714277898304762
    },
    {
      "doc": 47,
      "topic": 11,
      "similarity": 0.7815393750799431
    },
    {
      "doc": 47,
      "topic": 13,
      "similarity": 0.7559784226452253
    },
    {
      "doc": 47,
      "topic": 14,
      "similarity": 0.7721142502388326
    },
    {
      "doc": 47,
      "topic": 15,
      "similarity": 0.7823100656786004
    },
    {
      "doc": 47,
      "topic": 16,
      "similarity": 0.7959297747551732
    },
    {
      "doc": 47,
      "topic": 17,
      "similarity": 0.8149061835195861
    },
    {
      "doc": 47,
      "topic": 18,
      "similarity": 0.7585501507554191
    },
    {
      "doc": 47,
      "topic": 19,
      "similarity": 0.7951212111395367
    },
    {
      "doc": 47,
      "topic": 20,
      "similarity": 0.7673298340601891
    },
    {
      "doc": 47,
      "topic": 21,
      "similarity": 0.8131748901166957
    },
    {
      "doc": 47,
      "topic": 24,
      "similarity": 0.7811224953796714
    },
    {
      "doc": 48,
      "topic": 2,
      "similarity": 0.7698097962395902
    },
    {
      "doc": 48,
      "topic": 3,
      "similarity": 0.8185181180691294
    },
    {
      "doc": 48,
      "topic": 5,
      "similarity": 0.7852809489170509
    },
    {
      "doc": 48,
      "topic": 7,
      "similarity": 0.7846752998200931
    },
    {
      "doc": 48,
      "topic": 8,
      "similarity": 0.7664247766876173
    },
    {
      "doc": 48,
      "topic": 9,
      "similarity": 0.8388035800900415
    },
    {
      "doc": 48,
      "topic": 10,
      "similarity": 0.7728812831539806
    },
    {
      "doc": 48,
      "topic": 11,
      "similarity": 0.808045177089753
    },
    {
      "doc": 48,
      "topic": 12,
      "similarity": 0.7553973918839463
    },
    {
      "doc": 48,
      "topic": 13,
      "similarity": 0.7532526955637368
    },
    {
      "doc": 48,
      "topic": 14,
      "similarity": 0.7600382166685513
    },
    {
      "doc": 48,
      "topic": 15,
      "similarity": 0.7812923790872695
    },
    {
      "doc": 48,
      "topic": 16,
      "similarity": 0.7892972729370586
    },
    {
      "doc": 48,
      "topic": 17,
      "similarity": 0.7904475134212935
    },
    {
      "doc": 48,
      "topic": 18,
      "similarity": 0.792143808892
    },
    {
      "doc": 48,
      "topic": 19,
      "similarity": 0.8190720152624037
    },
    {
      "doc": 48,
      "topic": 20,
      "similarity": 0.8012401324631702
    },
    {
      "doc": 48,
      "topic": 21,
      "similarity": 0.8002969104216633
    },
    {
      "doc": 48,
      "topic": 23,
      "similarity": 0.7576252820116353
    },
    {
      "doc": 48,
      "topic": 24,
      "similarity": 0.7671118981283737
    },
    {
      "doc": 49,
      "topic": 3,
      "similarity": 0.7792806689438332
    },
    {
      "doc": 49,
      "topic": 5,
      "similarity": 0.7845695247077749
    },
    {
      "doc": 49,
      "topic": 7,
      "similarity": 0.7561345173139049
    },
    {
      "doc": 49,
      "topic": 8,
      "similarity": 0.7538107886478419
    },
    {
      "doc": 49,
      "topic": 9,
      "similarity": 0.8099612518104685
    },
    {
      "doc": 49,
      "topic": 10,
      "similarity": 0.755889138025756
    },
    {
      "doc": 49,
      "topic": 11,
      "similarity": 0.7667781893747532
    },
    {
      "doc": 49,
      "topic": 15,
      "similarity": 0.7712184995116648
    },
    {
      "doc": 49,
      "topic": 16,
      "similarity": 0.8037752619477179
    },
    {
      "doc": 49,
      "topic": 17,
      "similarity": 0.8367643640535516
    },
    {
      "doc": 49,
      "topic": 19,
      "similarity": 0.7910454166158789
    },
    {
      "doc": 49,
      "topic": 20,
      "similarity": 0.7760753519224663
    },
    {
      "doc": 49,
      "topic": 21,
      "similarity": 0.8271834355312508
    },
    {
      "doc": 50,
      "topic": 0,
      "similarity": 0.7500575021324379
    },
    {
      "doc": 50,
      "topic": 2,
      "similarity": 0.7642744388012813
    },
    {
      "doc": 50,
      "topic": 3,
      "similarity": 0.7877845326939097
    },
    {
      "doc": 50,
      "topic": 5,
      "similarity": 0.7817964317326157
    },
    {
      "doc": 50,
      "topic": 7,
      "similarity": 0.7563904996258884
    },
    {
      "doc": 50,
      "topic": 8,
      "similarity": 0.7701124955092012
    },
    {
      "doc": 50,
      "topic": 9,
      "similarity": 0.8137763947021402
    },
    {
      "doc": 50,
      "topic": 10,
      "similarity": 0.7649231500536497
    },
    {
      "doc": 50,
      "topic": 11,
      "similarity": 0.8153812054272466
    },
    {
      "doc": 50,
      "topic": 12,
      "similarity": 0.798062971324084
    },
    {
      "doc": 50,
      "topic": 13,
      "similarity": 0.7513622104804059
    },
    {
      "doc": 50,
      "topic": 14,
      "similarity": 0.7572909229709881
    },
    {
      "doc": 50,
      "topic": 15,
      "similarity": 0.7943415817563972
    },
    {
      "doc": 50,
      "topic": 16,
      "similarity": 0.8090556773705796
    },
    {
      "doc": 50,
      "topic": 17,
      "similarity": 0.8135466909966963
    },
    {
      "doc": 50,
      "topic": 18,
      "similarity": 0.7676666227604507
    },
    {
      "doc": 50,
      "topic": 19,
      "similarity": 0.7969958125798526
    },
    {
      "doc": 50,
      "topic": 20,
      "similarity": 0.7960950916071263
    },
    {
      "doc": 50,
      "topic": 21,
      "similarity": 0.7885217238421265
    },
    {
      "doc": 50,
      "topic": 23,
      "similarity": 0.7507260452682122
    },
    {
      "doc": 50,
      "topic": 24,
      "similarity": 0.7604586262068336
    },
    {
      "doc": 51,
      "topic": 2,
      "similarity": 0.7532631738957063
    },
    {
      "doc": 51,
      "topic": 3,
      "similarity": 0.7980619209675782
    },
    {
      "doc": 51,
      "topic": 5,
      "similarity": 0.806082909436682
    },
    {
      "doc": 51,
      "topic": 6,
      "similarity": 0.7799263134827087
    },
    {
      "doc": 51,
      "topic": 7,
      "similarity": 0.7624433049625172
    },
    {
      "doc": 51,
      "topic": 8,
      "similarity": 0.7955478820736653
    },
    {
      "doc": 51,
      "topic": 9,
      "similarity": 0.8179435931897052
    },
    {
      "doc": 51,
      "topic": 10,
      "similarity": 0.7697907836167085
    },
    {
      "doc": 51,
      "topic": 11,
      "similarity": 0.7725402251651796
    },
    {
      "doc": 51,
      "topic": 12,
      "similarity": 0.7979380919030226
    },
    {
      "doc": 51,
      "topic": 13,
      "similarity": 0.7779981365931082
    },
    {
      "doc": 51,
      "topic": 14,
      "similarity": 0.75862901475271
    },
    {
      "doc": 51,
      "topic": 15,
      "similarity": 0.7862968376710395
    },
    {
      "doc": 51,
      "topic": 16,
      "similarity": 0.8052521261257586
    },
    {
      "doc": 51,
      "topic": 17,
      "similarity": 0.8099145242202214
    },
    {
      "doc": 51,
      "topic": 18,
      "similarity": 0.7731999498100042
    },
    {
      "doc": 51,
      "topic": 19,
      "similarity": 0.8157518182123095
    },
    {
      "doc": 51,
      "topic": 20,
      "similarity": 0.7799673800226389
    },
    {
      "doc": 51,
      "topic": 21,
      "similarity": 0.804883477904053
    },
    {
      "doc": 51,
      "topic": 23,
      "similarity": 0.7812391029860919
    },
    {
      "doc": 51,
      "topic": 24,
      "similarity": 0.7842045249200821
    },
    {
      "doc": 52,
      "topic": 2,
      "similarity": 0.7570182991365753
    },
    {
      "doc": 52,
      "topic": 3,
      "similarity": 0.782512280207226
    },
    {
      "doc": 52,
      "topic": 5,
      "similarity": 0.795833737036209
    },
    {
      "doc": 52,
      "topic": 7,
      "similarity": 0.7537340079841641
    },
    {
      "doc": 52,
      "topic": 8,
      "similarity": 0.7564972904388736
    },
    {
      "doc": 52,
      "topic": 9,
      "similarity": 0.8139166948354881
    },
    {
      "doc": 52,
      "topic": 10,
      "similarity": 0.7706204983281639
    },
    {
      "doc": 52,
      "topic": 11,
      "similarity": 0.7875861206314594
    },
    {
      "doc": 52,
      "topic": 13,
      "similarity": 0.7566971678420154
    },
    {
      "doc": 52,
      "topic": 14,
      "similarity": 0.756197716281676
    },
    {
      "doc": 52,
      "topic": 15,
      "similarity": 0.7776580088874576
    },
    {
      "doc": 52,
      "topic": 16,
      "similarity": 0.7944870355045026
    },
    {
      "doc": 52,
      "topic": 17,
      "similarity": 0.8075757553254338
    },
    {
      "doc": 52,
      "topic": 18,
      "similarity": 0.7520643972145776
    },
    {
      "doc": 52,
      "topic": 19,
      "similarity": 0.7896737656626247
    },
    {
      "doc": 52,
      "topic": 20,
      "similarity": 0.7937895266675702
    },
    {
      "doc": 52,
      "topic": 21,
      "similarity": 0.86846510159089
    },
    {
      "doc": 52,
      "topic": 23,
      "similarity": 0.7543786805927774
    },
    {
      "doc": 52,
      "topic": 24,
      "similarity": 0.7788024409855367
    },
    {
      "doc": 53,
      "topic": 2,
      "similarity": 0.7666099393705015
    },
    {
      "doc": 53,
      "topic": 3,
      "similarity": 0.7982910300285888
    },
    {
      "doc": 53,
      "topic": 5,
      "similarity": 0.7895361057543022
    },
    {
      "doc": 53,
      "topic": 7,
      "similarity": 0.7815870112892745
    },
    {
      "doc": 53,
      "topic": 8,
      "similarity": 0.7782417066681231
    },
    {
      "doc": 53,
      "topic": 9,
      "similarity": 0.8323793061948053
    },
    {
      "doc": 53,
      "topic": 10,
      "similarity": 0.7875084911690655
    },
    {
      "doc": 53,
      "topic": 11,
      "similarity": 0.834782058106516
    },
    {
      "doc": 53,
      "topic": 13,
      "similarity": 0.7688488649303419
    },
    {
      "doc": 53,
      "topic": 14,
      "similarity": 0.7667719382066205
    },
    {
      "doc": 53,
      "topic": 15,
      "similarity": 0.7969433555766761
    },
    {
      "doc": 53,
      "topic": 16,
      "similarity": 0.8106565602033357
    },
    {
      "doc": 53,
      "topic": 17,
      "similarity": 0.8126644147040211
    },
    {
      "doc": 53,
      "topic": 18,
      "similarity": 0.7963273705431798
    },
    {
      "doc": 53,
      "topic": 19,
      "similarity": 0.8160708388981508
    },
    {
      "doc": 53,
      "topic": 20,
      "similarity": 0.7814090865368691
    },
    {
      "doc": 53,
      "topic": 21,
      "similarity": 0.8659116060789546
    },
    {
      "doc": 54,
      "topic": 2,
      "similarity": 0.7625072069213785
    },
    {
      "doc": 54,
      "topic": 3,
      "similarity": 0.7918022340051974
    },
    {
      "doc": 54,
      "topic": 5,
      "similarity": 0.8109663154970314
    },
    {
      "doc": 54,
      "topic": 7,
      "similarity": 0.7699057234801565
    },
    {
      "doc": 54,
      "topic": 8,
      "similarity": 0.7754073996391012
    },
    {
      "doc": 54,
      "topic": 9,
      "similarity": 0.8087077226907643
    },
    {
      "doc": 54,
      "topic": 10,
      "similarity": 0.7711044350288248
    },
    {
      "doc": 54,
      "topic": 11,
      "similarity": 0.7773754027467998
    },
    {
      "doc": 54,
      "topic": 13,
      "similarity": 0.7610338262155492
    },
    {
      "doc": 54,
      "topic": 14,
      "similarity": 0.7609594659657455
    },
    {
      "doc": 54,
      "topic": 15,
      "similarity": 0.7685056785250037
    },
    {
      "doc": 54,
      "topic": 16,
      "similarity": 0.8011402495688071
    },
    {
      "doc": 54,
      "topic": 17,
      "similarity": 0.7959324525455553
    },
    {
      "doc": 54,
      "topic": 18,
      "similarity": 0.7619854987908031
    },
    {
      "doc": 54,
      "topic": 19,
      "similarity": 0.8299814016531133
    },
    {
      "doc": 54,
      "topic": 20,
      "similarity": 0.8124654924459763
    },
    {
      "doc": 54,
      "topic": 21,
      "similarity": 0.7933483487592818
    },
    {
      "doc": 55,
      "topic": 3,
      "similarity": 0.7532622221222622
    },
    {
      "doc": 55,
      "topic": 5,
      "similarity": 0.751805000403946
    },
    {
      "doc": 55,
      "topic": 8,
      "similarity": 0.7507603448814625
    },
    {
      "doc": 55,
      "topic": 9,
      "similarity": 0.7794361470402739
    },
    {
      "doc": 55,
      "topic": 11,
      "similarity": 0.7768486658095174
    },
    {
      "doc": 55,
      "topic": 12,
      "similarity": 0.761672510194889
    },
    {
      "doc": 55,
      "topic": 15,
      "similarity": 0.7772840230416659
    },
    {
      "doc": 55,
      "topic": 16,
      "similarity": 0.763140791020784
    },
    {
      "doc": 55,
      "topic": 17,
      "similarity": 0.7671285869504326
    },
    {
      "doc": 55,
      "topic": 18,
      "similarity": 0.7650360782969101
    },
    {
      "doc": 55,
      "topic": 19,
      "similarity": 0.7708789486666248
    },
    {
      "doc": 55,
      "topic": 20,
      "similarity": 0.7692539858493237
    },
    {
      "doc": 56,
      "topic": 2,
      "similarity": 0.7680397084556245
    },
    {
      "doc": 56,
      "topic": 3,
      "similarity": 0.7945966670305898
    },
    {
      "doc": 56,
      "topic": 4,
      "similarity": 0.7594051491281206
    },
    {
      "doc": 56,
      "topic": 5,
      "similarity": 0.8194789822671698
    },
    {
      "doc": 56,
      "topic": 7,
      "similarity": 0.7761877486545078
    },
    {
      "doc": 56,
      "topic": 8,
      "similarity": 0.791192817299285
    },
    {
      "doc": 56,
      "topic": 9,
      "similarity": 0.822815650865528
    },
    {
      "doc": 56,
      "topic": 10,
      "similarity": 0.8007107251246426
    },
    {
      "doc": 56,
      "topic": 11,
      "similarity": 0.7895836924143229
    },
    {
      "doc": 56,
      "topic": 13,
      "similarity": 0.7725664887816839
    },
    {
      "doc": 56,
      "topic": 14,
      "similarity": 0.7788186219691509
    },
    {
      "doc": 56,
      "topic": 15,
      "similarity": 0.7776503303831355
    },
    {
      "doc": 56,
      "topic": 16,
      "similarity": 0.8104774008446158
    },
    {
      "doc": 56,
      "topic": 17,
      "similarity": 0.8026410962263022
    },
    {
      "doc": 56,
      "topic": 18,
      "similarity": 0.773521209617455
    },
    {
      "doc": 56,
      "topic": 19,
      "similarity": 0.8061427372240675
    },
    {
      "doc": 56,
      "topic": 20,
      "similarity": 0.8128478118101101
    },
    {
      "doc": 56,
      "topic": 21,
      "similarity": 0.802377768215871
    },
    {
      "doc": 56,
      "topic": 23,
      "similarity": 0.752652682056747
    },
    {
      "doc": 56,
      "topic": 24,
      "similarity": 0.7682088492555417
    },
    {
      "doc": 57,
      "topic": 2,
      "similarity": 0.7744572508010005
    },
    {
      "doc": 57,
      "topic": 3,
      "similarity": 0.7889062764579088
    },
    {
      "doc": 57,
      "topic": 5,
      "similarity": 0.7982682480146499
    },
    {
      "doc": 57,
      "topic": 7,
      "similarity": 0.755311899565888
    },
    {
      "doc": 57,
      "topic": 8,
      "similarity": 0.755379065817721
    },
    {
      "doc": 57,
      "topic": 9,
      "similarity": 0.8022681769811851
    },
    {
      "doc": 57,
      "topic": 10,
      "similarity": 0.8003039295141577
    },
    {
      "doc": 57,
      "topic": 11,
      "similarity": 0.7668925375074283
    },
    {
      "doc": 57,
      "topic": 13,
      "similarity": 0.7511667555664066
    },
    {
      "doc": 57,
      "topic": 14,
      "similarity": 0.7586161664826911
    },
    {
      "doc": 57,
      "topic": 15,
      "similarity": 0.7654188222676699
    },
    {
      "doc": 57,
      "topic": 16,
      "similarity": 0.798930642283304
    },
    {
      "doc": 57,
      "topic": 17,
      "similarity": 0.8030164812780223
    },
    {
      "doc": 57,
      "topic": 18,
      "similarity": 0.7767938998851999
    },
    {
      "doc": 57,
      "topic": 19,
      "similarity": 0.8018276441076629
    },
    {
      "doc": 57,
      "topic": 20,
      "similarity": 0.8031915378303718
    },
    {
      "doc": 57,
      "topic": 21,
      "similarity": 0.8000706345820361
    },
    {
      "doc": 57,
      "topic": 23,
      "similarity": 0.7626299419803612
    },
    {
      "doc": 58,
      "topic": 1,
      "similarity": 0.7581903311081061
    },
    {
      "doc": 58,
      "topic": 2,
      "similarity": 0.7728223879422673
    },
    {
      "doc": 58,
      "topic": 3,
      "similarity": 0.8040249557048914
    },
    {
      "doc": 58,
      "topic": 4,
      "similarity": 0.7702202835687668
    },
    {
      "doc": 58,
      "topic": 5,
      "similarity": 0.7945905914857291
    },
    {
      "doc": 58,
      "topic": 7,
      "similarity": 0.7992604316883961
    },
    {
      "doc": 58,
      "topic": 8,
      "similarity": 0.7968097770055401
    },
    {
      "doc": 58,
      "topic": 9,
      "similarity": 0.8574038419274562
    },
    {
      "doc": 58,
      "topic": 10,
      "similarity": 0.7713081108087527
    },
    {
      "doc": 58,
      "topic": 11,
      "similarity": 0.7859950588219093
    },
    {
      "doc": 58,
      "topic": 13,
      "similarity": 0.7791859371182803
    },
    {
      "doc": 58,
      "topic": 14,
      "similarity": 0.7738162255945867
    },
    {
      "doc": 58,
      "topic": 15,
      "similarity": 0.7911306575517598
    },
    {
      "doc": 58,
      "topic": 16,
      "similarity": 0.8099147987290874
    },
    {
      "doc": 58,
      "topic": 17,
      "similarity": 0.8122368058636861
    },
    {
      "doc": 58,
      "topic": 19,
      "similarity": 0.7925946734296095
    },
    {
      "doc": 58,
      "topic": 20,
      "similarity": 0.7831965530606553
    },
    {
      "doc": 58,
      "topic": 21,
      "similarity": 0.8245742373141154
    },
    {
      "doc": 58,
      "topic": 23,
      "similarity": 0.7543128570502586
    },
    {
      "doc": 58,
      "topic": 24,
      "similarity": 0.7802016628170707
    },
    {
      "doc": 59,
      "topic": 3,
      "similarity": 0.7881520760576297
    },
    {
      "doc": 59,
      "topic": 4,
      "similarity": 0.7640988558422608
    },
    {
      "doc": 59,
      "topic": 5,
      "similarity": 0.7783669795583698
    },
    {
      "doc": 59,
      "topic": 7,
      "similarity": 0.7723642587420849
    },
    {
      "doc": 59,
      "topic": 9,
      "similarity": 0.7939943395090571
    },
    {
      "doc": 59,
      "topic": 10,
      "similarity": 0.7550791028459599
    },
    {
      "doc": 59,
      "topic": 11,
      "similarity": 0.8238253728834163
    },
    {
      "doc": 59,
      "topic": 16,
      "similarity": 0.7887386558307293
    },
    {
      "doc": 59,
      "topic": 17,
      "similarity": 0.782735481774389
    },
    {
      "doc": 59,
      "topic": 19,
      "similarity": 0.7839994771380202
    },
    {
      "doc": 59,
      "topic": 20,
      "similarity": 0.7575393330054928
    },
    {
      "doc": 59,
      "topic": 21,
      "similarity": 0.7799648092065574
    },
    {
      "doc": 59,
      "topic": 24,
      "similarity": 0.7787114262423195
    },
    {
      "doc": 60,
      "topic": 3,
      "similarity": 0.7847408211825498
    },
    {
      "doc": 60,
      "topic": 5,
      "similarity": 0.7580586992167775
    },
    {
      "doc": 60,
      "topic": 7,
      "similarity": 0.7512297553487121
    },
    {
      "doc": 60,
      "topic": 8,
      "similarity": 0.7510451066761553
    },
    {
      "doc": 60,
      "topic": 9,
      "similarity": 0.8024311669059999
    },
    {
      "doc": 60,
      "topic": 10,
      "similarity": 0.7977111560432306
    },
    {
      "doc": 60,
      "topic": 11,
      "similarity": 0.7523256915673779
    },
    {
      "doc": 60,
      "topic": 16,
      "similarity": 0.7599958498349729
    },
    {
      "doc": 60,
      "topic": 18,
      "similarity": 0.7636331552679624
    },
    {
      "doc": 60,
      "topic": 19,
      "similarity": 0.7776595898212237
    },
    {
      "doc": 60,
      "topic": 20,
      "similarity": 0.765348055609419
    },
    {
      "doc": 60,
      "topic": 21,
      "similarity": 0.7699154597554888
    },
    {
      "doc": 60,
      "topic": 24,
      "similarity": 0.7745354752352159
    },
    {
      "doc": 61,
      "topic": 2,
      "similarity": 0.775814426092316
    },
    {
      "doc": 61,
      "topic": 3,
      "similarity": 0.796998850422865
    },
    {
      "doc": 61,
      "topic": 5,
      "similarity": 0.7934957810181628
    },
    {
      "doc": 61,
      "topic": 7,
      "similarity": 0.8182610589547766
    },
    {
      "doc": 61,
      "topic": 8,
      "similarity": 0.8050541833696018
    },
    {
      "doc": 61,
      "topic": 9,
      "similarity": 0.8303688701272759
    },
    {
      "doc": 61,
      "topic": 10,
      "similarity": 0.7897911313652176
    },
    {
      "doc": 61,
      "topic": 11,
      "similarity": 0.8314004414903876
    },
    {
      "doc": 61,
      "topic": 13,
      "similarity": 0.7787994803660869
    },
    {
      "doc": 61,
      "topic": 14,
      "similarity": 0.7745631553316494
    },
    {
      "doc": 61,
      "topic": 15,
      "similarity": 0.7803920302701527
    },
    {
      "doc": 61,
      "topic": 16,
      "similarity": 0.8022549919731066
    },
    {
      "doc": 61,
      "topic": 17,
      "similarity": 0.7938222052151092
    },
    {
      "doc": 61,
      "topic": 18,
      "similarity": 0.7651892866408089
    },
    {
      "doc": 61,
      "topic": 19,
      "similarity": 0.7953036166605089
    },
    {
      "doc": 61,
      "topic": 20,
      "similarity": 0.7639380677850963
    },
    {
      "doc": 61,
      "topic": 21,
      "similarity": 0.7865288940144394
    },
    {
      "doc": 61,
      "topic": 22,
      "similarity": 0.7519400320626848
    },
    {
      "doc": 61,
      "topic": 24,
      "similarity": 0.7685704371993056
    },
    {
      "doc": 62,
      "topic": 1,
      "similarity": 0.7558090305323484
    },
    {
      "doc": 62,
      "topic": 2,
      "similarity": 0.7653218611234441
    },
    {
      "doc": 62,
      "topic": 3,
      "similarity": 0.7864823685856909
    },
    {
      "doc": 62,
      "topic": 4,
      "similarity": 0.7633326261970623
    },
    {
      "doc": 62,
      "topic": 5,
      "similarity": 0.7865206237619919
    },
    {
      "doc": 62,
      "topic": 7,
      "similarity": 0.7872605070313593
    },
    {
      "doc": 62,
      "topic": 8,
      "similarity": 0.7609945944960174
    },
    {
      "doc": 62,
      "topic": 9,
      "similarity": 0.8239172797866793
    },
    {
      "doc": 62,
      "topic": 10,
      "similarity": 0.7843583209907937
    },
    {
      "doc": 62,
      "topic": 11,
      "similarity": 0.7829703173408278
    },
    {
      "doc": 62,
      "topic": 13,
      "similarity": 0.7703157549278322
    },
    {
      "doc": 62,
      "topic": 14,
      "similarity": 0.7713947841256005
    },
    {
      "doc": 62,
      "topic": 15,
      "similarity": 0.7898709292517686
    },
    {
      "doc": 62,
      "topic": 16,
      "similarity": 0.8035214319386487
    },
    {
      "doc": 62,
      "topic": 17,
      "similarity": 0.8147616461221378
    },
    {
      "doc": 62,
      "topic": 18,
      "similarity": 0.7524444933825923
    },
    {
      "doc": 62,
      "topic": 19,
      "similarity": 0.8063249732318551
    },
    {
      "doc": 62,
      "topic": 20,
      "similarity": 0.7580914825341084
    },
    {
      "doc": 62,
      "topic": 21,
      "similarity": 0.8287338543587242
    },
    {
      "doc": 63,
      "topic": 2,
      "similarity": 0.7533215280290917
    },
    {
      "doc": 63,
      "topic": 3,
      "similarity": 0.7998740236445331
    },
    {
      "doc": 63,
      "topic": 4,
      "similarity": 0.7737809838465939
    },
    {
      "doc": 63,
      "topic": 5,
      "similarity": 0.7857895685271327
    },
    {
      "doc": 63,
      "topic": 7,
      "similarity": 0.7792441850092418
    },
    {
      "doc": 63,
      "topic": 8,
      "similarity": 0.7560582326385283
    },
    {
      "doc": 63,
      "topic": 9,
      "similarity": 0.8056897303899618
    },
    {
      "doc": 63,
      "topic": 10,
      "similarity": 0.759028524951394
    },
    {
      "doc": 63,
      "topic": 11,
      "similarity": 0.7749249347699538
    },
    {
      "doc": 63,
      "topic": 13,
      "similarity": 0.7543208781076582
    },
    {
      "doc": 63,
      "topic": 15,
      "similarity": 0.7583710582981411
    },
    {
      "doc": 63,
      "topic": 16,
      "similarity": 0.8028640100366474
    },
    {
      "doc": 63,
      "topic": 17,
      "similarity": 0.7808256527822917
    },
    {
      "doc": 63,
      "topic": 19,
      "similarity": 0.786060438999933
    },
    {
      "doc": 63,
      "topic": 21,
      "similarity": 0.7929931143422293
    },
    {
      "doc": 63,
      "topic": 24,
      "similarity": 0.7785712635999238
    },
    {
      "doc": 64,
      "topic": 3,
      "similarity": 0.7660620139901168
    },
    {
      "doc": 64,
      "topic": 5,
      "similarity": 0.7609902010334664
    },
    {
      "doc": 64,
      "topic": 6,
      "similarity": 0.7502291925727881
    },
    {
      "doc": 64,
      "topic": 8,
      "similarity": 0.7691819855998754
    },
    {
      "doc": 64,
      "topic": 9,
      "similarity": 0.800796209533724
    },
    {
      "doc": 64,
      "topic": 10,
      "similarity": 0.7592909294522271
    },
    {
      "doc": 64,
      "topic": 11,
      "similarity": 0.7977192421602917
    },
    {
      "doc": 64,
      "topic": 12,
      "similarity": 0.7714829841549282
    },
    {
      "doc": 64,
      "topic": 13,
      "similarity": 0.7503491534930408
    },
    {
      "doc": 64,
      "topic": 14,
      "similarity": 0.7515312677964219
    },
    {
      "doc": 64,
      "topic": 15,
      "similarity": 0.7961409309969034
    },
    {
      "doc": 64,
      "topic": 16,
      "similarity": 0.7610319480342982
    },
    {
      "doc": 64,
      "topic": 17,
      "similarity": 0.777432328287559
    },
    {
      "doc": 64,
      "topic": 18,
      "similarity": 0.7796354949233543
    },
    {
      "doc": 64,
      "topic": 19,
      "similarity": 0.7903839164529421
    },
    {
      "doc": 64,
      "topic": 20,
      "similarity": 0.7616729280051332
    },
    {
      "doc": 64,
      "topic": 21,
      "similarity": 0.7865083441111108
    },
    {
      "doc": 64,
      "topic": 24,
      "similarity": 0.7533031290964227
    },
    {
      "doc": 65,
      "topic": 1,
      "similarity": 0.8074955372712861
    },
    {
      "doc": 65,
      "topic": 2,
      "similarity": 0.8065986468647094
    },
    {
      "doc": 65,
      "topic": 3,
      "similarity": 0.8174034510616996
    },
    {
      "doc": 65,
      "topic": 4,
      "similarity": 0.7687036464120509
    },
    {
      "doc": 65,
      "topic": 5,
      "similarity": 0.8246252597095802
    },
    {
      "doc": 65,
      "topic": 7,
      "similarity": 0.8001293387059005
    },
    {
      "doc": 65,
      "topic": 8,
      "similarity": 0.7813494517778989
    },
    {
      "doc": 65,
      "topic": 9,
      "similarity": 0.8462653257990598
    },
    {
      "doc": 65,
      "topic": 10,
      "similarity": 0.8098858674505675
    },
    {
      "doc": 65,
      "topic": 11,
      "similarity": 0.8139691834441068
    },
    {
      "doc": 65,
      "topic": 13,
      "similarity": 0.805692294344082
    },
    {
      "doc": 65,
      "topic": 14,
      "similarity": 0.8162319404222493
    },
    {
      "doc": 65,
      "topic": 15,
      "similarity": 0.7974853787586222
    },
    {
      "doc": 65,
      "topic": 16,
      "similarity": 0.8364164843651867
    },
    {
      "doc": 65,
      "topic": 17,
      "similarity": 0.8263557145440529
    },
    {
      "doc": 65,
      "topic": 18,
      "similarity": 0.7605904070321197
    },
    {
      "doc": 65,
      "topic": 19,
      "similarity": 0.8132451384040791
    },
    {
      "doc": 65,
      "topic": 20,
      "similarity": 0.7703427836167511
    },
    {
      "doc": 65,
      "topic": 21,
      "similarity": 0.8203780512198509
    },
    {
      "doc": 65,
      "topic": 23,
      "similarity": 0.7754585356116406
    },
    {
      "doc": 66,
      "topic": 0,
      "similarity": 0.765565884487644
    },
    {
      "doc": 66,
      "topic": 2,
      "similarity": 0.7578379025229109
    },
    {
      "doc": 66,
      "topic": 3,
      "similarity": 0.7791233732005165
    },
    {
      "doc": 66,
      "topic": 5,
      "similarity": 0.8077837944790417
    },
    {
      "doc": 66,
      "topic": 7,
      "similarity": 0.7586857615463752
    },
    {
      "doc": 66,
      "topic": 8,
      "similarity": 0.7676992077923144
    },
    {
      "doc": 66,
      "topic": 9,
      "similarity": 0.8293198503407718
    },
    {
      "doc": 66,
      "topic": 10,
      "similarity": 0.7739554143800783
    },
    {
      "doc": 66,
      "topic": 11,
      "similarity": 0.8195076742342409
    },
    {
      "doc": 66,
      "topic": 15,
      "similarity": 0.7792632972845026
    },
    {
      "doc": 66,
      "topic": 16,
      "similarity": 0.8052524762589262
    },
    {
      "doc": 66,
      "topic": 17,
      "similarity": 0.7975278523941776
    },
    {
      "doc": 66,
      "topic": 18,
      "similarity": 0.755748638211154
    },
    {
      "doc": 66,
      "topic": 19,
      "similarity": 0.7841318533036503
    },
    {
      "doc": 66,
      "topic": 20,
      "similarity": 0.7743470751062863
    },
    {
      "doc": 66,
      "topic": 21,
      "similarity": 0.8076339493083285
    },
    {
      "doc": 66,
      "topic": 24,
      "similarity": 0.7751755555350692
    },
    {
      "doc": 67,
      "topic": 2,
      "similarity": 0.7525691589020963
    },
    {
      "doc": 67,
      "topic": 3,
      "similarity": 0.8181487112657029
    },
    {
      "doc": 67,
      "topic": 4,
      "similarity": 0.7546344149675434
    },
    {
      "doc": 67,
      "topic": 5,
      "similarity": 0.7997272114642447
    },
    {
      "doc": 67,
      "topic": 7,
      "similarity": 0.7802826364461447
    },
    {
      "doc": 67,
      "topic": 8,
      "similarity": 0.7741736684685354
    },
    {
      "doc": 67,
      "topic": 9,
      "similarity": 0.78944853133868
    },
    {
      "doc": 67,
      "topic": 10,
      "similarity": 0.7681391185679093
    },
    {
      "doc": 67,
      "topic": 11,
      "similarity": 0.777507556738038
    },
    {
      "doc": 67,
      "topic": 16,
      "similarity": 0.7850840921821586
    },
    {
      "doc": 67,
      "topic": 17,
      "similarity": 0.767473494299925
    },
    {
      "doc": 67,
      "topic": 18,
      "similarity": 0.7701748900892499
    },
    {
      "doc": 67,
      "topic": 19,
      "similarity": 0.8395623324572138
    },
    {
      "doc": 67,
      "topic": 20,
      "similarity": 0.7685742075197071
    },
    {
      "doc": 67,
      "topic": 21,
      "similarity": 0.759478801975271
    },
    {
      "doc": 68,
      "topic": 0,
      "similarity": 0.7589848337365436
    },
    {
      "doc": 68,
      "topic": 3,
      "similarity": 0.779666970288144
    },
    {
      "doc": 68,
      "topic": 7,
      "similarity": 0.7676965490183111
    },
    {
      "doc": 68,
      "topic": 9,
      "similarity": 0.8172199262423737
    },
    {
      "doc": 68,
      "topic": 11,
      "similarity": 0.7783459192666151
    },
    {
      "doc": 68,
      "topic": 15,
      "similarity": 0.7590254198444684
    },
    {
      "doc": 68,
      "topic": 16,
      "similarity": 0.7737492027283385
    },
    {
      "doc": 68,
      "topic": 17,
      "similarity": 0.7605719153298152
    },
    {
      "doc": 68,
      "topic": 18,
      "similarity": 0.7652672389637564
    },
    {
      "doc": 68,
      "topic": 19,
      "similarity": 0.7779040720500013
    },
    {
      "doc": 68,
      "topic": 20,
      "similarity": 0.7656680194954602
    },
    {
      "doc": 68,
      "topic": 21,
      "similarity": 0.7630797883785976
    },
    {
      "doc": 68,
      "topic": 23,
      "similarity": 0.7559187498737847
    },
    {
      "doc": 68,
      "topic": 24,
      "similarity": 0.7532631785811073
    },
    {
      "doc": 69,
      "topic": 2,
      "similarity": 0.7506900942461561
    },
    {
      "doc": 69,
      "topic": 3,
      "similarity": 0.781757875037664
    },
    {
      "doc": 69,
      "topic": 5,
      "similarity": 0.7786557784882828
    },
    {
      "doc": 69,
      "topic": 7,
      "similarity": 0.7780344615908797
    },
    {
      "doc": 69,
      "topic": 8,
      "similarity": 0.7860244612005645
    },
    {
      "doc": 69,
      "topic": 9,
      "similarity": 0.8327478926172135
    },
    {
      "doc": 69,
      "topic": 10,
      "similarity": 0.7765695067199962
    },
    {
      "doc": 69,
      "topic": 11,
      "similarity": 0.7950587967813573
    },
    {
      "doc": 69,
      "topic": 12,
      "similarity": 0.7744789829575777
    },
    {
      "doc": 69,
      "topic": 13,
      "similarity": 0.7507309879629114
    },
    {
      "doc": 69,
      "topic": 14,
      "similarity": 0.7611354905186432
    },
    {
      "doc": 69,
      "topic": 15,
      "similarity": 0.7762471202919827
    },
    {
      "doc": 69,
      "topic": 16,
      "similarity": 0.7774492385318731
    },
    {
      "doc": 69,
      "topic": 17,
      "similarity": 0.7759879589493023
    },
    {
      "doc": 69,
      "topic": 18,
      "similarity": 0.8083391521640937
    },
    {
      "doc": 69,
      "topic": 19,
      "similarity": 0.8218761050734832
    },
    {
      "doc": 69,
      "topic": 20,
      "similarity": 0.7709552756620496
    },
    {
      "doc": 69,
      "topic": 21,
      "similarity": 0.7708484207430826
    },
    {
      "doc": 69,
      "topic": 22,
      "similarity": 0.7543128903119307
    },
    {
      "doc": 69,
      "topic": 24,
      "similarity": 0.7584341865531284
    },
    {
      "doc": 70,
      "topic": 3,
      "similarity": 0.7532983169861278
    },
    {
      "doc": 70,
      "topic": 5,
      "similarity": 0.7657295568442459
    },
    {
      "doc": 70,
      "topic": 9,
      "similarity": 0.7877361078709627
    },
    {
      "doc": 70,
      "topic": 11,
      "similarity": 0.7892163973922893
    },
    {
      "doc": 70,
      "topic": 15,
      "similarity": 0.7612328518862841
    },
    {
      "doc": 70,
      "topic": 16,
      "similarity": 0.7773580465910896
    },
    {
      "doc": 70,
      "topic": 17,
      "similarity": 0.7663237960157664
    },
    {
      "doc": 70,
      "topic": 19,
      "similarity": 0.756434587495935
    },
    {
      "doc": 70,
      "topic": 20,
      "similarity": 0.774566087397218
    },
    {
      "doc": 70,
      "topic": 21,
      "similarity": 0.7720612949605145
    },
    {
      "doc": 70,
      "topic": 24,
      "similarity": 0.7506478886672249
    },
    {
      "doc": 71,
      "topic": 2,
      "similarity": 0.7755013590649988
    },
    {
      "doc": 71,
      "topic": 3,
      "similarity": 0.8014015442388266
    },
    {
      "doc": 71,
      "topic": 4,
      "similarity": 0.7530644904530057
    },
    {
      "doc": 71,
      "topic": 5,
      "similarity": 0.8015122772043378
    },
    {
      "doc": 71,
      "topic": 7,
      "similarity": 0.7901379220150258
    },
    {
      "doc": 71,
      "topic": 8,
      "similarity": 0.8038256385017625
    },
    {
      "doc": 71,
      "topic": 9,
      "similarity": 0.8511393492961845
    },
    {
      "doc": 71,
      "topic": 10,
      "similarity": 0.8021829167806587
    },
    {
      "doc": 71,
      "topic": 11,
      "similarity": 0.7883251088456449
    },
    {
      "doc": 71,
      "topic": 12,
      "similarity": 0.8047281973616586
    },
    {
      "doc": 71,
      "topic": 13,
      "similarity": 0.7718008965114134
    },
    {
      "doc": 71,
      "topic": 14,
      "similarity": 0.7691544853951964
    },
    {
      "doc": 71,
      "topic": 15,
      "similarity": 0.8107723354889055
    },
    {
      "doc": 71,
      "topic": 16,
      "similarity": 0.8134561501069504
    },
    {
      "doc": 71,
      "topic": 17,
      "similarity": 0.8253036579936454
    },
    {
      "doc": 71,
      "topic": 18,
      "similarity": 0.7756058409809371
    },
    {
      "doc": 71,
      "topic": 19,
      "similarity": 0.8185557204085019
    },
    {
      "doc": 71,
      "topic": 20,
      "similarity": 0.7973668198084072
    },
    {
      "doc": 71,
      "topic": 21,
      "similarity": 0.8030083047557185
    },
    {
      "doc": 71,
      "topic": 22,
      "similarity": 0.7590389913948209
    },
    {
      "doc": 71,
      "topic": 23,
      "similarity": 0.7606129656266227
    },
    {
      "doc": 71,
      "topic": 24,
      "similarity": 0.7745819791478082
    },
    {
      "doc": 72,
      "topic": 2,
      "similarity": 0.7606427821353405
    },
    {
      "doc": 72,
      "topic": 3,
      "similarity": 0.7877747362054079
    },
    {
      "doc": 72,
      "topic": 4,
      "similarity": 0.765933051419595
    },
    {
      "doc": 72,
      "topic": 5,
      "similarity": 0.8008095863179796
    },
    {
      "doc": 72,
      "topic": 7,
      "similarity": 0.775653565803596
    },
    {
      "doc": 72,
      "topic": 8,
      "similarity": 0.7680331432310702
    },
    {
      "doc": 72,
      "topic": 9,
      "similarity": 0.8090826448699779
    },
    {
      "doc": 72,
      "topic": 10,
      "similarity": 0.7619925857837696
    },
    {
      "doc": 72,
      "topic": 11,
      "similarity": 0.7993750118762093
    },
    {
      "doc": 72,
      "topic": 13,
      "similarity": 0.7593067565484315
    },
    {
      "doc": 72,
      "topic": 15,
      "similarity": 0.780715467297834
    },
    {
      "doc": 72,
      "topic": 16,
      "similarity": 0.8076545069651246
    },
    {
      "doc": 72,
      "topic": 17,
      "similarity": 0.7968023884132979
    },
    {
      "doc": 72,
      "topic": 18,
      "similarity": 0.761921393392383
    },
    {
      "doc": 72,
      "topic": 19,
      "similarity": 0.8162792037835268
    },
    {
      "doc": 72,
      "topic": 20,
      "similarity": 0.7765646889030492
    },
    {
      "doc": 72,
      "topic": 21,
      "similarity": 0.7899013004634018
    },
    {
      "doc": 72,
      "topic": 23,
      "similarity": 0.7660756201297922
    },
    {
      "doc": 72,
      "topic": 24,
      "similarity": 0.7566481454084127
    },
    {
      "doc": 73,
      "topic": 1,
      "similarity": 0.7639124237758487
    },
    {
      "doc": 73,
      "topic": 2,
      "similarity": 0.7819432263195122
    },
    {
      "doc": 73,
      "topic": 3,
      "similarity": 0.8036074808425111
    },
    {
      "doc": 73,
      "topic": 4,
      "similarity": 0.7518355440956824
    },
    {
      "doc": 73,
      "topic": 5,
      "similarity": 0.7964549023609471
    },
    {
      "doc": 73,
      "topic": 7,
      "similarity": 0.7805165656447363
    },
    {
      "doc": 73,
      "topic": 8,
      "similarity": 0.778341757532914
    },
    {
      "doc": 73,
      "topic": 9,
      "similarity": 0.8390436385151783
    },
    {
      "doc": 73,
      "topic": 10,
      "similarity": 0.7986306516391648
    },
    {
      "doc": 73,
      "topic": 11,
      "similarity": 0.785109568519195
    },
    {
      "doc": 73,
      "topic": 12,
      "similarity": 0.7654628301599572
    },
    {
      "doc": 73,
      "topic": 13,
      "similarity": 0.7728624910957561
    },
    {
      "doc": 73,
      "topic": 14,
      "similarity": 0.8053105531012593
    },
    {
      "doc": 73,
      "topic": 15,
      "similarity": 0.7904359523095291
    },
    {
      "doc": 73,
      "topic": 16,
      "similarity": 0.8092985341546016
    },
    {
      "doc": 73,
      "topic": 17,
      "similarity": 0.789709930162264
    },
    {
      "doc": 73,
      "topic": 18,
      "similarity": 0.8068911747984474
    },
    {
      "doc": 73,
      "topic": 19,
      "similarity": 0.8535000249186244
    },
    {
      "doc": 73,
      "topic": 20,
      "similarity": 0.7693466895288567
    },
    {
      "doc": 73,
      "topic": 21,
      "similarity": 0.7906679953175288
    },
    {
      "doc": 73,
      "topic": 23,
      "similarity": 0.7786430419299556
    },
    {
      "doc": 74,
      "topic": 1,
      "similarity": 0.7589049137840481
    },
    {
      "doc": 74,
      "topic": 2,
      "similarity": 0.7848631142784727
    },
    {
      "doc": 74,
      "topic": 3,
      "similarity": 0.8035212337309204
    },
    {
      "doc": 74,
      "topic": 4,
      "similarity": 0.7760413281953961
    },
    {
      "doc": 74,
      "topic": 5,
      "similarity": 0.8191895203498567
    },
    {
      "doc": 74,
      "topic": 7,
      "similarity": 0.7968660156595502
    },
    {
      "doc": 74,
      "topic": 8,
      "similarity": 0.7995294058134207
    },
    {
      "doc": 74,
      "topic": 9,
      "similarity": 0.8463142026495709
    },
    {
      "doc": 74,
      "topic": 10,
      "similarity": 0.8226498528875625
    },
    {
      "doc": 74,
      "topic": 11,
      "similarity": 0.7892794280420411
    },
    {
      "doc": 74,
      "topic": 12,
      "similarity": 0.7562430682186246
    },
    {
      "doc": 74,
      "topic": 13,
      "similarity": 0.782332651955782
    },
    {
      "doc": 74,
      "topic": 14,
      "similarity": 0.7879723414277766
    },
    {
      "doc": 74,
      "topic": 15,
      "similarity": 0.7930030693662539
    },
    {
      "doc": 74,
      "topic": 16,
      "similarity": 0.8155575792538583
    },
    {
      "doc": 74,
      "topic": 17,
      "similarity": 0.8134346151077705
    },
    {
      "doc": 74,
      "topic": 18,
      "similarity": 0.8036017810367768
    },
    {
      "doc": 74,
      "topic": 19,
      "similarity": 0.8340560980362236
    },
    {
      "doc": 74,
      "topic": 20,
      "similarity": 0.7944501430845895
    },
    {
      "doc": 74,
      "topic": 21,
      "similarity": 0.8075504895532958
    },
    {
      "doc": 74,
      "topic": 23,
      "similarity": 0.7566367376734133
    },
    {
      "doc": 74,
      "topic": 24,
      "similarity": 0.7611022423131496
    },
    {
      "doc": 75,
      "topic": 3,
      "similarity": 0.7933219159206109
    },
    {
      "doc": 75,
      "topic": 5,
      "similarity": 0.7946335477273642
    },
    {
      "doc": 75,
      "topic": 9,
      "similarity": 0.7973835940059991
    },
    {
      "doc": 75,
      "topic": 11,
      "similarity": 0.7509752020221702
    },
    {
      "doc": 75,
      "topic": 16,
      "similarity": 0.7831228283293941
    },
    {
      "doc": 75,
      "topic": 17,
      "similarity": 0.757121489782243
    },
    {
      "doc": 75,
      "topic": 19,
      "similarity": 0.7662318152427627
    },
    {
      "doc": 75,
      "topic": 20,
      "similarity": 0.7655665764337375
    },
    {
      "doc": 75,
      "topic": 21,
      "similarity": 0.7806747198297269
    },
    {
      "doc": 75,
      "topic": 24,
      "similarity": 0.7641557211720498
    },
    {
      "doc": 76,
      "topic": 3,
      "similarity": 0.770660401478357
    },
    {
      "doc": 76,
      "topic": 5,
      "similarity": 0.7900045082884289
    },
    {
      "doc": 76,
      "topic": 8,
      "similarity": 0.7523509974643104
    },
    {
      "doc": 76,
      "topic": 9,
      "similarity": 0.7927598179111843
    },
    {
      "doc": 76,
      "topic": 10,
      "similarity": 0.752011044626217
    },
    {
      "doc": 76,
      "topic": 13,
      "similarity": 0.7520436708354381
    },
    {
      "doc": 76,
      "topic": 15,
      "similarity": 0.7594269102448146
    },
    {
      "doc": 76,
      "topic": 16,
      "similarity": 0.7722698661790095
    },
    {
      "doc": 76,
      "topic": 17,
      "similarity": 0.778399386674425
    },
    {
      "doc": 76,
      "topic": 19,
      "similarity": 0.7655540383137561
    },
    {
      "doc": 76,
      "topic": 20,
      "similarity": 0.7650880145027169
    },
    {
      "doc": 76,
      "topic": 21,
      "similarity": 0.8273959604993564
    },
    {
      "doc": 77,
      "topic": 3,
      "similarity": 0.7644669800722356
    },
    {
      "doc": 77,
      "topic": 5,
      "similarity": 0.7648079056906116
    },
    {
      "doc": 77,
      "topic": 8,
      "similarity": 0.756782149356062
    },
    {
      "doc": 77,
      "topic": 9,
      "similarity": 0.8174291318762754
    },
    {
      "doc": 77,
      "topic": 10,
      "similarity": 0.7566125537950884
    },
    {
      "doc": 77,
      "topic": 11,
      "similarity": 0.75716437664896
    },
    {
      "doc": 77,
      "topic": 15,
      "similarity": 0.7612326363881922
    },
    {
      "doc": 77,
      "topic": 16,
      "similarity": 0.7825305608627744
    },
    {
      "doc": 77,
      "topic": 17,
      "similarity": 0.7824864590655235
    },
    {
      "doc": 77,
      "topic": 18,
      "similarity": 0.7598839327699312
    },
    {
      "doc": 77,
      "topic": 19,
      "similarity": 0.7873956820972755
    },
    {
      "doc": 77,
      "topic": 20,
      "similarity": 0.7778935957471859
    },
    {
      "doc": 77,
      "topic": 21,
      "similarity": 0.7941424017725403
    },
    {
      "doc": 77,
      "topic": 24,
      "similarity": 0.7815304499169509
    },
    {
      "doc": 78,
      "topic": 2,
      "similarity": 0.7547280962756338
    },
    {
      "doc": 78,
      "topic": 3,
      "similarity": 0.7724376113517386
    },
    {
      "doc": 78,
      "topic": 5,
      "similarity": 0.7603974041406212
    },
    {
      "doc": 78,
      "topic": 6,
      "similarity": 0.7544379825528652
    },
    {
      "doc": 78,
      "topic": 7,
      "similarity": 0.764384279818243
    },
    {
      "doc": 78,
      "topic": 8,
      "similarity": 0.7824097125400514
    },
    {
      "doc": 78,
      "topic": 9,
      "similarity": 0.8257246133796449
    },
    {
      "doc": 78,
      "topic": 10,
      "similarity": 0.7686730705344968
    },
    {
      "doc": 78,
      "topic": 11,
      "similarity": 0.7822216203072835
    },
    {
      "doc": 78,
      "topic": 12,
      "similarity": 0.8365018480385623
    },
    {
      "doc": 78,
      "topic": 13,
      "similarity": 0.8007394321143814
    },
    {
      "doc": 78,
      "topic": 14,
      "similarity": 0.7571212356547318
    },
    {
      "doc": 78,
      "topic": 15,
      "similarity": 0.7849146472110848
    },
    {
      "doc": 78,
      "topic": 16,
      "similarity": 0.7777815509523819
    },
    {
      "doc": 78,
      "topic": 17,
      "similarity": 0.7868949886149704
    },
    {
      "doc": 78,
      "topic": 18,
      "similarity": 0.7807196404764868
    },
    {
      "doc": 78,
      "topic": 19,
      "similarity": 0.7772650867837119
    },
    {
      "doc": 78,
      "topic": 20,
      "similarity": 0.7662776357879694
    },
    {
      "doc": 78,
      "topic": 21,
      "similarity": 0.7782755080593429
    },
    {
      "doc": 78,
      "topic": 22,
      "similarity": 0.7681169682983993
    },
    {
      "doc": 79,
      "topic": 1,
      "similarity": 0.7558990729649601
    },
    {
      "doc": 79,
      "topic": 2,
      "similarity": 0.7805584669755401
    },
    {
      "doc": 79,
      "topic": 3,
      "similarity": 0.7984921743950434
    },
    {
      "doc": 79,
      "topic": 4,
      "similarity": 0.7657320545263224
    },
    {
      "doc": 79,
      "topic": 5,
      "similarity": 0.8040091401152283
    },
    {
      "doc": 79,
      "topic": 7,
      "similarity": 0.7987167052595718
    },
    {
      "doc": 79,
      "topic": 8,
      "similarity": 0.7937457780352377
    },
    {
      "doc": 79,
      "topic": 9,
      "similarity": 0.8533330883557818
    },
    {
      "doc": 79,
      "topic": 10,
      "similarity": 0.7854661875673588
    },
    {
      "doc": 79,
      "topic": 11,
      "similarity": 0.8165201718249391
    },
    {
      "doc": 79,
      "topic": 13,
      "similarity": 0.7932504620871951
    },
    {
      "doc": 79,
      "topic": 14,
      "similarity": 0.7780026917037298
    },
    {
      "doc": 79,
      "topic": 15,
      "similarity": 0.8138871501910582
    },
    {
      "doc": 79,
      "topic": 16,
      "similarity": 0.8047271391926595
    },
    {
      "doc": 79,
      "topic": 17,
      "similarity": 0.8272666873562818
    },
    {
      "doc": 79,
      "topic": 19,
      "similarity": 0.8002751064284725
    },
    {
      "doc": 79,
      "topic": 20,
      "similarity": 0.7678340061099199
    },
    {
      "doc": 79,
      "topic": 21,
      "similarity": 0.8242307602001796
    },
    {
      "doc": 80,
      "topic": 1,
      "similarity": 0.7774456193341264
    },
    {
      "doc": 80,
      "topic": 2,
      "similarity": 0.7820120292216552
    },
    {
      "doc": 80,
      "topic": 3,
      "similarity": 0.824125218788013
    },
    {
      "doc": 80,
      "topic": 4,
      "similarity": 0.7816108437677461
    },
    {
      "doc": 80,
      "topic": 5,
      "similarity": 0.816825068476439
    },
    {
      "doc": 80,
      "topic": 7,
      "similarity": 0.7936664729667411
    },
    {
      "doc": 80,
      "topic": 8,
      "similarity": 0.7912356362413634
    },
    {
      "doc": 80,
      "topic": 9,
      "similarity": 0.8321200119456845
    },
    {
      "doc": 80,
      "topic": 10,
      "similarity": 0.8081608999908321
    },
    {
      "doc": 80,
      "topic": 11,
      "similarity": 0.7959560138363884
    },
    {
      "doc": 80,
      "topic": 13,
      "similarity": 0.7986852580875508
    },
    {
      "doc": 80,
      "topic": 14,
      "similarity": 0.7911258488105234
    },
    {
      "doc": 80,
      "topic": 15,
      "similarity": 0.7894034144772555
    },
    {
      "doc": 80,
      "topic": 16,
      "similarity": 0.8318221749065662
    },
    {
      "doc": 80,
      "topic": 17,
      "similarity": 0.8198506787051234
    },
    {
      "doc": 80,
      "topic": 18,
      "similarity": 0.8147426925322564
    },
    {
      "doc": 80,
      "topic": 19,
      "similarity": 0.8625350366473556
    },
    {
      "doc": 80,
      "topic": 20,
      "similarity": 0.8027653430048733
    },
    {
      "doc": 80,
      "topic": 21,
      "similarity": 0.8245047348271396
    },
    {
      "doc": 80,
      "topic": 23,
      "similarity": 0.7659053142320428
    },
    {
      "doc": 80,
      "topic": 24,
      "similarity": 0.757411688522277
    },
    {
      "doc": 81,
      "topic": 3,
      "similarity": 0.7716660800319682
    },
    {
      "doc": 81,
      "topic": 4,
      "similarity": 0.7664582816723723
    },
    {
      "doc": 81,
      "topic": 5,
      "similarity": 0.7874792792563846
    },
    {
      "doc": 81,
      "topic": 6,
      "similarity": 0.7594538099039938
    },
    {
      "doc": 81,
      "topic": 7,
      "similarity": 0.7562958774117863
    },
    {
      "doc": 81,
      "topic": 8,
      "similarity": 0.7681251827217617
    },
    {
      "doc": 81,
      "topic": 9,
      "similarity": 0.8022524743115631
    },
    {
      "doc": 81,
      "topic": 10,
      "similarity": 0.808734714750391
    },
    {
      "doc": 81,
      "topic": 11,
      "similarity": 0.7661517943553133
    },
    {
      "doc": 81,
      "topic": 14,
      "similarity": 0.7590757598254406
    },
    {
      "doc": 81,
      "topic": 15,
      "similarity": 0.7643740700649647
    },
    {
      "doc": 81,
      "topic": 16,
      "similarity": 0.7854767839175013
    },
    {
      "doc": 81,
      "topic": 17,
      "similarity": 0.7830246423751618
    },
    {
      "doc": 81,
      "topic": 18,
      "similarity": 0.7701914974049071
    },
    {
      "doc": 81,
      "topic": 19,
      "similarity": 0.7913198727431298
    },
    {
      "doc": 81,
      "topic": 20,
      "similarity": 0.7643192433892249
    },
    {
      "doc": 81,
      "topic": 21,
      "similarity": 0.7816413842055934
    },
    {
      "doc": 81,
      "topic": 23,
      "similarity": 0.772109831574334
    },
    {
      "doc": 82,
      "topic": 1,
      "similarity": 0.7607980050784328
    },
    {
      "doc": 82,
      "topic": 2,
      "similarity": 0.7675434400203033
    },
    {
      "doc": 82,
      "topic": 3,
      "similarity": 0.8105800820926207
    },
    {
      "doc": 82,
      "topic": 4,
      "similarity": 0.7551607840066048
    },
    {
      "doc": 82,
      "topic": 5,
      "similarity": 0.7874237290537176
    },
    {
      "doc": 82,
      "topic": 7,
      "similarity": 0.7624523438422685
    },
    {
      "doc": 82,
      "topic": 8,
      "similarity": 0.7718419031275208
    },
    {
      "doc": 82,
      "topic": 9,
      "similarity": 0.8058550383451892
    },
    {
      "doc": 82,
      "topic": 10,
      "similarity": 0.7776639853354488
    },
    {
      "doc": 82,
      "topic": 11,
      "similarity": 0.775254671095341
    },
    {
      "doc": 82,
      "topic": 13,
      "similarity": 0.7596908858680242
    },
    {
      "doc": 82,
      "topic": 14,
      "similarity": 0.7753211070002074
    },
    {
      "doc": 82,
      "topic": 15,
      "similarity": 0.7592671556661179
    },
    {
      "doc": 82,
      "topic": 16,
      "similarity": 0.7829299980724647
    },
    {
      "doc": 82,
      "topic": 17,
      "similarity": 0.7910161088151264
    },
    {
      "doc": 82,
      "topic": 18,
      "similarity": 0.7809438698817167
    },
    {
      "doc": 82,
      "topic": 19,
      "similarity": 0.827386878462686
    },
    {
      "doc": 82,
      "topic": 20,
      "similarity": 0.777507917189963
    },
    {
      "doc": 82,
      "topic": 21,
      "similarity": 0.7821196482311721
    },
    {
      "doc": 83,
      "topic": 1,
      "similarity": 0.759028791757364
    },
    {
      "doc": 83,
      "topic": 2,
      "similarity": 0.7761718153747373
    },
    {
      "doc": 83,
      "topic": 3,
      "similarity": 0.8210458744196915
    },
    {
      "doc": 83,
      "topic": 4,
      "similarity": 0.7639817911530058
    },
    {
      "doc": 83,
      "topic": 5,
      "similarity": 0.8198749587929078
    },
    {
      "doc": 83,
      "topic": 7,
      "similarity": 0.7952793049623713
    },
    {
      "doc": 83,
      "topic": 8,
      "similarity": 0.7890233386055283
    },
    {
      "doc": 83,
      "topic": 9,
      "similarity": 0.8585992184616821
    },
    {
      "doc": 83,
      "topic": 10,
      "similarity": 0.7917680087822986
    },
    {
      "doc": 83,
      "topic": 11,
      "similarity": 0.7925819876942165
    },
    {
      "doc": 83,
      "topic": 12,
      "similarity": 0.7523555920991645
    },
    {
      "doc": 83,
      "topic": 13,
      "similarity": 0.7763167630593208
    },
    {
      "doc": 83,
      "topic": 14,
      "similarity": 0.7778929920336821
    },
    {
      "doc": 83,
      "topic": 15,
      "similarity": 0.7993989774885818
    },
    {
      "doc": 83,
      "topic": 16,
      "similarity": 0.8204038582198346
    },
    {
      "doc": 83,
      "topic": 17,
      "similarity": 0.8237690171492726
    },
    {
      "doc": 83,
      "topic": 18,
      "similarity": 0.7842167747823596
    },
    {
      "doc": 83,
      "topic": 19,
      "similarity": 0.821302153344283
    },
    {
      "doc": 83,
      "topic": 20,
      "similarity": 0.7912320533572493
    },
    {
      "doc": 83,
      "topic": 21,
      "similarity": 0.862279423215664
    },
    {
      "doc": 83,
      "topic": 23,
      "similarity": 0.7623231128628524
    },
    {
      "doc": 83,
      "topic": 24,
      "similarity": 0.7758382945831135
    },
    {
      "doc": 84,
      "topic": 1,
      "similarity": 0.7605815555000496
    },
    {
      "doc": 84,
      "topic": 2,
      "similarity": 0.7795538774796668
    },
    {
      "doc": 84,
      "topic": 3,
      "similarity": 0.8120020955334349
    },
    {
      "doc": 84,
      "topic": 4,
      "similarity": 0.7729063517655963
    },
    {
      "doc": 84,
      "topic": 5,
      "similarity": 0.797132588094811
    },
    {
      "doc": 84,
      "topic": 7,
      "similarity": 0.7900179903314538
    },
    {
      "doc": 84,
      "topic": 8,
      "similarity": 0.7816818264629941
    },
    {
      "doc": 84,
      "topic": 9,
      "similarity": 0.8250682731179084
    },
    {
      "doc": 84,
      "topic": 10,
      "similarity": 0.7961723839323228
    },
    {
      "doc": 84,
      "topic": 11,
      "similarity": 0.8013057958943147
    },
    {
      "doc": 84,
      "topic": 13,
      "similarity": 0.7703283764931017
    },
    {
      "doc": 84,
      "topic": 14,
      "similarity": 0.7784618472975534
    },
    {
      "doc": 84,
      "topic": 15,
      "similarity": 0.7861626419360168
    },
    {
      "doc": 84,
      "topic": 16,
      "similarity": 0.816442268136294
    },
    {
      "doc": 84,
      "topic": 17,
      "similarity": 0.8114315171465449
    },
    {
      "doc": 84,
      "topic": 18,
      "similarity": 0.7796726387571575
    },
    {
      "doc": 84,
      "topic": 19,
      "similarity": 0.8243325570856241
    },
    {
      "doc": 84,
      "topic": 20,
      "similarity": 0.789567387683678
    },
    {
      "doc": 84,
      "topic": 21,
      "similarity": 0.8048547735975027
    },
    {
      "doc": 84,
      "topic": 24,
      "similarity": 0.7633851904414943
    },
    {
      "doc": 85,
      "topic": 2,
      "similarity": 0.7729555208329351
    },
    {
      "doc": 85,
      "topic": 3,
      "similarity": 0.7934041357362961
    },
    {
      "doc": 85,
      "topic": 5,
      "similarity": 0.7862833192456089
    },
    {
      "doc": 85,
      "topic": 7,
      "similarity": 0.7606377224283751
    },
    {
      "doc": 85,
      "topic": 8,
      "similarity": 0.760434623992255
    },
    {
      "doc": 85,
      "topic": 9,
      "similarity": 0.8217735482931269
    },
    {
      "doc": 85,
      "topic": 10,
      "similarity": 0.7705133020350702
    },
    {
      "doc": 85,
      "topic": 11,
      "similarity": 0.7861584519445756
    },
    {
      "doc": 85,
      "topic": 12,
      "similarity": 0.7605864970996005
    },
    {
      "doc": 85,
      "topic": 13,
      "similarity": 0.7610610545062263
    },
    {
      "doc": 85,
      "topic": 14,
      "similarity": 0.7640983565162678
    },
    {
      "doc": 85,
      "topic": 15,
      "similarity": 0.7780353184349021
    },
    {
      "doc": 85,
      "topic": 16,
      "similarity": 0.798247929871224
    },
    {
      "doc": 85,
      "topic": 17,
      "similarity": 0.8158625110787795
    },
    {
      "doc": 85,
      "topic": 18,
      "similarity": 0.7613084546590974
    },
    {
      "doc": 85,
      "topic": 19,
      "similarity": 0.8004098725711299
    },
    {
      "doc": 85,
      "topic": 20,
      "similarity": 0.8007404668393204
    },
    {
      "doc": 85,
      "topic": 21,
      "similarity": 0.842457233914613
    },
    {
      "doc": 85,
      "topic": 23,
      "similarity": 0.7830526317989512
    },
    {
      "doc": 85,
      "topic": 24,
      "similarity": 0.7728885153753124
    },
    {
      "doc": 86,
      "topic": 1,
      "similarity": 0.7939011470747738
    },
    {
      "doc": 86,
      "topic": 2,
      "similarity": 0.8145770945567989
    },
    {
      "doc": 86,
      "topic": 3,
      "similarity": 0.8426057586898583
    },
    {
      "doc": 86,
      "topic": 4,
      "similarity": 0.7964092574704703
    },
    {
      "doc": 86,
      "topic": 5,
      "similarity": 0.8285068898549195
    },
    {
      "doc": 86,
      "topic": 6,
      "similarity": 0.7666009023910253
    },
    {
      "doc": 86,
      "topic": 7,
      "similarity": 0.8396198332723687
    },
    {
      "doc": 86,
      "topic": 8,
      "similarity": 0.8218735240646137
    },
    {
      "doc": 86,
      "topic": 9,
      "similarity": 0.8484201300733362
    },
    {
      "doc": 86,
      "topic": 10,
      "similarity": 0.8168968720709865
    },
    {
      "doc": 86,
      "topic": 11,
      "similarity": 0.8192282178011362
    },
    {
      "doc": 86,
      "topic": 13,
      "similarity": 0.8091163016644004
    },
    {
      "doc": 86,
      "topic": 14,
      "similarity": 0.8229823505228699
    },
    {
      "doc": 86,
      "topic": 15,
      "similarity": 0.8024984524060818
    },
    {
      "doc": 86,
      "topic": 16,
      "similarity": 0.8316832993087946
    },
    {
      "doc": 86,
      "topic": 17,
      "similarity": 0.8249833832698705
    },
    {
      "doc": 86,
      "topic": 18,
      "similarity": 0.7852532617076058
    },
    {
      "doc": 86,
      "topic": 19,
      "similarity": 0.8515324267831426
    },
    {
      "doc": 86,
      "topic": 20,
      "similarity": 0.785465079506612
    },
    {
      "doc": 86,
      "topic": 21,
      "similarity": 0.817575218335884
    },
    {
      "doc": 86,
      "topic": 23,
      "similarity": 0.8093955246304708
    },
    {
      "doc": 86,
      "topic": 24,
      "similarity": 0.7524243352867515
    },
    {
      "doc": 87,
      "topic": 2,
      "similarity": 0.7511969271323582
    },
    {
      "doc": 87,
      "topic": 3,
      "similarity": 0.779846001400897
    },
    {
      "doc": 87,
      "topic": 5,
      "similarity": 0.7578157871736456
    },
    {
      "doc": 87,
      "topic": 7,
      "similarity": 0.7839848271447838
    },
    {
      "doc": 87,
      "topic": 9,
      "similarity": 0.7957298003718216
    },
    {
      "doc": 87,
      "topic": 11,
      "similarity": 0.7521864432553623
    },
    {
      "doc": 87,
      "topic": 15,
      "similarity": 0.7601967970660256
    },
    {
      "doc": 87,
      "topic": 16,
      "similarity": 0.8176292119040808
    },
    {
      "doc": 87,
      "topic": 17,
      "similarity": 0.7636362046968491
    },
    {
      "doc": 87,
      "topic": 19,
      "similarity": 0.7897601893875412
    },
    {
      "doc": 87,
      "topic": 21,
      "similarity": 0.7629373045883405
    },
    {
      "doc": 88,
      "topic": 1,
      "similarity": 0.7614680665539717
    },
    {
      "doc": 88,
      "topic": 2,
      "similarity": 0.7504342124788377
    },
    {
      "doc": 88,
      "topic": 3,
      "similarity": 0.7886252374113847
    },
    {
      "doc": 88,
      "topic": 5,
      "similarity": 0.7876902143428892
    },
    {
      "doc": 88,
      "topic": 7,
      "similarity": 0.7751318039752091
    },
    {
      "doc": 88,
      "topic": 8,
      "similarity": 0.7974522237200277
    },
    {
      "doc": 88,
      "topic": 9,
      "similarity": 0.806537781536548
    },
    {
      "doc": 88,
      "topic": 10,
      "similarity": 0.7763827887753442
    },
    {
      "doc": 88,
      "topic": 11,
      "similarity": 0.763333146615387
    },
    {
      "doc": 88,
      "topic": 13,
      "similarity": 0.8207037347569132
    },
    {
      "doc": 88,
      "topic": 14,
      "similarity": 0.7756430809715017
    },
    {
      "doc": 88,
      "topic": 15,
      "similarity": 0.7572378301348988
    },
    {
      "doc": 88,
      "topic": 16,
      "similarity": 0.7917347478216106
    },
    {
      "doc": 88,
      "topic": 17,
      "similarity": 0.7678938446985342
    },
    {
      "doc": 88,
      "topic": 18,
      "similarity": 0.7594601325161836
    },
    {
      "doc": 88,
      "topic": 19,
      "similarity": 0.8004426795391335
    },
    {
      "doc": 88,
      "topic": 20,
      "similarity": 0.7940025037018195
    },
    {
      "doc": 88,
      "topic": 21,
      "similarity": 0.7877077954956974
    },
    {
      "doc": 88,
      "topic": 22,
      "similarity": 0.7640532361640167
    },
    {
      "doc": 89,
      "topic": 2,
      "similarity": 0.760856924472206
    },
    {
      "doc": 89,
      "topic": 3,
      "similarity": 0.8059068869480321
    },
    {
      "doc": 89,
      "topic": 4,
      "similarity": 0.7652025842237627
    },
    {
      "doc": 89,
      "topic": 5,
      "similarity": 0.8047979278548847
    },
    {
      "doc": 89,
      "topic": 7,
      "similarity": 0.7958380431240032
    },
    {
      "doc": 89,
      "topic": 8,
      "similarity": 0.7855811103982099
    },
    {
      "doc": 89,
      "topic": 9,
      "similarity": 0.8455386721301952
    },
    {
      "doc": 89,
      "topic": 10,
      "similarity": 0.788071820294287
    },
    {
      "doc": 89,
      "topic": 11,
      "similarity": 0.7925853696775689
    },
    {
      "doc": 89,
      "topic": 13,
      "similarity": 0.7610829159345986
    },
    {
      "doc": 89,
      "topic": 14,
      "similarity": 0.7602792222355672
    },
    {
      "doc": 89,
      "topic": 15,
      "similarity": 0.7928959535143845
    },
    {
      "doc": 89,
      "topic": 16,
      "similarity": 0.8144004810845462
    },
    {
      "doc": 89,
      "topic": 17,
      "similarity": 0.8116608912212628
    },
    {
      "doc": 89,
      "topic": 19,
      "similarity": 0.8026507411104606
    },
    {
      "doc": 89,
      "topic": 20,
      "similarity": 0.7919605034282007
    },
    {
      "doc": 89,
      "topic": 21,
      "similarity": 0.8237766177339195
    },
    {
      "doc": 89,
      "topic": 24,
      "similarity": 0.8119892444355675
    },
    {
      "doc": 90,
      "topic": 1,
      "similarity": 0.7542048172874422
    },
    {
      "doc": 90,
      "topic": 2,
      "similarity": 0.7826623550010922
    },
    {
      "doc": 90,
      "topic": 3,
      "similarity": 0.7973080203819389
    },
    {
      "doc": 90,
      "topic": 4,
      "similarity": 0.7610602466194452
    },
    {
      "doc": 90,
      "topic": 5,
      "similarity": 0.7926072410716403
    },
    {
      "doc": 90,
      "topic": 7,
      "similarity": 0.8055361435889767
    },
    {
      "doc": 90,
      "topic": 8,
      "similarity": 0.8604870023318605
    },
    {
      "doc": 90,
      "topic": 9,
      "similarity": 0.8258261612676185
    },
    {
      "doc": 90,
      "topic": 10,
      "similarity": 0.7993055784659383
    },
    {
      "doc": 90,
      "topic": 11,
      "similarity": 0.7733980878366095
    },
    {
      "doc": 90,
      "topic": 12,
      "similarity": 0.7511585181670812
    },
    {
      "doc": 90,
      "topic": 13,
      "similarity": 0.78121206725937
    },
    {
      "doc": 90,
      "topic": 14,
      "similarity": 0.7913206607916373
    },
    {
      "doc": 90,
      "topic": 15,
      "similarity": 0.7892520303995613
    },
    {
      "doc": 90,
      "topic": 16,
      "similarity": 0.7986052417362055
    },
    {
      "doc": 90,
      "topic": 17,
      "similarity": 0.7881739882443681
    },
    {
      "doc": 90,
      "topic": 18,
      "similarity": 0.7550336609226267
    },
    {
      "doc": 90,
      "topic": 19,
      "similarity": 0.7999196686821632
    },
    {
      "doc": 90,
      "topic": 20,
      "similarity": 0.7629326909587831
    },
    {
      "doc": 90,
      "topic": 21,
      "similarity": 0.7875885273175995
    },
    {
      "doc": 91,
      "topic": 0,
      "similarity": 0.7800548874458151
    },
    {
      "doc": 91,
      "topic": 1,
      "similarity": 0.7744386074387818
    },
    {
      "doc": 91,
      "topic": 2,
      "similarity": 0.7844203148072676
    },
    {
      "doc": 91,
      "topic": 3,
      "similarity": 0.7977435104476926
    },
    {
      "doc": 91,
      "topic": 4,
      "similarity": 0.7570485150409711
    },
    {
      "doc": 91,
      "topic": 5,
      "similarity": 0.7893709358098312
    },
    {
      "doc": 91,
      "topic": 7,
      "similarity": 0.7814242580408688
    },
    {
      "doc": 91,
      "topic": 8,
      "similarity": 0.7697625983125795
    },
    {
      "doc": 91,
      "topic": 9,
      "similarity": 0.8179570417833546
    },
    {
      "doc": 91,
      "topic": 10,
      "similarity": 0.7915157165245618
    },
    {
      "doc": 91,
      "topic": 11,
      "similarity": 0.8073381959878145
    },
    {
      "doc": 91,
      "topic": 13,
      "similarity": 0.7579219525099304
    },
    {
      "doc": 91,
      "topic": 14,
      "similarity": 0.7842383838107848
    },
    {
      "doc": 91,
      "topic": 15,
      "similarity": 0.7819435839375886
    },
    {
      "doc": 91,
      "topic": 16,
      "similarity": 0.8229669197744948
    },
    {
      "doc": 91,
      "topic": 17,
      "similarity": 0.7900477827109145
    },
    {
      "doc": 91,
      "topic": 18,
      "similarity": 0.7531258918042176
    },
    {
      "doc": 91,
      "topic": 19,
      "similarity": 0.7993911977578491
    },
    {
      "doc": 91,
      "topic": 20,
      "similarity": 0.7741339300116434
    },
    {
      "doc": 91,
      "topic": 21,
      "similarity": 0.7929690857441853
    },
    {
      "doc": 91,
      "topic": 23,
      "similarity": 0.7763770675003263
    },
    {
      "doc": 91,
      "topic": 24,
      "similarity": 0.7524504254768299
    },
    {
      "doc": 92,
      "topic": 2,
      "similarity": 0.7649346518055452
    },
    {
      "doc": 92,
      "topic": 3,
      "similarity": 0.7927786737731543
    },
    {
      "doc": 92,
      "topic": 4,
      "similarity": 0.7529496951956057
    },
    {
      "doc": 92,
      "topic": 5,
      "similarity": 0.8173994735362976
    },
    {
      "doc": 92,
      "topic": 7,
      "similarity": 0.7935796328710031
    },
    {
      "doc": 92,
      "topic": 8,
      "similarity": 0.7857399831134521
    },
    {
      "doc": 92,
      "topic": 9,
      "similarity": 0.8716348081668067
    },
    {
      "doc": 92,
      "topic": 10,
      "similarity": 0.7914036068934862
    },
    {
      "doc": 92,
      "topic": 11,
      "similarity": 0.7850631598462408
    },
    {
      "doc": 92,
      "topic": 13,
      "similarity": 0.7528334916052871
    },
    {
      "doc": 92,
      "topic": 14,
      "similarity": 0.7608167617361012
    },
    {
      "doc": 92,
      "topic": 15,
      "similarity": 0.7770921288239624
    },
    {
      "doc": 92,
      "topic": 16,
      "similarity": 0.8054592217850974
    },
    {
      "doc": 92,
      "topic": 17,
      "similarity": 0.803295748530964
    },
    {
      "doc": 92,
      "topic": 19,
      "similarity": 0.8049069661249355
    },
    {
      "doc": 92,
      "topic": 20,
      "similarity": 0.7640050608157514
    },
    {
      "doc": 92,
      "topic": 21,
      "similarity": 0.8019364963934165
    },
    {
      "doc": 92,
      "topic": 24,
      "similarity": 0.784780844172688
    },
    {
      "doc": 93,
      "topic": 3,
      "similarity": 0.7871989061701278
    },
    {
      "doc": 93,
      "topic": 5,
      "similarity": 0.7781636301260998
    },
    {
      "doc": 93,
      "topic": 6,
      "similarity": 0.7589190483067454
    },
    {
      "doc": 93,
      "topic": 7,
      "similarity": 0.7704737659175513
    },
    {
      "doc": 93,
      "topic": 9,
      "similarity": 0.8371522098186953
    },
    {
      "doc": 93,
      "topic": 10,
      "similarity": 0.7597244065252678
    },
    {
      "doc": 93,
      "topic": 11,
      "similarity": 0.7703692533375559
    },
    {
      "doc": 93,
      "topic": 14,
      "similarity": 0.7582605451263136
    },
    {
      "doc": 93,
      "topic": 15,
      "similarity": 0.7749225243779306
    },
    {
      "doc": 93,
      "topic": 16,
      "similarity": 0.7765909201046328
    },
    {
      "doc": 93,
      "topic": 17,
      "similarity": 0.7858161674288497
    },
    {
      "doc": 93,
      "topic": 19,
      "similarity": 0.7816079236868819
    },
    {
      "doc": 93,
      "topic": 20,
      "similarity": 0.7623233059969569
    },
    {
      "doc": 93,
      "topic": 21,
      "similarity": 0.7942403717188653
    },
    {
      "doc": 93,
      "topic": 23,
      "similarity": 0.7950256625157269
    },
    {
      "doc": 93,
      "topic": 24,
      "similarity": 0.7596732156322094
    },
    {
      "doc": 94,
      "topic": 2,
      "similarity": 0.7614591801286278
    },
    {
      "doc": 94,
      "topic": 3,
      "similarity": 0.8107719689911006
    },
    {
      "doc": 94,
      "topic": 5,
      "similarity": 0.7747626237295849
    },
    {
      "doc": 94,
      "topic": 7,
      "similarity": 0.7977767873694083
    },
    {
      "doc": 94,
      "topic": 8,
      "similarity": 0.7837951940499702
    },
    {
      "doc": 94,
      "topic": 9,
      "similarity": 0.8433865299277689
    },
    {
      "doc": 94,
      "topic": 10,
      "similarity": 0.7666277089065073
    },
    {
      "doc": 94,
      "topic": 11,
      "similarity": 0.7648045512943982
    },
    {
      "doc": 94,
      "topic": 13,
      "similarity": 0.7572399632859852
    },
    {
      "doc": 94,
      "topic": 14,
      "similarity": 0.7723562654360416
    },
    {
      "doc": 94,
      "topic": 15,
      "similarity": 0.7668931337366898
    },
    {
      "doc": 94,
      "topic": 16,
      "similarity": 0.7776498462791323
    },
    {
      "doc": 94,
      "topic": 17,
      "similarity": 0.7776825751270899
    },
    {
      "doc": 94,
      "topic": 18,
      "similarity": 0.7595911817358917
    },
    {
      "doc": 94,
      "topic": 19,
      "similarity": 0.7936992511487889
    },
    {
      "doc": 94,
      "topic": 20,
      "similarity": 0.7589898274168622
    },
    {
      "doc": 94,
      "topic": 21,
      "similarity": 0.7954672043900864
    },
    {
      "doc": 94,
      "topic": 24,
      "similarity": 0.7553492986717174
    },
    {
      "doc": 95,
      "topic": 1,
      "similarity": 0.7672823003274187
    },
    {
      "doc": 95,
      "topic": 2,
      "similarity": 0.7848090758278646
    },
    {
      "doc": 95,
      "topic": 3,
      "similarity": 0.824055870165871
    },
    {
      "doc": 95,
      "topic": 4,
      "similarity": 0.7804442045488019
    },
    {
      "doc": 95,
      "topic": 5,
      "similarity": 0.8162281212095108
    },
    {
      "doc": 95,
      "topic": 7,
      "similarity": 0.840192492933525
    },
    {
      "doc": 95,
      "topic": 8,
      "similarity": 0.8227666408850222
    },
    {
      "doc": 95,
      "topic": 9,
      "similarity": 0.8301053939422847
    },
    {
      "doc": 95,
      "topic": 10,
      "similarity": 0.7982399610566135
    },
    {
      "doc": 95,
      "topic": 11,
      "similarity": 0.803849682034964
    },
    {
      "doc": 95,
      "topic": 13,
      "similarity": 0.7923767231520502
    },
    {
      "doc": 95,
      "topic": 14,
      "similarity": 0.7839473330620746
    },
    {
      "doc": 95,
      "topic": 15,
      "similarity": 0.7812427203161244
    },
    {
      "doc": 95,
      "topic": 16,
      "similarity": 0.828191812862469
    },
    {
      "doc": 95,
      "topic": 17,
      "similarity": 0.8019339432408107
    },
    {
      "doc": 95,
      "topic": 18,
      "similarity": 0.8077734894291815
    },
    {
      "doc": 95,
      "topic": 19,
      "similarity": 0.8449363134010013
    },
    {
      "doc": 95,
      "topic": 20,
      "similarity": 0.7862897775845358
    },
    {
      "doc": 95,
      "topic": 21,
      "similarity": 0.8017384569626395
    },
    {
      "doc": 96,
      "topic": 3,
      "similarity": 0.7688273202452808
    },
    {
      "doc": 96,
      "topic": 5,
      "similarity": 0.7676662521950993
    },
    {
      "doc": 96,
      "topic": 9,
      "similarity": 0.8039560886677051
    },
    {
      "doc": 96,
      "topic": 10,
      "similarity": 0.7774304154309756
    },
    {
      "doc": 96,
      "topic": 11,
      "similarity": 0.7637141180206571
    },
    {
      "doc": 96,
      "topic": 15,
      "similarity": 0.7741344444243947
    },
    {
      "doc": 96,
      "topic": 16,
      "similarity": 0.7856404789007961
    },
    {
      "doc": 96,
      "topic": 17,
      "similarity": 0.7904643065977214
    },
    {
      "doc": 96,
      "topic": 19,
      "similarity": 0.7721237825415973
    },
    {
      "doc": 96,
      "topic": 20,
      "similarity": 0.7650719764350592
    },
    {
      "doc": 96,
      "topic": 21,
      "similarity": 0.7888170386055586
    },
    {
      "doc": 96,
      "topic": 24,
      "similarity": 0.7597405729228048
    },
    {
      "doc": 97,
      "topic": 2,
      "similarity": 0.7626529830197253
    },
    {
      "doc": 97,
      "topic": 3,
      "similarity": 0.7952531601305723
    },
    {
      "doc": 97,
      "topic": 5,
      "similarity": 0.7685000757300122
    },
    {
      "doc": 97,
      "topic": 7,
      "similarity": 0.7630051848136337
    },
    {
      "doc": 97,
      "topic": 8,
      "similarity": 0.7651519805996589
    },
    {
      "doc": 97,
      "topic": 9,
      "similarity": 0.812439464652914
    },
    {
      "doc": 97,
      "topic": 10,
      "similarity": 0.759432771590374
    },
    {
      "doc": 97,
      "topic": 11,
      "similarity": 0.7705019793051218
    },
    {
      "doc": 97,
      "topic": 15,
      "similarity": 0.7611818818736706
    },
    {
      "doc": 97,
      "topic": 16,
      "similarity": 0.7851943133258661
    },
    {
      "doc": 97,
      "topic": 17,
      "similarity": 0.776878838565999
    },
    {
      "doc": 97,
      "topic": 19,
      "similarity": 0.7762468773974157
    },
    {
      "doc": 97,
      "topic": 20,
      "similarity": 0.7948033233731462
    },
    {
      "doc": 97,
      "topic": 21,
      "similarity": 0.7921530684811318
    },
    {
      "doc": 97,
      "topic": 23,
      "similarity": 0.7694477038677177
    },
    {
      "doc": 97,
      "topic": 24,
      "similarity": 0.7635178970631996
    },
    {
      "doc": 98,
      "topic": 2,
      "similarity": 0.7863568393026845
    },
    {
      "doc": 98,
      "topic": 3,
      "similarity": 0.794492815054959
    },
    {
      "doc": 98,
      "topic": 5,
      "similarity": 0.7689577522564884
    },
    {
      "doc": 98,
      "topic": 7,
      "similarity": 0.765541771774883
    },
    {
      "doc": 98,
      "topic": 8,
      "similarity": 0.7698024620232266
    },
    {
      "doc": 98,
      "topic": 9,
      "similarity": 0.8113043210693662
    },
    {
      "doc": 98,
      "topic": 10,
      "similarity": 0.7800279649271544
    },
    {
      "doc": 98,
      "topic": 11,
      "similarity": 0.7905465546675524
    },
    {
      "doc": 98,
      "topic": 12,
      "similarity": 0.7583925763657351
    },
    {
      "doc": 98,
      "topic": 13,
      "similarity": 0.772995580569144
    },
    {
      "doc": 98,
      "topic": 14,
      "similarity": 0.7759946423998246
    },
    {
      "doc": 98,
      "topic": 15,
      "similarity": 0.7901221064636932
    },
    {
      "doc": 98,
      "topic": 16,
      "similarity": 0.7976939089133644
    },
    {
      "doc": 98,
      "topic": 17,
      "similarity": 0.7968233265090598
    },
    {
      "doc": 98,
      "topic": 18,
      "similarity": 0.8171181954593572
    },
    {
      "doc": 98,
      "topic": 19,
      "similarity": 0.8466119677485616
    },
    {
      "doc": 98,
      "topic": 20,
      "similarity": 0.7958064523800239
    },
    {
      "doc": 98,
      "topic": 21,
      "similarity": 0.7899426483171992
    },
    {
      "doc": 99,
      "topic": 1,
      "similarity": 0.7646766096624253
    },
    {
      "doc": 99,
      "topic": 2,
      "similarity": 0.7679860152316131
    },
    {
      "doc": 99,
      "topic": 3,
      "similarity": 0.7957256247502927
    },
    {
      "doc": 99,
      "topic": 5,
      "similarity": 0.8089641397006565
    },
    {
      "doc": 99,
      "topic": 7,
      "similarity": 0.7789622542067542
    },
    {
      "doc": 99,
      "topic": 8,
      "similarity": 0.7886911001477163
    },
    {
      "doc": 99,
      "topic": 9,
      "similarity": 0.8425140425322882
    },
    {
      "doc": 99,
      "topic": 10,
      "similarity": 0.8099621433895952
    },
    {
      "doc": 99,
      "topic": 11,
      "similarity": 0.7968584067840668
    },
    {
      "doc": 99,
      "topic": 13,
      "similarity": 0.7906749160594785
    },
    {
      "doc": 99,
      "topic": 14,
      "similarity": 0.7779653183506878
    },
    {
      "doc": 99,
      "topic": 15,
      "similarity": 0.7845069227000734
    },
    {
      "doc": 99,
      "topic": 16,
      "similarity": 0.8192642909761041
    },
    {
      "doc": 99,
      "topic": 17,
      "similarity": 0.8150244347199938
    },
    {
      "doc": 99,
      "topic": 18,
      "similarity": 0.7643363312347636
    },
    {
      "doc": 99,
      "topic": 19,
      "similarity": 0.8105611599241288
    },
    {
      "doc": 99,
      "topic": 20,
      "similarity": 0.7832474097044234
    },
    {
      "doc": 99,
      "topic": 21,
      "similarity": 0.8476458760040403
    },
    {
      "doc": 99,
      "topic": 23,
      "similarity": 0.7553049962734237
    },
    {
      "doc": 99,
      "topic": 24,
      "similarity": 0.7584376683586149
    },
    {
      "doc": 100,
      "topic": 2,
      "similarity": 0.7988528962083437
    },
    {
      "doc": 100,
      "topic": 3,
      "similarity": 0.7961372876497407
    },
    {
      "doc": 100,
      "topic": 4,
      "similarity": 0.7524765134180217
    },
    {
      "doc": 100,
      "topic": 5,
      "similarity": 0.8197229269141358
    },
    {
      "doc": 100,
      "topic": 7,
      "similarity": 0.7896197055323857
    },
    {
      "doc": 100,
      "topic": 8,
      "similarity": 0.7817427622585531
    },
    {
      "doc": 100,
      "topic": 9,
      "similarity": 0.8308476933199593
    },
    {
      "doc": 100,
      "topic": 10,
      "similarity": 0.7841838478320745
    },
    {
      "doc": 100,
      "topic": 11,
      "similarity": 0.7850770824782257
    },
    {
      "doc": 100,
      "topic": 13,
      "similarity": 0.7568106614231874
    },
    {
      "doc": 100,
      "topic": 14,
      "similarity": 0.759241308387577
    },
    {
      "doc": 100,
      "topic": 15,
      "similarity": 0.7735340555032285
    },
    {
      "doc": 100,
      "topic": 16,
      "similarity": 0.8013777014389488
    },
    {
      "doc": 100,
      "topic": 17,
      "similarity": 0.8144888931601011
    },
    {
      "doc": 100,
      "topic": 18,
      "similarity": 0.7630263075118994
    },
    {
      "doc": 100,
      "topic": 19,
      "similarity": 0.8196338195698571
    },
    {
      "doc": 100,
      "topic": 20,
      "similarity": 0.7693062606113087
    },
    {
      "doc": 100,
      "topic": 21,
      "similarity": 0.8079354352626801
    },
    {
      "doc": 100,
      "topic": 23,
      "similarity": 0.7583674202742224
    },
    {
      "doc": 100,
      "topic": 24,
      "similarity": 0.7740139646519374
    },
    {
      "doc": 101,
      "topic": 3,
      "similarity": 0.7889393329351482
    },
    {
      "doc": 101,
      "topic": 5,
      "similarity": 0.7809712104055327
    },
    {
      "doc": 101,
      "topic": 6,
      "similarity": 0.7662304386698069
    },
    {
      "doc": 101,
      "topic": 7,
      "similarity": 0.7701270385120851
    },
    {
      "doc": 101,
      "topic": 9,
      "similarity": 0.8368220284411602
    },
    {
      "doc": 101,
      "topic": 11,
      "similarity": 0.7753043319797562
    },
    {
      "doc": 101,
      "topic": 16,
      "similarity": 0.7730263896329991
    },
    {
      "doc": 101,
      "topic": 17,
      "similarity": 0.7715569513197035
    },
    {
      "doc": 101,
      "topic": 19,
      "similarity": 0.7859473636159606
    },
    {
      "doc": 101,
      "topic": 20,
      "similarity": 0.7538458644809266
    },
    {
      "doc": 101,
      "topic": 21,
      "similarity": 0.7808802599806863
    },
    {
      "doc": 101,
      "topic": 23,
      "similarity": 0.7890655746522196
    },
    {
      "doc": 101,
      "topic": 24,
      "similarity": 0.7846233681333564
    },
    {
      "doc": 102,
      "topic": 3,
      "similarity": 0.7703929583492848
    },
    {
      "doc": 102,
      "topic": 5,
      "similarity": 0.7584763303072654
    },
    {
      "doc": 102,
      "topic": 7,
      "similarity": 0.781385924695141
    },
    {
      "doc": 102,
      "topic": 8,
      "similarity": 0.7644292253400169
    },
    {
      "doc": 102,
      "topic": 9,
      "similarity": 0.8195874457073671
    },
    {
      "doc": 102,
      "topic": 11,
      "similarity": 0.7539308001871333
    },
    {
      "doc": 102,
      "topic": 15,
      "similarity": 0.7504012101947333
    },
    {
      "doc": 102,
      "topic": 16,
      "similarity": 0.7699556954661355
    },
    {
      "doc": 102,
      "topic": 17,
      "similarity": 0.7557548668301175
    },
    {
      "doc": 102,
      "topic": 19,
      "similarity": 0.7607364931837459
    },
    {
      "doc": 102,
      "topic": 20,
      "similarity": 0.7575288217919262
    },
    {
      "doc": 102,
      "topic": 21,
      "similarity": 0.7577161636164769
    },
    {
      "doc": 102,
      "topic": 24,
      "similarity": 0.754727720871243
    },
    {
      "doc": 103,
      "topic": 3,
      "similarity": 0.7637809876559356
    },
    {
      "doc": 103,
      "topic": 9,
      "similarity": 0.772556351195523
    },
    {
      "doc": 103,
      "topic": 11,
      "similarity": 0.7559981367523854
    },
    {
      "doc": 103,
      "topic": 16,
      "similarity": 0.7508602215697
    },
    {
      "doc": 103,
      "topic": 19,
      "similarity": 0.7603093282276748
    },
    {
      "doc": 103,
      "topic": 24,
      "similarity": 0.7839548154663842
    },
    {
      "doc": 104,
      "topic": 1,
      "similarity": 0.75903675045984
    },
    {
      "doc": 104,
      "topic": 2,
      "similarity": 0.7876757335066994
    },
    {
      "doc": 104,
      "topic": 3,
      "similarity": 0.8083370869005394
    },
    {
      "doc": 104,
      "topic": 4,
      "similarity": 0.7690510404071813
    },
    {
      "doc": 104,
      "topic": 5,
      "similarity": 0.8050769089985589
    },
    {
      "doc": 104,
      "topic": 7,
      "similarity": 0.8007510448594701
    },
    {
      "doc": 104,
      "topic": 8,
      "similarity": 0.7951074146791304
    },
    {
      "doc": 104,
      "topic": 9,
      "similarity": 0.8459497656353974
    },
    {
      "doc": 104,
      "topic": 10,
      "similarity": 0.8178794013590878
    },
    {
      "doc": 104,
      "topic": 11,
      "similarity": 0.8066265424522568
    },
    {
      "doc": 104,
      "topic": 13,
      "similarity": 0.7852631891287437
    },
    {
      "doc": 104,
      "topic": 14,
      "similarity": 0.7615976204099832
    },
    {
      "doc": 104,
      "topic": 15,
      "similarity": 0.8049737219722417
    },
    {
      "doc": 104,
      "topic": 16,
      "similarity": 0.8298592644714022
    },
    {
      "doc": 104,
      "topic": 17,
      "similarity": 0.832291518398981
    },
    {
      "doc": 104,
      "topic": 18,
      "similarity": 0.78518175683061
    },
    {
      "doc": 104,
      "topic": 19,
      "similarity": 0.8191566118159191
    },
    {
      "doc": 104,
      "topic": 20,
      "similarity": 0.8019035691983223
    },
    {
      "doc": 104,
      "topic": 21,
      "similarity": 0.8542073267251666
    },
    {
      "doc": 104,
      "topic": 24,
      "similarity": 0.7592518674364821
    },
    {
      "doc": 105,
      "topic": 3,
      "similarity": 0.793372536385753
    },
    {
      "doc": 105,
      "topic": 4,
      "similarity": 0.8363127697410299
    },
    {
      "doc": 105,
      "topic": 5,
      "similarity": 0.7964597576241521
    },
    {
      "doc": 105,
      "topic": 7,
      "similarity": 0.7895004353670879
    },
    {
      "doc": 105,
      "topic": 8,
      "similarity": 0.7627784489590779
    },
    {
      "doc": 105,
      "topic": 9,
      "similarity": 0.8111813187167809
    },
    {
      "doc": 105,
      "topic": 10,
      "similarity": 0.7995201972090777
    },
    {
      "doc": 105,
      "topic": 11,
      "similarity": 0.7600763855512445
    },
    {
      "doc": 105,
      "topic": 13,
      "similarity": 0.757586693098078
    },
    {
      "doc": 105,
      "topic": 14,
      "similarity": 0.7518996610042228
    },
    {
      "doc": 105,
      "topic": 15,
      "similarity": 0.758610462242858
    },
    {
      "doc": 105,
      "topic": 16,
      "similarity": 0.7996925466581994
    },
    {
      "doc": 105,
      "topic": 17,
      "similarity": 0.7791923125675315
    },
    {
      "doc": 105,
      "topic": 19,
      "similarity": 0.7846678929144917
    },
    {
      "doc": 105,
      "topic": 20,
      "similarity": 0.767272051802455
    },
    {
      "doc": 105,
      "topic": 21,
      "similarity": 0.779600579892752
    },
    {
      "doc": 105,
      "topic": 24,
      "similarity": 0.7616680854184146
    },
    {
      "doc": 106,
      "topic": 2,
      "similarity": 0.7631915276256955
    },
    {
      "doc": 106,
      "topic": 3,
      "similarity": 0.7993646835888931
    },
    {
      "doc": 106,
      "topic": 4,
      "similarity": 0.7617613302027092
    },
    {
      "doc": 106,
      "topic": 5,
      "similarity": 0.7831440678368496
    },
    {
      "doc": 106,
      "topic": 7,
      "similarity": 0.7788773783745431
    },
    {
      "doc": 106,
      "topic": 8,
      "similarity": 0.7821097844402108
    },
    {
      "doc": 106,
      "topic": 9,
      "similarity": 0.818192436294271
    },
    {
      "doc": 106,
      "topic": 10,
      "similarity": 0.7868459122728072
    },
    {
      "doc": 106,
      "topic": 11,
      "similarity": 0.7805252172301037
    },
    {
      "doc": 106,
      "topic": 13,
      "similarity": 0.7738579208680142
    },
    {
      "doc": 106,
      "topic": 14,
      "similarity": 0.7807099644381116
    },
    {
      "doc": 106,
      "topic": 15,
      "similarity": 0.7711414766900605
    },
    {
      "doc": 106,
      "topic": 16,
      "similarity": 0.7985105374195588
    },
    {
      "doc": 106,
      "topic": 17,
      "similarity": 0.7805899563284491
    },
    {
      "doc": 106,
      "topic": 18,
      "similarity": 0.7985090782001393
    },
    {
      "doc": 106,
      "topic": 19,
      "similarity": 0.8275964063510902
    },
    {
      "doc": 106,
      "topic": 20,
      "similarity": 0.7631082965618725
    },
    {
      "doc": 106,
      "topic": 21,
      "similarity": 0.7782743253198112
    },
    {
      "doc": 106,
      "topic": 22,
      "similarity": 0.7510783712922532
    },
    {
      "doc": 106,
      "topic": 24,
      "similarity": 0.7551154181309379
    },
    {
      "doc": 107,
      "topic": 0,
      "similarity": 0.7790367619524672
    },
    {
      "doc": 107,
      "topic": 2,
      "similarity": 0.7623198572730404
    },
    {
      "doc": 107,
      "topic": 3,
      "similarity": 0.7802600232590999
    },
    {
      "doc": 107,
      "topic": 5,
      "similarity": 0.7660763109213459
    },
    {
      "doc": 107,
      "topic": 7,
      "similarity": 0.7588722634859687
    },
    {
      "doc": 107,
      "topic": 8,
      "similarity": 0.7591838894285315
    },
    {
      "doc": 107,
      "topic": 9,
      "similarity": 0.7910187779732871
    },
    {
      "doc": 107,
      "topic": 11,
      "similarity": 0.7992082068639741
    },
    {
      "doc": 107,
      "topic": 15,
      "similarity": 0.7614672062760531
    },
    {
      "doc": 107,
      "topic": 16,
      "similarity": 0.7876563929084544
    },
    {
      "doc": 107,
      "topic": 17,
      "similarity": 0.7787782933514183
    },
    {
      "doc": 107,
      "topic": 18,
      "similarity": 0.751375443076403
    },
    {
      "doc": 107,
      "topic": 19,
      "similarity": 0.783101955830392
    },
    {
      "doc": 107,
      "topic": 20,
      "similarity": 0.7764173900792863
    },
    {
      "doc": 107,
      "topic": 21,
      "similarity": 0.7836004625892687
    },
    {
      "doc": 107,
      "topic": 23,
      "similarity": 0.7558832280555602
    },
    {
      "doc": 107,
      "topic": 24,
      "similarity": 0.753938518376012
    },
    {
      "doc": 108,
      "topic": 2,
      "similarity": 0.7687168701757383
    },
    {
      "doc": 108,
      "topic": 3,
      "similarity": 0.7993515120090121
    },
    {
      "doc": 108,
      "topic": 4,
      "similarity": 0.7623130299920936
    },
    {
      "doc": 108,
      "topic": 5,
      "similarity": 0.7795290314113732
    },
    {
      "doc": 108,
      "topic": 7,
      "similarity": 0.7704856424386664
    },
    {
      "doc": 108,
      "topic": 8,
      "similarity": 0.7586357538413627
    },
    {
      "doc": 108,
      "topic": 9,
      "similarity": 0.8067417345745186
    },
    {
      "doc": 108,
      "topic": 10,
      "similarity": 0.7715009653777184
    },
    {
      "doc": 108,
      "topic": 11,
      "similarity": 0.7597068884159863
    },
    {
      "doc": 108,
      "topic": 13,
      "similarity": 0.7571059540963216
    },
    {
      "doc": 108,
      "topic": 14,
      "similarity": 0.7562608143372016
    },
    {
      "doc": 108,
      "topic": 15,
      "similarity": 0.7649964467235836
    },
    {
      "doc": 108,
      "topic": 16,
      "similarity": 0.8178395925165365
    },
    {
      "doc": 108,
      "topic": 17,
      "similarity": 0.7860970212994529
    },
    {
      "doc": 108,
      "topic": 18,
      "similarity": 0.7697898620259739
    },
    {
      "doc": 108,
      "topic": 19,
      "similarity": 0.8201487597343672
    },
    {
      "doc": 108,
      "topic": 20,
      "similarity": 0.7792118549199474
    },
    {
      "doc": 108,
      "topic": 21,
      "similarity": 0.8131241496043928
    },
    {
      "doc": 108,
      "topic": 23,
      "similarity": 0.7562488442620633
    },
    {
      "doc": 108,
      "topic": 24,
      "similarity": 0.7550554715444473
    },
    {
      "doc": 109,
      "topic": 3,
      "similarity": 0.758005407123115
    },
    {
      "doc": 109,
      "topic": 4,
      "similarity": 0.831877163012508
    },
    {
      "doc": 109,
      "topic": 5,
      "similarity": 0.7504530877212846
    },
    {
      "doc": 109,
      "topic": 9,
      "similarity": 0.7628519941745868
    },
    {
      "doc": 109,
      "topic": 10,
      "similarity": 0.7647263373181098
    },
    {
      "doc": 109,
      "topic": 16,
      "similarity": 0.7638175995085448
    },
    {
      "doc": 109,
      "topic": 17,
      "similarity": 0.7548805150450043
    },
    {
      "doc": 109,
      "topic": 19,
      "similarity": 0.7709561552828208
    },
    {
      "doc": 109,
      "topic": 21,
      "similarity": 0.7582666976684667
    },
    {
      "doc": 110,
      "topic": 3,
      "similarity": 0.7929384791103286
    },
    {
      "doc": 110,
      "topic": 4,
      "similarity": 0.7786157860628037
    },
    {
      "doc": 110,
      "topic": 5,
      "similarity": 0.7902253725327731
    },
    {
      "doc": 110,
      "topic": 7,
      "similarity": 0.7868723916287284
    },
    {
      "doc": 110,
      "topic": 8,
      "similarity": 0.7681429088903572
    },
    {
      "doc": 110,
      "topic": 9,
      "similarity": 0.801266648560685
    },
    {
      "doc": 110,
      "topic": 10,
      "similarity": 0.7934605054607016
    },
    {
      "doc": 110,
      "topic": 11,
      "similarity": 0.7696985365013663
    },
    {
      "doc": 110,
      "topic": 13,
      "similarity": 0.7548046530860604
    },
    {
      "doc": 110,
      "topic": 15,
      "similarity": 0.7640776895468806
    },
    {
      "doc": 110,
      "topic": 16,
      "similarity": 0.7966043932239506
    },
    {
      "doc": 110,
      "topic": 17,
      "similarity": 0.7948766004676195
    },
    {
      "doc": 110,
      "topic": 19,
      "similarity": 0.7997919394160867
    },
    {
      "doc": 110,
      "topic": 20,
      "similarity": 0.7770843852802778
    },
    {
      "doc": 110,
      "topic": 21,
      "similarity": 0.793409692451754
    },
    {
      "doc": 111,
      "topic": 3,
      "similarity": 0.7610304695187818
    },
    {
      "doc": 111,
      "topic": 5,
      "similarity": 0.7804368700971713
    },
    {
      "doc": 111,
      "topic": 9,
      "similarity": 0.781128845080339
    },
    {
      "doc": 111,
      "topic": 19,
      "similarity": 0.7572610376399221
    },
    {
      "doc": 111,
      "topic": 20,
      "similarity": 0.7602345600692881
    },
    {
      "doc": 111,
      "topic": 21,
      "similarity": 0.7596826864077958
    },
    {
      "doc": 111,
      "topic": 23,
      "similarity": 0.7713877626765974
    },
    {
      "doc": 111,
      "topic": 24,
      "similarity": 0.7700186571837293
    },
    {
      "doc": 112,
      "topic": 1,
      "similarity": 0.7641042412947705
    },
    {
      "doc": 112,
      "topic": 2,
      "similarity": 0.7758976449817233
    },
    {
      "doc": 112,
      "topic": 3,
      "similarity": 0.7926660865671765
    },
    {
      "doc": 112,
      "topic": 5,
      "similarity": 0.7991577629365849
    },
    {
      "doc": 112,
      "topic": 7,
      "similarity": 0.7813338615692113
    },
    {
      "doc": 112,
      "topic": 8,
      "similarity": 0.7758582796700175
    },
    {
      "doc": 112,
      "topic": 9,
      "similarity": 0.8335334066783244
    },
    {
      "doc": 112,
      "topic": 10,
      "similarity": 0.7871659545636982
    },
    {
      "doc": 112,
      "topic": 11,
      "similarity": 0.7893114823224807
    },
    {
      "doc": 112,
      "topic": 13,
      "similarity": 0.7741642597656452
    },
    {
      "doc": 112,
      "topic": 14,
      "similarity": 0.7941972784376399
    },
    {
      "doc": 112,
      "topic": 15,
      "similarity": 0.7698081362202778
    },
    {
      "doc": 112,
      "topic": 16,
      "similarity": 0.7874235977893443
    },
    {
      "doc": 112,
      "topic": 17,
      "similarity": 0.8066837502005672
    },
    {
      "doc": 112,
      "topic": 18,
      "similarity": 0.7553706330438807
    },
    {
      "doc": 112,
      "topic": 19,
      "similarity": 0.804931415213298
    },
    {
      "doc": 112,
      "topic": 20,
      "similarity": 0.7675220414507344
    },
    {
      "doc": 112,
      "topic": 21,
      "similarity": 0.7929257912865872
    },
    {
      "doc": 112,
      "topic": 24,
      "similarity": 0.7542446917787579
    },
    {
      "doc": 113,
      "topic": 3,
      "similarity": 0.7875193610741926
    },
    {
      "doc": 113,
      "topic": 5,
      "similarity": 0.7918980360219524
    },
    {
      "doc": 113,
      "topic": 7,
      "similarity": 0.7758033853793698
    },
    {
      "doc": 113,
      "topic": 8,
      "similarity": 0.7766564757330209
    },
    {
      "doc": 113,
      "topic": 9,
      "similarity": 0.8287183068958207
    },
    {
      "doc": 113,
      "topic": 10,
      "similarity": 0.7708734151455973
    },
    {
      "doc": 113,
      "topic": 11,
      "similarity": 0.777691844064655
    },
    {
      "doc": 113,
      "topic": 12,
      "similarity": 0.7550128558959751
    },
    {
      "doc": 113,
      "topic": 13,
      "similarity": 0.7669192071562771
    },
    {
      "doc": 113,
      "topic": 14,
      "similarity": 0.7617047338933609
    },
    {
      "doc": 113,
      "topic": 15,
      "similarity": 0.765937361551567
    },
    {
      "doc": 113,
      "topic": 16,
      "similarity": 0.8002302332459548
    },
    {
      "doc": 113,
      "topic": 17,
      "similarity": 0.7882301632147827
    },
    {
      "doc": 113,
      "topic": 18,
      "similarity": 0.7803758175858417
    },
    {
      "doc": 113,
      "topic": 19,
      "similarity": 0.8109574287363723
    },
    {
      "doc": 113,
      "topic": 20,
      "similarity": 0.7717542626469437
    },
    {
      "doc": 113,
      "topic": 21,
      "similarity": 0.7914251679203154
    },
    {
      "doc": 113,
      "topic": 23,
      "similarity": 0.7601182349935133
    },
    {
      "doc": 113,
      "topic": 24,
      "similarity": 0.7605999597581448
    },
    {
      "doc": 114,
      "topic": 3,
      "similarity": 0.7790030749632203
    },
    {
      "doc": 114,
      "topic": 5,
      "similarity": 0.7776902848582817
    },
    {
      "doc": 114,
      "topic": 9,
      "similarity": 0.7970079152489332
    },
    {
      "doc": 114,
      "topic": 10,
      "similarity": 0.7500377095389363
    },
    {
      "doc": 114,
      "topic": 15,
      "similarity": 0.7637918078060204
    },
    {
      "doc": 114,
      "topic": 16,
      "similarity": 0.7815009922509881
    },
    {
      "doc": 114,
      "topic": 17,
      "similarity": 0.778502352617786
    },
    {
      "doc": 114,
      "topic": 18,
      "similarity": 0.7714536289258342
    },
    {
      "doc": 114,
      "topic": 19,
      "similarity": 0.782984091144319
    },
    {
      "doc": 114,
      "topic": 20,
      "similarity": 0.7642962314411206
    },
    {
      "doc": 114,
      "topic": 21,
      "similarity": 0.7871268179805986
    },
    {
      "doc": 114,
      "topic": 24,
      "similarity": 0.7515356086336188
    },
    {
      "doc": 115,
      "topic": 2,
      "similarity": 0.7519648949795339
    },
    {
      "doc": 115,
      "topic": 3,
      "similarity": 0.7777636881256004
    },
    {
      "doc": 115,
      "topic": 5,
      "similarity": 0.7795903420336174
    },
    {
      "doc": 115,
      "topic": 7,
      "similarity": 0.7992605218967826
    },
    {
      "doc": 115,
      "topic": 8,
      "similarity": 0.7807631360388934
    },
    {
      "doc": 115,
      "topic": 9,
      "similarity": 0.8236028069527459
    },
    {
      "doc": 115,
      "topic": 10,
      "similarity": 0.7582957420048745
    },
    {
      "doc": 115,
      "topic": 11,
      "similarity": 0.763988555562665
    },
    {
      "doc": 115,
      "topic": 15,
      "similarity": 0.7825096069805014
    },
    {
      "doc": 115,
      "topic": 16,
      "similarity": 0.783176594758304
    },
    {
      "doc": 115,
      "topic": 17,
      "similarity": 0.8036704473634727
    },
    {
      "doc": 115,
      "topic": 19,
      "similarity": 0.779853006708761
    },
    {
      "doc": 115,
      "topic": 20,
      "similarity": 0.7634894922801011
    },
    {
      "doc": 115,
      "topic": 21,
      "similarity": 0.8080984403615562
    },
    {
      "doc": 115,
      "topic": 24,
      "similarity": 0.7610941935652417
    },
    {
      "doc": 116,
      "topic": 3,
      "similarity": 0.7597425167107655
    },
    {
      "doc": 116,
      "topic": 4,
      "similarity": 0.8240143997427768
    },
    {
      "doc": 116,
      "topic": 10,
      "similarity": 0.7542820214781749
    },
    {
      "doc": 116,
      "topic": 16,
      "similarity": 0.7526959132414628
    },
    {
      "doc": 116,
      "topic": 24,
      "similarity": 0.7710537622762215
    },
    {
      "doc": 117,
      "topic": 3,
      "similarity": 0.7963991096155003
    },
    {
      "doc": 117,
      "topic": 4,
      "similarity": 0.7868712732214008
    },
    {
      "doc": 117,
      "topic": 5,
      "similarity": 0.7797429982104133
    },
    {
      "doc": 117,
      "topic": 7,
      "similarity": 0.7834765912643751
    },
    {
      "doc": 117,
      "topic": 8,
      "similarity": 0.7617129934710177
    },
    {
      "doc": 117,
      "topic": 9,
      "similarity": 0.8053565152020075
    },
    {
      "doc": 117,
      "topic": 10,
      "similarity": 0.7643351920471534
    },
    {
      "doc": 117,
      "topic": 11,
      "similarity": 0.7770350701992216
    },
    {
      "doc": 117,
      "topic": 16,
      "similarity": 0.7903802963395087
    },
    {
      "doc": 117,
      "topic": 17,
      "similarity": 0.7795124844055943
    },
    {
      "doc": 117,
      "topic": 19,
      "similarity": 0.7845783444108788
    },
    {
      "doc": 117,
      "topic": 20,
      "similarity": 0.7753781735983233
    },
    {
      "doc": 117,
      "topic": 21,
      "similarity": 0.7867338725295465
    },
    {
      "doc": 117,
      "topic": 24,
      "similarity": 0.757581155364488
    },
    {
      "doc": 118,
      "topic": 3,
      "similarity": 0.7847817358722494
    },
    {
      "doc": 118,
      "topic": 5,
      "similarity": 0.7811125675584668
    },
    {
      "doc": 118,
      "topic": 6,
      "similarity": 0.7631974244357409
    },
    {
      "doc": 118,
      "topic": 7,
      "similarity": 0.7695166818789361
    },
    {
      "doc": 118,
      "topic": 8,
      "similarity": 0.7524616717507114
    },
    {
      "doc": 118,
      "topic": 9,
      "similarity": 0.8216458429047659
    },
    {
      "doc": 118,
      "topic": 10,
      "similarity": 0.7559419853882001
    },
    {
      "doc": 118,
      "topic": 11,
      "similarity": 0.7706680763335269
    },
    {
      "doc": 118,
      "topic": 15,
      "similarity": 0.7646656618220796
    },
    {
      "doc": 118,
      "topic": 16,
      "similarity": 0.7875919557206715
    },
    {
      "doc": 118,
      "topic": 17,
      "similarity": 0.7934264928798938
    },
    {
      "doc": 118,
      "topic": 18,
      "similarity": 0.7628360203804908
    },
    {
      "doc": 118,
      "topic": 19,
      "similarity": 0.801183403366587
    },
    {
      "doc": 118,
      "topic": 20,
      "similarity": 0.7813477566879714
    },
    {
      "doc": 118,
      "topic": 21,
      "similarity": 0.7841969206240916
    },
    {
      "doc": 118,
      "topic": 23,
      "similarity": 0.7688725527654924
    },
    {
      "doc": 118,
      "topic": 24,
      "similarity": 0.773587350937571
    },
    {
      "doc": 119,
      "topic": 2,
      "similarity": 0.7695470951593784
    },
    {
      "doc": 119,
      "topic": 3,
      "similarity": 0.7874210349894134
    },
    {
      "doc": 119,
      "topic": 4,
      "similarity": 0.7516974106725494
    },
    {
      "doc": 119,
      "topic": 5,
      "similarity": 0.8058906069762686
    },
    {
      "doc": 119,
      "topic": 7,
      "similarity": 0.7635332721757749
    },
    {
      "doc": 119,
      "topic": 8,
      "similarity": 0.780803498282762
    },
    {
      "doc": 119,
      "topic": 9,
      "similarity": 0.7912467756046612
    },
    {
      "doc": 119,
      "topic": 10,
      "similarity": 0.7726973171906077
    },
    {
      "doc": 119,
      "topic": 11,
      "similarity": 0.7586515441681232
    },
    {
      "doc": 119,
      "topic": 12,
      "similarity": 0.765996364770114
    },
    {
      "doc": 119,
      "topic": 13,
      "similarity": 0.7637908078050921
    },
    {
      "doc": 119,
      "topic": 14,
      "similarity": 0.7527147133750586
    },
    {
      "doc": 119,
      "topic": 15,
      "similarity": 0.772294821691516
    },
    {
      "doc": 119,
      "topic": 16,
      "similarity": 0.8227277622371301
    },
    {
      "doc": 119,
      "topic": 17,
      "similarity": 0.8082507131643669
    },
    {
      "doc": 119,
      "topic": 18,
      "similarity": 0.7695091242965076
    },
    {
      "doc": 119,
      "topic": 19,
      "similarity": 0.8066501589188476
    },
    {
      "doc": 119,
      "topic": 20,
      "similarity": 0.7675918442966759
    },
    {
      "doc": 119,
      "topic": 21,
      "similarity": 0.7970442854147941
    },
    {
      "doc": 120,
      "topic": 1,
      "similarity": 0.7572460203787379
    },
    {
      "doc": 120,
      "topic": 2,
      "similarity": 0.751129375591517
    },
    {
      "doc": 120,
      "topic": 3,
      "similarity": 0.8081807155122105
    },
    {
      "doc": 120,
      "topic": 4,
      "similarity": 0.7906440613180029
    },
    {
      "doc": 120,
      "topic": 5,
      "similarity": 0.788379390745924
    },
    {
      "doc": 120,
      "topic": 7,
      "similarity": 0.7838770562468319
    },
    {
      "doc": 120,
      "topic": 8,
      "similarity": 0.7749609541947856
    },
    {
      "doc": 120,
      "topic": 9,
      "similarity": 0.8302423670597022
    },
    {
      "doc": 120,
      "topic": 10,
      "similarity": 0.7902474648227309
    },
    {
      "doc": 120,
      "topic": 11,
      "similarity": 0.7962157715535259
    },
    {
      "doc": 120,
      "topic": 13,
      "similarity": 0.7643864007610873
    },
    {
      "doc": 120,
      "topic": 14,
      "similarity": 0.7600320615869747
    },
    {
      "doc": 120,
      "topic": 15,
      "similarity": 0.7677558871225907
    },
    {
      "doc": 120,
      "topic": 16,
      "similarity": 0.8074881131930237
    },
    {
      "doc": 120,
      "topic": 17,
      "similarity": 0.7926384736630223
    },
    {
      "doc": 120,
      "topic": 19,
      "similarity": 0.7860877698562698
    },
    {
      "doc": 120,
      "topic": 21,
      "similarity": 0.8059551666366604
    },
    {
      "doc": 120,
      "topic": 24,
      "similarity": 0.8360143117709823
    },
    {
      "doc": 121,
      "topic": 3,
      "similarity": 0.8146451546840602
    },
    {
      "doc": 121,
      "topic": 5,
      "similarity": 0.7723211972899447
    },
    {
      "doc": 121,
      "topic": 7,
      "similarity": 0.7681735447359632
    },
    {
      "doc": 121,
      "topic": 8,
      "similarity": 0.7622745717684238
    },
    {
      "doc": 121,
      "topic": 9,
      "similarity": 0.7828557981332989
    },
    {
      "doc": 121,
      "topic": 16,
      "similarity": 0.7560998110883298
    },
    {
      "doc": 121,
      "topic": 17,
      "similarity": 0.7522015669744191
    },
    {
      "doc": 121,
      "topic": 19,
      "similarity": 0.783043002141496
    },
    {
      "doc": 121,
      "topic": 20,
      "similarity": 0.7554334132610425
    },
    {
      "doc": 121,
      "topic": 21,
      "similarity": 0.7573310663560342
    },
    {
      "doc": 122,
      "topic": 3,
      "similarity": 0.7901277273713845
    },
    {
      "doc": 122,
      "topic": 5,
      "similarity": 0.7910050946886147
    },
    {
      "doc": 122,
      "topic": 7,
      "similarity": 0.7678434715524168
    },
    {
      "doc": 122,
      "topic": 8,
      "similarity": 0.7721767947167673
    },
    {
      "doc": 122,
      "topic": 9,
      "similarity": 0.8172446676574088
    },
    {
      "doc": 122,
      "topic": 10,
      "similarity": 0.7678556361507518
    },
    {
      "doc": 122,
      "topic": 11,
      "similarity": 0.7640127168322176
    },
    {
      "doc": 122,
      "topic": 14,
      "similarity": 0.7579311643388966
    },
    {
      "doc": 122,
      "topic": 15,
      "similarity": 0.7657351800487252
    },
    {
      "doc": 122,
      "topic": 16,
      "similarity": 0.799156288137221
    },
    {
      "doc": 122,
      "topic": 17,
      "similarity": 0.789608532776124
    },
    {
      "doc": 122,
      "topic": 19,
      "similarity": 0.7933240967407986
    },
    {
      "doc": 122,
      "topic": 20,
      "similarity": 0.7779437151975699
    },
    {
      "doc": 122,
      "topic": 21,
      "similarity": 0.788479463585535
    },
    {
      "doc": 122,
      "topic": 23,
      "similarity": 0.7669192764532784
    },
    {
      "doc": 122,
      "topic": 24,
      "similarity": 0.7657643402083747
    },
    {
      "doc": 123,
      "topic": 3,
      "similarity": 0.7822009417721718
    },
    {
      "doc": 123,
      "topic": 5,
      "similarity": 0.7813278658985495
    },
    {
      "doc": 123,
      "topic": 7,
      "similarity": 0.7993275386990522
    },
    {
      "doc": 123,
      "topic": 8,
      "similarity": 0.7773202264475852
    },
    {
      "doc": 123,
      "topic": 9,
      "similarity": 0.8176128430191011
    },
    {
      "doc": 123,
      "topic": 10,
      "similarity": 0.7502237760212761
    },
    {
      "doc": 123,
      "topic": 11,
      "similarity": 0.7727031869088888
    },
    {
      "doc": 123,
      "topic": 13,
      "similarity": 0.7514543566226941
    },
    {
      "doc": 123,
      "topic": 16,
      "similarity": 0.7915072639321232
    },
    {
      "doc": 123,
      "topic": 17,
      "similarity": 0.7633652197689876
    },
    {
      "doc": 123,
      "topic": 19,
      "similarity": 0.7864890236672025
    },
    {
      "doc": 123,
      "topic": 20,
      "similarity": 0.7720913133024346
    },
    {
      "doc": 123,
      "topic": 21,
      "similarity": 0.783436633548346
    },
    {
      "doc": 124,
      "topic": 0,
      "similarity": 0.8464528556955432
    },
    {
      "doc": 124,
      "topic": 1,
      "similarity": 0.7780539943742616
    },
    {
      "doc": 124,
      "topic": 2,
      "similarity": 0.7572004638443943
    },
    {
      "doc": 124,
      "topic": 3,
      "similarity": 0.7926211548583137
    },
    {
      "doc": 124,
      "topic": 5,
      "similarity": 0.7620233360375148
    },
    {
      "doc": 124,
      "topic": 7,
      "similarity": 0.7575173252201627
    },
    {
      "doc": 124,
      "topic": 8,
      "similarity": 0.7533494317627157
    },
    {
      "doc": 124,
      "topic": 9,
      "similarity": 0.7922335930994568
    },
    {
      "doc": 124,
      "topic": 10,
      "similarity": 0.7644481571402547
    },
    {
      "doc": 124,
      "topic": 11,
      "similarity": 0.8224877116080913
    },
    {
      "doc": 124,
      "topic": 14,
      "similarity": 0.7546615908382364
    },
    {
      "doc": 124,
      "topic": 15,
      "similarity": 0.7768278342919229
    },
    {
      "doc": 124,
      "topic": 16,
      "similarity": 0.7920190061702981
    },
    {
      "doc": 124,
      "topic": 17,
      "similarity": 0.798976940135355
    },
    {
      "doc": 124,
      "topic": 19,
      "similarity": 0.7816287006324806
    },
    {
      "doc": 124,
      "topic": 20,
      "similarity": 0.7602383467171872
    },
    {
      "doc": 124,
      "topic": 21,
      "similarity": 0.7954638830283066
    },
    {
      "doc": 124,
      "topic": 23,
      "similarity": 0.7577958796188897
    },
    {
      "doc": 124,
      "topic": 24,
      "similarity": 0.7559893945582591
    },
    {
      "doc": 125,
      "topic": 3,
      "similarity": 0.7897993313943824
    },
    {
      "doc": 125,
      "topic": 5,
      "similarity": 0.7717455927900926
    },
    {
      "doc": 125,
      "topic": 7,
      "similarity": 0.7833662857991733
    },
    {
      "doc": 125,
      "topic": 8,
      "similarity": 0.7523045255726691
    },
    {
      "doc": 125,
      "topic": 9,
      "similarity": 0.8353482206670317
    },
    {
      "doc": 125,
      "topic": 10,
      "similarity": 0.7573149957236279
    },
    {
      "doc": 125,
      "topic": 11,
      "similarity": 0.7696553597045879
    },
    {
      "doc": 125,
      "topic": 15,
      "similarity": 0.7531091958328269
    },
    {
      "doc": 125,
      "topic": 16,
      "similarity": 0.7747448490951101
    },
    {
      "doc": 125,
      "topic": 17,
      "similarity": 0.7665171055233512
    },
    {
      "doc": 125,
      "topic": 19,
      "similarity": 0.7827406755789365
    },
    {
      "doc": 125,
      "topic": 21,
      "similarity": 0.77083415186367
    },
    {
      "doc": 125,
      "topic": 23,
      "similarity": 0.7763496250453474
    },
    {
      "doc": 125,
      "topic": 24,
      "similarity": 0.7567041547794846
    },
    {
      "doc": 126,
      "topic": 1,
      "similarity": 0.7636296080511069
    },
    {
      "doc": 126,
      "topic": 2,
      "similarity": 0.7866169125798882
    },
    {
      "doc": 126,
      "topic": 3,
      "similarity": 0.8127254304519826
    },
    {
      "doc": 126,
      "topic": 4,
      "similarity": 0.7649969152829222
    },
    {
      "doc": 126,
      "topic": 5,
      "similarity": 0.815940959353593
    },
    {
      "doc": 126,
      "topic": 7,
      "similarity": 0.8243436443639374
    },
    {
      "doc": 126,
      "topic": 8,
      "similarity": 0.8288555059627036
    },
    {
      "doc": 126,
      "topic": 9,
      "similarity": 0.8476161160669325
    },
    {
      "doc": 126,
      "topic": 10,
      "similarity": 0.8035471688444361
    },
    {
      "doc": 126,
      "topic": 11,
      "similarity": 0.804738913848314
    },
    {
      "doc": 126,
      "topic": 12,
      "similarity": 0.7824274545846887
    },
    {
      "doc": 126,
      "topic": 13,
      "similarity": 0.7983177163341941
    },
    {
      "doc": 126,
      "topic": 14,
      "similarity": 0.8196097610197347
    },
    {
      "doc": 126,
      "topic": 15,
      "similarity": 0.7978670947047518
    },
    {
      "doc": 126,
      "topic": 16,
      "similarity": 0.8097938636822382
    },
    {
      "doc": 126,
      "topic": 17,
      "similarity": 0.8132652583129866
    },
    {
      "doc": 126,
      "topic": 18,
      "similarity": 0.7666608473410961
    },
    {
      "doc": 126,
      "topic": 19,
      "similarity": 0.8184909898308326
    },
    {
      "doc": 126,
      "topic": 20,
      "similarity": 0.7753703325488949
    },
    {
      "doc": 126,
      "topic": 21,
      "similarity": 0.8077151915469792
    },
    {
      "doc": 126,
      "topic": 22,
      "similarity": 0.7509480248600334
    },
    {
      "doc": 127,
      "topic": 1,
      "similarity": 0.7648912268403728
    },
    {
      "doc": 127,
      "topic": 2,
      "similarity": 0.7912910115388043
    },
    {
      "doc": 127,
      "topic": 3,
      "similarity": 0.7961503829747428
    },
    {
      "doc": 127,
      "topic": 5,
      "similarity": 0.7968867645713886
    },
    {
      "doc": 127,
      "topic": 7,
      "similarity": 0.778644697680151
    },
    {
      "doc": 127,
      "topic": 8,
      "similarity": 0.7991714162635121
    },
    {
      "doc": 127,
      "topic": 9,
      "similarity": 0.8451234996525597
    },
    {
      "doc": 127,
      "topic": 10,
      "similarity": 0.8124975181060212
    },
    {
      "doc": 127,
      "topic": 11,
      "similarity": 0.8108184613585931
    },
    {
      "doc": 127,
      "topic": 12,
      "similarity": 0.8220454427077286
    },
    {
      "doc": 127,
      "topic": 13,
      "similarity": 0.7777691238679918
    },
    {
      "doc": 127,
      "topic": 14,
      "similarity": 0.7892330074594226
    },
    {
      "doc": 127,
      "topic": 15,
      "similarity": 0.8016879781901549
    },
    {
      "doc": 127,
      "topic": 16,
      "similarity": 0.8090073099522281
    },
    {
      "doc": 127,
      "topic": 17,
      "similarity": 0.8186623745672684
    },
    {
      "doc": 127,
      "topic": 18,
      "similarity": 0.7754040982649966
    },
    {
      "doc": 127,
      "topic": 19,
      "similarity": 0.8119742509603852
    },
    {
      "doc": 127,
      "topic": 20,
      "similarity": 0.7745196641330558
    },
    {
      "doc": 127,
      "topic": 21,
      "similarity": 0.8111271133065298
    },
    {
      "doc": 127,
      "topic": 22,
      "similarity": 0.7603571712279663
    },
    {
      "doc": 127,
      "topic": 23,
      "similarity": 0.762744428624393
    },
    {
      "doc": 127,
      "topic": 24,
      "similarity": 0.7861761705316067
    },
    {
      "doc": 128,
      "topic": 3,
      "similarity": 0.7630702533396626
    },
    {
      "doc": 128,
      "topic": 5,
      "similarity": 0.7586486413602315
    },
    {
      "doc": 128,
      "topic": 6,
      "similarity": 0.8338740343032849
    },
    {
      "doc": 128,
      "topic": 9,
      "similarity": 0.8016851468754937
    },
    {
      "doc": 128,
      "topic": 10,
      "similarity": 0.7637740327988876
    },
    {
      "doc": 128,
      "topic": 11,
      "similarity": 0.7867256913885462
    },
    {
      "doc": 128,
      "topic": 12,
      "similarity": 0.7576269278197616
    },
    {
      "doc": 128,
      "topic": 15,
      "similarity": 0.7663106131532745
    },
    {
      "doc": 128,
      "topic": 16,
      "similarity": 0.7756243438571471
    },
    {
      "doc": 128,
      "topic": 17,
      "similarity": 0.783084363554087
    },
    {
      "doc": 128,
      "topic": 18,
      "similarity": 0.7710098899661048
    },
    {
      "doc": 128,
      "topic": 19,
      "similarity": 0.7871457453245936
    },
    {
      "doc": 128,
      "topic": 20,
      "similarity": 0.7617853918745788
    },
    {
      "doc": 128,
      "topic": 21,
      "similarity": 0.7719083338754172
    },
    {
      "doc": 128,
      "topic": 23,
      "similarity": 0.7928868564099706
    },
    {
      "doc": 129,
      "topic": 0,
      "similarity": 0.8062140852045946
    },
    {
      "doc": 129,
      "topic": 1,
      "similarity": 0.7803917635723645
    },
    {
      "doc": 129,
      "topic": 2,
      "similarity": 0.7933096350100611
    },
    {
      "doc": 129,
      "topic": 3,
      "similarity": 0.7971943938662401
    },
    {
      "doc": 129,
      "topic": 5,
      "similarity": 0.7794770506512105
    },
    {
      "doc": 129,
      "topic": 7,
      "similarity": 0.7677376694068692
    },
    {
      "doc": 129,
      "topic": 8,
      "similarity": 0.7707490339687935
    },
    {
      "doc": 129,
      "topic": 9,
      "similarity": 0.8002087431747721
    },
    {
      "doc": 129,
      "topic": 10,
      "similarity": 0.7667757815581308
    },
    {
      "doc": 129,
      "topic": 11,
      "similarity": 0.8325686997568701
    },
    {
      "doc": 129,
      "topic": 13,
      "similarity": 0.7581674408442152
    },
    {
      "doc": 129,
      "topic": 14,
      "similarity": 0.769264863019227
    },
    {
      "doc": 129,
      "topic": 15,
      "similarity": 0.7861392021445573
    },
    {
      "doc": 129,
      "topic": 16,
      "similarity": 0.8028951632905413
    },
    {
      "doc": 129,
      "topic": 17,
      "similarity": 0.7940686469672373
    },
    {
      "doc": 129,
      "topic": 18,
      "similarity": 0.7556619619028534
    },
    {
      "doc": 129,
      "topic": 19,
      "similarity": 0.7775907624268186
    },
    {
      "doc": 129,
      "topic": 20,
      "similarity": 0.7667345112762158
    },
    {
      "doc": 129,
      "topic": 21,
      "similarity": 0.7942296775870722
    },
    {
      "doc": 129,
      "topic": 24,
      "similarity": 0.7503660889308817
    },
    {
      "doc": 130,
      "topic": 3,
      "similarity": 0.7794106944208377
    },
    {
      "doc": 130,
      "topic": 5,
      "similarity": 0.7684482000885705
    },
    {
      "doc": 130,
      "topic": 7,
      "similarity": 0.7556502265456538
    },
    {
      "doc": 130,
      "topic": 9,
      "similarity": 0.7892252185800096
    },
    {
      "doc": 130,
      "topic": 11,
      "similarity": 0.7685770472727187
    },
    {
      "doc": 130,
      "topic": 13,
      "similarity": 0.7522391649914396
    },
    {
      "doc": 130,
      "topic": 15,
      "similarity": 0.759012047320858
    },
    {
      "doc": 130,
      "topic": 16,
      "similarity": 0.8002368725412276
    },
    {
      "doc": 130,
      "topic": 17,
      "similarity": 0.7916910156330546
    },
    {
      "doc": 130,
      "topic": 19,
      "similarity": 0.8041912993040423
    },
    {
      "doc": 130,
      "topic": 20,
      "similarity": 0.7608577103276717
    },
    {
      "doc": 130,
      "topic": 21,
      "similarity": 0.7849288473676114
    },
    {
      "doc": 131,
      "topic": 2,
      "similarity": 0.7655604031272257
    },
    {
      "doc": 131,
      "topic": 3,
      "similarity": 0.7764768743050123
    },
    {
      "doc": 131,
      "topic": 5,
      "similarity": 0.7891556324362073
    },
    {
      "doc": 131,
      "topic": 7,
      "similarity": 0.7657463399166737
    },
    {
      "doc": 131,
      "topic": 8,
      "similarity": 0.7651491888931914
    },
    {
      "doc": 131,
      "topic": 9,
      "similarity": 0.8240287773352022
    },
    {
      "doc": 131,
      "topic": 10,
      "similarity": 0.7675878596679075
    },
    {
      "doc": 131,
      "topic": 11,
      "similarity": 0.7629508956817471
    },
    {
      "doc": 131,
      "topic": 12,
      "similarity": 0.7665210667384503
    },
    {
      "doc": 131,
      "topic": 13,
      "similarity": 0.7589979507070759
    },
    {
      "doc": 131,
      "topic": 14,
      "similarity": 0.7585977710623949
    },
    {
      "doc": 131,
      "topic": 15,
      "similarity": 0.7826357922376479
    },
    {
      "doc": 131,
      "topic": 16,
      "similarity": 0.7972195647045218
    },
    {
      "doc": 131,
      "topic": 17,
      "similarity": 0.805682804006797
    },
    {
      "doc": 131,
      "topic": 18,
      "similarity": 0.7603035461697316
    },
    {
      "doc": 131,
      "topic": 19,
      "similarity": 0.7870660768697396
    },
    {
      "doc": 131,
      "topic": 20,
      "similarity": 0.7811714775218183
    },
    {
      "doc": 131,
      "topic": 21,
      "similarity": 0.7940915506839672
    },
    {
      "doc": 131,
      "topic": 23,
      "similarity": 0.7555672383522585
    },
    {
      "doc": 131,
      "topic": 24,
      "similarity": 0.7623228960072328
    },
    {
      "doc": 132,
      "topic": 0,
      "similarity": 0.76593539966034
    },
    {
      "doc": 132,
      "topic": 1,
      "similarity": 0.7549831602836479
    },
    {
      "doc": 132,
      "topic": 2,
      "similarity": 0.7593675492278351
    },
    {
      "doc": 132,
      "topic": 3,
      "similarity": 0.7919125881484659
    },
    {
      "doc": 132,
      "topic": 5,
      "similarity": 0.7658835194700461
    },
    {
      "doc": 132,
      "topic": 7,
      "similarity": 0.7685913925587712
    },
    {
      "doc": 132,
      "topic": 9,
      "similarity": 0.8098768703604898
    },
    {
      "doc": 132,
      "topic": 10,
      "similarity": 0.7575404290441508
    },
    {
      "doc": 132,
      "topic": 11,
      "similarity": 0.7923660448815419
    },
    {
      "doc": 132,
      "topic": 14,
      "similarity": 0.7525562980285336
    },
    {
      "doc": 132,
      "topic": 15,
      "similarity": 0.7728407962630987
    },
    {
      "doc": 132,
      "topic": 16,
      "similarity": 0.7999896699155089
    },
    {
      "doc": 132,
      "topic": 17,
      "similarity": 0.786393250071252
    },
    {
      "doc": 132,
      "topic": 19,
      "similarity": 0.7875369796258601
    },
    {
      "doc": 132,
      "topic": 20,
      "similarity": 0.7756853233768232
    },
    {
      "doc": 132,
      "topic": 21,
      "similarity": 0.7851498956932389
    },
    {
      "doc": 132,
      "topic": 23,
      "similarity": 0.7658132376655631
    },
    {
      "doc": 133,
      "topic": 3,
      "similarity": 0.772438107265277
    },
    {
      "doc": 133,
      "topic": 4,
      "similarity": 0.8173628252954759
    },
    {
      "doc": 133,
      "topic": 5,
      "similarity": 0.7662719048536744
    },
    {
      "doc": 133,
      "topic": 7,
      "similarity": 0.7741890381344244
    },
    {
      "doc": 133,
      "topic": 9,
      "similarity": 0.7752104453155748
    },
    {
      "doc": 133,
      "topic": 10,
      "similarity": 0.7515311387940138
    },
    {
      "doc": 133,
      "topic": 16,
      "similarity": 0.7823194885160513
    },
    {
      "doc": 133,
      "topic": 17,
      "similarity": 0.7639069203978649
    },
    {
      "doc": 133,
      "topic": 19,
      "similarity": 0.7691730093746229
    },
    {
      "doc": 133,
      "topic": 24,
      "similarity": 0.758566353739924
    },
    {
      "doc": 134,
      "topic": 3,
      "similarity": 0.7816358276267289
    },
    {
      "doc": 134,
      "topic": 5,
      "similarity": 0.7686571440346529
    },
    {
      "doc": 134,
      "topic": 8,
      "similarity": 0.7606414162716362
    },
    {
      "doc": 134,
      "topic": 9,
      "similarity": 0.8205472735302083
    },
    {
      "doc": 134,
      "topic": 11,
      "similarity": 0.7591565371028723
    },
    {
      "doc": 134,
      "topic": 12,
      "similarity": 0.7520974718053931
    },
    {
      "doc": 134,
      "topic": 15,
      "similarity": 0.7764769331455545
    },
    {
      "doc": 134,
      "topic": 16,
      "similarity": 0.774244210769601
    },
    {
      "doc": 134,
      "topic": 17,
      "similarity": 0.793937768707853
    },
    {
      "doc": 134,
      "topic": 18,
      "similarity": 0.7584861871725719
    },
    {
      "doc": 134,
      "topic": 19,
      "similarity": 0.7953476483206384
    },
    {
      "doc": 134,
      "topic": 20,
      "similarity": 0.7677921899159892
    },
    {
      "doc": 134,
      "topic": 21,
      "similarity": 0.8023501753906628
    },
    {
      "doc": 134,
      "topic": 23,
      "similarity": 0.7603331230397355
    },
    {
      "doc": 134,
      "topic": 24,
      "similarity": 0.7847651312918207
    },
    {
      "doc": 135,
      "topic": 2,
      "similarity": 0.7681284552126724
    },
    {
      "doc": 135,
      "topic": 3,
      "similarity": 0.7817312967925082
    },
    {
      "doc": 135,
      "topic": 5,
      "similarity": 0.7812981305156677
    },
    {
      "doc": 135,
      "topic": 7,
      "similarity": 0.7601924810724786
    },
    {
      "doc": 135,
      "topic": 8,
      "similarity": 0.7681447341556216
    },
    {
      "doc": 135,
      "topic": 9,
      "similarity": 0.818084344878258
    },
    {
      "doc": 135,
      "topic": 10,
      "similarity": 0.7775042754570402
    },
    {
      "doc": 135,
      "topic": 11,
      "similarity": 0.7705059188788802
    },
    {
      "doc": 135,
      "topic": 13,
      "similarity": 0.767986636848041
    },
    {
      "doc": 135,
      "topic": 14,
      "similarity": 0.762346246222048
    },
    {
      "doc": 135,
      "topic": 15,
      "similarity": 0.7917254125158876
    },
    {
      "doc": 135,
      "topic": 16,
      "similarity": 0.7958058599161192
    },
    {
      "doc": 135,
      "topic": 17,
      "similarity": 0.8562815562760633
    },
    {
      "doc": 135,
      "topic": 18,
      "similarity": 0.7559095234232376
    },
    {
      "doc": 135,
      "topic": 19,
      "similarity": 0.7973545314386511
    },
    {
      "doc": 135,
      "topic": 20,
      "similarity": 0.7971373278638365
    },
    {
      "doc": 135,
      "topic": 21,
      "similarity": 0.8203637021673084
    },
    {
      "doc": 135,
      "topic": 23,
      "similarity": 0.750584829948298
    },
    {
      "doc": 135,
      "topic": 24,
      "similarity": 0.7588527795292745
    },
    {
      "doc": 136,
      "topic": 3,
      "similarity": 0.7674415133511739
    },
    {
      "doc": 136,
      "topic": 5,
      "similarity": 0.76186108816841
    },
    {
      "doc": 136,
      "topic": 7,
      "similarity": 0.7654719217139881
    },
    {
      "doc": 136,
      "topic": 8,
      "similarity": 0.7699380157043447
    },
    {
      "doc": 136,
      "topic": 9,
      "similarity": 0.8051571516584829
    },
    {
      "doc": 136,
      "topic": 10,
      "similarity": 0.7558073368714063
    },
    {
      "doc": 136,
      "topic": 11,
      "similarity": 0.7571655852662335
    },
    {
      "doc": 136,
      "topic": 15,
      "similarity": 0.763553516239468
    },
    {
      "doc": 136,
      "topic": 16,
      "similarity": 0.7641525863876367
    },
    {
      "doc": 136,
      "topic": 17,
      "similarity": 0.7837180252645864
    },
    {
      "doc": 136,
      "topic": 19,
      "similarity": 0.7827028400061261
    },
    {
      "doc": 136,
      "topic": 20,
      "similarity": 0.7566432386120063
    },
    {
      "doc": 136,
      "topic": 21,
      "similarity": 0.7866332798525152
    },
    {
      "doc": 137,
      "topic": 1,
      "similarity": 0.7520388887060304
    },
    {
      "doc": 137,
      "topic": 2,
      "similarity": 0.7650850293681617
    },
    {
      "doc": 137,
      "topic": 3,
      "similarity": 0.7981975778047805
    },
    {
      "doc": 137,
      "topic": 4,
      "similarity": 0.7572555162638361
    },
    {
      "doc": 137,
      "topic": 5,
      "similarity": 0.7999274845634335
    },
    {
      "doc": 137,
      "topic": 7,
      "similarity": 0.7945637380691308
    },
    {
      "doc": 137,
      "topic": 8,
      "similarity": 0.8095817278386117
    },
    {
      "doc": 137,
      "topic": 9,
      "similarity": 0.8331906836359736
    },
    {
      "doc": 137,
      "topic": 10,
      "similarity": 0.7883792381030557
    },
    {
      "doc": 137,
      "topic": 11,
      "similarity": 0.7930256670073483
    },
    {
      "doc": 137,
      "topic": 12,
      "similarity": 0.7534597910612956
    },
    {
      "doc": 137,
      "topic": 13,
      "similarity": 0.8086347501095014
    },
    {
      "doc": 137,
      "topic": 14,
      "similarity": 0.7681448299072655
    },
    {
      "doc": 137,
      "topic": 15,
      "similarity": 0.7635980042879272
    },
    {
      "doc": 137,
      "topic": 16,
      "similarity": 0.8068963252813757
    },
    {
      "doc": 137,
      "topic": 17,
      "similarity": 0.7906345804705144
    },
    {
      "doc": 137,
      "topic": 18,
      "similarity": 0.787695232042744
    },
    {
      "doc": 137,
      "topic": 19,
      "similarity": 0.8501094071197299
    },
    {
      "doc": 137,
      "topic": 20,
      "similarity": 0.8031148988304526
    },
    {
      "doc": 137,
      "topic": 21,
      "similarity": 0.7978524828731869
    },
    {
      "doc": 137,
      "topic": 23,
      "similarity": 0.753279560485219
    },
    {
      "doc": 137,
      "topic": 24,
      "similarity": 0.7582179228625341
    },
    {
      "doc": 138,
      "topic": 3,
      "similarity": 0.7596253504149247
    },
    {
      "doc": 138,
      "topic": 5,
      "similarity": 0.7656283482608
    },
    {
      "doc": 138,
      "topic": 7,
      "similarity": 0.7506443027481093
    },
    {
      "doc": 138,
      "topic": 9,
      "similarity": 0.7838122695182834
    },
    {
      "doc": 138,
      "topic": 10,
      "similarity": 0.7655913303781122
    },
    {
      "doc": 138,
      "topic": 11,
      "similarity": 0.7527945629503887
    },
    {
      "doc": 138,
      "topic": 13,
      "similarity": 0.7516164255245452
    },
    {
      "doc": 138,
      "topic": 15,
      "similarity": 0.757491923219001
    },
    {
      "doc": 138,
      "topic": 16,
      "similarity": 0.7745452074568515
    },
    {
      "doc": 138,
      "topic": 17,
      "similarity": 0.7917461000517036
    },
    {
      "doc": 138,
      "topic": 18,
      "similarity": 0.7581034308672597
    },
    {
      "doc": 138,
      "topic": 19,
      "similarity": 0.7952065912216486
    },
    {
      "doc": 138,
      "topic": 20,
      "similarity": 0.7640390556307323
    },
    {
      "doc": 138,
      "topic": 21,
      "similarity": 0.7803369593345615
    },
    {
      "doc": 139,
      "topic": 2,
      "similarity": 0.7659116881527676
    },
    {
      "doc": 139,
      "topic": 3,
      "similarity": 0.7923841236945598
    },
    {
      "doc": 139,
      "topic": 4,
      "similarity": 0.7537361847320456
    },
    {
      "doc": 139,
      "topic": 5,
      "similarity": 0.8083521942081794
    },
    {
      "doc": 139,
      "topic": 7,
      "similarity": 0.7850137208522003
    },
    {
      "doc": 139,
      "topic": 8,
      "similarity": 0.7637645433871094
    },
    {
      "doc": 139,
      "topic": 9,
      "similarity": 0.816558813083738
    },
    {
      "doc": 139,
      "topic": 10,
      "similarity": 0.7685082032396453
    },
    {
      "doc": 139,
      "topic": 11,
      "similarity": 0.7782500531547544
    },
    {
      "doc": 139,
      "topic": 13,
      "similarity": 0.7576882776745755
    },
    {
      "doc": 139,
      "topic": 14,
      "similarity": 0.7557013509663704
    },
    {
      "doc": 139,
      "topic": 15,
      "similarity": 0.7728375694212721
    },
    {
      "doc": 139,
      "topic": 16,
      "similarity": 0.8152750153697469
    },
    {
      "doc": 139,
      "topic": 17,
      "similarity": 0.8073758885849367
    },
    {
      "doc": 139,
      "topic": 18,
      "similarity": 0.7550814014430878
    },
    {
      "doc": 139,
      "topic": 19,
      "similarity": 0.8145380409720314
    },
    {
      "doc": 139,
      "topic": 20,
      "similarity": 0.7821900798135125
    },
    {
      "doc": 139,
      "topic": 21,
      "similarity": 0.7993458177306522
    },
    {
      "doc": 139,
      "topic": 23,
      "similarity": 0.7524889415606306
    },
    {
      "doc": 139,
      "topic": 24,
      "similarity": 0.7540890162690086
    },
    {
      "doc": 140,
      "topic": 2,
      "similarity": 0.780451979938944
    },
    {
      "doc": 140,
      "topic": 3,
      "similarity": 0.7958975076494675
    },
    {
      "doc": 140,
      "topic": 4,
      "similarity": 0.7577792003524914
    },
    {
      "doc": 140,
      "topic": 5,
      "similarity": 0.8129200346827105
    },
    {
      "doc": 140,
      "topic": 7,
      "similarity": 0.7885585615752129
    },
    {
      "doc": 140,
      "topic": 8,
      "similarity": 0.7785269458480615
    },
    {
      "doc": 140,
      "topic": 9,
      "similarity": 0.8324620786876215
    },
    {
      "doc": 140,
      "topic": 10,
      "similarity": 0.7763901502606747
    },
    {
      "doc": 140,
      "topic": 11,
      "similarity": 0.8328702563516923
    },
    {
      "doc": 140,
      "topic": 13,
      "similarity": 0.7711920825989299
    },
    {
      "doc": 140,
      "topic": 14,
      "similarity": 0.7739610946631834
    },
    {
      "doc": 140,
      "topic": 15,
      "similarity": 0.7948529190921573
    },
    {
      "doc": 140,
      "topic": 16,
      "similarity": 0.8215869296018661
    },
    {
      "doc": 140,
      "topic": 17,
      "similarity": 0.8171415892015849
    },
    {
      "doc": 140,
      "topic": 18,
      "similarity": 0.7709338670329325
    },
    {
      "doc": 140,
      "topic": 19,
      "similarity": 0.8048359348830542
    },
    {
      "doc": 140,
      "topic": 20,
      "similarity": 0.7920414067022279
    },
    {
      "doc": 140,
      "topic": 21,
      "similarity": 0.8426693138137535
    },
    {
      "doc": 140,
      "topic": 23,
      "similarity": 0.7552320283094538
    },
    {
      "doc": 140,
      "topic": 24,
      "similarity": 0.7548230056066193
    },
    {
      "doc": 141,
      "topic": 3,
      "similarity": 0.7736413176572174
    },
    {
      "doc": 141,
      "topic": 5,
      "similarity": 0.7651662009441094
    },
    {
      "doc": 141,
      "topic": 8,
      "similarity": 0.766252548574099
    },
    {
      "doc": 141,
      "topic": 9,
      "similarity": 0.8059039539093371
    },
    {
      "doc": 141,
      "topic": 15,
      "similarity": 0.7518656622653007
    },
    {
      "doc": 141,
      "topic": 17,
      "similarity": 0.7735807163399061
    },
    {
      "doc": 141,
      "topic": 18,
      "similarity": 0.7606377516542228
    },
    {
      "doc": 141,
      "topic": 19,
      "similarity": 0.7762043601747621
    },
    {
      "doc": 141,
      "topic": 20,
      "similarity": 0.7781714012499918
    },
    {
      "doc": 141,
      "topic": 21,
      "similarity": 0.7764141643238155
    },
    {
      "doc": 141,
      "topic": 23,
      "similarity": 0.7606486465519492
    },
    {
      "doc": 141,
      "topic": 24,
      "similarity": 0.7757897939352348
    },
    {
      "doc": 142,
      "topic": 2,
      "similarity": 0.7703338826851909
    },
    {
      "doc": 142,
      "topic": 3,
      "similarity": 0.8101760521476825
    },
    {
      "doc": 142,
      "topic": 5,
      "similarity": 0.792386283988721
    },
    {
      "doc": 142,
      "topic": 7,
      "similarity": 0.7912131128274766
    },
    {
      "doc": 142,
      "topic": 8,
      "similarity": 0.7737723878730559
    },
    {
      "doc": 142,
      "topic": 9,
      "similarity": 0.8468783444968786
    },
    {
      "doc": 142,
      "topic": 10,
      "similarity": 0.779147657555043
    },
    {
      "doc": 142,
      "topic": 11,
      "similarity": 0.7894682357504447
    },
    {
      "doc": 142,
      "topic": 13,
      "similarity": 0.7538295648985074
    },
    {
      "doc": 142,
      "topic": 14,
      "similarity": 0.7731100412964647
    },
    {
      "doc": 142,
      "topic": 15,
      "similarity": 0.7900152740835389
    },
    {
      "doc": 142,
      "topic": 16,
      "similarity": 0.8107526495318036
    },
    {
      "doc": 142,
      "topic": 17,
      "similarity": 0.7932736831292926
    },
    {
      "doc": 142,
      "topic": 18,
      "similarity": 0.7742467501713199
    },
    {
      "doc": 142,
      "topic": 19,
      "similarity": 0.8171624506627326
    },
    {
      "doc": 142,
      "topic": 20,
      "similarity": 0.7923630494652818
    },
    {
      "doc": 142,
      "topic": 21,
      "similarity": 0.7991231910943555
    },
    {
      "doc": 142,
      "topic": 24,
      "similarity": 0.7503945855444183
    },
    {
      "doc": 143,
      "topic": 1,
      "similarity": 0.7696023695694684
    },
    {
      "doc": 143,
      "topic": 2,
      "similarity": 0.7947160079416985
    },
    {
      "doc": 143,
      "topic": 3,
      "similarity": 0.8068567391949769
    },
    {
      "doc": 143,
      "topic": 4,
      "similarity": 0.7522602996852528
    },
    {
      "doc": 143,
      "topic": 5,
      "similarity": 0.8013357014567055
    },
    {
      "doc": 143,
      "topic": 7,
      "similarity": 0.7757021564675505
    },
    {
      "doc": 143,
      "topic": 8,
      "similarity": 0.7868625188357155
    },
    {
      "doc": 143,
      "topic": 9,
      "similarity": 0.8116244138274125
    },
    {
      "doc": 143,
      "topic": 10,
      "similarity": 0.7798192552397454
    },
    {
      "doc": 143,
      "topic": 11,
      "similarity": 0.795877760214577
    },
    {
      "doc": 143,
      "topic": 13,
      "similarity": 0.7724300456517647
    },
    {
      "doc": 143,
      "topic": 14,
      "similarity": 0.7890666953951851
    },
    {
      "doc": 143,
      "topic": 15,
      "similarity": 0.8106279816646877
    },
    {
      "doc": 143,
      "topic": 16,
      "similarity": 0.8173357279182821
    },
    {
      "doc": 143,
      "topic": 17,
      "similarity": 0.8241265378751548
    },
    {
      "doc": 143,
      "topic": 18,
      "similarity": 0.7771829984533877
    },
    {
      "doc": 143,
      "topic": 19,
      "similarity": 0.8135740594021215
    },
    {
      "doc": 143,
      "topic": 20,
      "similarity": 0.7764747125463762
    },
    {
      "doc": 143,
      "topic": 21,
      "similarity": 0.8009168659872002
    },
    {
      "doc": 143,
      "topic": 23,
      "similarity": 0.7657597151889851
    },
    {
      "doc": 144,
      "topic": 3,
      "similarity": 0.7596874049883706
    },
    {
      "doc": 144,
      "topic": 5,
      "similarity": 0.7710746872834634
    },
    {
      "doc": 144,
      "topic": 9,
      "similarity": 0.7835596550200351
    },
    {
      "doc": 144,
      "topic": 11,
      "similarity": 0.7941699934192015
    },
    {
      "doc": 144,
      "topic": 15,
      "similarity": 0.7666835725773606
    },
    {
      "doc": 144,
      "topic": 16,
      "similarity": 0.7852959816343509
    },
    {
      "doc": 144,
      "topic": 17,
      "similarity": 0.7802219004230259
    },
    {
      "doc": 144,
      "topic": 19,
      "similarity": 0.7764005415577853
    },
    {
      "doc": 144,
      "topic": 20,
      "similarity": 0.7586155969208725
    },
    {
      "doc": 144,
      "topic": 21,
      "similarity": 0.7798418766989391
    },
    {
      "doc": 145,
      "topic": 1,
      "similarity": 0.7618526296882174
    },
    {
      "doc": 145,
      "topic": 2,
      "similarity": 0.7802862379313821
    },
    {
      "doc": 145,
      "topic": 3,
      "similarity": 0.8238192486594642
    },
    {
      "doc": 145,
      "topic": 4,
      "similarity": 0.7697526109347093
    },
    {
      "doc": 145,
      "topic": 5,
      "similarity": 0.8037771413008442
    },
    {
      "doc": 145,
      "topic": 7,
      "similarity": 0.7844509178239767
    },
    {
      "doc": 145,
      "topic": 8,
      "similarity": 0.7819344651374199
    },
    {
      "doc": 145,
      "topic": 9,
      "similarity": 0.8208214100974617
    },
    {
      "doc": 145,
      "topic": 10,
      "similarity": 0.7858193915210864
    },
    {
      "doc": 145,
      "topic": 11,
      "similarity": 0.8001754741014913
    },
    {
      "doc": 145,
      "topic": 13,
      "similarity": 0.7841040241943038
    },
    {
      "doc": 145,
      "topic": 14,
      "similarity": 0.7869044030677608
    },
    {
      "doc": 145,
      "topic": 15,
      "similarity": 0.7884347729869388
    },
    {
      "doc": 145,
      "topic": 16,
      "similarity": 0.8038459460767489
    },
    {
      "doc": 145,
      "topic": 17,
      "similarity": 0.8013621342051562
    },
    {
      "doc": 145,
      "topic": 18,
      "similarity": 0.762607529769863
    },
    {
      "doc": 145,
      "topic": 19,
      "similarity": 0.8009522521388606
    },
    {
      "doc": 145,
      "topic": 20,
      "similarity": 0.7714671744741504
    },
    {
      "doc": 145,
      "topic": 21,
      "similarity": 0.8071272116279282
    },
    {
      "doc": 145,
      "topic": 23,
      "similarity": 0.759024656899925
    },
    {
      "doc": 146,
      "topic": 1,
      "similarity": 0.7506463678644599
    },
    {
      "doc": 146,
      "topic": 2,
      "similarity": 0.7625678786009734
    },
    {
      "doc": 146,
      "topic": 3,
      "similarity": 0.7954686500028918
    },
    {
      "doc": 146,
      "topic": 4,
      "similarity": 0.7672280454122428
    },
    {
      "doc": 146,
      "topic": 5,
      "similarity": 0.8080312512333383
    },
    {
      "doc": 146,
      "topic": 7,
      "similarity": 0.7920121740433307
    },
    {
      "doc": 146,
      "topic": 8,
      "similarity": 0.7884500147613697
    },
    {
      "doc": 146,
      "topic": 9,
      "similarity": 0.8239513496398965
    },
    {
      "doc": 146,
      "topic": 10,
      "similarity": 0.8260405403184786
    },
    {
      "doc": 146,
      "topic": 11,
      "similarity": 0.7713376584521346
    },
    {
      "doc": 146,
      "topic": 13,
      "similarity": 0.7612962452816335
    },
    {
      "doc": 146,
      "topic": 14,
      "similarity": 0.7606741052927949
    },
    {
      "doc": 146,
      "topic": 15,
      "similarity": 0.7787141580636957
    },
    {
      "doc": 146,
      "topic": 16,
      "similarity": 0.7938518209897977
    },
    {
      "doc": 146,
      "topic": 17,
      "similarity": 0.7920918814452486
    },
    {
      "doc": 146,
      "topic": 18,
      "similarity": 0.7760096808587088
    },
    {
      "doc": 146,
      "topic": 19,
      "similarity": 0.7964220166355511
    },
    {
      "doc": 146,
      "topic": 20,
      "similarity": 0.7603581502539868
    },
    {
      "doc": 146,
      "topic": 21,
      "similarity": 0.7996288150878298
    },
    {
      "doc": 147,
      "topic": 3,
      "similarity": 0.7855030701463405
    },
    {
      "doc": 147,
      "topic": 4,
      "similarity": 0.7593071668258288
    },
    {
      "doc": 147,
      "topic": 5,
      "similarity": 0.7797031098298595
    },
    {
      "doc": 147,
      "topic": 7,
      "similarity": 0.7599788258089183
    },
    {
      "doc": 147,
      "topic": 8,
      "similarity": 0.7812329908354905
    },
    {
      "doc": 147,
      "topic": 9,
      "similarity": 0.7870773523630273
    },
    {
      "doc": 147,
      "topic": 16,
      "similarity": 0.7559151991932436
    },
    {
      "doc": 147,
      "topic": 17,
      "similarity": 0.7637436491710381
    },
    {
      "doc": 147,
      "topic": 19,
      "similarity": 0.7631025747940867
    },
    {
      "doc": 147,
      "topic": 21,
      "similarity": 0.7633433287530279
    },
    {
      "doc": 147,
      "topic": 24,
      "similarity": 0.7554185544356611
    },
    {
      "doc": 148,
      "topic": 2,
      "similarity": 0.7708690119502583
    },
    {
      "doc": 148,
      "topic": 3,
      "similarity": 0.7911531290280382
    },
    {
      "doc": 148,
      "topic": 5,
      "similarity": 0.8032800483051418
    },
    {
      "doc": 148,
      "topic": 7,
      "similarity": 0.7603583370858713
    },
    {
      "doc": 148,
      "topic": 8,
      "similarity": 0.7529857108812511
    },
    {
      "doc": 148,
      "topic": 9,
      "similarity": 0.7968099696799255
    },
    {
      "doc": 148,
      "topic": 10,
      "similarity": 0.7707138581641579
    },
    {
      "doc": 148,
      "topic": 11,
      "similarity": 0.7659737542639984
    },
    {
      "doc": 148,
      "topic": 13,
      "similarity": 0.7566081900028349
    },
    {
      "doc": 148,
      "topic": 15,
      "similarity": 0.7603046991397813
    },
    {
      "doc": 148,
      "topic": 16,
      "similarity": 0.8015800576992957
    },
    {
      "doc": 148,
      "topic": 17,
      "similarity": 0.7989421075654132
    },
    {
      "doc": 148,
      "topic": 19,
      "similarity": 0.7936716400113238
    },
    {
      "doc": 148,
      "topic": 20,
      "similarity": 0.7712930602512214
    },
    {
      "doc": 148,
      "topic": 21,
      "similarity": 0.7954653657523572
    },
    {
      "doc": 148,
      "topic": 23,
      "similarity": 0.7511652349199214
    },
    {
      "doc": 149,
      "topic": 1,
      "similarity": 0.7609059260081844
    },
    {
      "doc": 149,
      "topic": 2,
      "similarity": 0.7729873481165163
    },
    {
      "doc": 149,
      "topic": 3,
      "similarity": 0.7994127776532768
    },
    {
      "doc": 149,
      "topic": 5,
      "similarity": 0.821125443657215
    },
    {
      "doc": 149,
      "topic": 7,
      "similarity": 0.7685990151363344
    },
    {
      "doc": 149,
      "topic": 8,
      "similarity": 0.7724780689624208
    },
    {
      "doc": 149,
      "topic": 9,
      "similarity": 0.8076293138933415
    },
    {
      "doc": 149,
      "topic": 10,
      "similarity": 0.7683828079828872
    },
    {
      "doc": 149,
      "topic": 11,
      "similarity": 0.7831635225106889
    },
    {
      "doc": 149,
      "topic": 13,
      "similarity": 0.7716252685587359
    },
    {
      "doc": 149,
      "topic": 14,
      "similarity": 0.7763836834808368
    },
    {
      "doc": 149,
      "topic": 15,
      "similarity": 0.7756639691622865
    },
    {
      "doc": 149,
      "topic": 16,
      "similarity": 0.8132636948352887
    },
    {
      "doc": 149,
      "topic": 17,
      "similarity": 0.7959711813550836
    },
    {
      "doc": 149,
      "topic": 18,
      "similarity": 0.7556313055862238
    },
    {
      "doc": 149,
      "topic": 19,
      "similarity": 0.8000056321122927
    },
    {
      "doc": 149,
      "topic": 20,
      "similarity": 0.7970547003404665
    },
    {
      "doc": 149,
      "topic": 21,
      "similarity": 0.7969705210413996
    },
    {
      "doc": 149,
      "topic": 23,
      "similarity": 0.7626289145951689
    },
    {
      "doc": 149,
      "topic": 24,
      "similarity": 0.7542955268512617
    },
    {
      "doc": 150,
      "topic": 1,
      "similarity": 0.7523683546346188
    },
    {
      "doc": 150,
      "topic": 2,
      "similarity": 0.7697138023043144
    },
    {
      "doc": 150,
      "topic": 3,
      "similarity": 0.7944761036535543
    },
    {
      "doc": 150,
      "topic": 4,
      "similarity": 0.7516958741716822
    },
    {
      "doc": 150,
      "topic": 5,
      "similarity": 0.8058377928359045
    },
    {
      "doc": 150,
      "topic": 7,
      "similarity": 0.7769277517479934
    },
    {
      "doc": 150,
      "topic": 8,
      "similarity": 0.7758784618014346
    },
    {
      "doc": 150,
      "topic": 9,
      "similarity": 0.831253129229532
    },
    {
      "doc": 150,
      "topic": 10,
      "similarity": 0.7976226409269809
    },
    {
      "doc": 150,
      "topic": 11,
      "similarity": 0.799029837709999
    },
    {
      "doc": 150,
      "topic": 13,
      "similarity": 0.7892318447135339
    },
    {
      "doc": 150,
      "topic": 14,
      "similarity": 0.7840608147649784
    },
    {
      "doc": 150,
      "topic": 15,
      "similarity": 0.7826269489891465
    },
    {
      "doc": 150,
      "topic": 16,
      "similarity": 0.8179948555207782
    },
    {
      "doc": 150,
      "topic": 17,
      "similarity": 0.7992815983074971
    },
    {
      "doc": 150,
      "topic": 18,
      "similarity": 0.7675256982229111
    },
    {
      "doc": 150,
      "topic": 19,
      "similarity": 0.8128354966181547
    },
    {
      "doc": 150,
      "topic": 20,
      "similarity": 0.7780796216310651
    },
    {
      "doc": 150,
      "topic": 21,
      "similarity": 0.8073525929433334
    },
    {
      "doc": 150,
      "topic": 24,
      "similarity": 0.7669374664065977
    },
    {
      "doc": 151,
      "topic": 1,
      "similarity": 0.7560776091751003
    },
    {
      "doc": 151,
      "topic": 2,
      "similarity": 0.7810903459634352
    },
    {
      "doc": 151,
      "topic": 3,
      "similarity": 0.7906237461218382
    },
    {
      "doc": 151,
      "topic": 5,
      "similarity": 0.7967551066897266
    },
    {
      "doc": 151,
      "topic": 7,
      "similarity": 0.7873780732470769
    },
    {
      "doc": 151,
      "topic": 8,
      "similarity": 0.7990967389397636
    },
    {
      "doc": 151,
      "topic": 9,
      "similarity": 0.847698896674597
    },
    {
      "doc": 151,
      "topic": 10,
      "similarity": 0.7973434117780076
    },
    {
      "doc": 151,
      "topic": 11,
      "similarity": 0.7888932730352016
    },
    {
      "doc": 151,
      "topic": 12,
      "similarity": 0.7670267215973292
    },
    {
      "doc": 151,
      "topic": 13,
      "similarity": 0.771272234658056
    },
    {
      "doc": 151,
      "topic": 14,
      "similarity": 0.7722172761869721
    },
    {
      "doc": 151,
      "topic": 15,
      "similarity": 0.8157075261244909
    },
    {
      "doc": 151,
      "topic": 16,
      "similarity": 0.8053951501671465
    },
    {
      "doc": 151,
      "topic": 17,
      "similarity": 0.8225626176141064
    },
    {
      "doc": 151,
      "topic": 18,
      "similarity": 0.7670248138139937
    },
    {
      "doc": 151,
      "topic": 19,
      "similarity": 0.7942141492565549
    },
    {
      "doc": 151,
      "topic": 20,
      "similarity": 0.7584560276340123
    },
    {
      "doc": 151,
      "topic": 21,
      "similarity": 0.8173961257200913
    },
    {
      "doc": 151,
      "topic": 24,
      "similarity": 0.7723176284263564
    },
    {
      "doc": 152,
      "topic": 2,
      "similarity": 0.7735056070489221
    },
    {
      "doc": 152,
      "topic": 3,
      "similarity": 0.7850854624252736
    },
    {
      "doc": 152,
      "topic": 5,
      "similarity": 0.7932791350380953
    },
    {
      "doc": 152,
      "topic": 7,
      "similarity": 0.7812688399007377
    },
    {
      "doc": 152,
      "topic": 8,
      "similarity": 0.7688361523186836
    },
    {
      "doc": 152,
      "topic": 9,
      "similarity": 0.8432052204514203
    },
    {
      "doc": 152,
      "topic": 10,
      "similarity": 0.7826753885656228
    },
    {
      "doc": 152,
      "topic": 11,
      "similarity": 0.7777049652977601
    },
    {
      "doc": 152,
      "topic": 12,
      "similarity": 0.764635436160384
    },
    {
      "doc": 152,
      "topic": 13,
      "similarity": 0.7763895572126017
    },
    {
      "doc": 152,
      "topic": 14,
      "similarity": 0.7638024453901996
    },
    {
      "doc": 152,
      "topic": 15,
      "similarity": 0.7880064702431593
    },
    {
      "doc": 152,
      "topic": 16,
      "similarity": 0.8149101228075928
    },
    {
      "doc": 152,
      "topic": 17,
      "similarity": 0.7937722090435374
    },
    {
      "doc": 152,
      "topic": 18,
      "similarity": 0.784677439064816
    },
    {
      "doc": 152,
      "topic": 19,
      "similarity": 0.8040430619339143
    },
    {
      "doc": 152,
      "topic": 20,
      "similarity": 0.8215420915783687
    },
    {
      "doc": 152,
      "topic": 21,
      "similarity": 0.7981792936910886
    },
    {
      "doc": 152,
      "topic": 24,
      "similarity": 0.7533023448511539
    },
    {
      "doc": 153,
      "topic": 9,
      "similarity": 0.7762351456683918
    },
    {
      "doc": 153,
      "topic": 16,
      "similarity": 0.7600636301038154
    },
    {
      "doc": 153,
      "topic": 17,
      "similarity": 0.7695017111905371
    },
    {
      "doc": 153,
      "topic": 19,
      "similarity": 0.7647811812034733
    },
    {
      "doc": 153,
      "topic": 20,
      "similarity": 0.7603283826207807
    },
    {
      "doc": 153,
      "topic": 21,
      "similarity": 0.765562842680331
    },
    {
      "doc": 154,
      "topic": 2,
      "similarity": 0.7575340691471629
    },
    {
      "doc": 154,
      "topic": 3,
      "similarity": 0.7908903056439286
    },
    {
      "doc": 154,
      "topic": 4,
      "similarity": 0.7558740988196808
    },
    {
      "doc": 154,
      "topic": 5,
      "similarity": 0.7847457992828257
    },
    {
      "doc": 154,
      "topic": 8,
      "similarity": 0.750333518882394
    },
    {
      "doc": 154,
      "topic": 9,
      "similarity": 0.7852080591779083
    },
    {
      "doc": 154,
      "topic": 10,
      "similarity": 0.7525370240787891
    },
    {
      "doc": 154,
      "topic": 15,
      "similarity": 0.7669206720649292
    },
    {
      "doc": 154,
      "topic": 16,
      "similarity": 0.8048143946722366
    },
    {
      "doc": 154,
      "topic": 17,
      "similarity": 0.8009937614911863
    },
    {
      "doc": 154,
      "topic": 19,
      "similarity": 0.8079883859147738
    },
    {
      "doc": 154,
      "topic": 20,
      "similarity": 0.7606463971828388
    },
    {
      "doc": 154,
      "topic": 21,
      "similarity": 0.8014006034091643
    },
    {
      "doc": 155,
      "topic": 3,
      "similarity": 0.7524046315669267
    },
    {
      "doc": 155,
      "topic": 9,
      "similarity": 0.7738324138033388
    },
    {
      "doc": 155,
      "topic": 16,
      "similarity": 0.7589086634556632
    },
    {
      "doc": 155,
      "topic": 17,
      "similarity": 0.7536540911259209
    },
    {
      "doc": 155,
      "topic": 21,
      "similarity": 0.7570638853746781
    },
    {
      "doc": 155,
      "topic": 24,
      "similarity": 0.7964739054866665
    },
    {
      "doc": 156,
      "topic": 3,
      "similarity": 0.7643641853092493
    },
    {
      "doc": 156,
      "topic": 5,
      "similarity": 0.7640703849825103
    },
    {
      "doc": 156,
      "topic": 7,
      "similarity": 0.7699287325004956
    },
    {
      "doc": 156,
      "topic": 9,
      "similarity": 0.8076045968486272
    },
    {
      "doc": 156,
      "topic": 10,
      "similarity": 0.7728806389155194
    },
    {
      "doc": 156,
      "topic": 11,
      "similarity": 0.7870611331405454
    },
    {
      "doc": 156,
      "topic": 15,
      "similarity": 0.7688920985014824
    },
    {
      "doc": 156,
      "topic": 16,
      "similarity": 0.7707427904416768
    },
    {
      "doc": 156,
      "topic": 17,
      "similarity": 0.7677591932407382
    },
    {
      "doc": 156,
      "topic": 18,
      "similarity": 0.7682468073287683
    },
    {
      "doc": 156,
      "topic": 19,
      "similarity": 0.7843111538922413
    },
    {
      "doc": 156,
      "topic": 20,
      "similarity": 0.7807957308061853
    },
    {
      "doc": 156,
      "topic": 21,
      "similarity": 0.7635616003304175
    },
    {
      "doc": 156,
      "topic": 23,
      "similarity": 0.753236427009287
    },
    {
      "doc": 157,
      "topic": 2,
      "similarity": 0.7586156595448388
    },
    {
      "doc": 157,
      "topic": 3,
      "similarity": 0.7863166836745904
    },
    {
      "doc": 157,
      "topic": 5,
      "similarity": 0.8010634671602366
    },
    {
      "doc": 157,
      "topic": 7,
      "similarity": 0.7511496768833055
    },
    {
      "doc": 157,
      "topic": 8,
      "similarity": 0.7619365819973993
    },
    {
      "doc": 157,
      "topic": 9,
      "similarity": 0.8149048790565024
    },
    {
      "doc": 157,
      "topic": 10,
      "similarity": 0.7613411972662552
    },
    {
      "doc": 157,
      "topic": 11,
      "similarity": 0.7793487509698636
    },
    {
      "doc": 157,
      "topic": 12,
      "similarity": 0.7579718032702714
    },
    {
      "doc": 157,
      "topic": 13,
      "similarity": 0.7526151535570833
    },
    {
      "doc": 157,
      "topic": 15,
      "similarity": 0.7626794769730735
    },
    {
      "doc": 157,
      "topic": 16,
      "similarity": 0.8078890818529713
    },
    {
      "doc": 157,
      "topic": 17,
      "similarity": 0.7964352281980248
    },
    {
      "doc": 157,
      "topic": 18,
      "similarity": 0.7708413215699458
    },
    {
      "doc": 157,
      "topic": 19,
      "similarity": 0.8077179076211214
    },
    {
      "doc": 157,
      "topic": 20,
      "similarity": 0.8078931018122315
    },
    {
      "doc": 157,
      "topic": 21,
      "similarity": 0.7932107648172839
    },
    {
      "doc": 157,
      "topic": 23,
      "similarity": 0.755314627193755
    },
    {
      "doc": 157,
      "topic": 24,
      "similarity": 0.7735550756890746
    },
    {
      "doc": 158,
      "topic": 9,
      "similarity": 0.7609072545582795
    },
    {
      "doc": 158,
      "topic": 19,
      "similarity": 0.7619649729713436
    },
    {
      "doc": 159,
      "topic": 2,
      "similarity": 0.762925931929287
    },
    {
      "doc": 159,
      "topic": 3,
      "similarity": 0.810134101914288
    },
    {
      "doc": 159,
      "topic": 4,
      "similarity": 0.7542573427724193
    },
    {
      "doc": 159,
      "topic": 5,
      "similarity": 0.8029508738928451
    },
    {
      "doc": 159,
      "topic": 7,
      "similarity": 0.8012970409613648
    },
    {
      "doc": 159,
      "topic": 8,
      "similarity": 0.770388034761862
    },
    {
      "doc": 159,
      "topic": 9,
      "similarity": 0.8439711485080139
    },
    {
      "doc": 159,
      "topic": 10,
      "similarity": 0.7704079540476186
    },
    {
      "doc": 159,
      "topic": 11,
      "similarity": 0.7780939922071034
    },
    {
      "doc": 159,
      "topic": 13,
      "similarity": 0.7588488604196729
    },
    {
      "doc": 159,
      "topic": 14,
      "similarity": 0.7617761252843444
    },
    {
      "doc": 159,
      "topic": 15,
      "similarity": 0.7728089312630844
    },
    {
      "doc": 159,
      "topic": 16,
      "similarity": 0.8003224283853096
    },
    {
      "doc": 159,
      "topic": 17,
      "similarity": 0.8046112071987186
    },
    {
      "doc": 159,
      "topic": 18,
      "similarity": 0.7742130445053117
    },
    {
      "doc": 159,
      "topic": 19,
      "similarity": 0.8284885520201293
    },
    {
      "doc": 159,
      "topic": 20,
      "similarity": 0.7790706671499801
    },
    {
      "doc": 159,
      "topic": 21,
      "similarity": 0.7946620477507198
    },
    {
      "doc": 159,
      "topic": 23,
      "similarity": 0.7679095914352484
    },
    {
      "doc": 159,
      "topic": 24,
      "similarity": 0.774753618134798
    },
    {
      "doc": 160,
      "topic": 1,
      "similarity": 0.7559114763116513
    },
    {
      "doc": 160,
      "topic": 3,
      "similarity": 0.8085125001807201
    },
    {
      "doc": 160,
      "topic": 5,
      "similarity": 0.7778547534561685
    },
    {
      "doc": 160,
      "topic": 6,
      "similarity": 0.8605044632683152
    },
    {
      "doc": 160,
      "topic": 7,
      "similarity": 0.771894477543053
    },
    {
      "doc": 160,
      "topic": 8,
      "similarity": 0.7545662976879343
    },
    {
      "doc": 160,
      "topic": 9,
      "similarity": 0.8291042625418413
    },
    {
      "doc": 160,
      "topic": 10,
      "similarity": 0.7712979561397038
    },
    {
      "doc": 160,
      "topic": 11,
      "similarity": 0.7768265571436203
    },
    {
      "doc": 160,
      "topic": 12,
      "similarity": 0.7510559711140424
    },
    {
      "doc": 160,
      "topic": 13,
      "similarity": 0.7578443064719494
    },
    {
      "doc": 160,
      "topic": 14,
      "similarity": 0.7672503562543384
    },
    {
      "doc": 160,
      "topic": 15,
      "similarity": 0.7772364277188359
    },
    {
      "doc": 160,
      "topic": 16,
      "similarity": 0.7830696856217959
    },
    {
      "doc": 160,
      "topic": 17,
      "similarity": 0.7936721436810781
    },
    {
      "doc": 160,
      "topic": 18,
      "similarity": 0.7795878081873983
    },
    {
      "doc": 160,
      "topic": 19,
      "similarity": 0.809092484617011
    },
    {
      "doc": 160,
      "topic": 20,
      "similarity": 0.7750729079654443
    },
    {
      "doc": 160,
      "topic": 21,
      "similarity": 0.7939489386291282
    },
    {
      "doc": 160,
      "topic": 23,
      "similarity": 0.8119349050031793
    },
    {
      "doc": 160,
      "topic": 24,
      "similarity": 0.7603273550213084
    },
    {
      "doc": 161,
      "topic": 1,
      "similarity": 0.7743391794147826
    },
    {
      "doc": 161,
      "topic": 3,
      "similarity": 0.7612478456400414
    },
    {
      "doc": 161,
      "topic": 5,
      "similarity": 0.7625116905420217
    },
    {
      "doc": 161,
      "topic": 8,
      "similarity": 0.7672213620868513
    },
    {
      "doc": 161,
      "topic": 9,
      "similarity": 0.7738213022738106
    },
    {
      "doc": 161,
      "topic": 10,
      "similarity": 0.7641299917871937
    },
    {
      "doc": 161,
      "topic": 19,
      "similarity": 0.7552792037638972
    },
    {
      "doc": 161,
      "topic": 21,
      "similarity": 0.7667539571667042
    },
    {
      "doc": 162,
      "topic": 3,
      "similarity": 0.7739380332044102
    },
    {
      "doc": 162,
      "topic": 6,
      "similarity": 0.7530809884726578
    },
    {
      "doc": 162,
      "topic": 7,
      "similarity": 0.7517115898601143
    },
    {
      "doc": 162,
      "topic": 8,
      "similarity": 0.7784862525118754
    },
    {
      "doc": 162,
      "topic": 9,
      "similarity": 0.7715543052879164
    },
    {
      "doc": 162,
      "topic": 10,
      "similarity": 0.7709068705967079
    },
    {
      "doc": 162,
      "topic": 11,
      "similarity": 0.7705133465313877
    },
    {
      "doc": 162,
      "topic": 13,
      "similarity": 0.7508187224976329
    },
    {
      "doc": 162,
      "topic": 14,
      "similarity": 0.7644609960658073
    },
    {
      "doc": 162,
      "topic": 16,
      "similarity": 0.756195517591308
    },
    {
      "doc": 162,
      "topic": 17,
      "similarity": 0.7544844577618586
    },
    {
      "doc": 162,
      "topic": 18,
      "similarity": 0.8403540849170741
    },
    {
      "doc": 162,
      "topic": 19,
      "similarity": 0.8348101274086448
    },
    {
      "doc": 162,
      "topic": 20,
      "similarity": 0.7591867566657059
    },
    {
      "doc": 162,
      "topic": 22,
      "similarity": 0.7696063069471254
    },
    {
      "doc": 162,
      "topic": 23,
      "similarity": 0.7693754724809359
    },
    {
      "doc": 163,
      "topic": 1,
      "similarity": 0.7502301014216293
    },
    {
      "doc": 163,
      "topic": 2,
      "similarity": 0.7698820595626157
    },
    {
      "doc": 163,
      "topic": 3,
      "similarity": 0.8029349409518114
    },
    {
      "doc": 163,
      "topic": 4,
      "similarity": 0.7627113482477932
    },
    {
      "doc": 163,
      "topic": 5,
      "similarity": 0.7990805515470621
    },
    {
      "doc": 163,
      "topic": 7,
      "similarity": 0.8038934206248616
    },
    {
      "doc": 163,
      "topic": 8,
      "similarity": 0.7869152436718693
    },
    {
      "doc": 163,
      "topic": 9,
      "similarity": 0.8336247973876406
    },
    {
      "doc": 163,
      "topic": 10,
      "similarity": 0.7924044687167656
    },
    {
      "doc": 163,
      "topic": 11,
      "similarity": 0.8070914222444613
    },
    {
      "doc": 163,
      "topic": 13,
      "similarity": 0.7717556291103472
    },
    {
      "doc": 163,
      "topic": 14,
      "similarity": 0.7796723277818683
    },
    {
      "doc": 163,
      "topic": 15,
      "similarity": 0.7928928474770813
    },
    {
      "doc": 163,
      "topic": 16,
      "similarity": 0.8523896095267216
    },
    {
      "doc": 163,
      "topic": 17,
      "similarity": 0.8013276554134998
    },
    {
      "doc": 163,
      "topic": 18,
      "similarity": 0.7988768333491865
    },
    {
      "doc": 163,
      "topic": 19,
      "similarity": 0.8483243391143895
    },
    {
      "doc": 163,
      "topic": 20,
      "similarity": 0.7721925399886296
    },
    {
      "doc": 163,
      "topic": 21,
      "similarity": 0.8140413696957752
    },
    {
      "doc": 163,
      "topic": 24,
      "similarity": 0.7701271525327336
    },
    {
      "doc": 164,
      "topic": 5,
      "similarity": 0.751684465756218
    },
    {
      "doc": 164,
      "topic": 9,
      "similarity": 0.755850667881921
    },
    {
      "doc": 164,
      "topic": 11,
      "similarity": 0.7624620189626111
    },
    {
      "doc": 164,
      "topic": 16,
      "similarity": 0.7638563897503532
    },
    {
      "doc": 164,
      "topic": 19,
      "similarity": 0.769338718825408
    },
    {
      "doc": 165,
      "topic": 2,
      "similarity": 0.7511267924503026
    },
    {
      "doc": 165,
      "topic": 3,
      "similarity": 0.7719513437751925
    },
    {
      "doc": 165,
      "topic": 5,
      "similarity": 0.7730354498900637
    },
    {
      "doc": 165,
      "topic": 6,
      "similarity": 0.7721881816017294
    },
    {
      "doc": 165,
      "topic": 7,
      "similarity": 0.7509014234949707
    },
    {
      "doc": 165,
      "topic": 8,
      "similarity": 0.757032074637566
    },
    {
      "doc": 165,
      "topic": 9,
      "similarity": 0.8060809153389737
    },
    {
      "doc": 165,
      "topic": 10,
      "similarity": 0.766676858081468
    },
    {
      "doc": 165,
      "topic": 11,
      "similarity": 0.7724167503094717
    },
    {
      "doc": 165,
      "topic": 12,
      "similarity": 0.7677243140770241
    },
    {
      "doc": 165,
      "topic": 14,
      "similarity": 0.759237251192544
    },
    {
      "doc": 165,
      "topic": 15,
      "similarity": 0.79358592332742
    },
    {
      "doc": 165,
      "topic": 16,
      "similarity": 0.7867751448187805
    },
    {
      "doc": 165,
      "topic": 17,
      "similarity": 0.8192530483734384
    },
    {
      "doc": 165,
      "topic": 18,
      "similarity": 0.7683252508475418
    },
    {
      "doc": 165,
      "topic": 19,
      "similarity": 0.8015520523971941
    },
    {
      "doc": 165,
      "topic": 20,
      "similarity": 0.7821740114347528
    },
    {
      "doc": 165,
      "topic": 21,
      "similarity": 0.7984423406733951
    },
    {
      "doc": 165,
      "topic": 22,
      "similarity": 0.7663040949924037
    },
    {
      "doc": 166,
      "topic": 1,
      "similarity": 0.7530511359459044
    },
    {
      "doc": 166,
      "topic": 2,
      "similarity": 0.7631903838290298
    },
    {
      "doc": 166,
      "topic": 3,
      "similarity": 0.8060331592737693
    },
    {
      "doc": 166,
      "topic": 5,
      "similarity": 0.7894268861947018
    },
    {
      "doc": 166,
      "topic": 7,
      "similarity": 0.762583289433322
    },
    {
      "doc": 166,
      "topic": 8,
      "similarity": 0.7796257936232619
    },
    {
      "doc": 166,
      "topic": 9,
      "similarity": 0.8128951994001568
    },
    {
      "doc": 166,
      "topic": 10,
      "similarity": 0.7753586315728163
    },
    {
      "doc": 166,
      "topic": 11,
      "similarity": 0.7731087223983794
    },
    {
      "doc": 166,
      "topic": 12,
      "similarity": 0.7606825173475006
    },
    {
      "doc": 166,
      "topic": 13,
      "similarity": 0.7729594380418546
    },
    {
      "doc": 166,
      "topic": 14,
      "similarity": 0.7585295281341402
    },
    {
      "doc": 166,
      "topic": 15,
      "similarity": 0.7931084322163509
    },
    {
      "doc": 166,
      "topic": 16,
      "similarity": 0.8099380572078781
    },
    {
      "doc": 166,
      "topic": 17,
      "similarity": 0.8274128329087891
    },
    {
      "doc": 166,
      "topic": 18,
      "similarity": 0.7640101930104469
    },
    {
      "doc": 166,
      "topic": 19,
      "similarity": 0.846463647367952
    },
    {
      "doc": 166,
      "topic": 20,
      "similarity": 0.817097990995186
    },
    {
      "doc": 166,
      "topic": 21,
      "similarity": 0.8081362635880396
    },
    {
      "doc": 166,
      "topic": 22,
      "similarity": 0.7587691008449964
    },
    {
      "doc": 166,
      "topic": 23,
      "similarity": 0.7621117873238978
    },
    {
      "doc": 166,
      "topic": 24,
      "similarity": 0.7610151216956581
    },
    {
      "doc": 167,
      "topic": 2,
      "similarity": 0.7547587948657724
    },
    {
      "doc": 167,
      "topic": 3,
      "similarity": 0.7775857926011502
    },
    {
      "doc": 167,
      "topic": 5,
      "similarity": 0.78336801428793
    },
    {
      "doc": 167,
      "topic": 7,
      "similarity": 0.7695039405655273
    },
    {
      "doc": 167,
      "topic": 8,
      "similarity": 0.7751522703516055
    },
    {
      "doc": 167,
      "topic": 9,
      "similarity": 0.8178141726861493
    },
    {
      "doc": 167,
      "topic": 10,
      "similarity": 0.8026670895675139
    },
    {
      "doc": 167,
      "topic": 11,
      "similarity": 0.7777158357237295
    },
    {
      "doc": 167,
      "topic": 12,
      "similarity": 0.7528558585095577
    },
    {
      "doc": 167,
      "topic": 13,
      "similarity": 0.7647718270333556
    },
    {
      "doc": 167,
      "topic": 14,
      "similarity": 0.7593506109224064
    },
    {
      "doc": 167,
      "topic": 15,
      "similarity": 0.7840286855541246
    },
    {
      "doc": 167,
      "topic": 16,
      "similarity": 0.7863947107966271
    },
    {
      "doc": 167,
      "topic": 17,
      "similarity": 0.7854861046477851
    },
    {
      "doc": 167,
      "topic": 18,
      "similarity": 0.7786742003911501
    },
    {
      "doc": 167,
      "topic": 19,
      "similarity": 0.785112002476823
    },
    {
      "doc": 167,
      "topic": 20,
      "similarity": 0.7838834398595567
    },
    {
      "doc": 167,
      "topic": 21,
      "similarity": 0.7936498484007198
    },
    {
      "doc": 167,
      "topic": 24,
      "similarity": 0.7693627986759172
    },
    {
      "doc": 168,
      "topic": 3,
      "similarity": 0.7673832934667809
    },
    {
      "doc": 168,
      "topic": 5,
      "similarity": 0.7664301550669101
    },
    {
      "doc": 168,
      "topic": 7,
      "similarity": 0.7555042835384099
    },
    {
      "doc": 168,
      "topic": 9,
      "similarity": 0.7890791344290196
    },
    {
      "doc": 168,
      "topic": 11,
      "similarity": 0.7575413880214907
    },
    {
      "doc": 168,
      "topic": 15,
      "similarity": 0.7501029410398393
    },
    {
      "doc": 168,
      "topic": 16,
      "similarity": 0.7864630500782888
    },
    {
      "doc": 168,
      "topic": 17,
      "similarity": 0.7940984935833059
    },
    {
      "doc": 168,
      "topic": 19,
      "similarity": 0.7716525302607411
    },
    {
      "doc": 168,
      "topic": 20,
      "similarity": 0.7736533607525647
    },
    {
      "doc": 168,
      "topic": 21,
      "similarity": 0.8141073471559708
    },
    {
      "doc": 168,
      "topic": 24,
      "similarity": 0.7529616515284323
    },
    {
      "doc": 169,
      "topic": 3,
      "similarity": 0.7906066330776436
    },
    {
      "doc": 169,
      "topic": 5,
      "similarity": 0.7884870453791815
    },
    {
      "doc": 169,
      "topic": 7,
      "similarity": 0.7630228326527874
    },
    {
      "doc": 169,
      "topic": 8,
      "similarity": 0.750709867488352
    },
    {
      "doc": 169,
      "topic": 9,
      "similarity": 0.8074734850683243
    },
    {
      "doc": 169,
      "topic": 10,
      "similarity": 0.7575597333688118
    },
    {
      "doc": 169,
      "topic": 11,
      "similarity": 0.7588286714193675
    },
    {
      "doc": 169,
      "topic": 13,
      "similarity": 0.7531839431115617
    },
    {
      "doc": 169,
      "topic": 15,
      "similarity": 0.7624548228015658
    },
    {
      "doc": 169,
      "topic": 16,
      "similarity": 0.7831575751311532
    },
    {
      "doc": 169,
      "topic": 17,
      "similarity": 0.783111330567515
    },
    {
      "doc": 169,
      "topic": 18,
      "similarity": 0.7718034063339257
    },
    {
      "doc": 169,
      "topic": 19,
      "similarity": 0.8127278468014726
    },
    {
      "doc": 169,
      "topic": 20,
      "similarity": 0.7792978237358814
    },
    {
      "doc": 169,
      "topic": 21,
      "similarity": 0.7893507019045114
    },
    {
      "doc": 169,
      "topic": 22,
      "similarity": 0.7511591478738241
    },
    {
      "doc": 169,
      "topic": 23,
      "similarity": 0.7615009024674843
    },
    {
      "doc": 169,
      "topic": 24,
      "similarity": 0.7666711790206789
    },
    {
      "doc": 170,
      "topic": 1,
      "similarity": 0.762381823818426
    },
    {
      "doc": 170,
      "topic": 2,
      "similarity": 0.7614698338057445
    },
    {
      "doc": 170,
      "topic": 3,
      "similarity": 0.7931998979333156
    },
    {
      "doc": 170,
      "topic": 5,
      "similarity": 0.7887975316643949
    },
    {
      "doc": 170,
      "topic": 7,
      "similarity": 0.793137530877695
    },
    {
      "doc": 170,
      "topic": 8,
      "similarity": 0.7601186225039199
    },
    {
      "doc": 170,
      "topic": 9,
      "similarity": 0.8693196492788714
    },
    {
      "doc": 170,
      "topic": 10,
      "similarity": 0.7741586048744582
    },
    {
      "doc": 170,
      "topic": 11,
      "similarity": 0.779763807555315
    },
    {
      "doc": 170,
      "topic": 13,
      "similarity": 0.7528690887119985
    },
    {
      "doc": 170,
      "topic": 14,
      "similarity": 0.7621009639094727
    },
    {
      "doc": 170,
      "topic": 15,
      "similarity": 0.7621543558864946
    },
    {
      "doc": 170,
      "topic": 16,
      "similarity": 0.794652326774283
    },
    {
      "doc": 170,
      "topic": 17,
      "similarity": 0.7817939010899315
    },
    {
      "doc": 170,
      "topic": 19,
      "similarity": 0.789665700425759
    },
    {
      "doc": 170,
      "topic": 20,
      "similarity": 0.7654273308255624
    },
    {
      "doc": 170,
      "topic": 21,
      "similarity": 0.7935852234568297
    },
    {
      "doc": 170,
      "topic": 23,
      "similarity": 0.7965334284155561
    },
    {
      "doc": 170,
      "topic": 24,
      "similarity": 0.7849573489951988
    },
    {
      "doc": 171,
      "topic": 1,
      "similarity": 0.7867784333569879
    },
    {
      "doc": 171,
      "topic": 2,
      "similarity": 0.8104912058522069
    },
    {
      "doc": 171,
      "topic": 3,
      "similarity": 0.8224267136568046
    },
    {
      "doc": 171,
      "topic": 4,
      "similarity": 0.7615373750465265
    },
    {
      "doc": 171,
      "topic": 5,
      "similarity": 0.8431898916334807
    },
    {
      "doc": 171,
      "topic": 7,
      "similarity": 0.799037563125347
    },
    {
      "doc": 171,
      "topic": 8,
      "similarity": 0.8081876212576574
    },
    {
      "doc": 171,
      "topic": 9,
      "similarity": 0.8385529994992347
    },
    {
      "doc": 171,
      "topic": 10,
      "similarity": 0.8168092057202125
    },
    {
      "doc": 171,
      "topic": 11,
      "similarity": 0.8200234887678999
    },
    {
      "doc": 171,
      "topic": 12,
      "similarity": 0.7933976537818249
    },
    {
      "doc": 171,
      "topic": 13,
      "similarity": 0.807327309111763
    },
    {
      "doc": 171,
      "topic": 14,
      "similarity": 0.8169761078934141
    },
    {
      "doc": 171,
      "topic": 15,
      "similarity": 0.8093156238771498
    },
    {
      "doc": 171,
      "topic": 16,
      "similarity": 0.8395909005113315
    },
    {
      "doc": 171,
      "topic": 17,
      "similarity": 0.8265873491314915
    },
    {
      "doc": 171,
      "topic": 18,
      "similarity": 0.7805241879384036
    },
    {
      "doc": 171,
      "topic": 19,
      "similarity": 0.8353755123404443
    },
    {
      "doc": 171,
      "topic": 20,
      "similarity": 0.7942350665858793
    },
    {
      "doc": 171,
      "topic": 21,
      "similarity": 0.8159737141811474
    },
    {
      "doc": 171,
      "topic": 22,
      "similarity": 0.755510427536121
    },
    {
      "doc": 172,
      "topic": 1,
      "similarity": 0.767774334505636
    },
    {
      "doc": 172,
      "topic": 2,
      "similarity": 0.772033621286423
    },
    {
      "doc": 172,
      "topic": 3,
      "similarity": 0.799625696862935
    },
    {
      "doc": 172,
      "topic": 4,
      "similarity": 0.7696276633279012
    },
    {
      "doc": 172,
      "topic": 5,
      "similarity": 0.7978456246720043
    },
    {
      "doc": 172,
      "topic": 7,
      "similarity": 0.8156160340295155
    },
    {
      "doc": 172,
      "topic": 8,
      "similarity": 0.802044166473148
    },
    {
      "doc": 172,
      "topic": 9,
      "similarity": 0.852406117433892
    },
    {
      "doc": 172,
      "topic": 10,
      "similarity": 0.7902784123009583
    },
    {
      "doc": 172,
      "topic": 11,
      "similarity": 0.7870952637915336
    },
    {
      "doc": 172,
      "topic": 13,
      "similarity": 0.799382908351143
    },
    {
      "doc": 172,
      "topic": 14,
      "similarity": 0.7892636132045298
    },
    {
      "doc": 172,
      "topic": 15,
      "similarity": 0.7865048354347521
    },
    {
      "doc": 172,
      "topic": 16,
      "similarity": 0.7938162788930408
    },
    {
      "doc": 172,
      "topic": 17,
      "similarity": 0.7922073539764256
    },
    {
      "doc": 172,
      "topic": 19,
      "similarity": 0.7993784758384322
    },
    {
      "doc": 172,
      "topic": 20,
      "similarity": 0.778687894416787
    },
    {
      "doc": 172,
      "topic": 21,
      "similarity": 0.8192511915484778
    },
    {
      "doc": 172,
      "topic": 24,
      "similarity": 0.7616723807596142
    },
    {
      "doc": 173,
      "topic": 2,
      "similarity": 0.784487656923618
    },
    {
      "doc": 173,
      "topic": 3,
      "similarity": 0.7870897904166214
    },
    {
      "doc": 173,
      "topic": 5,
      "similarity": 0.7880005445468432
    },
    {
      "doc": 173,
      "topic": 7,
      "similarity": 0.7740648373216708
    },
    {
      "doc": 173,
      "topic": 8,
      "similarity": 0.7861306554363824
    },
    {
      "doc": 173,
      "topic": 9,
      "similarity": 0.8242774838650737
    },
    {
      "doc": 173,
      "topic": 10,
      "similarity": 0.7710679408691047
    },
    {
      "doc": 173,
      "topic": 11,
      "similarity": 0.7693368554988362
    },
    {
      "doc": 173,
      "topic": 12,
      "similarity": 0.7502084421028136
    },
    {
      "doc": 173,
      "topic": 13,
      "similarity": 0.7843334077633621
    },
    {
      "doc": 173,
      "topic": 14,
      "similarity": 0.776539612487101
    },
    {
      "doc": 173,
      "topic": 15,
      "similarity": 0.7624474219287815
    },
    {
      "doc": 173,
      "topic": 16,
      "similarity": 0.7928866676338224
    },
    {
      "doc": 173,
      "topic": 17,
      "similarity": 0.784545963670486
    },
    {
      "doc": 173,
      "topic": 18,
      "similarity": 0.7606799211979342
    },
    {
      "doc": 173,
      "topic": 19,
      "similarity": 0.7888317677241591
    },
    {
      "doc": 173,
      "topic": 20,
      "similarity": 0.7645338704429756
    },
    {
      "doc": 173,
      "topic": 21,
      "similarity": 0.7936894878769057
    },
    {
      "doc": 173,
      "topic": 22,
      "similarity": 0.7541344119493993
    },
    {
      "doc": 173,
      "topic": 23,
      "similarity": 0.7552496398627665
    },
    {
      "doc": 174,
      "topic": 0,
      "similarity": 0.7539433217644088
    },
    {
      "doc": 174,
      "topic": 3,
      "similarity": 0.7859836830032884
    },
    {
      "doc": 174,
      "topic": 5,
      "similarity": 0.7826244090156664
    },
    {
      "doc": 174,
      "topic": 6,
      "similarity": 0.7660375421647575
    },
    {
      "doc": 174,
      "topic": 7,
      "similarity": 0.7624057693638624
    },
    {
      "doc": 174,
      "topic": 8,
      "similarity": 0.7752876775261467
    },
    {
      "doc": 174,
      "topic": 9,
      "similarity": 0.8140598278877345
    },
    {
      "doc": 174,
      "topic": 10,
      "similarity": 0.7718078280768244
    },
    {
      "doc": 174,
      "topic": 11,
      "similarity": 0.8004078474408542
    },
    {
      "doc": 174,
      "topic": 12,
      "similarity": 0.7824281191580462
    },
    {
      "doc": 174,
      "topic": 13,
      "similarity": 0.750718083779533
    },
    {
      "doc": 174,
      "topic": 14,
      "similarity": 0.7534790237865951
    },
    {
      "doc": 174,
      "topic": 15,
      "similarity": 0.7871152207357165
    },
    {
      "doc": 174,
      "topic": 16,
      "similarity": 0.7904751813432785
    },
    {
      "doc": 174,
      "topic": 17,
      "similarity": 0.8181785927906673
    },
    {
      "doc": 174,
      "topic": 18,
      "similarity": 0.7788302281235944
    },
    {
      "doc": 174,
      "topic": 19,
      "similarity": 0.7895034082103262
    },
    {
      "doc": 174,
      "topic": 20,
      "similarity": 0.8017522698848848
    },
    {
      "doc": 174,
      "topic": 21,
      "similarity": 0.7996692974198516
    },
    {
      "doc": 174,
      "topic": 23,
      "similarity": 0.7655571591124704
    },
    {
      "doc": 174,
      "topic": 24,
      "similarity": 0.7659025592074387
    },
    {
      "doc": 175,
      "topic": 3,
      "similarity": 0.7709529516167718
    },
    {
      "doc": 175,
      "topic": 4,
      "similarity": 0.8219224951937789
    },
    {
      "doc": 175,
      "topic": 5,
      "similarity": 0.764087624744167
    },
    {
      "doc": 175,
      "topic": 9,
      "similarity": 0.7623516462326786
    },
    {
      "doc": 175,
      "topic": 10,
      "similarity": 0.7518208710066241
    },
    {
      "doc": 175,
      "topic": 16,
      "similarity": 0.784168321802346
    },
    {
      "doc": 175,
      "topic": 17,
      "similarity": 0.7666083797032313
    },
    {
      "doc": 175,
      "topic": 19,
      "similarity": 0.7718801663501049
    },
    {
      "doc": 175,
      "topic": 21,
      "similarity": 0.7525405461875032
    },
    {
      "doc": 176,
      "topic": 2,
      "similarity": 0.7584132687622016
    },
    {
      "doc": 176,
      "topic": 3,
      "similarity": 0.7802325908826133
    },
    {
      "doc": 176,
      "topic": 5,
      "similarity": 0.7980304760499427
    },
    {
      "doc": 176,
      "topic": 7,
      "similarity": 0.783161181737578
    },
    {
      "doc": 176,
      "topic": 8,
      "similarity": 0.7846228804561785
    },
    {
      "doc": 176,
      "topic": 9,
      "similarity": 0.8118910703534485
    },
    {
      "doc": 176,
      "topic": 10,
      "similarity": 0.7558521787575901
    },
    {
      "doc": 176,
      "topic": 15,
      "similarity": 0.7574768133360025
    },
    {
      "doc": 176,
      "topic": 16,
      "similarity": 0.795995322283476
    },
    {
      "doc": 176,
      "topic": 17,
      "similarity": 0.7767557249158686
    },
    {
      "doc": 176,
      "topic": 19,
      "similarity": 0.7753265307917121
    },
    {
      "doc": 176,
      "topic": 20,
      "similarity": 0.7755961429940704
    },
    {
      "doc": 176,
      "topic": 21,
      "similarity": 0.7796478066303817
    },
    {
      "doc": 176,
      "topic": 24,
      "similarity": 0.7786241808741449
    },
    {
      "doc": 177,
      "topic": 1,
      "similarity": 0.767038326720231
    },
    {
      "doc": 177,
      "topic": 2,
      "similarity": 0.7793860277461917
    },
    {
      "doc": 177,
      "topic": 3,
      "similarity": 0.8012823084345011
    },
    {
      "doc": 177,
      "topic": 4,
      "similarity": 0.7561827450556912
    },
    {
      "doc": 177,
      "topic": 5,
      "similarity": 0.8063660948980345
    },
    {
      "doc": 177,
      "topic": 7,
      "similarity": 0.7892582325866127
    },
    {
      "doc": 177,
      "topic": 8,
      "similarity": 0.7995149547240242
    },
    {
      "doc": 177,
      "topic": 9,
      "similarity": 0.8052520095447676
    },
    {
      "doc": 177,
      "topic": 10,
      "similarity": 0.7853893469057228
    },
    {
      "doc": 177,
      "topic": 11,
      "similarity": 0.797321307575849
    },
    {
      "doc": 177,
      "topic": 13,
      "similarity": 0.777756529521374
    },
    {
      "doc": 177,
      "topic": 14,
      "similarity": 0.782886482901053
    },
    {
      "doc": 177,
      "topic": 15,
      "similarity": 0.7844036705511708
    },
    {
      "doc": 177,
      "topic": 16,
      "similarity": 0.8304526262432642
    },
    {
      "doc": 177,
      "topic": 17,
      "similarity": 0.8081953185144939
    },
    {
      "doc": 177,
      "topic": 19,
      "similarity": 0.8084586002323672
    },
    {
      "doc": 177,
      "topic": 20,
      "similarity": 0.7660797670525324
    },
    {
      "doc": 177,
      "topic": 21,
      "similarity": 0.7947725683882674
    },
    {
      "doc": 178,
      "topic": 1,
      "similarity": 0.7509783025610035
    },
    {
      "doc": 178,
      "topic": 2,
      "similarity": 0.7740095502896279
    },
    {
      "doc": 178,
      "topic": 3,
      "similarity": 0.7989827097508762
    },
    {
      "doc": 178,
      "topic": 5,
      "similarity": 0.8212619046133767
    },
    {
      "doc": 178,
      "topic": 7,
      "similarity": 0.7799182455681518
    },
    {
      "doc": 178,
      "topic": 8,
      "similarity": 0.7845819235460016
    },
    {
      "doc": 178,
      "topic": 9,
      "similarity": 0.8293437002969523
    },
    {
      "doc": 178,
      "topic": 10,
      "similarity": 0.7935517625621363
    },
    {
      "doc": 178,
      "topic": 11,
      "similarity": 0.7871131349026699
    },
    {
      "doc": 178,
      "topic": 12,
      "similarity": 0.7662563264033891
    },
    {
      "doc": 178,
      "topic": 13,
      "similarity": 0.7793365412706166
    },
    {
      "doc": 178,
      "topic": 14,
      "similarity": 0.7698499339135362
    },
    {
      "doc": 178,
      "topic": 15,
      "similarity": 0.7777637213362772
    },
    {
      "doc": 178,
      "topic": 16,
      "similarity": 0.8284987421018056
    },
    {
      "doc": 178,
      "topic": 17,
      "similarity": 0.8162575464054593
    },
    {
      "doc": 178,
      "topic": 18,
      "similarity": 0.7633731807507025
    },
    {
      "doc": 178,
      "topic": 19,
      "similarity": 0.8133426965684748
    },
    {
      "doc": 178,
      "topic": 20,
      "similarity": 0.800595160396498
    },
    {
      "doc": 178,
      "topic": 21,
      "similarity": 0.8112854987755886
    },
    {
      "doc": 178,
      "topic": 24,
      "similarity": 0.7649940185834504
    },
    {
      "doc": 179,
      "topic": 0,
      "similarity": 0.7839554171693125
    },
    {
      "doc": 179,
      "topic": 3,
      "similarity": 0.7672501893623109
    },
    {
      "doc": 179,
      "topic": 6,
      "similarity": 0.7734224856693318
    },
    {
      "doc": 179,
      "topic": 9,
      "similarity": 0.7848818732175936
    },
    {
      "doc": 179,
      "topic": 11,
      "similarity": 0.7711980604084792
    },
    {
      "doc": 179,
      "topic": 15,
      "similarity": 0.7654236881761777
    },
    {
      "doc": 179,
      "topic": 16,
      "similarity": 0.7747788650476052
    },
    {
      "doc": 179,
      "topic": 17,
      "similarity": 0.7975513354012334
    },
    {
      "doc": 179,
      "topic": 19,
      "similarity": 0.7667476742228082
    },
    {
      "doc": 179,
      "topic": 20,
      "similarity": 0.7820233186118914
    },
    {
      "doc": 179,
      "topic": 21,
      "similarity": 0.7770459284535691
    },
    {
      "doc": 179,
      "topic": 23,
      "similarity": 0.7817740128030969
    },
    {
      "doc": 180,
      "topic": 1,
      "similarity": 0.7748927346292827
    },
    {
      "doc": 180,
      "topic": 2,
      "similarity": 0.7695524237047169
    },
    {
      "doc": 180,
      "topic": 3,
      "similarity": 0.7942030298701344
    },
    {
      "doc": 180,
      "topic": 4,
      "similarity": 0.7504486874883283
    },
    {
      "doc": 180,
      "topic": 5,
      "similarity": 0.7882901004653246
    },
    {
      "doc": 180,
      "topic": 7,
      "similarity": 0.7555782437207271
    },
    {
      "doc": 180,
      "topic": 9,
      "similarity": 0.8192270657975519
    },
    {
      "doc": 180,
      "topic": 10,
      "similarity": 0.7691052087795803
    },
    {
      "doc": 180,
      "topic": 11,
      "similarity": 0.770465222483612
    },
    {
      "doc": 180,
      "topic": 13,
      "similarity": 0.7542505476187151
    },
    {
      "doc": 180,
      "topic": 14,
      "similarity": 0.7888724299430444
    },
    {
      "doc": 180,
      "topic": 15,
      "similarity": 0.7715460897722402
    },
    {
      "doc": 180,
      "topic": 16,
      "similarity": 0.8163751272515553
    },
    {
      "doc": 180,
      "topic": 17,
      "similarity": 0.7817162661795758
    },
    {
      "doc": 180,
      "topic": 19,
      "similarity": 0.8013354462421552
    },
    {
      "doc": 180,
      "topic": 20,
      "similarity": 0.7687069088216774
    },
    {
      "doc": 180,
      "topic": 21,
      "similarity": 0.7919849219141841
    },
    {
      "doc": 180,
      "topic": 23,
      "similarity": 0.7995774038344644
    },
    {
      "doc": 181,
      "topic": 1,
      "similarity": 0.7537365357055604
    },
    {
      "doc": 181,
      "topic": 2,
      "similarity": 0.7700074022772093
    },
    {
      "doc": 181,
      "topic": 3,
      "similarity": 0.7948493430944863
    },
    {
      "doc": 181,
      "topic": 5,
      "similarity": 0.7653527052255228
    },
    {
      "doc": 181,
      "topic": 7,
      "similarity": 0.7669224210284663
    },
    {
      "doc": 181,
      "topic": 8,
      "similarity": 0.7528920107469519
    },
    {
      "doc": 181,
      "topic": 9,
      "similarity": 0.8272088975355284
    },
    {
      "doc": 181,
      "topic": 10,
      "similarity": 0.755150258180929
    },
    {
      "doc": 181,
      "topic": 11,
      "similarity": 0.7645820224008484
    },
    {
      "doc": 181,
      "topic": 14,
      "similarity": 0.7565903035973627
    },
    {
      "doc": 181,
      "topic": 15,
      "similarity": 0.7635972612913369
    },
    {
      "doc": 181,
      "topic": 16,
      "similarity": 0.7774244361221744
    },
    {
      "doc": 181,
      "topic": 17,
      "similarity": 0.7667477194426441
    },
    {
      "doc": 181,
      "topic": 18,
      "similarity": 0.7519660632172569
    },
    {
      "doc": 181,
      "topic": 19,
      "similarity": 0.810636198644191
    },
    {
      "doc": 181,
      "topic": 20,
      "similarity": 0.7631879544350031
    },
    {
      "doc": 181,
      "topic": 21,
      "similarity": 0.7966060818628602
    },
    {
      "doc": 181,
      "topic": 23,
      "similarity": 0.7907483243250502
    },
    {
      "doc": 181,
      "topic": 24,
      "similarity": 0.7553348845421033
    },
    {
      "doc": 182,
      "topic": 1,
      "similarity": 0.7663623616606985
    },
    {
      "doc": 182,
      "topic": 2,
      "similarity": 0.7877128847944829
    },
    {
      "doc": 182,
      "topic": 3,
      "similarity": 0.8055190622508385
    },
    {
      "doc": 182,
      "topic": 4,
      "similarity": 0.7649807697015947
    },
    {
      "doc": 182,
      "topic": 5,
      "similarity": 0.8089826008500929
    },
    {
      "doc": 182,
      "topic": 7,
      "similarity": 0.7927300406827328
    },
    {
      "doc": 182,
      "topic": 8,
      "similarity": 0.8073247637406236
    },
    {
      "doc": 182,
      "topic": 9,
      "similarity": 0.8519434476463104
    },
    {
      "doc": 182,
      "topic": 10,
      "similarity": 0.8108179195307716
    },
    {
      "doc": 182,
      "topic": 11,
      "similarity": 0.7948833503617244
    },
    {
      "doc": 182,
      "topic": 12,
      "similarity": 0.7681026295170713
    },
    {
      "doc": 182,
      "topic": 13,
      "similarity": 0.8148566813199783
    },
    {
      "doc": 182,
      "topic": 14,
      "similarity": 0.8358843946244437
    },
    {
      "doc": 182,
      "topic": 15,
      "similarity": 0.7994463614951322
    },
    {
      "doc": 182,
      "topic": 16,
      "similarity": 0.814517046149118
    },
    {
      "doc": 182,
      "topic": 17,
      "similarity": 0.8086170261071525
    },
    {
      "doc": 182,
      "topic": 18,
      "similarity": 0.796571103692387
    },
    {
      "doc": 182,
      "topic": 19,
      "similarity": 0.8393737655805388
    },
    {
      "doc": 182,
      "topic": 20,
      "similarity": 0.8059479286256804
    },
    {
      "doc": 182,
      "topic": 21,
      "similarity": 0.8072630399604248
    },
    {
      "doc": 182,
      "topic": 23,
      "similarity": 0.757287440648978
    },
    {
      "doc": 183,
      "topic": 1,
      "similarity": 0.7695261063672225
    },
    {
      "doc": 183,
      "topic": 2,
      "similarity": 0.7826990522423514
    },
    {
      "doc": 183,
      "topic": 3,
      "similarity": 0.7979469843679229
    },
    {
      "doc": 183,
      "topic": 5,
      "similarity": 0.7914763163782795
    },
    {
      "doc": 183,
      "topic": 7,
      "similarity": 0.7889486431430207
    },
    {
      "doc": 183,
      "topic": 8,
      "similarity": 0.7926163554530179
    },
    {
      "doc": 183,
      "topic": 9,
      "similarity": 0.8180332212344266
    },
    {
      "doc": 183,
      "topic": 10,
      "similarity": 0.7643300802541118
    },
    {
      "doc": 183,
      "topic": 11,
      "similarity": 0.7693367770261478
    },
    {
      "doc": 183,
      "topic": 13,
      "similarity": 0.7582818389677094
    },
    {
      "doc": 183,
      "topic": 14,
      "similarity": 0.7583560114386247
    },
    {
      "doc": 183,
      "topic": 15,
      "similarity": 0.7719324640001075
    },
    {
      "doc": 183,
      "topic": 16,
      "similarity": 0.80135031015994
    },
    {
      "doc": 183,
      "topic": 17,
      "similarity": 0.7852826559030499
    },
    {
      "doc": 183,
      "topic": 19,
      "similarity": 0.7841057702973705
    },
    {
      "doc": 183,
      "topic": 20,
      "similarity": 0.7781914823187124
    },
    {
      "doc": 183,
      "topic": 21,
      "similarity": 0.7899807325673075
    },
    {
      "doc": 183,
      "topic": 23,
      "similarity": 0.7916021735682757
    },
    {
      "doc": 183,
      "topic": 24,
      "similarity": 0.7606847107097067
    },
    {
      "doc": 184,
      "topic": 1,
      "similarity": 0.7825566257910791
    },
    {
      "doc": 184,
      "topic": 2,
      "similarity": 0.8143635397308632
    },
    {
      "doc": 184,
      "topic": 3,
      "similarity": 0.8130489037971675
    },
    {
      "doc": 184,
      "topic": 4,
      "similarity": 0.7651561406897801
    },
    {
      "doc": 184,
      "topic": 5,
      "similarity": 0.8129749343483814
    },
    {
      "doc": 184,
      "topic": 7,
      "similarity": 0.7905569787811985
    },
    {
      "doc": 184,
      "topic": 8,
      "similarity": 0.800398707351143
    },
    {
      "doc": 184,
      "topic": 9,
      "similarity": 0.8207129019044773
    },
    {
      "doc": 184,
      "topic": 10,
      "similarity": 0.8085127273624515
    },
    {
      "doc": 184,
      "topic": 11,
      "similarity": 0.8075846467335268
    },
    {
      "doc": 184,
      "topic": 12,
      "similarity": 0.804939946276715
    },
    {
      "doc": 184,
      "topic": 13,
      "similarity": 0.8427423986639132
    },
    {
      "doc": 184,
      "topic": 14,
      "similarity": 0.7890636068036961
    },
    {
      "doc": 184,
      "topic": 15,
      "similarity": 0.7948523813719092
    },
    {
      "doc": 184,
      "topic": 16,
      "similarity": 0.8263944972439025
    },
    {
      "doc": 184,
      "topic": 17,
      "similarity": 0.8228268760624537
    },
    {
      "doc": 184,
      "topic": 18,
      "similarity": 0.7899776411023854
    },
    {
      "doc": 184,
      "topic": 19,
      "similarity": 0.8215472657084426
    },
    {
      "doc": 184,
      "topic": 20,
      "similarity": 0.8099514283671292
    },
    {
      "doc": 184,
      "topic": 21,
      "similarity": 0.8289549497065666
    },
    {
      "doc": 184,
      "topic": 22,
      "similarity": 0.7506057860298574
    },
    {
      "doc": 184,
      "topic": 23,
      "similarity": 0.757369429803105
    },
    {
      "doc": 185,
      "topic": 1,
      "similarity": 0.752494267469434
    },
    {
      "doc": 185,
      "topic": 2,
      "similarity": 0.7645923407353185
    },
    {
      "doc": 185,
      "topic": 3,
      "similarity": 0.7887943092446319
    },
    {
      "doc": 185,
      "topic": 4,
      "similarity": 0.7657645926055605
    },
    {
      "doc": 185,
      "topic": 5,
      "similarity": 0.7981667729613632
    },
    {
      "doc": 185,
      "topic": 6,
      "similarity": 0.7565417627832198
    },
    {
      "doc": 185,
      "topic": 7,
      "similarity": 0.7641837789645937
    },
    {
      "doc": 185,
      "topic": 8,
      "similarity": 0.7681179092383591
    },
    {
      "doc": 185,
      "topic": 9,
      "similarity": 0.8056511713552853
    },
    {
      "doc": 185,
      "topic": 10,
      "similarity": 0.7827859437751108
    },
    {
      "doc": 185,
      "topic": 11,
      "similarity": 0.7743570847645692
    },
    {
      "doc": 185,
      "topic": 13,
      "similarity": 0.764745310125257
    },
    {
      "doc": 185,
      "topic": 14,
      "similarity": 0.7624984706428122
    },
    {
      "doc": 185,
      "topic": 15,
      "similarity": 0.7895908745902316
    },
    {
      "doc": 185,
      "topic": 16,
      "similarity": 0.8142123634505928
    },
    {
      "doc": 185,
      "topic": 17,
      "similarity": 0.8252834537489527
    },
    {
      "doc": 185,
      "topic": 18,
      "similarity": 0.7725138240582499
    },
    {
      "doc": 185,
      "topic": 19,
      "similarity": 0.8068812442882327
    },
    {
      "doc": 185,
      "topic": 20,
      "similarity": 0.7920651409913289
    },
    {
      "doc": 185,
      "topic": 21,
      "similarity": 0.8226979126232276
    },
    {
      "doc": 186,
      "topic": 1,
      "similarity": 0.759695974867229
    },
    {
      "doc": 186,
      "topic": 2,
      "similarity": 0.7817383347237425
    },
    {
      "doc": 186,
      "topic": 3,
      "similarity": 0.8071794115080531
    },
    {
      "doc": 186,
      "topic": 4,
      "similarity": 0.7653099233831177
    },
    {
      "doc": 186,
      "topic": 5,
      "similarity": 0.8178244170253667
    },
    {
      "doc": 186,
      "topic": 7,
      "similarity": 0.7872006172724341
    },
    {
      "doc": 186,
      "topic": 8,
      "similarity": 0.7786455025985991
    },
    {
      "doc": 186,
      "topic": 9,
      "similarity": 0.8353285156492439
    },
    {
      "doc": 186,
      "topic": 10,
      "similarity": 0.7795843307058954
    },
    {
      "doc": 186,
      "topic": 11,
      "similarity": 0.8018442254906831
    },
    {
      "doc": 186,
      "topic": 13,
      "similarity": 0.7773317859921949
    },
    {
      "doc": 186,
      "topic": 14,
      "similarity": 0.7679254805059643
    },
    {
      "doc": 186,
      "topic": 15,
      "similarity": 0.780371591258308
    },
    {
      "doc": 186,
      "topic": 16,
      "similarity": 0.8644327531495718
    },
    {
      "doc": 186,
      "topic": 17,
      "similarity": 0.8065016147589974
    },
    {
      "doc": 186,
      "topic": 18,
      "similarity": 0.7551679464848627
    },
    {
      "doc": 186,
      "topic": 19,
      "similarity": 0.807232293720154
    },
    {
      "doc": 186,
      "topic": 20,
      "similarity": 0.7826682143740309
    },
    {
      "doc": 186,
      "topic": 21,
      "similarity": 0.8100306689582739
    },
    {
      "doc": 186,
      "topic": 24,
      "similarity": 0.7673435966123439
    },
    {
      "doc": 187,
      "topic": 1,
      "similarity": 0.7512021673124968
    },
    {
      "doc": 187,
      "topic": 2,
      "similarity": 0.7776159328400547
    },
    {
      "doc": 187,
      "topic": 3,
      "similarity": 0.7859604285042573
    },
    {
      "doc": 187,
      "topic": 5,
      "similarity": 0.821812378814651
    },
    {
      "doc": 187,
      "topic": 7,
      "similarity": 0.7726024751197406
    },
    {
      "doc": 187,
      "topic": 8,
      "similarity": 0.7730502550738849
    },
    {
      "doc": 187,
      "topic": 9,
      "similarity": 0.8414269353045769
    },
    {
      "doc": 187,
      "topic": 10,
      "similarity": 0.7774081041174742
    },
    {
      "doc": 187,
      "topic": 11,
      "similarity": 0.7859969952786278
    },
    {
      "doc": 187,
      "topic": 13,
      "similarity": 0.7748812696215884
    },
    {
      "doc": 187,
      "topic": 14,
      "similarity": 0.7640187476909814
    },
    {
      "doc": 187,
      "topic": 15,
      "similarity": 0.7738827771770019
    },
    {
      "doc": 187,
      "topic": 16,
      "similarity": 0.8413090431243342
    },
    {
      "doc": 187,
      "topic": 17,
      "similarity": 0.8074516465380397
    },
    {
      "doc": 187,
      "topic": 19,
      "similarity": 0.7939741975473613
    },
    {
      "doc": 187,
      "topic": 20,
      "similarity": 0.7708692744670204
    },
    {
      "doc": 187,
      "topic": 21,
      "similarity": 0.8044008382158012
    },
    {
      "doc": 187,
      "topic": 23,
      "similarity": 0.7549654884093223
    },
    {
      "doc": 187,
      "topic": 24,
      "similarity": 0.7786875760925882
    },
    {
      "doc": 188,
      "topic": 3,
      "similarity": 0.7507151358622363
    },
    {
      "doc": 188,
      "topic": 9,
      "similarity": 0.7709714495495528
    },
    {
      "doc": 188,
      "topic": 17,
      "similarity": 0.7640847031966164
    },
    {
      "doc": 188,
      "topic": 19,
      "similarity": 0.7634625102678024
    },
    {
      "doc": 188,
      "topic": 21,
      "similarity": 0.7577781882166074
    },
    {
      "doc": 189,
      "topic": 3,
      "similarity": 0.7573568662923077
    },
    {
      "doc": 189,
      "topic": 5,
      "similarity": 0.7521316144851877
    },
    {
      "doc": 189,
      "topic": 9,
      "similarity": 0.8046001740428245
    },
    {
      "doc": 189,
      "topic": 16,
      "similarity": 0.7655047960918845
    },
    {
      "doc": 189,
      "topic": 17,
      "similarity": 0.7504567992920163
    },
    {
      "doc": 189,
      "topic": 19,
      "similarity": 0.7586742059955912
    },
    {
      "doc": 189,
      "topic": 21,
      "similarity": 0.7871768471370774
    },
    {
      "doc": 190,
      "topic": 1,
      "similarity": 0.7824536983964545
    },
    {
      "doc": 190,
      "topic": 2,
      "similarity": 0.8198666641927106
    },
    {
      "doc": 190,
      "topic": 3,
      "similarity": 0.8134397534734831
    },
    {
      "doc": 190,
      "topic": 4,
      "similarity": 0.7716189896565908
    },
    {
      "doc": 190,
      "topic": 5,
      "similarity": 0.8226085713155343
    },
    {
      "doc": 190,
      "topic": 6,
      "similarity": 0.7604192685662134
    },
    {
      "doc": 190,
      "topic": 7,
      "similarity": 0.7928230393507129
    },
    {
      "doc": 190,
      "topic": 8,
      "similarity": 0.8000693031756201
    },
    {
      "doc": 190,
      "topic": 9,
      "similarity": 0.8316460545386146
    },
    {
      "doc": 190,
      "topic": 10,
      "similarity": 0.7976336154844776
    },
    {
      "doc": 190,
      "topic": 11,
      "similarity": 0.785533825854378
    },
    {
      "doc": 190,
      "topic": 12,
      "similarity": 0.7595285896910239
    },
    {
      "doc": 190,
      "topic": 13,
      "similarity": 0.7994887313949727
    },
    {
      "doc": 190,
      "topic": 14,
      "similarity": 0.8554675676425864
    },
    {
      "doc": 190,
      "topic": 15,
      "similarity": 0.7877821691468587
    },
    {
      "doc": 190,
      "topic": 16,
      "similarity": 0.8217010455250652
    },
    {
      "doc": 190,
      "topic": 17,
      "similarity": 0.8018442005967058
    },
    {
      "doc": 190,
      "topic": 18,
      "similarity": 0.7755126128299127
    },
    {
      "doc": 190,
      "topic": 19,
      "similarity": 0.8301131033398637
    },
    {
      "doc": 190,
      "topic": 20,
      "similarity": 0.7900087436381737
    },
    {
      "doc": 190,
      "topic": 21,
      "similarity": 0.8039011628400892
    },
    {
      "doc": 190,
      "topic": 22,
      "similarity": 0.7787853412769818
    },
    {
      "doc": 190,
      "topic": 23,
      "similarity": 0.7811457299300208
    },
    {
      "doc": 191,
      "topic": 1,
      "similarity": 0.7586894479060029
    },
    {
      "doc": 191,
      "topic": 2,
      "similarity": 0.7726369428756771
    },
    {
      "doc": 191,
      "topic": 3,
      "similarity": 0.8027399704358089
    },
    {
      "doc": 191,
      "topic": 4,
      "similarity": 0.7546912065549022
    },
    {
      "doc": 191,
      "topic": 5,
      "similarity": 0.8021884457855807
    },
    {
      "doc": 191,
      "topic": 7,
      "similarity": 0.784485461004233
    },
    {
      "doc": 191,
      "topic": 8,
      "similarity": 0.7886052732280953
    },
    {
      "doc": 191,
      "topic": 9,
      "similarity": 0.8478959379341596
    },
    {
      "doc": 191,
      "topic": 10,
      "similarity": 0.7784873338035221
    },
    {
      "doc": 191,
      "topic": 11,
      "similarity": 0.7972659316545175
    },
    {
      "doc": 191,
      "topic": 12,
      "similarity": 0.7760156801803512
    },
    {
      "doc": 191,
      "topic": 13,
      "similarity": 0.7990727897461269
    },
    {
      "doc": 191,
      "topic": 14,
      "similarity": 0.7804870818123033
    },
    {
      "doc": 191,
      "topic": 15,
      "similarity": 0.7764355638621407
    },
    {
      "doc": 191,
      "topic": 16,
      "similarity": 0.8049802864596024
    },
    {
      "doc": 191,
      "topic": 17,
      "similarity": 0.8076008254096668
    },
    {
      "doc": 191,
      "topic": 18,
      "similarity": 0.7760982186209084
    },
    {
      "doc": 191,
      "topic": 19,
      "similarity": 0.80901400177236
    },
    {
      "doc": 191,
      "topic": 20,
      "similarity": 0.8015524932912775
    },
    {
      "doc": 191,
      "topic": 21,
      "similarity": 0.8148063142048035
    },
    {
      "doc": 191,
      "topic": 23,
      "similarity": 0.7599319082557737
    },
    {
      "doc": 191,
      "topic": 24,
      "similarity": 0.7762353315962766
    },
    {
      "doc": 192,
      "topic": 1,
      "similarity": 0.7799718284322216
    },
    {
      "doc": 192,
      "topic": 2,
      "similarity": 0.8086302506016445
    },
    {
      "doc": 192,
      "topic": 3,
      "similarity": 0.8254944246026208
    },
    {
      "doc": 192,
      "topic": 4,
      "similarity": 0.7579620580221795
    },
    {
      "doc": 192,
      "topic": 5,
      "similarity": 0.8119939990176865
    },
    {
      "doc": 192,
      "topic": 6,
      "similarity": 0.7540152162181721
    },
    {
      "doc": 192,
      "topic": 7,
      "similarity": 0.8065699359679811
    },
    {
      "doc": 192,
      "topic": 8,
      "similarity": 0.79743183238134
    },
    {
      "doc": 192,
      "topic": 9,
      "similarity": 0.8201393456189644
    },
    {
      "doc": 192,
      "topic": 10,
      "similarity": 0.7953507684660793
    },
    {
      "doc": 192,
      "topic": 11,
      "similarity": 0.799884471939645
    },
    {
      "doc": 192,
      "topic": 13,
      "similarity": 0.7767626472579695
    },
    {
      "doc": 192,
      "topic": 14,
      "similarity": 0.7829991282693567
    },
    {
      "doc": 192,
      "topic": 15,
      "similarity": 0.7880127153365737
    },
    {
      "doc": 192,
      "topic": 16,
      "similarity": 0.8114339575285725
    },
    {
      "doc": 192,
      "topic": 17,
      "similarity": 0.8179024228152755
    },
    {
      "doc": 192,
      "topic": 18,
      "similarity": 0.7608319321960367
    },
    {
      "doc": 192,
      "topic": 19,
      "similarity": 0.8154959460597648
    },
    {
      "doc": 192,
      "topic": 20,
      "similarity": 0.760286367057971
    },
    {
      "doc": 192,
      "topic": 21,
      "similarity": 0.7937069865922856
    },
    {
      "doc": 192,
      "topic": 22,
      "similarity": 0.7585047701661616
    },
    {
      "doc": 192,
      "topic": 23,
      "similarity": 0.7736805525633818
    },
    {
      "doc": 192,
      "topic": 24,
      "similarity": 0.7697009225769014
    },
    {
      "doc": 193,
      "topic": 1,
      "similarity": 0.7583952863571329
    },
    {
      "doc": 193,
      "topic": 2,
      "similarity": 0.7721832536118284
    },
    {
      "doc": 193,
      "topic": 3,
      "similarity": 0.806882935931028
    },
    {
      "doc": 193,
      "topic": 4,
      "similarity": 0.7501619633624925
    },
    {
      "doc": 193,
      "topic": 5,
      "similarity": 0.8076137144713034
    },
    {
      "doc": 193,
      "topic": 7,
      "similarity": 0.7878562858184213
    },
    {
      "doc": 193,
      "topic": 8,
      "similarity": 0.7804021322790329
    },
    {
      "doc": 193,
      "topic": 9,
      "similarity": 0.8407060922416856
    },
    {
      "doc": 193,
      "topic": 10,
      "similarity": 0.7818598589385309
    },
    {
      "doc": 193,
      "topic": 11,
      "similarity": 0.8405817828168918
    },
    {
      "doc": 193,
      "topic": 13,
      "similarity": 0.7714675004760359
    },
    {
      "doc": 193,
      "topic": 14,
      "similarity": 0.7702856362616748
    },
    {
      "doc": 193,
      "topic": 15,
      "similarity": 0.8049568316795084
    },
    {
      "doc": 193,
      "topic": 16,
      "similarity": 0.804690330162168
    },
    {
      "doc": 193,
      "topic": 17,
      "similarity": 0.813221699101738
    },
    {
      "doc": 193,
      "topic": 18,
      "similarity": 0.7784575158471385
    },
    {
      "doc": 193,
      "topic": 19,
      "similarity": 0.8102962186586398
    },
    {
      "doc": 193,
      "topic": 20,
      "similarity": 0.8142925613649336
    },
    {
      "doc": 193,
      "topic": 21,
      "similarity": 0.8174709194700663
    },
    {
      "doc": 193,
      "topic": 24,
      "similarity": 0.7614861575909649
    },
    {
      "doc": 194,
      "topic": 2,
      "similarity": 0.7590165473808839
    },
    {
      "doc": 194,
      "topic": 3,
      "similarity": 0.7906535418984338
    },
    {
      "doc": 194,
      "topic": 4,
      "similarity": 0.7793085782004754
    },
    {
      "doc": 194,
      "topic": 5,
      "similarity": 0.7686467550030731
    },
    {
      "doc": 194,
      "topic": 7,
      "similarity": 0.7705351530394902
    },
    {
      "doc": 194,
      "topic": 8,
      "similarity": 0.7532560095874649
    },
    {
      "doc": 194,
      "topic": 9,
      "similarity": 0.8186052319844703
    },
    {
      "doc": 194,
      "topic": 10,
      "similarity": 0.7505919981473073
    },
    {
      "doc": 194,
      "topic": 11,
      "similarity": 0.7599094705002362
    },
    {
      "doc": 194,
      "topic": 15,
      "similarity": 0.7579990628396903
    },
    {
      "doc": 194,
      "topic": 16,
      "similarity": 0.7978996935254191
    },
    {
      "doc": 194,
      "topic": 17,
      "similarity": 0.7896403870815882
    },
    {
      "doc": 194,
      "topic": 19,
      "similarity": 0.7801258522395098
    },
    {
      "doc": 194,
      "topic": 20,
      "similarity": 0.754504396436006
    },
    {
      "doc": 194,
      "topic": 21,
      "similarity": 0.7945105902589877
    },
    {
      "doc": 194,
      "topic": 23,
      "similarity": 0.7569106133973037
    },
    {
      "doc": 194,
      "topic": 24,
      "similarity": 0.7887177562822686
    },
    {
      "doc": 195,
      "topic": 2,
      "similarity": 0.7591492590426219
    },
    {
      "doc": 195,
      "topic": 3,
      "similarity": 0.7881283299642335
    },
    {
      "doc": 195,
      "topic": 4,
      "similarity": 0.7532680969728376
    },
    {
      "doc": 195,
      "topic": 5,
      "similarity": 0.7997733048763612
    },
    {
      "doc": 195,
      "topic": 7,
      "similarity": 0.8326759981155339
    },
    {
      "doc": 195,
      "topic": 8,
      "similarity": 0.7991573973735454
    },
    {
      "doc": 195,
      "topic": 9,
      "similarity": 0.8358360207832134
    },
    {
      "doc": 195,
      "topic": 10,
      "similarity": 0.7840829889563344
    },
    {
      "doc": 195,
      "topic": 11,
      "similarity": 0.7634454253218356
    },
    {
      "doc": 195,
      "topic": 13,
      "similarity": 0.7674396196424031
    },
    {
      "doc": 195,
      "topic": 14,
      "similarity": 0.7599606182020104
    },
    {
      "doc": 195,
      "topic": 15,
      "similarity": 0.7671628964595035
    },
    {
      "doc": 195,
      "topic": 16,
      "similarity": 0.7880137424782668
    },
    {
      "doc": 195,
      "topic": 17,
      "similarity": 0.8091587418348488
    },
    {
      "doc": 195,
      "topic": 18,
      "similarity": 0.7622594820923524
    },
    {
      "doc": 195,
      "topic": 19,
      "similarity": 0.7958517871861512
    },
    {
      "doc": 195,
      "topic": 20,
      "similarity": 0.780013114203004
    },
    {
      "doc": 195,
      "topic": 21,
      "similarity": 0.804624671082582
    },
    {
      "doc": 195,
      "topic": 24,
      "similarity": 0.7559808257346946
    },
    {
      "doc": 196,
      "topic": 1,
      "similarity": 0.7644187421536085
    },
    {
      "doc": 196,
      "topic": 2,
      "similarity": 0.7716066280249866
    },
    {
      "doc": 196,
      "topic": 3,
      "similarity": 0.7922741162272432
    },
    {
      "doc": 196,
      "topic": 4,
      "similarity": 0.7512980331341341
    },
    {
      "doc": 196,
      "topic": 5,
      "similarity": 0.7931925217805755
    },
    {
      "doc": 196,
      "topic": 7,
      "similarity": 0.7864644876157556
    },
    {
      "doc": 196,
      "topic": 8,
      "similarity": 0.8094519570075052
    },
    {
      "doc": 196,
      "topic": 9,
      "similarity": 0.8192040489664615
    },
    {
      "doc": 196,
      "topic": 10,
      "similarity": 0.7863482808153512
    },
    {
      "doc": 196,
      "topic": 11,
      "similarity": 0.7911863169484266
    },
    {
      "doc": 196,
      "topic": 13,
      "similarity": 0.8183153962799699
    },
    {
      "doc": 196,
      "topic": 14,
      "similarity": 0.7857956678373634
    },
    {
      "doc": 196,
      "topic": 15,
      "similarity": 0.7726001167129942
    },
    {
      "doc": 196,
      "topic": 16,
      "similarity": 0.8079935630537004
    },
    {
      "doc": 196,
      "topic": 17,
      "similarity": 0.8001887510334306
    },
    {
      "doc": 196,
      "topic": 18,
      "similarity": 0.765597261181011
    },
    {
      "doc": 196,
      "topic": 19,
      "similarity": 0.8044539054087422
    },
    {
      "doc": 196,
      "topic": 20,
      "similarity": 0.7828517799078046
    },
    {
      "doc": 196,
      "topic": 21,
      "similarity": 0.7939712707186155
    },
    {
      "doc": 196,
      "topic": 22,
      "similarity": 0.7697093552321557
    },
    {
      "doc": 197,
      "topic": 1,
      "similarity": 0.7684968360188345
    },
    {
      "doc": 197,
      "topic": 2,
      "similarity": 0.7915806287549448
    },
    {
      "doc": 197,
      "topic": 3,
      "similarity": 0.812750735235407
    },
    {
      "doc": 197,
      "topic": 4,
      "similarity": 0.7756148194376857
    },
    {
      "doc": 197,
      "topic": 5,
      "similarity": 0.8247053893964108
    },
    {
      "doc": 197,
      "topic": 7,
      "similarity": 0.7875831808349544
    },
    {
      "doc": 197,
      "topic": 8,
      "similarity": 0.7913231519234691
    },
    {
      "doc": 197,
      "topic": 9,
      "similarity": 0.8365950479560383
    },
    {
      "doc": 197,
      "topic": 10,
      "similarity": 0.8091465320556328
    },
    {
      "doc": 197,
      "topic": 11,
      "similarity": 0.7996533121473927
    },
    {
      "doc": 197,
      "topic": 13,
      "similarity": 0.7886282760795982
    },
    {
      "doc": 197,
      "topic": 14,
      "similarity": 0.7940512385977188
    },
    {
      "doc": 197,
      "topic": 15,
      "similarity": 0.8064887184047618
    },
    {
      "doc": 197,
      "topic": 16,
      "similarity": 0.8363299495875967
    },
    {
      "doc": 197,
      "topic": 17,
      "similarity": 0.8256859719264179
    },
    {
      "doc": 197,
      "topic": 18,
      "similarity": 0.7658820633859577
    },
    {
      "doc": 197,
      "topic": 19,
      "similarity": 0.8253442524414837
    },
    {
      "doc": 197,
      "topic": 20,
      "similarity": 0.8142524239347979
    },
    {
      "doc": 197,
      "topic": 21,
      "similarity": 0.8238971269536213
    },
    {
      "doc": 197,
      "topic": 23,
      "similarity": 0.7591432313039389
    },
    {
      "doc": 197,
      "topic": 24,
      "similarity": 0.7565205558414072
    },
    {
      "doc": 198,
      "topic": 1,
      "similarity": 0.755077647662862
    },
    {
      "doc": 198,
      "topic": 2,
      "similarity": 0.7804794641177668
    },
    {
      "doc": 198,
      "topic": 3,
      "similarity": 0.7910200051250162
    },
    {
      "doc": 198,
      "topic": 5,
      "similarity": 0.7911207776301578
    },
    {
      "doc": 198,
      "topic": 7,
      "similarity": 0.7712661315969522
    },
    {
      "doc": 198,
      "topic": 8,
      "similarity": 0.7812266541548993
    },
    {
      "doc": 198,
      "topic": 9,
      "similarity": 0.8246148172455041
    },
    {
      "doc": 198,
      "topic": 10,
      "similarity": 0.7826829774003743
    },
    {
      "doc": 198,
      "topic": 11,
      "similarity": 0.7952417149259234
    },
    {
      "doc": 198,
      "topic": 12,
      "similarity": 0.7528898029229321
    },
    {
      "doc": 198,
      "topic": 13,
      "similarity": 0.7715158427357135
    },
    {
      "doc": 198,
      "topic": 14,
      "similarity": 0.7857219959573319
    },
    {
      "doc": 198,
      "topic": 15,
      "similarity": 0.8233368441650541
    },
    {
      "doc": 198,
      "topic": 16,
      "similarity": 0.7905970631678778
    },
    {
      "doc": 198,
      "topic": 17,
      "similarity": 0.8247830341502129
    },
    {
      "doc": 198,
      "topic": 18,
      "similarity": 0.7613817868529952
    },
    {
      "doc": 198,
      "topic": 19,
      "similarity": 0.8090599187094213
    },
    {
      "doc": 198,
      "topic": 20,
      "similarity": 0.766615171362206
    },
    {
      "doc": 198,
      "topic": 21,
      "similarity": 0.8054723103087369
    },
    {
      "doc": 198,
      "topic": 23,
      "similarity": 0.7580370063558907
    },
    {
      "doc": 199,
      "topic": 2,
      "similarity": 0.7571864511286509
    },
    {
      "doc": 199,
      "topic": 3,
      "similarity": 0.7956680528946587
    },
    {
      "doc": 199,
      "topic": 5,
      "similarity": 0.7909842437951838
    },
    {
      "doc": 199,
      "topic": 7,
      "similarity": 0.8011159518207732
    },
    {
      "doc": 199,
      "topic": 8,
      "similarity": 0.7876771917915878
    },
    {
      "doc": 199,
      "topic": 9,
      "similarity": 0.8296799054461282
    },
    {
      "doc": 199,
      "topic": 10,
      "similarity": 0.7781653319810148
    },
    {
      "doc": 199,
      "topic": 11,
      "similarity": 0.7819931353272646
    },
    {
      "doc": 199,
      "topic": 12,
      "similarity": 0.758950193421
    },
    {
      "doc": 199,
      "topic": 13,
      "similarity": 0.7543769260386913
    },
    {
      "doc": 199,
      "topic": 14,
      "similarity": 0.7528101615209941
    },
    {
      "doc": 199,
      "topic": 15,
      "similarity": 0.7876872981695838
    },
    {
      "doc": 199,
      "topic": 16,
      "similarity": 0.8167983795599518
    },
    {
      "doc": 199,
      "topic": 17,
      "similarity": 0.8042646497095417
    },
    {
      "doc": 199,
      "topic": 18,
      "similarity": 0.7621492088893251
    },
    {
      "doc": 199,
      "topic": 19,
      "similarity": 0.8007591440395916
    },
    {
      "doc": 199,
      "topic": 20,
      "similarity": 0.7842683591187433
    },
    {
      "doc": 199,
      "topic": 21,
      "similarity": 0.8061827028204709
    },
    {
      "doc": 199,
      "topic": 24,
      "similarity": 0.7524420894339525
    },
    {
      "doc": 200,
      "topic": 3,
      "similarity": 0.7506720879466259
    },
    {
      "doc": 200,
      "topic": 5,
      "similarity": 0.7577953204674783
    },
    {
      "doc": 200,
      "topic": 9,
      "similarity": 0.8159115363529122
    },
    {
      "doc": 200,
      "topic": 10,
      "similarity": 0.7629315835654927
    },
    {
      "doc": 200,
      "topic": 12,
      "similarity": 0.7663150287611754
    },
    {
      "doc": 200,
      "topic": 17,
      "similarity": 0.7603134699846384
    },
    {
      "doc": 200,
      "topic": 18,
      "similarity": 0.7548815068575656
    },
    {
      "doc": 200,
      "topic": 19,
      "similarity": 0.7676386510519015
    },
    {
      "doc": 200,
      "topic": 20,
      "similarity": 0.7670456918473958
    },
    {
      "doc": 200,
      "topic": 21,
      "similarity": 0.7693076320905283
    },
    {
      "doc": 200,
      "topic": 24,
      "similarity": 0.7657189700380411
    },
    {
      "doc": 201,
      "topic": 2,
      "similarity": 0.7544119031875096
    },
    {
      "doc": 201,
      "topic": 3,
      "similarity": 0.767274102623722
    },
    {
      "doc": 201,
      "topic": 5,
      "similarity": 0.7859303878439725
    },
    {
      "doc": 201,
      "topic": 7,
      "similarity": 0.7720603870553747
    },
    {
      "doc": 201,
      "topic": 8,
      "similarity": 0.7690518182138241
    },
    {
      "doc": 201,
      "topic": 9,
      "similarity": 0.8147472437856786
    },
    {
      "doc": 201,
      "topic": 10,
      "similarity": 0.7705581568309896
    },
    {
      "doc": 201,
      "topic": 11,
      "similarity": 0.7560619589493023
    },
    {
      "doc": 201,
      "topic": 15,
      "similarity": 0.7685497229157021
    },
    {
      "doc": 201,
      "topic": 16,
      "similarity": 0.7573281853483588
    },
    {
      "doc": 201,
      "topic": 17,
      "similarity": 0.7745029285380937
    },
    {
      "doc": 201,
      "topic": 18,
      "similarity": 0.7581648721944275
    },
    {
      "doc": 201,
      "topic": 19,
      "similarity": 0.7991609652660162
    },
    {
      "doc": 201,
      "topic": 20,
      "similarity": 0.7659873334698387
    },
    {
      "doc": 201,
      "topic": 21,
      "similarity": 0.7651920489128714
    },
    {
      "doc": 202,
      "topic": 3,
      "similarity": 0.7639319054434033
    },
    {
      "doc": 202,
      "topic": 5,
      "similarity": 0.7673946951878731
    },
    {
      "doc": 202,
      "topic": 7,
      "similarity": 0.7545893978469471
    },
    {
      "doc": 202,
      "topic": 9,
      "similarity": 0.8278235049297857
    },
    {
      "doc": 202,
      "topic": 10,
      "similarity": 0.7811933290101578
    },
    {
      "doc": 202,
      "topic": 11,
      "similarity": 0.7521682039644596
    },
    {
      "doc": 202,
      "topic": 15,
      "similarity": 0.7513162092241837
    },
    {
      "doc": 202,
      "topic": 16,
      "similarity": 0.752887886393016
    },
    {
      "doc": 202,
      "topic": 17,
      "similarity": 0.7594717938649722
    },
    {
      "doc": 202,
      "topic": 18,
      "similarity": 0.7595858537664765
    },
    {
      "doc": 202,
      "topic": 19,
      "similarity": 0.7706370865092519
    },
    {
      "doc": 202,
      "topic": 20,
      "similarity": 0.7717136484396079
    },
    {
      "doc": 202,
      "topic": 21,
      "similarity": 0.7857483542805667
    },
    {
      "doc": 202,
      "topic": 23,
      "similarity": 0.7608064361342434
    },
    {
      "doc": 202,
      "topic": 24,
      "similarity": 0.7648506496630438
    },
    {
      "doc": 203,
      "topic": 1,
      "similarity": 0.7739002922731769
    },
    {
      "doc": 203,
      "topic": 2,
      "similarity": 0.7684648476572452
    },
    {
      "doc": 203,
      "topic": 3,
      "similarity": 0.8047355499564973
    },
    {
      "doc": 203,
      "topic": 4,
      "similarity": 0.7697440956482106
    },
    {
      "doc": 203,
      "topic": 5,
      "similarity": 0.7901344842788287
    },
    {
      "doc": 203,
      "topic": 6,
      "similarity": 0.7566643849491809
    },
    {
      "doc": 203,
      "topic": 7,
      "similarity": 0.7768578628708692
    },
    {
      "doc": 203,
      "topic": 8,
      "similarity": 0.7706249673262641
    },
    {
      "doc": 203,
      "topic": 9,
      "similarity": 0.8119602999461304
    },
    {
      "doc": 203,
      "topic": 10,
      "similarity": 0.8033961692121558
    },
    {
      "doc": 203,
      "topic": 11,
      "similarity": 0.79016058742817
    },
    {
      "doc": 203,
      "topic": 13,
      "similarity": 0.7582150267212858
    },
    {
      "doc": 203,
      "topic": 14,
      "similarity": 0.7701654008687095
    },
    {
      "doc": 203,
      "topic": 15,
      "similarity": 0.77824357245723
    },
    {
      "doc": 203,
      "topic": 16,
      "similarity": 0.8116260605569936
    },
    {
      "doc": 203,
      "topic": 17,
      "similarity": 0.7819408356549887
    },
    {
      "doc": 203,
      "topic": 18,
      "similarity": 0.7892591323049829
    },
    {
      "doc": 203,
      "topic": 19,
      "similarity": 0.8325539427233185
    },
    {
      "doc": 203,
      "topic": 20,
      "similarity": 0.7901346490549507
    },
    {
      "doc": 203,
      "topic": 21,
      "similarity": 0.7879694355878278
    },
    {
      "doc": 203,
      "topic": 23,
      "similarity": 0.774202631833041
    },
    {
      "doc": 204,
      "topic": 5,
      "similarity": 0.7556562874717635
    },
    {
      "doc": 204,
      "topic": 7,
      "similarity": 0.8179102765639457
    },
    {
      "doc": 204,
      "topic": 8,
      "similarity": 0.7689217301286281
    },
    {
      "doc": 204,
      "topic": 9,
      "similarity": 0.8231811084417568
    },
    {
      "doc": 204,
      "topic": 16,
      "similarity": 0.7818232153894688
    },
    {
      "doc": 205,
      "topic": 2,
      "similarity": 0.7860868133253986
    },
    {
      "doc": 205,
      "topic": 3,
      "similarity": 0.8101846518854774
    },
    {
      "doc": 205,
      "topic": 4,
      "similarity": 0.764491383452536
    },
    {
      "doc": 205,
      "topic": 5,
      "similarity": 0.8003696437351263
    },
    {
      "doc": 205,
      "topic": 7,
      "similarity": 0.7836577815712991
    },
    {
      "doc": 205,
      "topic": 8,
      "similarity": 0.7948392101147541
    },
    {
      "doc": 205,
      "topic": 9,
      "similarity": 0.8127512716211445
    },
    {
      "doc": 205,
      "topic": 10,
      "similarity": 0.77397405472428
    },
    {
      "doc": 205,
      "topic": 11,
      "similarity": 0.7743068731357358
    },
    {
      "doc": 205,
      "topic": 13,
      "similarity": 0.7796918397131776
    },
    {
      "doc": 205,
      "topic": 14,
      "similarity": 0.7781857175025138
    },
    {
      "doc": 205,
      "topic": 15,
      "similarity": 0.7656529524568993
    },
    {
      "doc": 205,
      "topic": 16,
      "similarity": 0.7882994636551635
    },
    {
      "doc": 205,
      "topic": 17,
      "similarity": 0.794707871114001
    },
    {
      "doc": 205,
      "topic": 18,
      "similarity": 0.7586326126834735
    },
    {
      "doc": 205,
      "topic": 19,
      "similarity": 0.8099905873727941
    },
    {
      "doc": 205,
      "topic": 20,
      "similarity": 0.761811370335606
    },
    {
      "doc": 205,
      "topic": 21,
      "similarity": 0.8005927437036163
    },
    {
      "doc": 206,
      "topic": 3,
      "similarity": 0.7709260084032727
    },
    {
      "doc": 206,
      "topic": 5,
      "similarity": 0.788188632943617
    },
    {
      "doc": 206,
      "topic": 7,
      "similarity": 0.778454507691568
    },
    {
      "doc": 206,
      "topic": 8,
      "similarity": 0.7785491905163195
    },
    {
      "doc": 206,
      "topic": 9,
      "similarity": 0.8304925381277203
    },
    {
      "doc": 206,
      "topic": 10,
      "similarity": 0.7696867094377604
    },
    {
      "doc": 206,
      "topic": 11,
      "similarity": 0.7710776622465835
    },
    {
      "doc": 206,
      "topic": 12,
      "similarity": 0.7742717436416453
    },
    {
      "doc": 206,
      "topic": 15,
      "similarity": 0.764262812814523
    },
    {
      "doc": 206,
      "topic": 16,
      "similarity": 0.7837484553556068
    },
    {
      "doc": 206,
      "topic": 17,
      "similarity": 0.7898289338674637
    },
    {
      "doc": 206,
      "topic": 18,
      "similarity": 0.7537821818445802
    },
    {
      "doc": 206,
      "topic": 19,
      "similarity": 0.7911019611344756
    },
    {
      "doc": 206,
      "topic": 20,
      "similarity": 0.7666171994945975
    },
    {
      "doc": 206,
      "topic": 21,
      "similarity": 0.7776543534397307
    },
    {
      "doc": 206,
      "topic": 24,
      "similarity": 0.7745200190477783
    },
    {
      "doc": 207,
      "topic": 1,
      "similarity": 0.7577283904750848
    },
    {
      "doc": 207,
      "topic": 2,
      "similarity": 0.7876005543083123
    },
    {
      "doc": 207,
      "topic": 3,
      "similarity": 0.7939462858779653
    },
    {
      "doc": 207,
      "topic": 4,
      "similarity": 0.7553942761609189
    },
    {
      "doc": 207,
      "topic": 5,
      "similarity": 0.7964712114783904
    },
    {
      "doc": 207,
      "topic": 7,
      "similarity": 0.7970903267431422
    },
    {
      "doc": 207,
      "topic": 8,
      "similarity": 0.8189737591721635
    },
    {
      "doc": 207,
      "topic": 9,
      "similarity": 0.8265125848496587
    },
    {
      "doc": 207,
      "topic": 10,
      "similarity": 0.7733990985294733
    },
    {
      "doc": 207,
      "topic": 11,
      "similarity": 0.7805754907808833
    },
    {
      "doc": 207,
      "topic": 12,
      "similarity": 0.8060783887898643
    },
    {
      "doc": 207,
      "topic": 13,
      "similarity": 0.8123773350012856
    },
    {
      "doc": 207,
      "topic": 14,
      "similarity": 0.8012577424211781
    },
    {
      "doc": 207,
      "topic": 15,
      "similarity": 0.779716709513992
    },
    {
      "doc": 207,
      "topic": 16,
      "similarity": 0.8093654237694942
    },
    {
      "doc": 207,
      "topic": 17,
      "similarity": 0.8024340711418335
    },
    {
      "doc": 207,
      "topic": 18,
      "similarity": 0.7606077560185333
    },
    {
      "doc": 207,
      "topic": 19,
      "similarity": 0.8167008056404453
    },
    {
      "doc": 207,
      "topic": 20,
      "similarity": 0.7831298829706347
    },
    {
      "doc": 207,
      "topic": 21,
      "similarity": 0.7923199282797999
    },
    {
      "doc": 207,
      "topic": 22,
      "similarity": 0.7706927002673241
    },
    {
      "doc": 208,
      "topic": 1,
      "similarity": 0.7766338239075298
    },
    {
      "doc": 208,
      "topic": 2,
      "similarity": 0.8012373891591685
    },
    {
      "doc": 208,
      "topic": 3,
      "similarity": 0.8171472775249583
    },
    {
      "doc": 208,
      "topic": 4,
      "similarity": 0.769175158585987
    },
    {
      "doc": 208,
      "topic": 5,
      "similarity": 0.8194251679626997
    },
    {
      "doc": 208,
      "topic": 7,
      "similarity": 0.838270491323385
    },
    {
      "doc": 208,
      "topic": 8,
      "similarity": 0.8251455876532051
    },
    {
      "doc": 208,
      "topic": 9,
      "similarity": 0.8467114637137813
    },
    {
      "doc": 208,
      "topic": 10,
      "similarity": 0.8033616522871576
    },
    {
      "doc": 208,
      "topic": 11,
      "similarity": 0.7991886121735908
    },
    {
      "doc": 208,
      "topic": 12,
      "similarity": 0.7756318994370168
    },
    {
      "doc": 208,
      "topic": 13,
      "similarity": 0.8089688830125821
    },
    {
      "doc": 208,
      "topic": 14,
      "similarity": 0.8019010898845234
    },
    {
      "doc": 208,
      "topic": 15,
      "similarity": 0.8081131474175802
    },
    {
      "doc": 208,
      "topic": 16,
      "similarity": 0.8417050607271184
    },
    {
      "doc": 208,
      "topic": 17,
      "similarity": 0.8285237168717677
    },
    {
      "doc": 208,
      "topic": 18,
      "similarity": 0.7750548278966252
    },
    {
      "doc": 208,
      "topic": 19,
      "similarity": 0.8373164334088369
    },
    {
      "doc": 208,
      "topic": 20,
      "similarity": 0.7894235605366454
    },
    {
      "doc": 208,
      "topic": 21,
      "similarity": 0.8145039610557239
    },
    {
      "doc": 208,
      "topic": 22,
      "similarity": 0.79625656563872
    },
    {
      "doc": 209,
      "topic": 1,
      "similarity": 0.7642143457426578
    },
    {
      "doc": 209,
      "topic": 2,
      "similarity": 0.7969770956015458
    },
    {
      "doc": 209,
      "topic": 3,
      "similarity": 0.8014148263161581
    },
    {
      "doc": 209,
      "topic": 4,
      "similarity": 0.7575441475339227
    },
    {
      "doc": 209,
      "topic": 5,
      "similarity": 0.8199373689338565
    },
    {
      "doc": 209,
      "topic": 7,
      "similarity": 0.7996548476120092
    },
    {
      "doc": 209,
      "topic": 8,
      "similarity": 0.7890208598911371
    },
    {
      "doc": 209,
      "topic": 9,
      "similarity": 0.8403528360265433
    },
    {
      "doc": 209,
      "topic": 10,
      "similarity": 0.8016763377664545
    },
    {
      "doc": 209,
      "topic": 11,
      "similarity": 0.8037763633863829
    },
    {
      "doc": 209,
      "topic": 13,
      "similarity": 0.7814251067031982
    },
    {
      "doc": 209,
      "topic": 14,
      "similarity": 0.8140145676613162
    },
    {
      "doc": 209,
      "topic": 15,
      "similarity": 0.7979014147872305
    },
    {
      "doc": 209,
      "topic": 16,
      "similarity": 0.8198355792075263
    },
    {
      "doc": 209,
      "topic": 17,
      "similarity": 0.8301479857057958
    },
    {
      "doc": 209,
      "topic": 18,
      "similarity": 0.764895115057077
    },
    {
      "doc": 209,
      "topic": 19,
      "similarity": 0.807457665049942
    },
    {
      "doc": 209,
      "topic": 20,
      "similarity": 0.7692638841468866
    },
    {
      "doc": 209,
      "topic": 21,
      "similarity": 0.8072236061877495
    },
    {
      "doc": 209,
      "topic": 23,
      "similarity": 0.7586360250319351
    },
    {
      "doc": 209,
      "topic": 24,
      "similarity": 0.7707545402567558
    },
    {
      "doc": 210,
      "topic": 3,
      "similarity": 0.7855980680994717
    },
    {
      "doc": 210,
      "topic": 4,
      "similarity": 0.767336136295213
    },
    {
      "doc": 210,
      "topic": 5,
      "similarity": 0.7836923871572491
    },
    {
      "doc": 210,
      "topic": 7,
      "similarity": 0.7877712308382405
    },
    {
      "doc": 210,
      "topic": 8,
      "similarity": 0.7605632866810056
    },
    {
      "doc": 210,
      "topic": 9,
      "similarity": 0.8045421005277558
    },
    {
      "doc": 210,
      "topic": 11,
      "similarity": 0.7816412947868242
    },
    {
      "doc": 210,
      "topic": 15,
      "similarity": 0.7620842260766023
    },
    {
      "doc": 210,
      "topic": 16,
      "similarity": 0.806163218717793
    },
    {
      "doc": 210,
      "topic": 17,
      "similarity": 0.7837404871145542
    },
    {
      "doc": 210,
      "topic": 19,
      "similarity": 0.8008157443579378
    },
    {
      "doc": 210,
      "topic": 20,
      "similarity": 0.7533199363561588
    },
    {
      "doc": 210,
      "topic": 21,
      "similarity": 0.7847264160572528
    },
    {
      "doc": 210,
      "topic": 23,
      "similarity": 0.752922400156028
    },
    {
      "doc": 210,
      "topic": 24,
      "similarity": 0.7571421258403355
    },
    {
      "doc": 211,
      "topic": 2,
      "similarity": 0.7520920498481577
    },
    {
      "doc": 211,
      "topic": 3,
      "similarity": 0.7860057827190619
    },
    {
      "doc": 211,
      "topic": 5,
      "similarity": 0.7642334938591715
    },
    {
      "doc": 211,
      "topic": 7,
      "similarity": 0.8122508989778996
    },
    {
      "doc": 211,
      "topic": 8,
      "similarity": 0.7999464422939165
    },
    {
      "doc": 211,
      "topic": 9,
      "similarity": 0.8229745181360506
    },
    {
      "doc": 211,
      "topic": 10,
      "similarity": 0.7586325217630671
    },
    {
      "doc": 211,
      "topic": 11,
      "similarity": 0.7620618221439976
    },
    {
      "doc": 211,
      "topic": 12,
      "similarity": 0.8033529288306741
    },
    {
      "doc": 211,
      "topic": 13,
      "similarity": 0.7520428770114488
    },
    {
      "doc": 211,
      "topic": 15,
      "similarity": 0.7576364559702745
    },
    {
      "doc": 211,
      "topic": 16,
      "similarity": 0.7670451404508374
    },
    {
      "doc": 211,
      "topic": 17,
      "similarity": 0.7675627112863095
    },
    {
      "doc": 211,
      "topic": 19,
      "similarity": 0.767569997654359
    },
    {
      "doc": 211,
      "topic": 20,
      "similarity": 0.7665458155365161
    },
    {
      "doc": 211,
      "topic": 21,
      "similarity": 0.7765226644870427
    },
    {
      "doc": 211,
      "topic": 22,
      "similarity": 0.7621705137690383
    },
    {
      "doc": 211,
      "topic": 24,
      "similarity": 0.7531885444038445
    },
    {
      "doc": 212,
      "topic": 2,
      "similarity": 0.7945464795146725
    },
    {
      "doc": 212,
      "topic": 3,
      "similarity": 0.8049125207550889
    },
    {
      "doc": 212,
      "topic": 4,
      "similarity": 0.7578974835201752
    },
    {
      "doc": 212,
      "topic": 5,
      "similarity": 0.7909779045191081
    },
    {
      "doc": 212,
      "topic": 7,
      "similarity": 0.8436721004102024
    },
    {
      "doc": 212,
      "topic": 8,
      "similarity": 0.8141156935532876
    },
    {
      "doc": 212,
      "topic": 9,
      "similarity": 0.8295754119866986
    },
    {
      "doc": 212,
      "topic": 10,
      "similarity": 0.7783479837430076
    },
    {
      "doc": 212,
      "topic": 11,
      "similarity": 0.7885454687037696
    },
    {
      "doc": 212,
      "topic": 12,
      "similarity": 0.7658068320616235
    },
    {
      "doc": 212,
      "topic": 13,
      "similarity": 0.777107255114063
    },
    {
      "doc": 212,
      "topic": 14,
      "similarity": 0.7585163207823132
    },
    {
      "doc": 212,
      "topic": 15,
      "similarity": 0.7794353052037226
    },
    {
      "doc": 212,
      "topic": 16,
      "similarity": 0.810421483743455
    },
    {
      "doc": 212,
      "topic": 17,
      "similarity": 0.7987770303429066
    },
    {
      "doc": 212,
      "topic": 19,
      "similarity": 0.7902821239178135
    },
    {
      "doc": 212,
      "topic": 20,
      "similarity": 0.8003811094840186
    },
    {
      "doc": 212,
      "topic": 21,
      "similarity": 0.8079186755175138
    },
    {
      "doc": 212,
      "topic": 24,
      "similarity": 0.7563301575984412
    },
    {
      "doc": 213,
      "topic": 1,
      "similarity": 0.7746290789936204
    },
    {
      "doc": 213,
      "topic": 2,
      "similarity": 0.7817452633509928
    },
    {
      "doc": 213,
      "topic": 3,
      "similarity": 0.8174597990482142
    },
    {
      "doc": 213,
      "topic": 4,
      "similarity": 0.7595087600196797
    },
    {
      "doc": 213,
      "topic": 5,
      "similarity": 0.8205539868269135
    },
    {
      "doc": 213,
      "topic": 7,
      "similarity": 0.7923131329237231
    },
    {
      "doc": 213,
      "topic": 8,
      "similarity": 0.7837526334779017
    },
    {
      "doc": 213,
      "topic": 9,
      "similarity": 0.8129307325336687
    },
    {
      "doc": 213,
      "topic": 10,
      "similarity": 0.8222627620648434
    },
    {
      "doc": 213,
      "topic": 11,
      "similarity": 0.7996925826996651
    },
    {
      "doc": 213,
      "topic": 12,
      "similarity": 0.7684007656657036
    },
    {
      "doc": 213,
      "topic": 13,
      "similarity": 0.7886838934260367
    },
    {
      "doc": 213,
      "topic": 14,
      "similarity": 0.7819765197995184
    },
    {
      "doc": 213,
      "topic": 15,
      "similarity": 0.8037207519793729
    },
    {
      "doc": 213,
      "topic": 16,
      "similarity": 0.8305281877018728
    },
    {
      "doc": 213,
      "topic": 17,
      "similarity": 0.8143386079542302
    },
    {
      "doc": 213,
      "topic": 18,
      "similarity": 0.7644993706359899
    },
    {
      "doc": 213,
      "topic": 19,
      "similarity": 0.8205811670367401
    },
    {
      "doc": 213,
      "topic": 20,
      "similarity": 0.7651277565445243
    },
    {
      "doc": 213,
      "topic": 21,
      "similarity": 0.8083129986128031
    },
    {
      "doc": 213,
      "topic": 23,
      "similarity": 0.7500999825173186
    },
    {
      "doc": 213,
      "topic": 24,
      "similarity": 0.75114501378881
    },
    {
      "doc": 214,
      "topic": 1,
      "similarity": 0.7521767493070785
    },
    {
      "doc": 214,
      "topic": 2,
      "similarity": 0.7764026203695855
    },
    {
      "doc": 214,
      "topic": 3,
      "similarity": 0.7988954523230042
    },
    {
      "doc": 214,
      "topic": 4,
      "similarity": 0.7604764017199559
    },
    {
      "doc": 214,
      "topic": 5,
      "similarity": 0.8078343075397598
    },
    {
      "doc": 214,
      "topic": 7,
      "similarity": 0.769600611146147
    },
    {
      "doc": 214,
      "topic": 8,
      "similarity": 0.7702593473289685
    },
    {
      "doc": 214,
      "topic": 9,
      "similarity": 0.801069200282378
    },
    {
      "doc": 214,
      "topic": 10,
      "similarity": 0.7673653104530489
    },
    {
      "doc": 214,
      "topic": 11,
      "similarity": 0.7927955600251831
    },
    {
      "doc": 214,
      "topic": 12,
      "similarity": 0.7520464049726434
    },
    {
      "doc": 214,
      "topic": 13,
      "similarity": 0.7789689894346048
    },
    {
      "doc": 214,
      "topic": 14,
      "similarity": 0.7682991583917869
    },
    {
      "doc": 214,
      "topic": 15,
      "similarity": 0.8072356211605346
    },
    {
      "doc": 214,
      "topic": 16,
      "similarity": 0.8362321855529048
    },
    {
      "doc": 214,
      "topic": 17,
      "similarity": 0.8229442544308655
    },
    {
      "doc": 214,
      "topic": 18,
      "similarity": 0.7825904488985382
    },
    {
      "doc": 214,
      "topic": 19,
      "similarity": 0.848869152366091
    },
    {
      "doc": 214,
      "topic": 20,
      "similarity": 0.7985653936073458
    },
    {
      "doc": 214,
      "topic": 21,
      "similarity": 0.8016054906229966
    },
    {
      "doc": 214,
      "topic": 22,
      "similarity": 0.7542890199507984
    },
    {
      "doc": 215,
      "topic": 0,
      "similarity": 0.7510686911532775
    },
    {
      "doc": 215,
      "topic": 1,
      "similarity": 0.7727474719545775
    },
    {
      "doc": 215,
      "topic": 3,
      "similarity": 0.7710397110839305
    },
    {
      "doc": 215,
      "topic": 5,
      "similarity": 0.753999750706132
    },
    {
      "doc": 215,
      "topic": 8,
      "similarity": 0.759353799841569
    },
    {
      "doc": 215,
      "topic": 9,
      "similarity": 0.7966357051789509
    },
    {
      "doc": 215,
      "topic": 11,
      "similarity": 0.7523388503612612
    },
    {
      "doc": 215,
      "topic": 12,
      "similarity": 0.7516310301749511
    },
    {
      "doc": 215,
      "topic": 15,
      "similarity": 0.7648373478131594
    },
    {
      "doc": 215,
      "topic": 16,
      "similarity": 0.7708983472065716
    },
    {
      "doc": 215,
      "topic": 19,
      "similarity": 0.7811135358164145
    },
    {
      "doc": 215,
      "topic": 20,
      "similarity": 0.7517928556482706
    },
    {
      "doc": 215,
      "topic": 21,
      "similarity": 0.7708546916408784
    },
    {
      "doc": 215,
      "topic": 24,
      "similarity": 0.7585591876207735
    },
    {
      "doc": 216,
      "topic": 1,
      "similarity": 0.7803184278146775
    },
    {
      "doc": 216,
      "topic": 2,
      "similarity": 0.8135701680311562
    },
    {
      "doc": 216,
      "topic": 3,
      "similarity": 0.8116968843202934
    },
    {
      "doc": 216,
      "topic": 4,
      "similarity": 0.765681366639822
    },
    {
      "doc": 216,
      "topic": 5,
      "similarity": 0.8231442213021786
    },
    {
      "doc": 216,
      "topic": 7,
      "similarity": 0.7868586068415871
    },
    {
      "doc": 216,
      "topic": 8,
      "similarity": 0.7932803839256845
    },
    {
      "doc": 216,
      "topic": 9,
      "similarity": 0.844160710960421
    },
    {
      "doc": 216,
      "topic": 10,
      "similarity": 0.81120739779395
    },
    {
      "doc": 216,
      "topic": 11,
      "similarity": 0.8021164425283748
    },
    {
      "doc": 216,
      "topic": 13,
      "similarity": 0.8056172586228534
    },
    {
      "doc": 216,
      "topic": 14,
      "similarity": 0.8091729410380958
    },
    {
      "doc": 216,
      "topic": 15,
      "similarity": 0.8119878787550499
    },
    {
      "doc": 216,
      "topic": 16,
      "similarity": 0.8416003905943
    },
    {
      "doc": 216,
      "topic": 17,
      "similarity": 0.8222044507564273
    },
    {
      "doc": 216,
      "topic": 18,
      "similarity": 0.7639848696029402
    },
    {
      "doc": 216,
      "topic": 19,
      "similarity": 0.816380598873831
    },
    {
      "doc": 216,
      "topic": 20,
      "similarity": 0.7968771177361916
    },
    {
      "doc": 216,
      "topic": 21,
      "similarity": 0.8293425893074631
    },
    {
      "doc": 216,
      "topic": 23,
      "similarity": 0.7931586157425097
    },
    {
      "doc": 216,
      "topic": 24,
      "similarity": 0.7546640040609203
    },
    {
      "doc": 217,
      "topic": 2,
      "similarity": 0.7736782166610816
    },
    {
      "doc": 217,
      "topic": 3,
      "similarity": 0.7854958757445192
    },
    {
      "doc": 217,
      "topic": 5,
      "similarity": 0.7761655425022876
    },
    {
      "doc": 217,
      "topic": 7,
      "similarity": 0.773649408303064
    },
    {
      "doc": 217,
      "topic": 8,
      "similarity": 0.7829532350257262
    },
    {
      "doc": 217,
      "topic": 9,
      "similarity": 0.8493324775646282
    },
    {
      "doc": 217,
      "topic": 10,
      "similarity": 0.7712629550315979
    },
    {
      "doc": 217,
      "topic": 11,
      "similarity": 0.7785366910611219
    },
    {
      "doc": 217,
      "topic": 12,
      "similarity": 0.7880330979214786
    },
    {
      "doc": 217,
      "topic": 13,
      "similarity": 0.7685664833714606
    },
    {
      "doc": 217,
      "topic": 14,
      "similarity": 0.773105543685371
    },
    {
      "doc": 217,
      "topic": 15,
      "similarity": 0.7969496701502334
    },
    {
      "doc": 217,
      "topic": 16,
      "similarity": 0.7762393747330526
    },
    {
      "doc": 217,
      "topic": 17,
      "similarity": 0.8022339787208244
    },
    {
      "doc": 217,
      "topic": 18,
      "similarity": 0.7522616023589451
    },
    {
      "doc": 217,
      "topic": 19,
      "similarity": 0.7931563204296607
    },
    {
      "doc": 217,
      "topic": 20,
      "similarity": 0.7611193061939435
    },
    {
      "doc": 217,
      "topic": 21,
      "similarity": 0.7921343326357126
    },
    {
      "doc": 217,
      "topic": 22,
      "similarity": 0.7628268677524512
    },
    {
      "doc": 218,
      "topic": 0,
      "similarity": 0.7570740467798521
    },
    {
      "doc": 218,
      "topic": 1,
      "similarity": 0.815180129580425
    },
    {
      "doc": 218,
      "topic": 2,
      "similarity": 0.7962257067305967
    },
    {
      "doc": 218,
      "topic": 3,
      "similarity": 0.8302666232809834
    },
    {
      "doc": 218,
      "topic": 4,
      "similarity": 0.7699514501971434
    },
    {
      "doc": 218,
      "topic": 5,
      "similarity": 0.8096708449100587
    },
    {
      "doc": 218,
      "topic": 6,
      "similarity": 0.7605447132672887
    },
    {
      "doc": 218,
      "topic": 7,
      "similarity": 0.8033056157875096
    },
    {
      "doc": 218,
      "topic": 8,
      "similarity": 0.7872816152664077
    },
    {
      "doc": 218,
      "topic": 9,
      "similarity": 0.8694027356609124
    },
    {
      "doc": 218,
      "topic": 10,
      "similarity": 0.8106400059494172
    },
    {
      "doc": 218,
      "topic": 11,
      "similarity": 0.8292219692722972
    },
    {
      "doc": 218,
      "topic": 12,
      "similarity": 0.7594283298767408
    },
    {
      "doc": 218,
      "topic": 13,
      "similarity": 0.7984998607554121
    },
    {
      "doc": 218,
      "topic": 14,
      "similarity": 0.8092332405658053
    },
    {
      "doc": 218,
      "topic": 15,
      "similarity": 0.811563659647059
    },
    {
      "doc": 218,
      "topic": 16,
      "similarity": 0.8276012899301165
    },
    {
      "doc": 218,
      "topic": 17,
      "similarity": 0.8128660813209057
    },
    {
      "doc": 218,
      "topic": 18,
      "similarity": 0.7636550293119897
    },
    {
      "doc": 218,
      "topic": 19,
      "similarity": 0.8379309041375117
    },
    {
      "doc": 218,
      "topic": 20,
      "similarity": 0.7868128149684495
    },
    {
      "doc": 218,
      "topic": 21,
      "similarity": 0.8240877259996066
    },
    {
      "doc": 218,
      "topic": 23,
      "similarity": 0.7919688072108023
    },
    {
      "doc": 218,
      "topic": 24,
      "similarity": 0.7538856133351364
    },
    {
      "doc": 219,
      "topic": 3,
      "similarity": 0.7891877857142539
    },
    {
      "doc": 219,
      "topic": 5,
      "similarity": 0.7634052126835009
    },
    {
      "doc": 219,
      "topic": 7,
      "similarity": 0.7644243841255316
    },
    {
      "doc": 219,
      "topic": 8,
      "similarity": 0.7640625535560522
    },
    {
      "doc": 219,
      "topic": 9,
      "similarity": 0.7961160247821045
    },
    {
      "doc": 219,
      "topic": 10,
      "similarity": 0.7521545506577646
    },
    {
      "doc": 219,
      "topic": 11,
      "similarity": 0.7642282436561509
    },
    {
      "doc": 219,
      "topic": 16,
      "similarity": 0.7862529405956632
    },
    {
      "doc": 219,
      "topic": 17,
      "similarity": 0.7713341991557666
    },
    {
      "doc": 219,
      "topic": 19,
      "similarity": 0.7833288495396905
    },
    {
      "doc": 219,
      "topic": 20,
      "similarity": 0.7668443714682293
    },
    {
      "doc": 219,
      "topic": 21,
      "similarity": 0.7737064639027248
    },
    {
      "doc": 220,
      "topic": 3,
      "similarity": 0.762375506309336
    },
    {
      "doc": 220,
      "topic": 5,
      "similarity": 0.781019928221412
    },
    {
      "doc": 220,
      "topic": 8,
      "similarity": 0.7561634900432554
    },
    {
      "doc": 220,
      "topic": 9,
      "similarity": 0.7998581011343388
    },
    {
      "doc": 220,
      "topic": 10,
      "similarity": 0.7657208641453762
    },
    {
      "doc": 220,
      "topic": 11,
      "similarity": 0.7786009766263903
    },
    {
      "doc": 220,
      "topic": 12,
      "similarity": 0.7603677272483528
    },
    {
      "doc": 220,
      "topic": 13,
      "similarity": 0.7660060989089298
    },
    {
      "doc": 220,
      "topic": 14,
      "similarity": 0.7603734109568219
    },
    {
      "doc": 220,
      "topic": 15,
      "similarity": 0.80426150866857
    },
    {
      "doc": 220,
      "topic": 16,
      "similarity": 0.7908403021878007
    },
    {
      "doc": 220,
      "topic": 17,
      "similarity": 0.7901302357813577
    },
    {
      "doc": 220,
      "topic": 18,
      "similarity": 0.7516619318926767
    },
    {
      "doc": 220,
      "topic": 19,
      "similarity": 0.7900108636384301
    },
    {
      "doc": 220,
      "topic": 20,
      "similarity": 0.773534680197869
    },
    {
      "doc": 220,
      "topic": 21,
      "similarity": 0.776076142482343
    },
    {
      "doc": 221,
      "topic": 1,
      "similarity": 0.7638516747521722
    },
    {
      "doc": 221,
      "topic": 2,
      "similarity": 0.7652070815691985
    },
    {
      "doc": 221,
      "topic": 3,
      "similarity": 0.7989595853192429
    },
    {
      "doc": 221,
      "topic": 4,
      "similarity": 0.7526368446653463
    },
    {
      "doc": 221,
      "topic": 5,
      "similarity": 0.8016749361411998
    },
    {
      "doc": 221,
      "topic": 7,
      "similarity": 0.7960565272233451
    },
    {
      "doc": 221,
      "topic": 8,
      "similarity": 0.7941426217551045
    },
    {
      "doc": 221,
      "topic": 9,
      "similarity": 0.8568357219626711
    },
    {
      "doc": 221,
      "topic": 10,
      "similarity": 0.7913066171353108
    },
    {
      "doc": 221,
      "topic": 11,
      "similarity": 0.8024152787105947
    },
    {
      "doc": 221,
      "topic": 12,
      "similarity": 0.7545234895989797
    },
    {
      "doc": 221,
      "topic": 13,
      "similarity": 0.79202042620384
    },
    {
      "doc": 221,
      "topic": 14,
      "similarity": 0.7826054981779969
    },
    {
      "doc": 221,
      "topic": 15,
      "similarity": 0.8046352861615828
    },
    {
      "doc": 221,
      "topic": 16,
      "similarity": 0.8012527089466429
    },
    {
      "doc": 221,
      "topic": 17,
      "similarity": 0.8043470472899913
    },
    {
      "doc": 221,
      "topic": 18,
      "similarity": 0.7548250497335787
    },
    {
      "doc": 221,
      "topic": 19,
      "similarity": 0.8027484818112428
    },
    {
      "doc": 221,
      "topic": 20,
      "similarity": 0.7744899443169926
    },
    {
      "doc": 221,
      "topic": 21,
      "similarity": 0.831302029224295
    },
    {
      "doc": 221,
      "topic": 24,
      "similarity": 0.7590451452143432
    },
    {
      "doc": 222,
      "topic": 2,
      "similarity": 0.75774433269065
    },
    {
      "doc": 222,
      "topic": 3,
      "similarity": 0.7827430291691977
    },
    {
      "doc": 222,
      "topic": 5,
      "similarity": 0.7911254134745165
    },
    {
      "doc": 222,
      "topic": 7,
      "similarity": 0.7500205998195567
    },
    {
      "doc": 222,
      "topic": 8,
      "similarity": 0.7618765340068985
    },
    {
      "doc": 222,
      "topic": 9,
      "similarity": 0.8040256801171972
    },
    {
      "doc": 222,
      "topic": 10,
      "similarity": 0.7715534147002461
    },
    {
      "doc": 222,
      "topic": 11,
      "similarity": 0.7655172285552603
    },
    {
      "doc": 222,
      "topic": 14,
      "similarity": 0.7622592660864194
    },
    {
      "doc": 222,
      "topic": 15,
      "similarity": 0.773375496102908
    },
    {
      "doc": 222,
      "topic": 16,
      "similarity": 0.796727098172127
    },
    {
      "doc": 222,
      "topic": 17,
      "similarity": 0.8158166953707322
    },
    {
      "doc": 222,
      "topic": 18,
      "similarity": 0.7636826411398688
    },
    {
      "doc": 222,
      "topic": 19,
      "similarity": 0.8171407894975422
    },
    {
      "doc": 222,
      "topic": 20,
      "similarity": 0.7887849229238475
    },
    {
      "doc": 222,
      "topic": 21,
      "similarity": 0.818268709273801
    },
    {
      "doc": 222,
      "topic": 23,
      "similarity": 0.7516954027167296
    },
    {
      "doc": 222,
      "topic": 24,
      "similarity": 0.765166406529002
    },
    {
      "doc": 223,
      "topic": 1,
      "similarity": 0.7501918312279253
    },
    {
      "doc": 223,
      "topic": 2,
      "similarity": 0.7885301058371851
    },
    {
      "doc": 223,
      "topic": 3,
      "similarity": 0.7960038609988441
    },
    {
      "doc": 223,
      "topic": 5,
      "similarity": 0.8021300919454029
    },
    {
      "doc": 223,
      "topic": 7,
      "similarity": 0.7969863193966401
    },
    {
      "doc": 223,
      "topic": 8,
      "similarity": 0.768926524769689
    },
    {
      "doc": 223,
      "topic": 9,
      "similarity": 0.8742536085847762
    },
    {
      "doc": 223,
      "topic": 10,
      "similarity": 0.7800688672477906
    },
    {
      "doc": 223,
      "topic": 11,
      "similarity": 0.7971842805004473
    },
    {
      "doc": 223,
      "topic": 13,
      "similarity": 0.7836628714750457
    },
    {
      "doc": 223,
      "topic": 14,
      "similarity": 0.7687493754760684
    },
    {
      "doc": 223,
      "topic": 15,
      "similarity": 0.7934083953037613
    },
    {
      "doc": 223,
      "topic": 16,
      "similarity": 0.8293399776365704
    },
    {
      "doc": 223,
      "topic": 17,
      "similarity": 0.8142344121938156
    },
    {
      "doc": 223,
      "topic": 18,
      "similarity": 0.7782263091828167
    },
    {
      "doc": 223,
      "topic": 19,
      "similarity": 0.8076087931354411
    },
    {
      "doc": 223,
      "topic": 20,
      "similarity": 0.8563459425813615
    },
    {
      "doc": 223,
      "topic": 21,
      "similarity": 0.8186752377669685
    },
    {
      "doc": 223,
      "topic": 23,
      "similarity": 0.7572170404548735
    },
    {
      "doc": 223,
      "topic": 24,
      "similarity": 0.7853326360207964
    },
    {
      "doc": 224,
      "topic": 3,
      "similarity": 0.7668778955705227
    },
    {
      "doc": 224,
      "topic": 5,
      "similarity": 0.7746077380775832
    },
    {
      "doc": 224,
      "topic": 7,
      "similarity": 0.758043831433987
    },
    {
      "doc": 224,
      "topic": 8,
      "similarity": 0.761816963123636
    },
    {
      "doc": 224,
      "topic": 9,
      "similarity": 0.8160460524685349
    },
    {
      "doc": 224,
      "topic": 10,
      "similarity": 0.7716112471333186
    },
    {
      "doc": 224,
      "topic": 11,
      "similarity": 0.7776471875518812
    },
    {
      "doc": 224,
      "topic": 12,
      "similarity": 0.7770718171518148
    },
    {
      "doc": 224,
      "topic": 15,
      "similarity": 0.8004594548695056
    },
    {
      "doc": 224,
      "topic": 16,
      "similarity": 0.7854287246812633
    },
    {
      "doc": 224,
      "topic": 17,
      "similarity": 0.7923809362738343
    },
    {
      "doc": 224,
      "topic": 18,
      "similarity": 0.7673200489799564
    },
    {
      "doc": 224,
      "topic": 19,
      "similarity": 0.8013423651411951
    },
    {
      "doc": 224,
      "topic": 20,
      "similarity": 0.7828455510234024
    },
    {
      "doc": 224,
      "topic": 21,
      "similarity": 0.7803161067146998
    },
    {
      "doc": 224,
      "topic": 23,
      "similarity": 0.751555457643212
    },
    {
      "doc": 224,
      "topic": 24,
      "similarity": 0.7562625173851123
    },
    {
      "doc": 225,
      "topic": 2,
      "similarity": 0.7535149853463833
    },
    {
      "doc": 225,
      "topic": 3,
      "similarity": 0.7613044014090141
    },
    {
      "doc": 225,
      "topic": 7,
      "similarity": 0.7730311446307282
    },
    {
      "doc": 225,
      "topic": 9,
      "similarity": 0.8010909760086099
    },
    {
      "doc": 225,
      "topic": 10,
      "similarity": 0.7626948180320826
    },
    {
      "doc": 225,
      "topic": 11,
      "similarity": 0.750191231175596
    },
    {
      "doc": 225,
      "topic": 12,
      "similarity": 0.7631941906625407
    },
    {
      "doc": 225,
      "topic": 15,
      "similarity": 0.7748006254026818
    },
    {
      "doc": 225,
      "topic": 16,
      "similarity": 0.7826178898083027
    },
    {
      "doc": 225,
      "topic": 17,
      "similarity": 0.7831826216763322
    },
    {
      "doc": 225,
      "topic": 18,
      "similarity": 0.7524661305890454
    },
    {
      "doc": 225,
      "topic": 19,
      "similarity": 0.7841607488193423
    },
    {
      "doc": 225,
      "topic": 20,
      "similarity": 0.7666279864239998
    },
    {
      "doc": 225,
      "topic": 21,
      "similarity": 0.7769432924471005
    },
    {
      "doc": 226,
      "topic": 3,
      "similarity": 0.7888518504072806
    },
    {
      "doc": 226,
      "topic": 5,
      "similarity": 0.7801964247259694
    },
    {
      "doc": 226,
      "topic": 7,
      "similarity": 0.7635304326951586
    },
    {
      "doc": 226,
      "topic": 8,
      "similarity": 0.7769237133829087
    },
    {
      "doc": 226,
      "topic": 9,
      "similarity": 0.8122927724854493
    },
    {
      "doc": 226,
      "topic": 10,
      "similarity": 0.759061736915133
    },
    {
      "doc": 226,
      "topic": 11,
      "similarity": 0.7815864519338083
    },
    {
      "doc": 226,
      "topic": 12,
      "similarity": 0.7621645566463564
    },
    {
      "doc": 226,
      "topic": 15,
      "similarity": 0.7948731615172914
    },
    {
      "doc": 226,
      "topic": 16,
      "similarity": 0.799722352255114
    },
    {
      "doc": 226,
      "topic": 17,
      "similarity": 0.7962763057192379
    },
    {
      "doc": 226,
      "topic": 18,
      "similarity": 0.7558651605842488
    },
    {
      "doc": 226,
      "topic": 19,
      "similarity": 0.7914270254751404
    },
    {
      "doc": 226,
      "topic": 20,
      "similarity": 0.7673556890740915
    },
    {
      "doc": 226,
      "topic": 21,
      "similarity": 0.7821550062215492
    },
    {
      "doc": 226,
      "topic": 24,
      "similarity": 0.7685964074488588
    },
    {
      "doc": 227,
      "topic": 3,
      "similarity": 0.7743124830360999
    },
    {
      "doc": 227,
      "topic": 5,
      "similarity": 0.7812432815034648
    },
    {
      "doc": 227,
      "topic": 7,
      "similarity": 0.7733943636225254
    },
    {
      "doc": 227,
      "topic": 8,
      "similarity": 0.7609714899564556
    },
    {
      "doc": 227,
      "topic": 9,
      "similarity": 0.8349033570173159
    },
    {
      "doc": 227,
      "topic": 11,
      "similarity": 0.7699235881676528
    },
    {
      "doc": 227,
      "topic": 15,
      "similarity": 0.7567943496608263
    },
    {
      "doc": 227,
      "topic": 16,
      "similarity": 0.7911702334175464
    },
    {
      "doc": 227,
      "topic": 17,
      "similarity": 0.78225319693557
    },
    {
      "doc": 227,
      "topic": 19,
      "similarity": 0.7756679096097396
    },
    {
      "doc": 227,
      "topic": 20,
      "similarity": 0.7713049420541684
    },
    {
      "doc": 227,
      "topic": 21,
      "similarity": 0.8006396671540326
    },
    {
      "doc": 227,
      "topic": 24,
      "similarity": 0.7874951838427892
    },
    {
      "doc": 228,
      "topic": 1,
      "similarity": 0.7502532927507563
    },
    {
      "doc": 228,
      "topic": 3,
      "similarity": 0.7860622167968929
    },
    {
      "doc": 228,
      "topic": 5,
      "similarity": 0.8005047802763114
    },
    {
      "doc": 228,
      "topic": 7,
      "similarity": 0.7580262546378954
    },
    {
      "doc": 228,
      "topic": 9,
      "similarity": 0.8040753632902792
    },
    {
      "doc": 228,
      "topic": 10,
      "similarity": 0.7514229983490908
    },
    {
      "doc": 228,
      "topic": 11,
      "similarity": 0.7570719249283001
    },
    {
      "doc": 228,
      "topic": 15,
      "similarity": 0.7511426292780424
    },
    {
      "doc": 228,
      "topic": 16,
      "similarity": 0.7737812302997764
    },
    {
      "doc": 228,
      "topic": 17,
      "similarity": 0.7637252332023555
    },
    {
      "doc": 228,
      "topic": 19,
      "similarity": 0.764301925196656
    },
    {
      "doc": 228,
      "topic": 20,
      "similarity": 0.7655662640971457
    },
    {
      "doc": 228,
      "topic": 21,
      "similarity": 0.786251126241478
    },
    {
      "doc": 228,
      "topic": 23,
      "similarity": 0.7868647857044949
    },
    {
      "doc": 228,
      "topic": 24,
      "similarity": 0.7649674490433951
    },
    {
      "doc": 229,
      "topic": 1,
      "similarity": 0.7564792427525475
    },
    {
      "doc": 229,
      "topic": 2,
      "similarity": 0.7572186414576586
    },
    {
      "doc": 229,
      "topic": 3,
      "similarity": 0.7792826386309528
    },
    {
      "doc": 229,
      "topic": 4,
      "similarity": 0.7608378932730129
    },
    {
      "doc": 229,
      "topic": 5,
      "similarity": 0.782014088849198
    },
    {
      "doc": 229,
      "topic": 7,
      "similarity": 0.7669236575512406
    },
    {
      "doc": 229,
      "topic": 8,
      "similarity": 0.7591532177579927
    },
    {
      "doc": 229,
      "topic": 9,
      "similarity": 0.7923122826705975
    },
    {
      "doc": 229,
      "topic": 10,
      "similarity": 0.7812271744011505
    },
    {
      "doc": 229,
      "topic": 11,
      "similarity": 0.7876386899328122
    },
    {
      "doc": 229,
      "topic": 13,
      "similarity": 0.7680962517967649
    },
    {
      "doc": 229,
      "topic": 14,
      "similarity": 0.7677238717815756
    },
    {
      "doc": 229,
      "topic": 15,
      "similarity": 0.7742926836744531
    },
    {
      "doc": 229,
      "topic": 16,
      "similarity": 0.8096341605889305
    },
    {
      "doc": 229,
      "topic": 17,
      "similarity": 0.8080519188389875
    },
    {
      "doc": 229,
      "topic": 19,
      "similarity": 0.8021426093826362
    },
    {
      "doc": 229,
      "topic": 21,
      "similarity": 0.7799442437654212
    },
    {
      "doc": 230,
      "topic": 1,
      "similarity": 0.7561076834205586
    },
    {
      "doc": 230,
      "topic": 2,
      "similarity": 0.7801975161255856
    },
    {
      "doc": 230,
      "topic": 3,
      "similarity": 0.8046154925264728
    },
    {
      "doc": 230,
      "topic": 4,
      "similarity": 0.7570995341087108
    },
    {
      "doc": 230,
      "topic": 5,
      "similarity": 0.8061405344368139
    },
    {
      "doc": 230,
      "topic": 7,
      "similarity": 0.7998322973481559
    },
    {
      "doc": 230,
      "topic": 8,
      "similarity": 0.7968586831370407
    },
    {
      "doc": 230,
      "topic": 9,
      "similarity": 0.8750987860048335
    },
    {
      "doc": 230,
      "topic": 10,
      "similarity": 0.7935564159419485
    },
    {
      "doc": 230,
      "topic": 11,
      "similarity": 0.7894848050612163
    },
    {
      "doc": 230,
      "topic": 12,
      "similarity": 0.7595797360004388
    },
    {
      "doc": 230,
      "topic": 13,
      "similarity": 0.7893437554613842
    },
    {
      "doc": 230,
      "topic": 14,
      "similarity": 0.7910127545602068
    },
    {
      "doc": 230,
      "topic": 15,
      "similarity": 0.7863635244631451
    },
    {
      "doc": 230,
      "topic": 16,
      "similarity": 0.8184897790171936
    },
    {
      "doc": 230,
      "topic": 17,
      "similarity": 0.7954301227989878
    },
    {
      "doc": 230,
      "topic": 18,
      "similarity": 0.7594544746889518
    },
    {
      "doc": 230,
      "topic": 19,
      "similarity": 0.8150242437911238
    },
    {
      "doc": 230,
      "topic": 20,
      "similarity": 0.77335378527928
    },
    {
      "doc": 230,
      "topic": 21,
      "similarity": 0.8209646017267472
    },
    {
      "doc": 230,
      "topic": 22,
      "similarity": 0.758823821412496
    },
    {
      "doc": 230,
      "topic": 23,
      "similarity": 0.7622613504533446
    },
    {
      "doc": 230,
      "topic": 24,
      "similarity": 0.799896397764325
    },
    {
      "doc": 231,
      "topic": 1,
      "similarity": 0.7537609962550801
    },
    {
      "doc": 231,
      "topic": 2,
      "similarity": 0.7727707961154822
    },
    {
      "doc": 231,
      "topic": 3,
      "similarity": 0.785798672466756
    },
    {
      "doc": 231,
      "topic": 4,
      "similarity": 0.7562212107422293
    },
    {
      "doc": 231,
      "topic": 5,
      "similarity": 0.7864449213801634
    },
    {
      "doc": 231,
      "topic": 7,
      "similarity": 0.7829556573299195
    },
    {
      "doc": 231,
      "topic": 8,
      "similarity": 0.7651954982244462
    },
    {
      "doc": 231,
      "topic": 9,
      "similarity": 0.8095799577686176
    },
    {
      "doc": 231,
      "topic": 10,
      "similarity": 0.7650483987644316
    },
    {
      "doc": 231,
      "topic": 11,
      "similarity": 0.7815521160597341
    },
    {
      "doc": 231,
      "topic": 13,
      "similarity": 0.7556851785954942
    },
    {
      "doc": 231,
      "topic": 15,
      "similarity": 0.768746940050903
    },
    {
      "doc": 231,
      "topic": 16,
      "similarity": 0.8342173111566847
    },
    {
      "doc": 231,
      "topic": 17,
      "similarity": 0.7958485429831114
    },
    {
      "doc": 231,
      "topic": 19,
      "similarity": 0.7930302570223647
    },
    {
      "doc": 231,
      "topic": 20,
      "similarity": 0.7611034866176577
    },
    {
      "doc": 231,
      "topic": 21,
      "similarity": 0.793571774621784
    },
    {
      "doc": 231,
      "topic": 24,
      "similarity": 0.7704275559762184
    },
    {
      "doc": 232,
      "topic": 3,
      "similarity": 0.7662260716476317
    },
    {
      "doc": 232,
      "topic": 4,
      "similarity": 0.8346119243385112
    },
    {
      "doc": 232,
      "topic": 5,
      "similarity": 0.7569160098671855
    },
    {
      "doc": 232,
      "topic": 7,
      "similarity": 0.7572002909474461
    },
    {
      "doc": 232,
      "topic": 9,
      "similarity": 0.766446469201787
    },
    {
      "doc": 232,
      "topic": 16,
      "similarity": 0.7613994361755202
    },
    {
      "doc": 232,
      "topic": 17,
      "similarity": 0.7512956079070043
    },
    {
      "doc": 232,
      "topic": 19,
      "similarity": 0.7627780242418453
    },
    {
      "doc": 233,
      "topic": 1,
      "similarity": 0.7800607028152506
    },
    {
      "doc": 233,
      "topic": 2,
      "similarity": 0.8009664045506386
    },
    {
      "doc": 233,
      "topic": 3,
      "similarity": 0.8245149738056305
    },
    {
      "doc": 233,
      "topic": 4,
      "similarity": 0.7718277484226296
    },
    {
      "doc": 233,
      "topic": 5,
      "similarity": 0.8102103428994837
    },
    {
      "doc": 233,
      "topic": 7,
      "similarity": 0.7946686642615886
    },
    {
      "doc": 233,
      "topic": 8,
      "similarity": 0.8037388734923795
    },
    {
      "doc": 233,
      "topic": 9,
      "similarity": 0.8319243027361314
    },
    {
      "doc": 233,
      "topic": 10,
      "similarity": 0.803227353871806
    },
    {
      "doc": 233,
      "topic": 11,
      "similarity": 0.7992518499536665
    },
    {
      "doc": 233,
      "topic": 13,
      "similarity": 0.7952277967650002
    },
    {
      "doc": 233,
      "topic": 14,
      "similarity": 0.8139561850523087
    },
    {
      "doc": 233,
      "topic": 15,
      "similarity": 0.7895042592601105
    },
    {
      "doc": 233,
      "topic": 16,
      "similarity": 0.8187923153493415
    },
    {
      "doc": 233,
      "topic": 17,
      "similarity": 0.8040552246732043
    },
    {
      "doc": 233,
      "topic": 18,
      "similarity": 0.7990781712000721
    },
    {
      "doc": 233,
      "topic": 19,
      "similarity": 0.8534063377922638
    },
    {
      "doc": 233,
      "topic": 20,
      "similarity": 0.7643246406738798
    },
    {
      "doc": 233,
      "topic": 21,
      "similarity": 0.7947580693829757
    },
    {
      "doc": 233,
      "topic": 23,
      "similarity": 0.7653997038657732
    },
    {
      "doc": 234,
      "topic": 3,
      "similarity": 0.771002074920979
    },
    {
      "doc": 234,
      "topic": 5,
      "similarity": 0.7674514593556709
    },
    {
      "doc": 234,
      "topic": 8,
      "similarity": 0.7510784104746772
    },
    {
      "doc": 234,
      "topic": 9,
      "similarity": 0.796264044038367
    },
    {
      "doc": 234,
      "topic": 10,
      "similarity": 0.7781120609815435
    },
    {
      "doc": 234,
      "topic": 11,
      "similarity": 0.7623749053142896
    },
    {
      "doc": 234,
      "topic": 15,
      "similarity": 0.7640663622234959
    },
    {
      "doc": 234,
      "topic": 16,
      "similarity": 0.7689562466772586
    },
    {
      "doc": 234,
      "topic": 17,
      "similarity": 0.8100148911920546
    },
    {
      "doc": 234,
      "topic": 18,
      "similarity": 0.7581856167151418
    },
    {
      "doc": 234,
      "topic": 19,
      "similarity": 0.7823763606000906
    },
    {
      "doc": 234,
      "topic": 20,
      "similarity": 0.7775177265243923
    },
    {
      "doc": 234,
      "topic": 21,
      "similarity": 0.7902269754140034
    },
    {
      "doc": 234,
      "topic": 24,
      "similarity": 0.7701901674638854
    },
    {
      "doc": 235,
      "topic": 1,
      "similarity": 0.7508877692612905
    },
    {
      "doc": 235,
      "topic": 2,
      "similarity": 0.7821584134195274
    },
    {
      "doc": 235,
      "topic": 3,
      "similarity": 0.8048437217921596
    },
    {
      "doc": 235,
      "topic": 4,
      "similarity": 0.754299636258548
    },
    {
      "doc": 235,
      "topic": 5,
      "similarity": 0.8031550812049176
    },
    {
      "doc": 235,
      "topic": 7,
      "similarity": 0.8109636544637823
    },
    {
      "doc": 235,
      "topic": 8,
      "similarity": 0.7923164924730459
    },
    {
      "doc": 235,
      "topic": 9,
      "similarity": 0.8450703880031362
    },
    {
      "doc": 235,
      "topic": 10,
      "similarity": 0.8136416333849859
    },
    {
      "doc": 235,
      "topic": 11,
      "similarity": 0.790328868112203
    },
    {
      "doc": 235,
      "topic": 13,
      "similarity": 0.7734179903496464
    },
    {
      "doc": 235,
      "topic": 14,
      "similarity": 0.7825469869587827
    },
    {
      "doc": 235,
      "topic": 15,
      "similarity": 0.7842133157755027
    },
    {
      "doc": 235,
      "topic": 16,
      "similarity": 0.7917917874497247
    },
    {
      "doc": 235,
      "topic": 17,
      "similarity": 0.7926064938780937
    },
    {
      "doc": 235,
      "topic": 18,
      "similarity": 0.7723603800940597
    },
    {
      "doc": 235,
      "topic": 19,
      "similarity": 0.8115679394135169
    },
    {
      "doc": 235,
      "topic": 20,
      "similarity": 0.7784234707231907
    },
    {
      "doc": 235,
      "topic": 21,
      "similarity": 0.8241978898065347
    },
    {
      "doc": 235,
      "topic": 24,
      "similarity": 0.7503849692017444
    },
    {
      "doc": 236,
      "topic": 2,
      "similarity": 0.7754297840656933
    },
    {
      "doc": 236,
      "topic": 3,
      "similarity": 0.7911862035900188
    },
    {
      "doc": 236,
      "topic": 5,
      "similarity": 0.7925936068172359
    },
    {
      "doc": 236,
      "topic": 7,
      "similarity": 0.7843801128711781
    },
    {
      "doc": 236,
      "topic": 8,
      "similarity": 0.7918236988123545
    },
    {
      "doc": 236,
      "topic": 9,
      "similarity": 0.8454052194197854
    },
    {
      "doc": 236,
      "topic": 10,
      "similarity": 0.7694790480050517
    },
    {
      "doc": 236,
      "topic": 11,
      "similarity": 0.7788810270571305
    },
    {
      "doc": 236,
      "topic": 12,
      "similarity": 0.7561636936075821
    },
    {
      "doc": 236,
      "topic": 13,
      "similarity": 0.7807967278641119
    },
    {
      "doc": 236,
      "topic": 14,
      "similarity": 0.7779056143938078
    },
    {
      "doc": 236,
      "topic": 15,
      "similarity": 0.7975760536343646
    },
    {
      "doc": 236,
      "topic": 16,
      "similarity": 0.8035685607348885
    },
    {
      "doc": 236,
      "topic": 17,
      "similarity": 0.7953567563372982
    },
    {
      "doc": 236,
      "topic": 18,
      "similarity": 0.7513795408466243
    },
    {
      "doc": 236,
      "topic": 19,
      "similarity": 0.8031707847605908
    },
    {
      "doc": 236,
      "topic": 20,
      "similarity": 0.7789955666671132
    },
    {
      "doc": 236,
      "topic": 21,
      "similarity": 0.7928899797817937
    },
    {
      "doc": 237,
      "topic": 2,
      "similarity": 0.7681616656958092
    },
    {
      "doc": 237,
      "topic": 3,
      "similarity": 0.7903925906253554
    },
    {
      "doc": 237,
      "topic": 5,
      "similarity": 0.8027032750771288
    },
    {
      "doc": 237,
      "topic": 7,
      "similarity": 0.7676745996799459
    },
    {
      "doc": 237,
      "topic": 8,
      "similarity": 0.7681617748313923
    },
    {
      "doc": 237,
      "topic": 9,
      "similarity": 0.8118178982455774
    },
    {
      "doc": 237,
      "topic": 10,
      "similarity": 0.7842781268358193
    },
    {
      "doc": 237,
      "topic": 11,
      "similarity": 0.7889849359417938
    },
    {
      "doc": 237,
      "topic": 12,
      "similarity": 0.767128432571989
    },
    {
      "doc": 237,
      "topic": 13,
      "similarity": 0.7747038840843549
    },
    {
      "doc": 237,
      "topic": 14,
      "similarity": 0.7762265720769965
    },
    {
      "doc": 237,
      "topic": 15,
      "similarity": 0.7881952261413262
    },
    {
      "doc": 237,
      "topic": 16,
      "similarity": 0.7979509239788697
    },
    {
      "doc": 237,
      "topic": 17,
      "similarity": 0.8056089996192475
    },
    {
      "doc": 237,
      "topic": 18,
      "similarity": 0.7824691688242288
    },
    {
      "doc": 237,
      "topic": 19,
      "similarity": 0.8251540488411837
    },
    {
      "doc": 237,
      "topic": 20,
      "similarity": 0.824038569183647
    },
    {
      "doc": 237,
      "topic": 21,
      "similarity": 0.8071445572730395
    },
    {
      "doc": 237,
      "topic": 22,
      "similarity": 0.7764159850109942
    },
    {
      "doc": 238,
      "topic": 1,
      "similarity": 0.753910562451229
    },
    {
      "doc": 238,
      "topic": 2,
      "similarity": 0.7601108887855557
    },
    {
      "doc": 238,
      "topic": 3,
      "similarity": 0.7991044214096176
    },
    {
      "doc": 238,
      "topic": 4,
      "similarity": 0.7789068912334938
    },
    {
      "doc": 238,
      "topic": 5,
      "similarity": 0.800517046279336
    },
    {
      "doc": 238,
      "topic": 7,
      "similarity": 0.7838194119286641
    },
    {
      "doc": 238,
      "topic": 8,
      "similarity": 0.7808232939043128
    },
    {
      "doc": 238,
      "topic": 9,
      "similarity": 0.8271960521408256
    },
    {
      "doc": 238,
      "topic": 10,
      "similarity": 0.8338500433144604
    },
    {
      "doc": 238,
      "topic": 11,
      "similarity": 0.7844262024981801
    },
    {
      "doc": 238,
      "topic": 13,
      "similarity": 0.7703846644224227
    },
    {
      "doc": 238,
      "topic": 14,
      "similarity": 0.7853743385481834
    },
    {
      "doc": 238,
      "topic": 15,
      "similarity": 0.7781767707069267
    },
    {
      "doc": 238,
      "topic": 16,
      "similarity": 0.7988810614899209
    },
    {
      "doc": 238,
      "topic": 17,
      "similarity": 0.7909603545690617
    },
    {
      "doc": 238,
      "topic": 18,
      "similarity": 0.7711363575569651
    },
    {
      "doc": 238,
      "topic": 19,
      "similarity": 0.8087376995099601
    },
    {
      "doc": 238,
      "topic": 20,
      "similarity": 0.7852759018527353
    },
    {
      "doc": 238,
      "topic": 21,
      "similarity": 0.8080544375112679
    },
    {
      "doc": 238,
      "topic": 23,
      "similarity": 0.7650393556594103
    },
    {
      "doc": 238,
      "topic": 24,
      "similarity": 0.7529105122470132
    },
    {
      "doc": 239,
      "topic": 3,
      "similarity": 0.7663344862256148
    },
    {
      "doc": 239,
      "topic": 5,
      "similarity": 0.774537258317318
    },
    {
      "doc": 239,
      "topic": 7,
      "similarity": 0.7556177035021808
    },
    {
      "doc": 239,
      "topic": 8,
      "similarity": 0.769821104907685
    },
    {
      "doc": 239,
      "topic": 9,
      "similarity": 0.7802181521471626
    },
    {
      "doc": 239,
      "topic": 12,
      "similarity": 0.7500282147313644
    },
    {
      "doc": 239,
      "topic": 13,
      "similarity": 0.7681928224592223
    },
    {
      "doc": 239,
      "topic": 16,
      "similarity": 0.7644945272249762
    },
    {
      "doc": 239,
      "topic": 17,
      "similarity": 0.7537843912257222
    },
    {
      "doc": 239,
      "topic": 18,
      "similarity": 0.7526143484504969
    },
    {
      "doc": 239,
      "topic": 19,
      "similarity": 0.766008580329064
    },
    {
      "doc": 239,
      "topic": 21,
      "similarity": 0.7663534128885516
    },
    {
      "doc": 239,
      "topic": 24,
      "similarity": 0.7642837877953478
    },
    {
      "doc": 240,
      "topic": 2,
      "similarity": 0.7518402937281078
    },
    {
      "doc": 240,
      "topic": 3,
      "similarity": 0.7784809509865465
    },
    {
      "doc": 240,
      "topic": 5,
      "similarity": 0.7901153353256696
    },
    {
      "doc": 240,
      "topic": 7,
      "similarity": 0.769617854703237
    },
    {
      "doc": 240,
      "topic": 8,
      "similarity": 0.7764584365527135
    },
    {
      "doc": 240,
      "topic": 9,
      "similarity": 0.8322621001906865
    },
    {
      "doc": 240,
      "topic": 10,
      "similarity": 0.7632463729586645
    },
    {
      "doc": 240,
      "topic": 11,
      "similarity": 0.7831291540639328
    },
    {
      "doc": 240,
      "topic": 12,
      "similarity": 0.7731008798155301
    },
    {
      "doc": 240,
      "topic": 14,
      "similarity": 0.7548101317341077
    },
    {
      "doc": 240,
      "topic": 15,
      "similarity": 0.7611668741198604
    },
    {
      "doc": 240,
      "topic": 16,
      "similarity": 0.7815773784295279
    },
    {
      "doc": 240,
      "topic": 17,
      "similarity": 0.7842644883115399
    },
    {
      "doc": 240,
      "topic": 19,
      "similarity": 0.775662385249028
    },
    {
      "doc": 240,
      "topic": 20,
      "similarity": 0.780239914260314
    },
    {
      "doc": 240,
      "topic": 21,
      "similarity": 0.7950407297554114
    },
    {
      "doc": 240,
      "topic": 23,
      "similarity": 0.7687516436346565
    },
    {
      "doc": 240,
      "topic": 24,
      "similarity": 0.775329374676217
    },
    {
      "doc": 241,
      "topic": 2,
      "similarity": 0.7760744579157858
    },
    {
      "doc": 241,
      "topic": 3,
      "similarity": 0.7892349034902058
    },
    {
      "doc": 241,
      "topic": 4,
      "similarity": 0.7528783159014828
    },
    {
      "doc": 241,
      "topic": 5,
      "similarity": 0.7893363077160528
    },
    {
      "doc": 241,
      "topic": 7,
      "similarity": 0.8146973805650117
    },
    {
      "doc": 241,
      "topic": 8,
      "similarity": 0.8074432058664487
    },
    {
      "doc": 241,
      "topic": 9,
      "similarity": 0.8355590593307674
    },
    {
      "doc": 241,
      "topic": 10,
      "similarity": 0.79125389337069
    },
    {
      "doc": 241,
      "topic": 11,
      "similarity": 0.8074726965372417
    },
    {
      "doc": 241,
      "topic": 13,
      "similarity": 0.7884903618552928
    },
    {
      "doc": 241,
      "topic": 14,
      "similarity": 0.7801200452276714
    },
    {
      "doc": 241,
      "topic": 15,
      "similarity": 0.7872535315713969
    },
    {
      "doc": 241,
      "topic": 16,
      "similarity": 0.8048924200673271
    },
    {
      "doc": 241,
      "topic": 17,
      "similarity": 0.7956258895771526
    },
    {
      "doc": 241,
      "topic": 18,
      "similarity": 0.7941605317577584
    },
    {
      "doc": 241,
      "topic": 19,
      "similarity": 0.8297705212755707
    },
    {
      "doc": 241,
      "topic": 20,
      "similarity": 0.7687155285588246
    },
    {
      "doc": 241,
      "topic": 21,
      "similarity": 0.8036865931406706
    },
    {
      "doc": 241,
      "topic": 22,
      "similarity": 0.7518536633115
    },
    {
      "doc": 242,
      "topic": 3,
      "similarity": 0.7654575056744837
    },
    {
      "doc": 242,
      "topic": 5,
      "similarity": 0.7628963920481922
    },
    {
      "doc": 242,
      "topic": 9,
      "similarity": 0.7996137570119578
    },
    {
      "doc": 242,
      "topic": 16,
      "similarity": 0.7711117455626509
    },
    {
      "doc": 242,
      "topic": 17,
      "similarity": 0.7576129641493388
    },
    {
      "doc": 242,
      "topic": 19,
      "similarity": 0.7845251310452069
    },
    {
      "doc": 242,
      "topic": 20,
      "similarity": 0.757902365220656
    },
    {
      "doc": 242,
      "topic": 21,
      "similarity": 0.779908855353469
    },
    {
      "doc": 242,
      "topic": 24,
      "similarity": 0.7656484521693175
    },
    {
      "doc": 243,
      "topic": 2,
      "similarity": 0.7584862708304593
    },
    {
      "doc": 243,
      "topic": 3,
      "similarity": 0.7704568288992093
    },
    {
      "doc": 243,
      "topic": 5,
      "similarity": 0.7715297874583144
    },
    {
      "doc": 243,
      "topic": 7,
      "similarity": 0.7646927837483224
    },
    {
      "doc": 243,
      "topic": 8,
      "similarity": 0.7938344507137384
    },
    {
      "doc": 243,
      "topic": 9,
      "similarity": 0.8212832683602357
    },
    {
      "doc": 243,
      "topic": 10,
      "similarity": 0.7789503553738569
    },
    {
      "doc": 243,
      "topic": 11,
      "similarity": 0.7694577168707445
    },
    {
      "doc": 243,
      "topic": 12,
      "similarity": 0.7764396527716482
    },
    {
      "doc": 243,
      "topic": 14,
      "similarity": 0.7536979231009979
    },
    {
      "doc": 243,
      "topic": 15,
      "similarity": 0.787801122095298
    },
    {
      "doc": 243,
      "topic": 16,
      "similarity": 0.7897381104855711
    },
    {
      "doc": 243,
      "topic": 17,
      "similarity": 0.7794139598646852
    },
    {
      "doc": 243,
      "topic": 18,
      "similarity": 0.7691594281676192
    },
    {
      "doc": 243,
      "topic": 19,
      "similarity": 0.7934246506208149
    },
    {
      "doc": 243,
      "topic": 20,
      "similarity": 0.7956223372208889
    },
    {
      "doc": 243,
      "topic": 21,
      "similarity": 0.7771439880008093
    },
    {
      "doc": 244,
      "topic": 1,
      "similarity": 0.7504758424097379
    },
    {
      "doc": 244,
      "topic": 2,
      "similarity": 0.7862626971305858
    },
    {
      "doc": 244,
      "topic": 3,
      "similarity": 0.8009450015116036
    },
    {
      "doc": 244,
      "topic": 5,
      "similarity": 0.8030183184600825
    },
    {
      "doc": 244,
      "topic": 7,
      "similarity": 0.776264442630849
    },
    {
      "doc": 244,
      "topic": 8,
      "similarity": 0.7940432402263927
    },
    {
      "doc": 244,
      "topic": 9,
      "similarity": 0.8347594665828071
    },
    {
      "doc": 244,
      "topic": 10,
      "similarity": 0.7792877923684566
    },
    {
      "doc": 244,
      "topic": 11,
      "similarity": 0.7812896452574601
    },
    {
      "doc": 244,
      "topic": 12,
      "similarity": 0.8726892904902832
    },
    {
      "doc": 244,
      "topic": 13,
      "similarity": 0.785722996761421
    },
    {
      "doc": 244,
      "topic": 14,
      "similarity": 0.7740300773916984
    },
    {
      "doc": 244,
      "topic": 15,
      "similarity": 0.8020314754702736
    },
    {
      "doc": 244,
      "topic": 16,
      "similarity": 0.8138318608838246
    },
    {
      "doc": 244,
      "topic": 17,
      "similarity": 0.8219966050751326
    },
    {
      "doc": 244,
      "topic": 18,
      "similarity": 0.7819086810105605
    },
    {
      "doc": 244,
      "topic": 19,
      "similarity": 0.8115517614964817
    },
    {
      "doc": 244,
      "topic": 20,
      "similarity": 0.7966608532807782
    },
    {
      "doc": 244,
      "topic": 21,
      "similarity": 0.814347657778466
    },
    {
      "doc": 244,
      "topic": 22,
      "similarity": 0.7616029612873336
    },
    {
      "doc": 244,
      "topic": 24,
      "similarity": 0.7634811558531672
    },
    {
      "doc": 245,
      "topic": 3,
      "similarity": 0.7943765254745643
    },
    {
      "doc": 245,
      "topic": 4,
      "similarity": 0.7526762005481702
    },
    {
      "doc": 245,
      "topic": 5,
      "similarity": 0.8349275714354282
    },
    {
      "doc": 245,
      "topic": 7,
      "similarity": 0.782377605410753
    },
    {
      "doc": 245,
      "topic": 8,
      "similarity": 0.7587449434853061
    },
    {
      "doc": 245,
      "topic": 9,
      "similarity": 0.797029362017333
    },
    {
      "doc": 245,
      "topic": 10,
      "similarity": 0.7630096479797045
    },
    {
      "doc": 245,
      "topic": 11,
      "similarity": 0.7809057756325274
    },
    {
      "doc": 245,
      "topic": 15,
      "similarity": 0.7604540769200772
    },
    {
      "doc": 245,
      "topic": 16,
      "similarity": 0.7867463777766949
    },
    {
      "doc": 245,
      "topic": 17,
      "similarity": 0.7742644073856049
    },
    {
      "doc": 245,
      "topic": 18,
      "similarity": 0.754643310919065
    },
    {
      "doc": 245,
      "topic": 19,
      "similarity": 0.7856850484228632
    },
    {
      "doc": 245,
      "topic": 20,
      "similarity": 0.784660977595328
    },
    {
      "doc": 245,
      "topic": 21,
      "similarity": 0.7772978373205917
    },
    {
      "doc": 245,
      "topic": 23,
      "similarity": 0.7604757015482956
    },
    {
      "doc": 245,
      "topic": 24,
      "similarity": 0.7511990628875987
    },
    {
      "doc": 246,
      "topic": 2,
      "similarity": 0.7704885787909466
    },
    {
      "doc": 246,
      "topic": 3,
      "similarity": 0.7802532840226365
    },
    {
      "doc": 246,
      "topic": 4,
      "similarity": 0.7626563746503535
    },
    {
      "doc": 246,
      "topic": 5,
      "similarity": 0.7761068850393121
    },
    {
      "doc": 246,
      "topic": 7,
      "similarity": 0.7696304808438553
    },
    {
      "doc": 246,
      "topic": 8,
      "similarity": 0.767534043404738
    },
    {
      "doc": 246,
      "topic": 9,
      "similarity": 0.801924244821801
    },
    {
      "doc": 246,
      "topic": 10,
      "similarity": 0.8445005883541353
    },
    {
      "doc": 246,
      "topic": 11,
      "similarity": 0.7761870159056556
    },
    {
      "doc": 246,
      "topic": 14,
      "similarity": 0.7509354892801995
    },
    {
      "doc": 246,
      "topic": 15,
      "similarity": 0.7630717168542103
    },
    {
      "doc": 246,
      "topic": 16,
      "similarity": 0.7948355802118348
    },
    {
      "doc": 246,
      "topic": 17,
      "similarity": 0.791806876973845
    },
    {
      "doc": 246,
      "topic": 18,
      "similarity": 0.7626925059408103
    },
    {
      "doc": 246,
      "topic": 19,
      "similarity": 0.7985750532667122
    },
    {
      "doc": 246,
      "topic": 20,
      "similarity": 0.771026788809695
    },
    {
      "doc": 246,
      "topic": 21,
      "similarity": 0.788229021100038
    },
    {
      "doc": 247,
      "topic": 3,
      "similarity": 0.8043615117294324
    },
    {
      "doc": 247,
      "topic": 5,
      "similarity": 0.82235486883302
    },
    {
      "doc": 247,
      "topic": 7,
      "similarity": 0.7675825819621147
    },
    {
      "doc": 247,
      "topic": 8,
      "similarity": 0.755380294933477
    },
    {
      "doc": 247,
      "topic": 9,
      "similarity": 0.8107760870618339
    },
    {
      "doc": 247,
      "topic": 10,
      "similarity": 0.7625124037765652
    },
    {
      "doc": 247,
      "topic": 11,
      "similarity": 0.777595716485492
    },
    {
      "doc": 247,
      "topic": 15,
      "similarity": 0.7519781425611013
    },
    {
      "doc": 247,
      "topic": 16,
      "similarity": 0.7845072416043387
    },
    {
      "doc": 247,
      "topic": 17,
      "similarity": 0.7894259213446738
    },
    {
      "doc": 247,
      "topic": 19,
      "similarity": 0.7768860670433618
    },
    {
      "doc": 247,
      "topic": 20,
      "similarity": 0.7687847920958188
    },
    {
      "doc": 247,
      "topic": 21,
      "similarity": 0.7998301649531057
    },
    {
      "doc": 247,
      "topic": 23,
      "similarity": 0.7702536850950743
    },
    {
      "doc": 247,
      "topic": 24,
      "similarity": 0.7919871351406881
    },
    {
      "doc": 248,
      "topic": 1,
      "similarity": 0.8028937695869163
    },
    {
      "doc": 248,
      "topic": 2,
      "similarity": 0.7976939142211753
    },
    {
      "doc": 248,
      "topic": 3,
      "similarity": 0.8033260757092355
    },
    {
      "doc": 248,
      "topic": 4,
      "similarity": 0.7541233928307632
    },
    {
      "doc": 248,
      "topic": 5,
      "similarity": 0.7920672029970074
    },
    {
      "doc": 248,
      "topic": 7,
      "similarity": 0.7730858477578253
    },
    {
      "doc": 248,
      "topic": 8,
      "similarity": 0.761771479082615
    },
    {
      "doc": 248,
      "topic": 9,
      "similarity": 0.8192632127705874
    },
    {
      "doc": 248,
      "topic": 10,
      "similarity": 0.7839861693116866
    },
    {
      "doc": 248,
      "topic": 11,
      "similarity": 0.7684601968890975
    },
    {
      "doc": 248,
      "topic": 13,
      "similarity": 0.7711025033623718
    },
    {
      "doc": 248,
      "topic": 14,
      "similarity": 0.780862507279435
    },
    {
      "doc": 248,
      "topic": 15,
      "similarity": 0.7708448817916138
    },
    {
      "doc": 248,
      "topic": 16,
      "similarity": 0.8246994459543837
    },
    {
      "doc": 248,
      "topic": 17,
      "similarity": 0.7977910438261401
    },
    {
      "doc": 248,
      "topic": 19,
      "similarity": 0.7980504516774669
    },
    {
      "doc": 248,
      "topic": 20,
      "similarity": 0.7771042697806951
    },
    {
      "doc": 248,
      "topic": 21,
      "similarity": 0.8082951478410125
    },
    {
      "doc": 248,
      "topic": 23,
      "similarity": 0.7876226502230061
    },
    {
      "doc": 249,
      "topic": 1,
      "similarity": 0.7700120263703709
    },
    {
      "doc": 249,
      "topic": 2,
      "similarity": 0.7737909010625802
    },
    {
      "doc": 249,
      "topic": 3,
      "similarity": 0.8201057554592209
    },
    {
      "doc": 249,
      "topic": 4,
      "similarity": 0.771004808120913
    },
    {
      "doc": 249,
      "topic": 5,
      "similarity": 0.8126074413917067
    },
    {
      "doc": 249,
      "topic": 7,
      "similarity": 0.8021657421442966
    },
    {
      "doc": 249,
      "topic": 8,
      "similarity": 0.7981133453707041
    },
    {
      "doc": 249,
      "topic": 9,
      "similarity": 0.82984022409176
    },
    {
      "doc": 249,
      "topic": 10,
      "similarity": 0.8380920038677488
    },
    {
      "doc": 249,
      "topic": 11,
      "similarity": 0.8041572582878294
    },
    {
      "doc": 249,
      "topic": 13,
      "similarity": 0.774918501945468
    },
    {
      "doc": 249,
      "topic": 14,
      "similarity": 0.7899990737143096
    },
    {
      "doc": 249,
      "topic": 15,
      "similarity": 0.7967252829889081
    },
    {
      "doc": 249,
      "topic": 16,
      "similarity": 0.8342452426981333
    },
    {
      "doc": 249,
      "topic": 17,
      "similarity": 0.8106402357739337
    },
    {
      "doc": 249,
      "topic": 18,
      "similarity": 0.7802975645955855
    },
    {
      "doc": 249,
      "topic": 19,
      "similarity": 0.8244982543243686
    },
    {
      "doc": 249,
      "topic": 20,
      "similarity": 0.8117385055582333
    },
    {
      "doc": 249,
      "topic": 21,
      "similarity": 0.8202124285814474
    },
    {
      "doc": 249,
      "topic": 23,
      "similarity": 0.7667362079006073
    },
    {
      "doc": 249,
      "topic": 24,
      "similarity": 0.7667188036823138
    },
    {
      "doc": 250,
      "topic": 1,
      "similarity": 0.7523002465523446
    },
    {
      "doc": 250,
      "topic": 2,
      "similarity": 0.7701376475679349
    },
    {
      "doc": 250,
      "topic": 3,
      "similarity": 0.8026943760257736
    },
    {
      "doc": 250,
      "topic": 4,
      "similarity": 0.7669974778474616
    },
    {
      "doc": 250,
      "topic": 5,
      "similarity": 0.7822584104215101
    },
    {
      "doc": 250,
      "topic": 6,
      "similarity": 0.7713279648142678
    },
    {
      "doc": 250,
      "topic": 7,
      "similarity": 0.7724402208300113
    },
    {
      "doc": 250,
      "topic": 8,
      "similarity": 0.7708336203024428
    },
    {
      "doc": 250,
      "topic": 9,
      "similarity": 0.8056924202276738
    },
    {
      "doc": 250,
      "topic": 10,
      "similarity": 0.8226162000070656
    },
    {
      "doc": 250,
      "topic": 11,
      "similarity": 0.7836982357835669
    },
    {
      "doc": 250,
      "topic": 13,
      "similarity": 0.765545778229766
    },
    {
      "doc": 250,
      "topic": 14,
      "similarity": 0.7869686378785437
    },
    {
      "doc": 250,
      "topic": 15,
      "similarity": 0.7820331946168242
    },
    {
      "doc": 250,
      "topic": 16,
      "similarity": 0.7826729634537354
    },
    {
      "doc": 250,
      "topic": 17,
      "similarity": 0.7930201475733142
    },
    {
      "doc": 250,
      "topic": 18,
      "similarity": 0.8136091266533597
    },
    {
      "doc": 250,
      "topic": 19,
      "similarity": 0.8389317409615329
    },
    {
      "doc": 250,
      "topic": 20,
      "similarity": 0.7650457170229132
    },
    {
      "doc": 250,
      "topic": 21,
      "similarity": 0.7892709071205817
    },
    {
      "doc": 250,
      "topic": 23,
      "similarity": 0.764348118886331
    },
    {
      "doc": 251,
      "topic": 1,
      "similarity": 0.7806352860751488
    },
    {
      "doc": 251,
      "topic": 2,
      "similarity": 0.7614122906829438
    },
    {
      "doc": 251,
      "topic": 3,
      "similarity": 0.7741234310007228
    },
    {
      "doc": 251,
      "topic": 5,
      "similarity": 0.7786380656233883
    },
    {
      "doc": 251,
      "topic": 7,
      "similarity": 0.7624800757702874
    },
    {
      "doc": 251,
      "topic": 8,
      "similarity": 0.7686708476985947
    },
    {
      "doc": 251,
      "topic": 9,
      "similarity": 0.8014199360568578
    },
    {
      "doc": 251,
      "topic": 14,
      "similarity": 0.7583859410534257
    },
    {
      "doc": 251,
      "topic": 16,
      "similarity": 0.7665979013412546
    },
    {
      "doc": 251,
      "topic": 19,
      "similarity": 0.7663862364432749
    },
    {
      "doc": 251,
      "topic": 20,
      "similarity": 0.7569594871693102
    },
    {
      "doc": 251,
      "topic": 21,
      "similarity": 0.7597016354850641
    },
    {
      "doc": 251,
      "topic": 23,
      "similarity": 0.7668483454522014
    },
    {
      "doc": 252,
      "topic": 1,
      "similarity": 0.7553606827581175
    },
    {
      "doc": 252,
      "topic": 2,
      "similarity": 0.78456048335815
    },
    {
      "doc": 252,
      "topic": 3,
      "similarity": 0.7987124016547305
    },
    {
      "doc": 252,
      "topic": 5,
      "similarity": 0.7864901145555883
    },
    {
      "doc": 252,
      "topic": 7,
      "similarity": 0.7731960207188247
    },
    {
      "doc": 252,
      "topic": 8,
      "similarity": 0.7917465772711367
    },
    {
      "doc": 252,
      "topic": 9,
      "similarity": 0.8140721886705194
    },
    {
      "doc": 252,
      "topic": 10,
      "similarity": 0.7969541439437972
    },
    {
      "doc": 252,
      "topic": 11,
      "similarity": 0.770975366282575
    },
    {
      "doc": 252,
      "topic": 12,
      "similarity": 0.7764838989375066
    },
    {
      "doc": 252,
      "topic": 13,
      "similarity": 0.762498937945936
    },
    {
      "doc": 252,
      "topic": 14,
      "similarity": 0.7593169079721768
    },
    {
      "doc": 252,
      "topic": 15,
      "similarity": 0.7994737490086155
    },
    {
      "doc": 252,
      "topic": 16,
      "similarity": 0.8002954912508251
    },
    {
      "doc": 252,
      "topic": 17,
      "similarity": 0.8110844888703638
    },
    {
      "doc": 252,
      "topic": 18,
      "similarity": 0.7762532954305468
    },
    {
      "doc": 252,
      "topic": 19,
      "similarity": 0.8189475000487254
    },
    {
      "doc": 252,
      "topic": 20,
      "similarity": 0.7858024840500113
    },
    {
      "doc": 252,
      "topic": 21,
      "similarity": 0.8128170620084444
    },
    {
      "doc": 252,
      "topic": 23,
      "similarity": 0.7510391346174549
    },
    {
      "doc": 252,
      "topic": 24,
      "similarity": 0.7587067678654658
    },
    {
      "doc": 253,
      "topic": 3,
      "similarity": 0.7881887328289383
    },
    {
      "doc": 253,
      "topic": 5,
      "similarity": 0.7868456264209263
    },
    {
      "doc": 253,
      "topic": 7,
      "similarity": 0.7879088026813048
    },
    {
      "doc": 253,
      "topic": 9,
      "similarity": 0.8166792704187439
    },
    {
      "doc": 253,
      "topic": 11,
      "similarity": 0.7643433995898675
    },
    {
      "doc": 253,
      "topic": 15,
      "similarity": 0.7611006375650438
    },
    {
      "doc": 253,
      "topic": 16,
      "similarity": 0.7756150284742874
    },
    {
      "doc": 253,
      "topic": 17,
      "similarity": 0.7774166489515129
    },
    {
      "doc": 253,
      "topic": 19,
      "similarity": 0.7857592354067436
    },
    {
      "doc": 253,
      "topic": 20,
      "similarity": 0.7756189496208148
    },
    {
      "doc": 253,
      "topic": 21,
      "similarity": 0.7761032643626588
    },
    {
      "doc": 254,
      "topic": 2,
      "similarity": 0.7685140813271389
    },
    {
      "doc": 254,
      "topic": 3,
      "similarity": 0.7797228386768968
    },
    {
      "doc": 254,
      "topic": 5,
      "similarity": 0.8007993455207734
    },
    {
      "doc": 254,
      "topic": 7,
      "similarity": 0.7729034360963564
    },
    {
      "doc": 254,
      "topic": 8,
      "similarity": 0.7732204461454812
    },
    {
      "doc": 254,
      "topic": 9,
      "similarity": 0.8100043400385497
    },
    {
      "doc": 254,
      "topic": 10,
      "similarity": 0.7691344457801347
    },
    {
      "doc": 254,
      "topic": 11,
      "similarity": 0.7691720750092996
    },
    {
      "doc": 254,
      "topic": 13,
      "similarity": 0.7597860651703632
    },
    {
      "doc": 254,
      "topic": 14,
      "similarity": 0.7615773230107838
    },
    {
      "doc": 254,
      "topic": 15,
      "similarity": 0.8007556759242069
    },
    {
      "doc": 254,
      "topic": 16,
      "similarity": 0.7977698958586108
    },
    {
      "doc": 254,
      "topic": 17,
      "similarity": 0.8182817003403047
    },
    {
      "doc": 254,
      "topic": 18,
      "similarity": 0.7528547401483954
    },
    {
      "doc": 254,
      "topic": 19,
      "similarity": 0.7957569014224548
    },
    {
      "doc": 254,
      "topic": 20,
      "similarity": 0.7760957491697544
    },
    {
      "doc": 254,
      "topic": 21,
      "similarity": 0.8039442251194552
    },
    {
      "doc": 254,
      "topic": 23,
      "similarity": 0.7557288974935183
    },
    {
      "doc": 255,
      "topic": 2,
      "similarity": 0.7538660117453515
    },
    {
      "doc": 255,
      "topic": 3,
      "similarity": 0.7662966141138171
    },
    {
      "doc": 255,
      "topic": 5,
      "similarity": 0.7560371494329937
    },
    {
      "doc": 255,
      "topic": 6,
      "similarity": 0.7510108294894713
    },
    {
      "doc": 255,
      "topic": 8,
      "similarity": 0.78124748915486
    },
    {
      "doc": 255,
      "topic": 9,
      "similarity": 0.8126734046098331
    },
    {
      "doc": 255,
      "topic": 10,
      "similarity": 0.7654211343009798
    },
    {
      "doc": 255,
      "topic": 11,
      "similarity": 0.7707628229103817
    },
    {
      "doc": 255,
      "topic": 12,
      "similarity": 0.8254443029008974
    },
    {
      "doc": 255,
      "topic": 13,
      "similarity": 0.7555070468160375
    },
    {
      "doc": 255,
      "topic": 14,
      "similarity": 0.7586033610256333
    },
    {
      "doc": 255,
      "topic": 15,
      "similarity": 0.800115769294
    },
    {
      "doc": 255,
      "topic": 16,
      "similarity": 0.778524076366282
    },
    {
      "doc": 255,
      "topic": 17,
      "similarity": 0.7984411913531931
    },
    {
      "doc": 255,
      "topic": 18,
      "similarity": 0.7618489125666228
    },
    {
      "doc": 255,
      "topic": 19,
      "similarity": 0.7920894898953977
    },
    {
      "doc": 255,
      "topic": 20,
      "similarity": 0.7667585494544391
    },
    {
      "doc": 255,
      "topic": 21,
      "similarity": 0.7773283003554138
    },
    {
      "doc": 255,
      "topic": 22,
      "similarity": 0.7609621454554072
    },
    {
      "doc": 256,
      "topic": 2,
      "similarity": 0.7552581254642926
    },
    {
      "doc": 256,
      "topic": 3,
      "similarity": 0.7770938230408222
    },
    {
      "doc": 256,
      "topic": 5,
      "similarity": 0.7768942902670682
    },
    {
      "doc": 256,
      "topic": 7,
      "similarity": 0.7523596628828019
    },
    {
      "doc": 256,
      "topic": 8,
      "similarity": 0.7791155064462205
    },
    {
      "doc": 256,
      "topic": 9,
      "similarity": 0.8122991502907266
    },
    {
      "doc": 256,
      "topic": 11,
      "similarity": 0.7907247712309636
    },
    {
      "doc": 256,
      "topic": 12,
      "similarity": 0.7514223205963624
    },
    {
      "doc": 256,
      "topic": 14,
      "similarity": 0.7505111877838011
    },
    {
      "doc": 256,
      "topic": 15,
      "similarity": 0.7921150552305194
    },
    {
      "doc": 256,
      "topic": 16,
      "similarity": 0.7918951690503342
    },
    {
      "doc": 256,
      "topic": 17,
      "similarity": 0.8056416876119755
    },
    {
      "doc": 256,
      "topic": 18,
      "similarity": 0.7718513005092391
    },
    {
      "doc": 256,
      "topic": 19,
      "similarity": 0.7993927879254565
    },
    {
      "doc": 256,
      "topic": 20,
      "similarity": 0.7795945012844028
    },
    {
      "doc": 256,
      "topic": 21,
      "similarity": 0.7887798249121852
    },
    {
      "doc": 256,
      "topic": 23,
      "similarity": 0.7568555125079695
    },
    {
      "doc": 256,
      "topic": 24,
      "similarity": 0.7535226012808873
    },
    {
      "doc": 257,
      "topic": 1,
      "similarity": 0.7687171692677366
    },
    {
      "doc": 257,
      "topic": 2,
      "similarity": 0.8434129396012324
    },
    {
      "doc": 257,
      "topic": 3,
      "similarity": 0.8117575078735858
    },
    {
      "doc": 257,
      "topic": 4,
      "similarity": 0.7507086964417441
    },
    {
      "doc": 257,
      "topic": 5,
      "similarity": 0.8049391067729915
    },
    {
      "doc": 257,
      "topic": 7,
      "similarity": 0.7797702974430349
    },
    {
      "doc": 257,
      "topic": 8,
      "similarity": 0.7846813788048401
    },
    {
      "doc": 257,
      "topic": 9,
      "similarity": 0.821714096911965
    },
    {
      "doc": 257,
      "topic": 10,
      "similarity": 0.7906936457292785
    },
    {
      "doc": 257,
      "topic": 11,
      "similarity": 0.7856483875123419
    },
    {
      "doc": 257,
      "topic": 12,
      "similarity": 0.7556510912170274
    },
    {
      "doc": 257,
      "topic": 13,
      "similarity": 0.785549730646165
    },
    {
      "doc": 257,
      "topic": 14,
      "similarity": 0.8564022395870268
    },
    {
      "doc": 257,
      "topic": 15,
      "similarity": 0.7868455533056257
    },
    {
      "doc": 257,
      "topic": 16,
      "similarity": 0.8245602937093547
    },
    {
      "doc": 257,
      "topic": 17,
      "similarity": 0.8009424973365682
    },
    {
      "doc": 257,
      "topic": 18,
      "similarity": 0.8061427737886243
    },
    {
      "doc": 257,
      "topic": 19,
      "similarity": 0.8502449171701155
    },
    {
      "doc": 257,
      "topic": 20,
      "similarity": 0.7785484050995919
    },
    {
      "doc": 257,
      "topic": 21,
      "similarity": 0.7924227671835409
    },
    {
      "doc": 257,
      "topic": 22,
      "similarity": 0.757837873626362
    },
    {
      "doc": 257,
      "topic": 23,
      "similarity": 0.7632195598781549
    },
    {
      "doc": 258,
      "topic": 0,
      "similarity": 0.7677372670751509
    },
    {
      "doc": 258,
      "topic": 2,
      "similarity": 0.790397895176279
    },
    {
      "doc": 258,
      "topic": 3,
      "similarity": 0.8129359798644831
    },
    {
      "doc": 258,
      "topic": 5,
      "similarity": 0.8049222400578635
    },
    {
      "doc": 258,
      "topic": 7,
      "similarity": 0.790972342598454
    },
    {
      "doc": 258,
      "topic": 8,
      "similarity": 0.7939955197954086
    },
    {
      "doc": 258,
      "topic": 9,
      "similarity": 0.8371302342384461
    },
    {
      "doc": 258,
      "topic": 10,
      "similarity": 0.7746102394441912
    },
    {
      "doc": 258,
      "topic": 11,
      "similarity": 0.8542516276314797
    },
    {
      "doc": 258,
      "topic": 12,
      "similarity": 0.7568594497819144
    },
    {
      "doc": 258,
      "topic": 13,
      "similarity": 0.7749740940932485
    },
    {
      "doc": 258,
      "topic": 14,
      "similarity": 0.7742455415637928
    },
    {
      "doc": 258,
      "topic": 15,
      "similarity": 0.8219952313104663
    },
    {
      "doc": 258,
      "topic": 16,
      "similarity": 0.8083847029454532
    },
    {
      "doc": 258,
      "topic": 17,
      "similarity": 0.8203694444909803
    },
    {
      "doc": 258,
      "topic": 18,
      "similarity": 0.775353030262681
    },
    {
      "doc": 258,
      "topic": 19,
      "similarity": 0.8067404384361919
    },
    {
      "doc": 258,
      "topic": 20,
      "similarity": 0.7816325285826049
    },
    {
      "doc": 258,
      "topic": 21,
      "similarity": 0.8274162570498196
    },
    {
      "doc": 258,
      "topic": 23,
      "similarity": 0.7528347141357377
    },
    {
      "doc": 258,
      "topic": 24,
      "similarity": 0.7580234544717396
    },
    {
      "doc": 259,
      "topic": 3,
      "similarity": 0.7992408175062955
    },
    {
      "doc": 259,
      "topic": 4,
      "similarity": 0.7665812043695593
    },
    {
      "doc": 259,
      "topic": 5,
      "similarity": 0.7807768038846403
    },
    {
      "doc": 259,
      "topic": 7,
      "similarity": 0.7811834180159747
    },
    {
      "doc": 259,
      "topic": 8,
      "similarity": 0.7697569208944579
    },
    {
      "doc": 259,
      "topic": 9,
      "similarity": 0.8182363955001368
    },
    {
      "doc": 259,
      "topic": 10,
      "similarity": 0.7749581799769433
    },
    {
      "doc": 259,
      "topic": 11,
      "similarity": 0.7729297838193885
    },
    {
      "doc": 259,
      "topic": 13,
      "similarity": 0.763826568575198
    },
    {
      "doc": 259,
      "topic": 14,
      "similarity": 0.7566324630920417
    },
    {
      "doc": 259,
      "topic": 15,
      "similarity": 0.77707958382024
    },
    {
      "doc": 259,
      "topic": 16,
      "similarity": 0.8028103029872348
    },
    {
      "doc": 259,
      "topic": 17,
      "similarity": 0.8049657765605815
    },
    {
      "doc": 259,
      "topic": 19,
      "similarity": 0.7979381471671413
    },
    {
      "doc": 259,
      "topic": 20,
      "similarity": 0.7782777510792603
    },
    {
      "doc": 259,
      "topic": 21,
      "similarity": 0.8104658572577412
    },
    {
      "doc": 259,
      "topic": 24,
      "similarity": 0.7771325514864061
    },
    {
      "doc": 260,
      "topic": 0,
      "similarity": 0.7653045580021433
    },
    {
      "doc": 260,
      "topic": 1,
      "similarity": 0.789225244633474
    },
    {
      "doc": 260,
      "topic": 2,
      "similarity": 0.7813537443296066
    },
    {
      "doc": 260,
      "topic": 3,
      "similarity": 0.8183210362463833
    },
    {
      "doc": 260,
      "topic": 4,
      "similarity": 0.7596380830282612
    },
    {
      "doc": 260,
      "topic": 5,
      "similarity": 0.7791450436570901
    },
    {
      "doc": 260,
      "topic": 6,
      "similarity": 0.7777077360060581
    },
    {
      "doc": 260,
      "topic": 7,
      "similarity": 0.8030726788978242
    },
    {
      "doc": 260,
      "topic": 8,
      "similarity": 0.7881370473897306
    },
    {
      "doc": 260,
      "topic": 9,
      "similarity": 0.8316086711543447
    },
    {
      "doc": 260,
      "topic": 10,
      "similarity": 0.7910964500764227
    },
    {
      "doc": 260,
      "topic": 11,
      "similarity": 0.7933929991883346
    },
    {
      "doc": 260,
      "topic": 12,
      "similarity": 0.7579652586555814
    },
    {
      "doc": 260,
      "topic": 13,
      "similarity": 0.7824670607955359
    },
    {
      "doc": 260,
      "topic": 14,
      "similarity": 0.7836853981322487
    },
    {
      "doc": 260,
      "topic": 15,
      "similarity": 0.802193443274195
    },
    {
      "doc": 260,
      "topic": 16,
      "similarity": 0.8134423623953531
    },
    {
      "doc": 260,
      "topic": 17,
      "similarity": 0.795054053646431
    },
    {
      "doc": 260,
      "topic": 18,
      "similarity": 0.7738113538382451
    },
    {
      "doc": 260,
      "topic": 19,
      "similarity": 0.8154566607930054
    },
    {
      "doc": 260,
      "topic": 20,
      "similarity": 0.7849791620764348
    },
    {
      "doc": 260,
      "topic": 21,
      "similarity": 0.7889659369016041
    },
    {
      "doc": 260,
      "topic": 22,
      "similarity": 0.7585222732028979
    },
    {
      "doc": 260,
      "topic": 23,
      "similarity": 0.7764352188499283
    },
    {
      "doc": 261,
      "topic": 3,
      "similarity": 0.7542700517273011
    },
    {
      "doc": 261,
      "topic": 5,
      "similarity": 0.7526157643269901
    },
    {
      "doc": 261,
      "topic": 7,
      "similarity": 0.7534814429643192
    },
    {
      "doc": 261,
      "topic": 9,
      "similarity": 0.7718448424854408
    },
    {
      "doc": 261,
      "topic": 15,
      "similarity": 0.7557665341466621
    },
    {
      "doc": 261,
      "topic": 16,
      "similarity": 0.7660710009313026
    },
    {
      "doc": 261,
      "topic": 17,
      "similarity": 0.811180004285987
    },
    {
      "doc": 261,
      "topic": 19,
      "similarity": 0.7530538011963662
    },
    {
      "doc": 261,
      "topic": 20,
      "similarity": 0.7525710416288989
    },
    {
      "doc": 261,
      "topic": 21,
      "similarity": 0.779014709481534
    },
    {
      "doc": 262,
      "topic": 1,
      "similarity": 0.7528835813054295
    },
    {
      "doc": 262,
      "topic": 2,
      "similarity": 0.7610716111197364
    },
    {
      "doc": 262,
      "topic": 3,
      "similarity": 0.7786560550127221
    },
    {
      "doc": 262,
      "topic": 5,
      "similarity": 0.7711439873866327
    },
    {
      "doc": 262,
      "topic": 8,
      "similarity": 0.7749895782617476
    },
    {
      "doc": 262,
      "topic": 9,
      "similarity": 0.7875543231263877
    },
    {
      "doc": 262,
      "topic": 10,
      "similarity": 0.7531163750977155
    },
    {
      "doc": 262,
      "topic": 11,
      "similarity": 0.7530976180396134
    },
    {
      "doc": 262,
      "topic": 13,
      "similarity": 0.763683160309024
    },
    {
      "doc": 262,
      "topic": 14,
      "similarity": 0.7544330299576715
    },
    {
      "doc": 262,
      "topic": 15,
      "similarity": 0.7533870243880418
    },
    {
      "doc": 262,
      "topic": 16,
      "similarity": 0.7790469379413967
    },
    {
      "doc": 262,
      "topic": 17,
      "similarity": 0.7659722286704707
    },
    {
      "doc": 262,
      "topic": 18,
      "similarity": 0.7619411700602663
    },
    {
      "doc": 262,
      "topic": 19,
      "similarity": 0.8302248989567518
    },
    {
      "doc": 262,
      "topic": 20,
      "similarity": 0.7706339671135158
    },
    {
      "doc": 262,
      "topic": 21,
      "similarity": 0.7728540348924269
    },
    {
      "doc": 262,
      "topic": 23,
      "similarity": 0.7528651932751101
    },
    {
      "doc": 263,
      "topic": 2,
      "similarity": 0.7653610417565292
    },
    {
      "doc": 263,
      "topic": 3,
      "similarity": 0.801870791995591
    },
    {
      "doc": 263,
      "topic": 4,
      "similarity": 0.7608722347567893
    },
    {
      "doc": 263,
      "topic": 5,
      "similarity": 0.7889307914305187
    },
    {
      "doc": 263,
      "topic": 7,
      "similarity": 0.7706261917276558
    },
    {
      "doc": 263,
      "topic": 8,
      "similarity": 0.7718856975292326
    },
    {
      "doc": 263,
      "topic": 9,
      "similarity": 0.8201988210228819
    },
    {
      "doc": 263,
      "topic": 10,
      "similarity": 0.8099191067003997
    },
    {
      "doc": 263,
      "topic": 11,
      "similarity": 0.7886396595594654
    },
    {
      "doc": 263,
      "topic": 12,
      "similarity": 0.7598675592330413
    },
    {
      "doc": 263,
      "topic": 13,
      "similarity": 0.7564205282320077
    },
    {
      "doc": 263,
      "topic": 14,
      "similarity": 0.7589239950305685
    },
    {
      "doc": 263,
      "topic": 15,
      "similarity": 0.7761032131965763
    },
    {
      "doc": 263,
      "topic": 16,
      "similarity": 0.7869743996560673
    },
    {
      "doc": 263,
      "topic": 17,
      "similarity": 0.7885371246121392
    },
    {
      "doc": 263,
      "topic": 18,
      "similarity": 0.7687773562641064
    },
    {
      "doc": 263,
      "topic": 19,
      "similarity": 0.811996215590129
    },
    {
      "doc": 263,
      "topic": 20,
      "similarity": 0.7721264935179932
    },
    {
      "doc": 263,
      "topic": 21,
      "similarity": 0.8020315180908939
    },
    {
      "doc": 263,
      "topic": 24,
      "similarity": 0.7693385944839967
    },
    {
      "doc": 264,
      "topic": 1,
      "similarity": 0.7544781839636372
    },
    {
      "doc": 264,
      "topic": 2,
      "similarity": 0.7675136952927765
    },
    {
      "doc": 264,
      "topic": 3,
      "similarity": 0.7996837814783041
    },
    {
      "doc": 264,
      "topic": 5,
      "similarity": 0.7997537107298465
    },
    {
      "doc": 264,
      "topic": 7,
      "similarity": 0.7917027724768777
    },
    {
      "doc": 264,
      "topic": 8,
      "similarity": 0.7615574757894455
    },
    {
      "doc": 264,
      "topic": 9,
      "similarity": 0.8287345422045618
    },
    {
      "doc": 264,
      "topic": 10,
      "similarity": 0.7800760421694496
    },
    {
      "doc": 264,
      "topic": 11,
      "similarity": 0.7698578531128466
    },
    {
      "doc": 264,
      "topic": 13,
      "similarity": 0.7546309332134297
    },
    {
      "doc": 264,
      "topic": 14,
      "similarity": 0.7509421514299306
    },
    {
      "doc": 264,
      "topic": 15,
      "similarity": 0.7818871797781494
    },
    {
      "doc": 264,
      "topic": 16,
      "similarity": 0.794656219336983
    },
    {
      "doc": 264,
      "topic": 17,
      "similarity": 0.8095901495904222
    },
    {
      "doc": 264,
      "topic": 18,
      "similarity": 0.7697555555431786
    },
    {
      "doc": 264,
      "topic": 19,
      "similarity": 0.810057652102978
    },
    {
      "doc": 264,
      "topic": 20,
      "similarity": 0.7885591149369723
    },
    {
      "doc": 264,
      "topic": 21,
      "similarity": 0.7910341879302635
    },
    {
      "doc": 264,
      "topic": 23,
      "similarity": 0.7513530309534231
    },
    {
      "doc": 264,
      "topic": 24,
      "similarity": 0.7591928158676335
    },
    {
      "doc": 265,
      "topic": 1,
      "similarity": 0.7650457863046844
    },
    {
      "doc": 265,
      "topic": 2,
      "similarity": 0.768389738115501
    },
    {
      "doc": 265,
      "topic": 3,
      "similarity": 0.7941151405197238
    },
    {
      "doc": 265,
      "topic": 5,
      "similarity": 0.7706927299696735
    },
    {
      "doc": 265,
      "topic": 7,
      "similarity": 0.7891126431758573
    },
    {
      "doc": 265,
      "topic": 8,
      "similarity": 0.769374902639276
    },
    {
      "doc": 265,
      "topic": 9,
      "similarity": 0.7941967989068547
    },
    {
      "doc": 265,
      "topic": 10,
      "similarity": 0.7981040968900338
    },
    {
      "doc": 265,
      "topic": 11,
      "similarity": 0.7866933041024823
    },
    {
      "doc": 265,
      "topic": 13,
      "similarity": 0.7631292749778532
    },
    {
      "doc": 265,
      "topic": 14,
      "similarity": 0.759343681641984
    },
    {
      "doc": 265,
      "topic": 15,
      "similarity": 0.8132928237884187
    },
    {
      "doc": 265,
      "topic": 16,
      "similarity": 0.8050707585889095
    },
    {
      "doc": 265,
      "topic": 17,
      "similarity": 0.803536934145912
    },
    {
      "doc": 265,
      "topic": 18,
      "similarity": 0.7570940071600557
    },
    {
      "doc": 265,
      "topic": 19,
      "similarity": 0.7963509629455306
    },
    {
      "doc": 265,
      "topic": 20,
      "similarity": 0.7809691581141445
    },
    {
      "doc": 265,
      "topic": 21,
      "similarity": 0.7746644502863186
    },
    {
      "doc": 266,
      "topic": 1,
      "similarity": 0.7737514913395149
    },
    {
      "doc": 266,
      "topic": 2,
      "similarity": 0.7809905175777392
    },
    {
      "doc": 266,
      "topic": 3,
      "similarity": 0.8158940900027533
    },
    {
      "doc": 266,
      "topic": 4,
      "similarity": 0.769674028325221
    },
    {
      "doc": 266,
      "topic": 5,
      "similarity": 0.8142333960588161
    },
    {
      "doc": 266,
      "topic": 7,
      "similarity": 0.7847881625475255
    },
    {
      "doc": 266,
      "topic": 8,
      "similarity": 0.781697609371776
    },
    {
      "doc": 266,
      "topic": 9,
      "similarity": 0.8525112725257111
    },
    {
      "doc": 266,
      "topic": 10,
      "similarity": 0.8091372465356782
    },
    {
      "doc": 266,
      "topic": 11,
      "similarity": 0.793653226816351
    },
    {
      "doc": 266,
      "topic": 12,
      "similarity": 0.756676056321449
    },
    {
      "doc": 266,
      "topic": 13,
      "similarity": 0.7842875140134803
    },
    {
      "doc": 266,
      "topic": 14,
      "similarity": 0.7792329505816622
    },
    {
      "doc": 266,
      "topic": 15,
      "similarity": 0.7931471715015807
    },
    {
      "doc": 266,
      "topic": 16,
      "similarity": 0.8190210460260754
    },
    {
      "doc": 266,
      "topic": 17,
      "similarity": 0.8382183709468333
    },
    {
      "doc": 266,
      "topic": 18,
      "similarity": 0.7788595950310492
    },
    {
      "doc": 266,
      "topic": 19,
      "similarity": 0.8141585167207677
    },
    {
      "doc": 266,
      "topic": 20,
      "similarity": 0.8030676021428661
    },
    {
      "doc": 266,
      "topic": 21,
      "similarity": 0.8379637297924705
    },
    {
      "doc": 266,
      "topic": 23,
      "similarity": 0.7593757318799822
    },
    {
      "doc": 266,
      "topic": 24,
      "similarity": 0.7785992034740994
    },
    {
      "doc": 267,
      "topic": 0,
      "similarity": 0.7720554964396227
    },
    {
      "doc": 267,
      "topic": 1,
      "similarity": 0.793861741857853
    },
    {
      "doc": 267,
      "topic": 2,
      "similarity": 0.7634815128292675
    },
    {
      "doc": 267,
      "topic": 3,
      "similarity": 0.7901230829851643
    },
    {
      "doc": 267,
      "topic": 5,
      "similarity": 0.7553407376083364
    },
    {
      "doc": 267,
      "topic": 7,
      "similarity": 0.7558927667345972
    },
    {
      "doc": 267,
      "topic": 9,
      "similarity": 0.793537186186153
    },
    {
      "doc": 267,
      "topic": 11,
      "similarity": 0.7836168796278143
    },
    {
      "doc": 267,
      "topic": 15,
      "similarity": 0.7605067223771712
    },
    {
      "doc": 267,
      "topic": 16,
      "similarity": 0.7664425479361447
    },
    {
      "doc": 267,
      "topic": 17,
      "similarity": 0.7545043750838032
    },
    {
      "doc": 267,
      "topic": 19,
      "similarity": 0.7826901111695624
    },
    {
      "doc": 267,
      "topic": 21,
      "similarity": 0.7852690866272098
    },
    {
      "doc": 267,
      "topic": 23,
      "similarity": 0.7578508380111855
    },
    {
      "doc": 268,
      "topic": 2,
      "similarity": 0.7722200882551984
    },
    {
      "doc": 268,
      "topic": 3,
      "similarity": 0.7914430317104634
    },
    {
      "doc": 268,
      "topic": 4,
      "similarity": 0.7503231387142737
    },
    {
      "doc": 268,
      "topic": 5,
      "similarity": 0.8075830310739113
    },
    {
      "doc": 268,
      "topic": 6,
      "similarity": 0.7556006120170763
    },
    {
      "doc": 268,
      "topic": 7,
      "similarity": 0.7738058910091801
    },
    {
      "doc": 268,
      "topic": 8,
      "similarity": 0.7749001255048237
    },
    {
      "doc": 268,
      "topic": 9,
      "similarity": 0.8147568042063326
    },
    {
      "doc": 268,
      "topic": 10,
      "similarity": 0.8177150396619066
    },
    {
      "doc": 268,
      "topic": 11,
      "similarity": 0.8021123927549927
    },
    {
      "doc": 268,
      "topic": 13,
      "similarity": 0.767915795690689
    },
    {
      "doc": 268,
      "topic": 14,
      "similarity": 0.7788167412942268
    },
    {
      "doc": 268,
      "topic": 15,
      "similarity": 0.7829642471545843
    },
    {
      "doc": 268,
      "topic": 16,
      "similarity": 0.8168797103403372
    },
    {
      "doc": 268,
      "topic": 17,
      "similarity": 0.8049388705101688
    },
    {
      "doc": 268,
      "topic": 18,
      "similarity": 0.7990374235238119
    },
    {
      "doc": 268,
      "topic": 19,
      "similarity": 0.8257234374235708
    },
    {
      "doc": 268,
      "topic": 20,
      "similarity": 0.7963551695991582
    },
    {
      "doc": 268,
      "topic": 21,
      "similarity": 0.804643266124923
    },
    {
      "doc": 268,
      "topic": 23,
      "similarity": 0.7644072205686323
    },
    {
      "doc": 269,
      "topic": 2,
      "similarity": 0.7650035174557611
    },
    {
      "doc": 269,
      "topic": 3,
      "similarity": 0.7917768879925183
    },
    {
      "doc": 269,
      "topic": 4,
      "similarity": 0.7713630428594265
    },
    {
      "doc": 269,
      "topic": 5,
      "similarity": 0.7927112138245632
    },
    {
      "doc": 269,
      "topic": 7,
      "similarity": 0.7992764150223486
    },
    {
      "doc": 269,
      "topic": 8,
      "similarity": 0.7772252861557838
    },
    {
      "doc": 269,
      "topic": 9,
      "similarity": 0.8151625655527074
    },
    {
      "doc": 269,
      "topic": 10,
      "similarity": 0.7723984601526398
    },
    {
      "doc": 269,
      "topic": 11,
      "similarity": 0.77572699456783
    },
    {
      "doc": 269,
      "topic": 13,
      "similarity": 0.7620815644228139
    },
    {
      "doc": 269,
      "topic": 14,
      "similarity": 0.7533091086786088
    },
    {
      "doc": 269,
      "topic": 15,
      "similarity": 0.7696974294886784
    },
    {
      "doc": 269,
      "topic": 16,
      "similarity": 0.8127107805981357
    },
    {
      "doc": 269,
      "topic": 17,
      "similarity": 0.796069411413734
    },
    {
      "doc": 269,
      "topic": 19,
      "similarity": 0.7892468027776249
    },
    {
      "doc": 269,
      "topic": 20,
      "similarity": 0.7765170514115614
    },
    {
      "doc": 269,
      "topic": 21,
      "similarity": 0.7966071085004306
    },
    {
      "doc": 270,
      "topic": 3,
      "similarity": 0.7648352942675184
    },
    {
      "doc": 270,
      "topic": 5,
      "similarity": 0.7694042933205317
    },
    {
      "doc": 270,
      "topic": 7,
      "similarity": 0.7541741234072081
    },
    {
      "doc": 270,
      "topic": 9,
      "similarity": 0.802717808821014
    },
    {
      "doc": 270,
      "topic": 10,
      "similarity": 0.7516048598492364
    },
    {
      "doc": 270,
      "topic": 11,
      "similarity": 0.7531204599206756
    },
    {
      "doc": 270,
      "topic": 14,
      "similarity": 0.7590124973219065
    },
    {
      "doc": 270,
      "topic": 15,
      "similarity": 0.7501327487522008
    },
    {
      "doc": 270,
      "topic": 16,
      "similarity": 0.7783960596914264
    },
    {
      "doc": 270,
      "topic": 17,
      "similarity": 0.7616359054269205
    },
    {
      "doc": 270,
      "topic": 18,
      "similarity": 0.7510170725918666
    },
    {
      "doc": 270,
      "topic": 19,
      "similarity": 0.7758772879767858
    },
    {
      "doc": 270,
      "topic": 20,
      "similarity": 0.7950773968529433
    },
    {
      "doc": 270,
      "topic": 21,
      "similarity": 0.7684361885160829
    },
    {
      "doc": 270,
      "topic": 23,
      "similarity": 0.7762901240984293
    },
    {
      "doc": 271,
      "topic": 3,
      "similarity": 0.7769202188696109
    },
    {
      "doc": 271,
      "topic": 4,
      "similarity": 0.7540724737265474
    },
    {
      "doc": 271,
      "topic": 5,
      "similarity": 0.7731293855071598
    },
    {
      "doc": 271,
      "topic": 7,
      "similarity": 0.7654751373464534
    },
    {
      "doc": 271,
      "topic": 9,
      "similarity": 0.7950882509760201
    },
    {
      "doc": 271,
      "topic": 10,
      "similarity": 0.7551560691980933
    },
    {
      "doc": 271,
      "topic": 11,
      "similarity": 0.7636188623961501
    },
    {
      "doc": 271,
      "topic": 15,
      "similarity": 0.7698615538729159
    },
    {
      "doc": 271,
      "topic": 16,
      "similarity": 0.795369205754139
    },
    {
      "doc": 271,
      "topic": 17,
      "similarity": 0.8355311279524825
    },
    {
      "doc": 271,
      "topic": 19,
      "similarity": 0.777436049763558
    },
    {
      "doc": 271,
      "topic": 20,
      "similarity": 0.783442028768333
    },
    {
      "doc": 271,
      "topic": 21,
      "similarity": 0.8068323297292748
    },
    {
      "doc": 272,
      "topic": 1,
      "similarity": 0.7667027344484679
    },
    {
      "doc": 272,
      "topic": 2,
      "similarity": 0.7746498618937977
    },
    {
      "doc": 272,
      "topic": 3,
      "similarity": 0.8014282449239688
    },
    {
      "doc": 272,
      "topic": 4,
      "similarity": 0.7519675295746543
    },
    {
      "doc": 272,
      "topic": 5,
      "similarity": 0.8270982697811671
    },
    {
      "doc": 272,
      "topic": 6,
      "similarity": 0.7612010928906574
    },
    {
      "doc": 272,
      "topic": 7,
      "similarity": 0.7797773935452504
    },
    {
      "doc": 272,
      "topic": 8,
      "similarity": 0.7865418267761427
    },
    {
      "doc": 272,
      "topic": 9,
      "similarity": 0.8351217185074827
    },
    {
      "doc": 272,
      "topic": 10,
      "similarity": 0.8095849352557138
    },
    {
      "doc": 272,
      "topic": 11,
      "similarity": 0.799292943247428
    },
    {
      "doc": 272,
      "topic": 13,
      "similarity": 0.774572179907159
    },
    {
      "doc": 272,
      "topic": 14,
      "similarity": 0.7891021427814428
    },
    {
      "doc": 272,
      "topic": 15,
      "similarity": 0.807892511554622
    },
    {
      "doc": 272,
      "topic": 16,
      "similarity": 0.8071358449551863
    },
    {
      "doc": 272,
      "topic": 17,
      "similarity": 0.809520727905667
    },
    {
      "doc": 272,
      "topic": 18,
      "similarity": 0.7698210856978257
    },
    {
      "doc": 272,
      "topic": 19,
      "similarity": 0.8124243945799441
    },
    {
      "doc": 272,
      "topic": 20,
      "similarity": 0.7869578437329441
    },
    {
      "doc": 272,
      "topic": 21,
      "similarity": 0.8175416568224387
    },
    {
      "doc": 272,
      "topic": 23,
      "similarity": 0.7836942006518605
    },
    {
      "doc": 272,
      "topic": 24,
      "similarity": 0.7571934921571548
    },
    {
      "doc": 273,
      "topic": 2,
      "similarity": 0.7790317854137052
    },
    {
      "doc": 273,
      "topic": 3,
      "similarity": 0.7945563753119048
    },
    {
      "doc": 273,
      "topic": 4,
      "similarity": 0.7503560261947584
    },
    {
      "doc": 273,
      "topic": 5,
      "similarity": 0.8190134118560684
    },
    {
      "doc": 273,
      "topic": 7,
      "similarity": 0.7793104573647047
    },
    {
      "doc": 273,
      "topic": 8,
      "similarity": 0.7913943159089316
    },
    {
      "doc": 273,
      "topic": 9,
      "similarity": 0.8338887578490305
    },
    {
      "doc": 273,
      "topic": 10,
      "similarity": 0.7592764265417701
    },
    {
      "doc": 273,
      "topic": 11,
      "similarity": 0.7739875178839132
    },
    {
      "doc": 273,
      "topic": 13,
      "similarity": 0.7756430286391778
    },
    {
      "doc": 273,
      "topic": 14,
      "similarity": 0.7607956579496151
    },
    {
      "doc": 273,
      "topic": 15,
      "similarity": 0.7835837318965903
    },
    {
      "doc": 273,
      "topic": 16,
      "similarity": 0.8080515586734902
    },
    {
      "doc": 273,
      "topic": 17,
      "similarity": 0.8133438717440751
    },
    {
      "doc": 273,
      "topic": 19,
      "similarity": 0.7954293249595193
    },
    {
      "doc": 273,
      "topic": 20,
      "similarity": 0.7894585828543874
    },
    {
      "doc": 273,
      "topic": 21,
      "similarity": 0.8057628528424118
    },
    {
      "doc": 274,
      "topic": 1,
      "similarity": 0.7819584057699152
    },
    {
      "doc": 274,
      "topic": 2,
      "similarity": 0.7861295054490304
    },
    {
      "doc": 274,
      "topic": 3,
      "similarity": 0.826158263890479
    },
    {
      "doc": 274,
      "topic": 4,
      "similarity": 0.7793277606359551
    },
    {
      "doc": 274,
      "topic": 5,
      "similarity": 0.8399452702616242
    },
    {
      "doc": 274,
      "topic": 6,
      "similarity": 0.7877012319203434
    },
    {
      "doc": 274,
      "topic": 7,
      "similarity": 0.7993726096449079
    },
    {
      "doc": 274,
      "topic": 8,
      "similarity": 0.7885886345553063
    },
    {
      "doc": 274,
      "topic": 9,
      "similarity": 0.8409427964322791
    },
    {
      "doc": 274,
      "topic": 10,
      "similarity": 0.8066773628099593
    },
    {
      "doc": 274,
      "topic": 11,
      "similarity": 0.8189361436541425
    },
    {
      "doc": 274,
      "topic": 13,
      "similarity": 0.791926258836059
    },
    {
      "doc": 274,
      "topic": 14,
      "similarity": 0.795697501109796
    },
    {
      "doc": 274,
      "topic": 15,
      "similarity": 0.8041385809859719
    },
    {
      "doc": 274,
      "topic": 16,
      "similarity": 0.8224312930347335
    },
    {
      "doc": 274,
      "topic": 17,
      "similarity": 0.8367792985356775
    },
    {
      "doc": 274,
      "topic": 18,
      "similarity": 0.7864168599067364
    },
    {
      "doc": 274,
      "topic": 19,
      "similarity": 0.8384561133061036
    },
    {
      "doc": 274,
      "topic": 20,
      "similarity": 0.7908794067811437
    },
    {
      "doc": 274,
      "topic": 21,
      "similarity": 0.8204103333256937
    },
    {
      "doc": 274,
      "topic": 23,
      "similarity": 0.7910415518131837
    },
    {
      "doc": 274,
      "topic": 24,
      "similarity": 0.7758665665745047
    },
    {
      "doc": 275,
      "topic": 3,
      "similarity": 0.7894529228999214
    },
    {
      "doc": 275,
      "topic": 5,
      "similarity": 0.7878484152842069
    },
    {
      "doc": 275,
      "topic": 7,
      "similarity": 0.7574446661767728
    },
    {
      "doc": 275,
      "topic": 8,
      "similarity": 0.7559560853012636
    },
    {
      "doc": 275,
      "topic": 9,
      "similarity": 0.8051945959558469
    },
    {
      "doc": 275,
      "topic": 10,
      "similarity": 0.7535075375919035
    },
    {
      "doc": 275,
      "topic": 11,
      "similarity": 0.7627269306338602
    },
    {
      "doc": 275,
      "topic": 16,
      "similarity": 0.769681495634223
    },
    {
      "doc": 275,
      "topic": 19,
      "similarity": 0.7658498182124841
    },
    {
      "doc": 275,
      "topic": 20,
      "similarity": 0.7517024310039797
    },
    {
      "doc": 275,
      "topic": 21,
      "similarity": 0.7572931199195114
    },
    {
      "doc": 275,
      "topic": 23,
      "similarity": 0.76476794003838
    },
    {
      "doc": 275,
      "topic": 24,
      "similarity": 0.7895037360917512
    },
    {
      "doc": 276,
      "topic": 3,
      "similarity": 0.766456372450063
    },
    {
      "doc": 276,
      "topic": 4,
      "similarity": 0.8264143604131458
    },
    {
      "doc": 276,
      "topic": 9,
      "similarity": 0.7523015385296985
    },
    {
      "doc": 277,
      "topic": 3,
      "similarity": 0.7547463272102706
    },
    {
      "doc": 277,
      "topic": 5,
      "similarity": 0.7661400198913711
    },
    {
      "doc": 277,
      "topic": 9,
      "similarity": 0.8073219139053674
    },
    {
      "doc": 277,
      "topic": 10,
      "similarity": 0.7534238977941878
    },
    {
      "doc": 277,
      "topic": 11,
      "similarity": 0.7561804416582928
    },
    {
      "doc": 277,
      "topic": 15,
      "similarity": 0.7604034362909015
    },
    {
      "doc": 277,
      "topic": 16,
      "similarity": 0.7785658185348057
    },
    {
      "doc": 277,
      "topic": 17,
      "similarity": 0.8257834012553456
    },
    {
      "doc": 277,
      "topic": 19,
      "similarity": 0.7920073741233634
    },
    {
      "doc": 277,
      "topic": 20,
      "similarity": 0.781528860097328
    },
    {
      "doc": 277,
      "topic": 21,
      "similarity": 0.8009036168086555
    },
    {
      "doc": 277,
      "topic": 24,
      "similarity": 0.7643220556311627
    },
    {
      "doc": 278,
      "topic": 1,
      "similarity": 0.7521097910070649
    },
    {
      "doc": 278,
      "topic": 2,
      "similarity": 0.7792278485876603
    },
    {
      "doc": 278,
      "topic": 3,
      "similarity": 0.7998705697158556
    },
    {
      "doc": 278,
      "topic": 4,
      "similarity": 0.7695635515034983
    },
    {
      "doc": 278,
      "topic": 5,
      "similarity": 0.8048309894046584
    },
    {
      "doc": 278,
      "topic": 7,
      "similarity": 0.770808089128569
    },
    {
      "doc": 278,
      "topic": 8,
      "similarity": 0.7699093455521704
    },
    {
      "doc": 278,
      "topic": 9,
      "similarity": 0.8131640947109696
    },
    {
      "doc": 278,
      "topic": 10,
      "similarity": 0.7743916191763209
    },
    {
      "doc": 278,
      "topic": 11,
      "similarity": 0.7642717323045837
    },
    {
      "doc": 278,
      "topic": 13,
      "similarity": 0.768086911387607
    },
    {
      "doc": 278,
      "topic": 14,
      "similarity": 0.7755355864919371
    },
    {
      "doc": 278,
      "topic": 15,
      "similarity": 0.7726860826768832
    },
    {
      "doc": 278,
      "topic": 16,
      "similarity": 0.8169955717141479
    },
    {
      "doc": 278,
      "topic": 17,
      "similarity": 0.79328114245251
    },
    {
      "doc": 278,
      "topic": 18,
      "similarity": 0.7575175549365711
    },
    {
      "doc": 278,
      "topic": 19,
      "similarity": 0.8267730530184009
    },
    {
      "doc": 278,
      "topic": 20,
      "similarity": 0.790581572537186
    },
    {
      "doc": 278,
      "topic": 21,
      "similarity": 0.8038118480788139
    },
    {
      "doc": 278,
      "topic": 23,
      "similarity": 0.7672090715570383
    },
    {
      "doc": 278,
      "topic": 24,
      "similarity": 0.7536504875426342
    },
    {
      "doc": 279,
      "topic": 2,
      "similarity": 0.7557406972430779
    },
    {
      "doc": 279,
      "topic": 3,
      "similarity": 0.8191358712213841
    },
    {
      "doc": 279,
      "topic": 4,
      "similarity": 0.7554409920079143
    },
    {
      "doc": 279,
      "topic": 5,
      "similarity": 0.7969400664065615
    },
    {
      "doc": 279,
      "topic": 7,
      "similarity": 0.7862182237484414
    },
    {
      "doc": 279,
      "topic": 8,
      "similarity": 0.7596957770878979
    },
    {
      "doc": 279,
      "topic": 9,
      "similarity": 0.8318343889638821
    },
    {
      "doc": 279,
      "topic": 10,
      "similarity": 0.7958004727472237
    },
    {
      "doc": 279,
      "topic": 11,
      "similarity": 0.7542312884175009
    },
    {
      "doc": 279,
      "topic": 13,
      "similarity": 0.7507995196567485
    },
    {
      "doc": 279,
      "topic": 14,
      "similarity": 0.7515825700843615
    },
    {
      "doc": 279,
      "topic": 15,
      "similarity": 0.7621032158096683
    },
    {
      "doc": 279,
      "topic": 16,
      "similarity": 0.7824278860209459
    },
    {
      "doc": 279,
      "topic": 17,
      "similarity": 0.7793334395111222
    },
    {
      "doc": 279,
      "topic": 18,
      "similarity": 0.7594656582578267
    },
    {
      "doc": 279,
      "topic": 19,
      "similarity": 0.8011037662752114
    },
    {
      "doc": 279,
      "topic": 20,
      "similarity": 0.7629901731036989
    },
    {
      "doc": 279,
      "topic": 21,
      "similarity": 0.7983216909804559
    },
    {
      "doc": 279,
      "topic": 23,
      "similarity": 0.7530536112890875
    },
    {
      "doc": 280,
      "topic": 2,
      "similarity": 0.7647733617911457
    },
    {
      "doc": 280,
      "topic": 3,
      "similarity": 0.7819642493522746
    },
    {
      "doc": 280,
      "topic": 5,
      "similarity": 0.7813656386284533
    },
    {
      "doc": 280,
      "topic": 7,
      "similarity": 0.7898247600900301
    },
    {
      "doc": 280,
      "topic": 8,
      "similarity": 0.7852280370391131
    },
    {
      "doc": 280,
      "topic": 9,
      "similarity": 0.8577713789433808
    },
    {
      "doc": 280,
      "topic": 10,
      "similarity": 0.7711195857156866
    },
    {
      "doc": 280,
      "topic": 11,
      "similarity": 0.7716761025265988
    },
    {
      "doc": 280,
      "topic": 14,
      "similarity": 0.7610901442311045
    },
    {
      "doc": 280,
      "topic": 15,
      "similarity": 0.7740085329109019
    },
    {
      "doc": 280,
      "topic": 16,
      "similarity": 0.7970755016631467
    },
    {
      "doc": 280,
      "topic": 17,
      "similarity": 0.7831683285180628
    },
    {
      "doc": 280,
      "topic": 18,
      "similarity": 0.7608208287329752
    },
    {
      "doc": 280,
      "topic": 19,
      "similarity": 0.7943434428360451
    },
    {
      "doc": 280,
      "topic": 20,
      "similarity": 0.7668370308016935
    },
    {
      "doc": 280,
      "topic": 21,
      "similarity": 0.7944334904796633
    },
    {
      "doc": 280,
      "topic": 23,
      "similarity": 0.7572112355347578
    },
    {
      "doc": 280,
      "topic": 24,
      "similarity": 0.7998796728521631
    },
    {
      "doc": 281,
      "topic": 1,
      "similarity": 0.7630819437258188
    },
    {
      "doc": 281,
      "topic": 2,
      "similarity": 0.8009328841085096
    },
    {
      "doc": 281,
      "topic": 3,
      "similarity": 0.8050017927367507
    },
    {
      "doc": 281,
      "topic": 5,
      "similarity": 0.8064708894801313
    },
    {
      "doc": 281,
      "topic": 6,
      "similarity": 0.7580765550267269
    },
    {
      "doc": 281,
      "topic": 7,
      "similarity": 0.7728624050587468
    },
    {
      "doc": 281,
      "topic": 8,
      "similarity": 0.7791265401682904
    },
    {
      "doc": 281,
      "topic": 9,
      "similarity": 0.8442131190404148
    },
    {
      "doc": 281,
      "topic": 10,
      "similarity": 0.7930772797254173
    },
    {
      "doc": 281,
      "topic": 11,
      "similarity": 0.7982165468947872
    },
    {
      "doc": 281,
      "topic": 12,
      "similarity": 0.7589325807692935
    },
    {
      "doc": 281,
      "topic": 13,
      "similarity": 0.7769669899417686
    },
    {
      "doc": 281,
      "topic": 14,
      "similarity": 0.8007090434600113
    },
    {
      "doc": 281,
      "topic": 15,
      "similarity": 0.8000542311018612
    },
    {
      "doc": 281,
      "topic": 16,
      "similarity": 0.8194294340604045
    },
    {
      "doc": 281,
      "topic": 17,
      "similarity": 0.8568576718761003
    },
    {
      "doc": 281,
      "topic": 18,
      "similarity": 0.7882076556424571
    },
    {
      "doc": 281,
      "topic": 19,
      "similarity": 0.814368277956525
    },
    {
      "doc": 281,
      "topic": 20,
      "similarity": 0.8041694994087363
    },
    {
      "doc": 281,
      "topic": 21,
      "similarity": 0.8313052573387268
    },
    {
      "doc": 281,
      "topic": 23,
      "similarity": 0.7807375036336909
    },
    {
      "doc": 281,
      "topic": 24,
      "similarity": 0.7636370520238639
    },
    {
      "doc": 282,
      "topic": 1,
      "similarity": 0.7571436201506708
    },
    {
      "doc": 282,
      "topic": 2,
      "similarity": 0.7714182457997858
    },
    {
      "doc": 282,
      "topic": 3,
      "similarity": 0.8086851731775296
    },
    {
      "doc": 282,
      "topic": 4,
      "similarity": 0.7675852504256333
    },
    {
      "doc": 282,
      "topic": 5,
      "similarity": 0.8100772856674789
    },
    {
      "doc": 282,
      "topic": 7,
      "similarity": 0.7979373849558671
    },
    {
      "doc": 282,
      "topic": 8,
      "similarity": 0.7829695084240353
    },
    {
      "doc": 282,
      "topic": 9,
      "similarity": 0.8587112934866498
    },
    {
      "doc": 282,
      "topic": 10,
      "similarity": 0.784963263377628
    },
    {
      "doc": 282,
      "topic": 11,
      "similarity": 0.8219588624235685
    },
    {
      "doc": 282,
      "topic": 13,
      "similarity": 0.7736679759733556
    },
    {
      "doc": 282,
      "topic": 14,
      "similarity": 0.7813418202245186
    },
    {
      "doc": 282,
      "topic": 15,
      "similarity": 0.7812327802545335
    },
    {
      "doc": 282,
      "topic": 16,
      "similarity": 0.7992606952605624
    },
    {
      "doc": 282,
      "topic": 17,
      "similarity": 0.808526107977206
    },
    {
      "doc": 282,
      "topic": 18,
      "similarity": 0.7655739140093739
    },
    {
      "doc": 282,
      "topic": 19,
      "similarity": 0.7958436515485694
    },
    {
      "doc": 282,
      "topic": 20,
      "similarity": 0.7585458130794552
    },
    {
      "doc": 282,
      "topic": 21,
      "similarity": 0.8110774599483749
    },
    {
      "doc": 282,
      "topic": 24,
      "similarity": 0.7921421421462953
    },
    {
      "doc": 283,
      "topic": 3,
      "similarity": 0.7818346400980921
    },
    {
      "doc": 283,
      "topic": 5,
      "similarity": 0.7761129837544767
    },
    {
      "doc": 283,
      "topic": 6,
      "similarity": 0.7549416187582497
    },
    {
      "doc": 283,
      "topic": 9,
      "similarity": 0.7941087835069809
    },
    {
      "doc": 283,
      "topic": 10,
      "similarity": 0.7523979621204462
    },
    {
      "doc": 283,
      "topic": 11,
      "similarity": 0.7521328117186945
    },
    {
      "doc": 283,
      "topic": 14,
      "similarity": 0.7524307109795553
    },
    {
      "doc": 283,
      "topic": 16,
      "similarity": 0.7692361446787708
    },
    {
      "doc": 283,
      "topic": 17,
      "similarity": 0.7734676934398768
    },
    {
      "doc": 283,
      "topic": 19,
      "similarity": 0.7820457241734314
    },
    {
      "doc": 283,
      "topic": 20,
      "similarity": 0.7660919519159795
    },
    {
      "doc": 283,
      "topic": 21,
      "similarity": 0.7712833591881431
    },
    {
      "doc": 283,
      "topic": 23,
      "similarity": 0.8280328083351145
    },
    {
      "doc": 284,
      "topic": 2,
      "similarity": 0.7661255348739613
    },
    {
      "doc": 284,
      "topic": 3,
      "similarity": 0.7879590366927578
    },
    {
      "doc": 284,
      "topic": 5,
      "similarity": 0.7984081378251371
    },
    {
      "doc": 284,
      "topic": 7,
      "similarity": 0.7865501597026926
    },
    {
      "doc": 284,
      "topic": 8,
      "similarity": 0.7664107207170722
    },
    {
      "doc": 284,
      "topic": 9,
      "similarity": 0.8625698556338289
    },
    {
      "doc": 284,
      "topic": 10,
      "similarity": 0.7716181848547676
    },
    {
      "doc": 284,
      "topic": 11,
      "similarity": 0.76367672236245
    },
    {
      "doc": 284,
      "topic": 12,
      "similarity": 0.7571628465994642
    },
    {
      "doc": 284,
      "topic": 13,
      "similarity": 0.7579787475657953
    },
    {
      "doc": 284,
      "topic": 14,
      "similarity": 0.7616532660085624
    },
    {
      "doc": 284,
      "topic": 15,
      "similarity": 0.775187826683683
    },
    {
      "doc": 284,
      "topic": 16,
      "similarity": 0.8027521572941027
    },
    {
      "doc": 284,
      "topic": 17,
      "similarity": 0.7985090293614855
    },
    {
      "doc": 284,
      "topic": 18,
      "similarity": 0.7544214214423829
    },
    {
      "doc": 284,
      "topic": 19,
      "similarity": 0.8026642865905186
    },
    {
      "doc": 284,
      "topic": 20,
      "similarity": 0.7982353905707521
    },
    {
      "doc": 284,
      "topic": 21,
      "similarity": 0.8082085923375993
    },
    {
      "doc": 284,
      "topic": 23,
      "similarity": 0.7793804766174579
    },
    {
      "doc": 284,
      "topic": 24,
      "similarity": 0.7898576021234577
    },
    {
      "doc": 285,
      "topic": 1,
      "similarity": 0.7613035332775727
    },
    {
      "doc": 285,
      "topic": 2,
      "similarity": 0.7556839355072023
    },
    {
      "doc": 285,
      "topic": 3,
      "similarity": 0.7824473891504342
    },
    {
      "doc": 285,
      "topic": 5,
      "similarity": 0.7953862042233387
    },
    {
      "doc": 285,
      "topic": 7,
      "similarity": 0.7612766089494727
    },
    {
      "doc": 285,
      "topic": 8,
      "similarity": 0.7647897181027548
    },
    {
      "doc": 285,
      "topic": 9,
      "similarity": 0.7952988636471698
    },
    {
      "doc": 285,
      "topic": 10,
      "similarity": 0.7748848106228815
    },
    {
      "doc": 285,
      "topic": 11,
      "similarity": 0.7797393470194987
    },
    {
      "doc": 285,
      "topic": 13,
      "similarity": 0.7525722531842354
    },
    {
      "doc": 285,
      "topic": 14,
      "similarity": 0.7570804661780576
    },
    {
      "doc": 285,
      "topic": 15,
      "similarity": 0.7756740514451242
    },
    {
      "doc": 285,
      "topic": 16,
      "similarity": 0.7892618879845347
    },
    {
      "doc": 285,
      "topic": 17,
      "similarity": 0.8037938693696217
    },
    {
      "doc": 285,
      "topic": 18,
      "similarity": 0.7587071039997291
    },
    {
      "doc": 285,
      "topic": 19,
      "similarity": 0.8029212680379697
    },
    {
      "doc": 285,
      "topic": 20,
      "similarity": 0.7867738910339993
    },
    {
      "doc": 285,
      "topic": 21,
      "similarity": 0.783854560042845
    },
    {
      "doc": 285,
      "topic": 22,
      "similarity": 0.7666321380997798
    },
    {
      "doc": 286,
      "topic": 1,
      "similarity": 0.7761280885333606
    },
    {
      "doc": 286,
      "topic": 2,
      "similarity": 0.7942547651882504
    },
    {
      "doc": 286,
      "topic": 3,
      "similarity": 0.8124114955025766
    },
    {
      "doc": 286,
      "topic": 4,
      "similarity": 0.7804732317167999
    },
    {
      "doc": 286,
      "topic": 5,
      "similarity": 0.8134040211029852
    },
    {
      "doc": 286,
      "topic": 7,
      "similarity": 0.8036852790686663
    },
    {
      "doc": 286,
      "topic": 8,
      "similarity": 0.7965449139137958
    },
    {
      "doc": 286,
      "topic": 9,
      "similarity": 0.8565863596204545
    },
    {
      "doc": 286,
      "topic": 10,
      "similarity": 0.8406429849270781
    },
    {
      "doc": 286,
      "topic": 11,
      "similarity": 0.8102614509183228
    },
    {
      "doc": 286,
      "topic": 13,
      "similarity": 0.7837878743112631
    },
    {
      "doc": 286,
      "topic": 14,
      "similarity": 0.7876486708057849
    },
    {
      "doc": 286,
      "topic": 15,
      "similarity": 0.7943981209304881
    },
    {
      "doc": 286,
      "topic": 16,
      "similarity": 0.8231967438337339
    },
    {
      "doc": 286,
      "topic": 17,
      "similarity": 0.8140342632501012
    },
    {
      "doc": 286,
      "topic": 18,
      "similarity": 0.7717078820904115
    },
    {
      "doc": 286,
      "topic": 19,
      "similarity": 0.8288866363939301
    },
    {
      "doc": 286,
      "topic": 20,
      "similarity": 0.7605536276811243
    },
    {
      "doc": 286,
      "topic": 21,
      "similarity": 0.8067211717192048
    },
    {
      "doc": 286,
      "topic": 23,
      "similarity": 0.750432497164963
    },
    {
      "doc": 286,
      "topic": 24,
      "similarity": 0.7651278007667738
    },
    {
      "doc": 287,
      "topic": 2,
      "similarity": 0.7584676527024911
    },
    {
      "doc": 287,
      "topic": 3,
      "similarity": 0.790991972947035
    },
    {
      "doc": 287,
      "topic": 4,
      "similarity": 0.7699156427915719
    },
    {
      "doc": 287,
      "topic": 5,
      "similarity": 0.800160118130399
    },
    {
      "doc": 287,
      "topic": 7,
      "similarity": 0.7899299066142913
    },
    {
      "doc": 287,
      "topic": 8,
      "similarity": 0.7924690192174144
    },
    {
      "doc": 287,
      "topic": 9,
      "similarity": 0.8233060822613102
    },
    {
      "doc": 287,
      "topic": 10,
      "similarity": 0.8241354440026686
    },
    {
      "doc": 287,
      "topic": 11,
      "similarity": 0.7739793258361256
    },
    {
      "doc": 287,
      "topic": 13,
      "similarity": 0.7565222856417554
    },
    {
      "doc": 287,
      "topic": 14,
      "similarity": 0.763233116885647
    },
    {
      "doc": 287,
      "topic": 15,
      "similarity": 0.7807972696128289
    },
    {
      "doc": 287,
      "topic": 16,
      "similarity": 0.8072831208562002
    },
    {
      "doc": 287,
      "topic": 17,
      "similarity": 0.7946375877846001
    },
    {
      "doc": 287,
      "topic": 18,
      "similarity": 0.7606227387661826
    },
    {
      "doc": 287,
      "topic": 19,
      "similarity": 0.7921501591218698
    },
    {
      "doc": 287,
      "topic": 20,
      "similarity": 0.7772419141960013
    },
    {
      "doc": 287,
      "topic": 21,
      "similarity": 0.8061539594320966
    },
    {
      "doc": 287,
      "topic": 24,
      "similarity": 0.7501082239845351
    },
    {
      "doc": 288,
      "topic": 1,
      "similarity": 0.7675277865563522
    },
    {
      "doc": 288,
      "topic": 2,
      "similarity": 0.7970804630322198
    },
    {
      "doc": 288,
      "topic": 3,
      "similarity": 0.8118277538229138
    },
    {
      "doc": 288,
      "topic": 4,
      "similarity": 0.7618218714128168
    },
    {
      "doc": 288,
      "topic": 5,
      "similarity": 0.8061565601256389
    },
    {
      "doc": 288,
      "topic": 7,
      "similarity": 0.7829545838829246
    },
    {
      "doc": 288,
      "topic": 8,
      "similarity": 0.8084744737565911
    },
    {
      "doc": 288,
      "topic": 9,
      "similarity": 0.8235406681305194
    },
    {
      "doc": 288,
      "topic": 10,
      "similarity": 0.803562901520466
    },
    {
      "doc": 288,
      "topic": 11,
      "similarity": 0.8003698503576459
    },
    {
      "doc": 288,
      "topic": 13,
      "similarity": 0.7930864281634277
    },
    {
      "doc": 288,
      "topic": 14,
      "similarity": 0.8199655159594439
    },
    {
      "doc": 288,
      "topic": 15,
      "similarity": 0.7971236950787298
    },
    {
      "doc": 288,
      "topic": 16,
      "similarity": 0.8238872699965126
    },
    {
      "doc": 288,
      "topic": 17,
      "similarity": 0.8194811690421365
    },
    {
      "doc": 288,
      "topic": 18,
      "similarity": 0.8016515774018763
    },
    {
      "doc": 288,
      "topic": 19,
      "similarity": 0.8609979525935225
    },
    {
      "doc": 288,
      "topic": 20,
      "similarity": 0.7835951193395297
    },
    {
      "doc": 288,
      "topic": 21,
      "similarity": 0.8285998175576056
    },
    {
      "doc": 288,
      "topic": 23,
      "similarity": 0.7608436313059143
    },
    {
      "doc": 289,
      "topic": 3,
      "similarity": 0.7593249259305203
    },
    {
      "doc": 289,
      "topic": 5,
      "similarity": 0.7716160182400126
    },
    {
      "doc": 289,
      "topic": 6,
      "similarity": 0.7535330492181262
    },
    {
      "doc": 289,
      "topic": 8,
      "similarity": 0.7554415196267535
    },
    {
      "doc": 289,
      "topic": 9,
      "similarity": 0.784265954643699
    },
    {
      "doc": 289,
      "topic": 11,
      "similarity": 0.7621300785244417
    },
    {
      "doc": 289,
      "topic": 15,
      "similarity": 0.805130779287883
    },
    {
      "doc": 289,
      "topic": 16,
      "similarity": 0.7562513758241539
    },
    {
      "doc": 289,
      "topic": 17,
      "similarity": 0.7822433877187307
    },
    {
      "doc": 289,
      "topic": 18,
      "similarity": 0.7556345379794935
    },
    {
      "doc": 289,
      "topic": 19,
      "similarity": 0.7749127319626122
    },
    {
      "doc": 289,
      "topic": 20,
      "similarity": 0.7629682611743218
    },
    {
      "doc": 289,
      "topic": 21,
      "similarity": 0.7652342060996241
    },
    {
      "doc": 289,
      "topic": 23,
      "similarity": 0.7500214549035186
    },
    {
      "doc": 289,
      "topic": 24,
      "similarity": 0.7599462179663548
    },
    {
      "doc": 290,
      "topic": 1,
      "similarity": 0.7697169328261886
    },
    {
      "doc": 290,
      "topic": 2,
      "similarity": 0.7899451697319042
    },
    {
      "doc": 290,
      "topic": 3,
      "similarity": 0.8113896723028989
    },
    {
      "doc": 290,
      "topic": 4,
      "similarity": 0.7692235714748201
    },
    {
      "doc": 290,
      "topic": 5,
      "similarity": 0.8172125062742224
    },
    {
      "doc": 290,
      "topic": 7,
      "similarity": 0.7991010894737111
    },
    {
      "doc": 290,
      "topic": 8,
      "similarity": 0.80569851285823
    },
    {
      "doc": 290,
      "topic": 9,
      "similarity": 0.8303899197305764
    },
    {
      "doc": 290,
      "topic": 10,
      "similarity": 0.7870978979279447
    },
    {
      "doc": 290,
      "topic": 11,
      "similarity": 0.778601938092831
    },
    {
      "doc": 290,
      "topic": 13,
      "similarity": 0.7973119757322287
    },
    {
      "doc": 290,
      "topic": 14,
      "similarity": 0.7958885810888112
    },
    {
      "doc": 290,
      "topic": 15,
      "similarity": 0.7730366233025013
    },
    {
      "doc": 290,
      "topic": 16,
      "similarity": 0.803150660992147
    },
    {
      "doc": 290,
      "topic": 17,
      "similarity": 0.792319026735247
    },
    {
      "doc": 290,
      "topic": 18,
      "similarity": 0.756289331563309
    },
    {
      "doc": 290,
      "topic": 19,
      "similarity": 0.8140561623378177
    },
    {
      "doc": 290,
      "topic": 20,
      "similarity": 0.7777021944615644
    },
    {
      "doc": 290,
      "topic": 21,
      "similarity": 0.8075461571405126
    },
    {
      "doc": 290,
      "topic": 22,
      "similarity": 0.7604681450481285
    },
    {
      "doc": 290,
      "topic": 23,
      "similarity": 0.7588882817564727
    },
    {
      "doc": 291,
      "topic": 5,
      "similarity": 0.7623428642277807
    },
    {
      "doc": 291,
      "topic": 7,
      "similarity": 0.7620647578319482
    },
    {
      "doc": 291,
      "topic": 9,
      "similarity": 0.8008025610810098
    },
    {
      "doc": 291,
      "topic": 16,
      "similarity": 0.7768224841194762
    },
    {
      "doc": 291,
      "topic": 19,
      "similarity": 0.7533411119738964
    },
    {
      "doc": 291,
      "topic": 20,
      "similarity": 0.7526522521813248
    },
    {
      "doc": 292,
      "topic": 1,
      "similarity": 0.7719218518053335
    },
    {
      "doc": 292,
      "topic": 2,
      "similarity": 0.7745566726762176
    },
    {
      "doc": 292,
      "topic": 3,
      "similarity": 0.8020698119690663
    },
    {
      "doc": 292,
      "topic": 4,
      "similarity": 0.7696910092831942
    },
    {
      "doc": 292,
      "topic": 5,
      "similarity": 0.8164007430532633
    },
    {
      "doc": 292,
      "topic": 7,
      "similarity": 0.787390634371062
    },
    {
      "doc": 292,
      "topic": 8,
      "similarity": 0.7900639906555545
    },
    {
      "doc": 292,
      "topic": 9,
      "similarity": 0.8257671867544515
    },
    {
      "doc": 292,
      "topic": 10,
      "similarity": 0.7825330614537287
    },
    {
      "doc": 292,
      "topic": 11,
      "similarity": 0.7897833050923987
    },
    {
      "doc": 292,
      "topic": 13,
      "similarity": 0.7817402297273249
    },
    {
      "doc": 292,
      "topic": 14,
      "similarity": 0.7740225885800702
    },
    {
      "doc": 292,
      "topic": 15,
      "similarity": 0.7863644576539071
    },
    {
      "doc": 292,
      "topic": 16,
      "similarity": 0.8139975820497436
    },
    {
      "doc": 292,
      "topic": 17,
      "similarity": 0.8112773549830608
    },
    {
      "doc": 292,
      "topic": 19,
      "similarity": 0.7980521380530194
    },
    {
      "doc": 292,
      "topic": 20,
      "similarity": 0.7719347521762571
    },
    {
      "doc": 292,
      "topic": 21,
      "similarity": 0.8093317302214559
    },
    {
      "doc": 292,
      "topic": 23,
      "similarity": 0.752639170807004
    },
    {
      "doc": 292,
      "topic": 24,
      "similarity": 0.7715045789791405
    },
    {
      "doc": 293,
      "topic": 2,
      "similarity": 0.7551366772724405
    },
    {
      "doc": 293,
      "topic": 3,
      "similarity": 0.7734598282160029
    },
    {
      "doc": 293,
      "topic": 5,
      "similarity": 0.773381823728579
    },
    {
      "doc": 293,
      "topic": 8,
      "similarity": 0.7522824341864784
    },
    {
      "doc": 293,
      "topic": 9,
      "similarity": 0.807547487805624
    },
    {
      "doc": 293,
      "topic": 10,
      "similarity": 0.7551897473518268
    },
    {
      "doc": 293,
      "topic": 11,
      "similarity": 0.7679077687890954
    },
    {
      "doc": 293,
      "topic": 13,
      "similarity": 0.7534890696527454
    },
    {
      "doc": 293,
      "topic": 14,
      "similarity": 0.7606547295885203
    },
    {
      "doc": 293,
      "topic": 15,
      "similarity": 0.7733756966139511
    },
    {
      "doc": 293,
      "topic": 16,
      "similarity": 0.7828946521323176
    },
    {
      "doc": 293,
      "topic": 17,
      "similarity": 0.7898146541043068
    },
    {
      "doc": 293,
      "topic": 18,
      "similarity": 0.7521106809802692
    },
    {
      "doc": 293,
      "topic": 19,
      "similarity": 0.809099007653275
    },
    {
      "doc": 293,
      "topic": 20,
      "similarity": 0.7816275994527621
    },
    {
      "doc": 293,
      "topic": 21,
      "similarity": 0.7834130958710604
    },
    {
      "doc": 293,
      "topic": 23,
      "similarity": 0.7503245934013258
    },
    {
      "doc": 294,
      "topic": 0,
      "similarity": 0.7605408318363631
    },
    {
      "doc": 294,
      "topic": 1,
      "similarity": 0.7829238646109246
    },
    {
      "doc": 294,
      "topic": 2,
      "similarity": 0.8015970976929334
    },
    {
      "doc": 294,
      "topic": 3,
      "similarity": 0.7969743924897325
    },
    {
      "doc": 294,
      "topic": 5,
      "similarity": 0.7690118687732443
    },
    {
      "doc": 294,
      "topic": 7,
      "similarity": 0.7656971956275527
    },
    {
      "doc": 294,
      "topic": 8,
      "similarity": 0.7722163073035432
    },
    {
      "doc": 294,
      "topic": 9,
      "similarity": 0.8149925745040434
    },
    {
      "doc": 294,
      "topic": 10,
      "similarity": 0.7749253942592105
    },
    {
      "doc": 294,
      "topic": 11,
      "similarity": 0.7579417276003697
    },
    {
      "doc": 294,
      "topic": 12,
      "similarity": 0.7657007289809397
    },
    {
      "doc": 294,
      "topic": 13,
      "similarity": 0.7727694677208288
    },
    {
      "doc": 294,
      "topic": 14,
      "similarity": 0.7625031044690781
    },
    {
      "doc": 294,
      "topic": 15,
      "similarity": 0.7793296370767474
    },
    {
      "doc": 294,
      "topic": 16,
      "similarity": 0.7945470228985053
    },
    {
      "doc": 294,
      "topic": 17,
      "similarity": 0.787632727742184
    },
    {
      "doc": 294,
      "topic": 19,
      "similarity": 0.7903663365560808
    },
    {
      "doc": 294,
      "topic": 20,
      "similarity": 0.7740484261162612
    },
    {
      "doc": 294,
      "topic": 21,
      "similarity": 0.7782355215181089
    },
    {
      "doc": 294,
      "topic": 23,
      "similarity": 0.7535021812608944
    },
    {
      "doc": 295,
      "topic": 3,
      "similarity": 0.7642856360979922
    },
    {
      "doc": 295,
      "topic": 5,
      "similarity": 0.7639797616342544
    },
    {
      "doc": 295,
      "topic": 8,
      "similarity": 0.7535344134287373
    },
    {
      "doc": 295,
      "topic": 9,
      "similarity": 0.8199721139172247
    },
    {
      "doc": 295,
      "topic": 11,
      "similarity": 0.7540473887400849
    },
    {
      "doc": 295,
      "topic": 12,
      "similarity": 0.7955141386652718
    },
    {
      "doc": 295,
      "topic": 15,
      "similarity": 0.7719175206264678
    },
    {
      "doc": 295,
      "topic": 16,
      "similarity": 0.770787401856206
    },
    {
      "doc": 295,
      "topic": 17,
      "similarity": 0.7952496898245126
    },
    {
      "doc": 295,
      "topic": 18,
      "similarity": 0.7686178313087098
    },
    {
      "doc": 295,
      "topic": 19,
      "similarity": 0.7743144826091993
    },
    {
      "doc": 295,
      "topic": 20,
      "similarity": 0.7815753346984159
    },
    {
      "doc": 295,
      "topic": 21,
      "similarity": 0.7902373021012686
    },
    {
      "doc": 295,
      "topic": 24,
      "similarity": 0.7793643928359584
    },
    {
      "doc": 296,
      "topic": 3,
      "similarity": 0.7626183369678778
    },
    {
      "doc": 296,
      "topic": 5,
      "similarity": 0.7706209156190749
    },
    {
      "doc": 296,
      "topic": 9,
      "similarity": 0.7699798671728321
    },
    {
      "doc": 296,
      "topic": 11,
      "similarity": 0.7531517324679488
    },
    {
      "doc": 296,
      "topic": 16,
      "similarity": 0.7782000027543504
    },
    {
      "doc": 296,
      "topic": 17,
      "similarity": 0.7606723245291697
    },
    {
      "doc": 296,
      "topic": 19,
      "similarity": 0.7736117975466648
    },
    {
      "doc": 296,
      "topic": 21,
      "similarity": 0.7753537814699752
    },
    {
      "doc": 296,
      "topic": 24,
      "similarity": 0.7559446041741084
    },
    {
      "doc": 297,
      "topic": 3,
      "similarity": 0.7859993399680739
    },
    {
      "doc": 297,
      "topic": 5,
      "similarity": 0.779459709850237
    },
    {
      "doc": 297,
      "topic": 7,
      "similarity": 0.771185150713445
    },
    {
      "doc": 297,
      "topic": 8,
      "similarity": 0.7516273720464075
    },
    {
      "doc": 297,
      "topic": 9,
      "similarity": 0.801619946931836
    },
    {
      "doc": 297,
      "topic": 10,
      "similarity": 0.7601396479842057
    },
    {
      "doc": 297,
      "topic": 11,
      "similarity": 0.7587131671484156
    },
    {
      "doc": 297,
      "topic": 15,
      "similarity": 0.7508114142711507
    },
    {
      "doc": 297,
      "topic": 16,
      "similarity": 0.7876662180341157
    },
    {
      "doc": 297,
      "topic": 17,
      "similarity": 0.7739246576641569
    },
    {
      "doc": 297,
      "topic": 19,
      "similarity": 0.7796799726657431
    },
    {
      "doc": 297,
      "topic": 20,
      "similarity": 0.7814247857198513
    },
    {
      "doc": 297,
      "topic": 21,
      "similarity": 0.7959679871895872
    },
    {
      "doc": 297,
      "topic": 23,
      "similarity": 0.7666138525119844
    },
    {
      "doc": 297,
      "topic": 24,
      "similarity": 0.7665721162955484
    },
    {
      "doc": 298,
      "topic": 2,
      "similarity": 0.7532210427169488
    },
    {
      "doc": 298,
      "topic": 3,
      "similarity": 0.7854978493532485
    },
    {
      "doc": 298,
      "topic": 5,
      "similarity": 0.7673763500004417
    },
    {
      "doc": 298,
      "topic": 7,
      "similarity": 0.7949402977931931
    },
    {
      "doc": 298,
      "topic": 8,
      "similarity": 0.7896639875025064
    },
    {
      "doc": 298,
      "topic": 9,
      "similarity": 0.8258413977943772
    },
    {
      "doc": 298,
      "topic": 10,
      "similarity": 0.7602781263299758
    },
    {
      "doc": 298,
      "topic": 11,
      "similarity": 0.7809572933140013
    },
    {
      "doc": 298,
      "topic": 13,
      "similarity": 0.7594392146895007
    },
    {
      "doc": 298,
      "topic": 14,
      "similarity": 0.7568314290904747
    },
    {
      "doc": 298,
      "topic": 15,
      "similarity": 0.7703903507465251
    },
    {
      "doc": 298,
      "topic": 16,
      "similarity": 0.7860803009773595
    },
    {
      "doc": 298,
      "topic": 17,
      "similarity": 0.7927944025636031
    },
    {
      "doc": 298,
      "topic": 18,
      "similarity": 0.7549664082869166
    },
    {
      "doc": 298,
      "topic": 19,
      "similarity": 0.7760104360300708
    },
    {
      "doc": 298,
      "topic": 20,
      "similarity": 0.7609550368786068
    },
    {
      "doc": 298,
      "topic": 21,
      "similarity": 0.7913854655553987
    },
    {
      "doc": 299,
      "topic": 3,
      "similarity": 0.7571539475022883
    },
    {
      "doc": 299,
      "topic": 5,
      "similarity": 0.776093274186746
    },
    {
      "doc": 299,
      "topic": 9,
      "similarity": 0.783424134804477
    },
    {
      "doc": 299,
      "topic": 17,
      "similarity": 0.7708785008561244
    },
    {
      "doc": 299,
      "topic": 19,
      "similarity": 0.7576889436049834
    },
    {
      "doc": 299,
      "topic": 20,
      "similarity": 0.7551275285059889
    },
    {
      "doc": 299,
      "topic": 21,
      "similarity": 0.7625874392815664
    },
    {
      "doc": 299,
      "topic": 24,
      "similarity": 0.7785082456891168
    },
    {
      "doc": 300,
      "topic": 2,
      "similarity": 0.7858655712787306
    },
    {
      "doc": 300,
      "topic": 3,
      "similarity": 0.8090301150206103
    },
    {
      "doc": 300,
      "topic": 4,
      "similarity": 0.7786090689775577
    },
    {
      "doc": 300,
      "topic": 5,
      "similarity": 0.7918498044978237
    },
    {
      "doc": 300,
      "topic": 7,
      "similarity": 0.7848142582064979
    },
    {
      "doc": 300,
      "topic": 8,
      "similarity": 0.7750934356151012
    },
    {
      "doc": 300,
      "topic": 9,
      "similarity": 0.8422134500326636
    },
    {
      "doc": 300,
      "topic": 10,
      "similarity": 0.8322678210425297
    },
    {
      "doc": 300,
      "topic": 11,
      "similarity": 0.7958357138957434
    },
    {
      "doc": 300,
      "topic": 13,
      "similarity": 0.764231308429973
    },
    {
      "doc": 300,
      "topic": 14,
      "similarity": 0.7697460724655038
    },
    {
      "doc": 300,
      "topic": 15,
      "similarity": 0.790020024226231
    },
    {
      "doc": 300,
      "topic": 16,
      "similarity": 0.8009434807170126
    },
    {
      "doc": 300,
      "topic": 17,
      "similarity": 0.807913483560177
    },
    {
      "doc": 300,
      "topic": 18,
      "similarity": 0.7976330621002564
    },
    {
      "doc": 300,
      "topic": 19,
      "similarity": 0.8200538205096264
    },
    {
      "doc": 300,
      "topic": 20,
      "similarity": 0.7812391554209763
    },
    {
      "doc": 300,
      "topic": 21,
      "similarity": 0.8266846398289096
    },
    {
      "doc": 300,
      "topic": 23,
      "similarity": 0.7521984790624582
    },
    {
      "doc": 300,
      "topic": 24,
      "similarity": 0.7838715289434323
    },
    {
      "doc": 301,
      "topic": 3,
      "similarity": 0.7902088984712563
    },
    {
      "doc": 301,
      "topic": 4,
      "similarity": 0.7578631144542116
    },
    {
      "doc": 301,
      "topic": 5,
      "similarity": 0.7991073588512996
    },
    {
      "doc": 301,
      "topic": 7,
      "similarity": 0.7642719436309029
    },
    {
      "doc": 301,
      "topic": 8,
      "similarity": 0.7555103043969268
    },
    {
      "doc": 301,
      "topic": 9,
      "similarity": 0.8045138912133376
    },
    {
      "doc": 301,
      "topic": 10,
      "similarity": 0.7728151914060747
    },
    {
      "doc": 301,
      "topic": 11,
      "similarity": 0.7785570432150086
    },
    {
      "doc": 301,
      "topic": 13,
      "similarity": 0.7628145483650743
    },
    {
      "doc": 301,
      "topic": 15,
      "similarity": 0.7622782916143637
    },
    {
      "doc": 301,
      "topic": 16,
      "similarity": 0.8095688306865542
    },
    {
      "doc": 301,
      "topic": 17,
      "similarity": 0.7944854908203854
    },
    {
      "doc": 301,
      "topic": 19,
      "similarity": 0.7967152359034788
    },
    {
      "doc": 301,
      "topic": 20,
      "similarity": 0.79989378054624
    },
    {
      "doc": 301,
      "topic": 21,
      "similarity": 0.8038599415685883
    },
    {
      "doc": 301,
      "topic": 23,
      "similarity": 0.7566034003991766
    },
    {
      "doc": 302,
      "topic": 1,
      "similarity": 0.7702564052951549
    },
    {
      "doc": 302,
      "topic": 2,
      "similarity": 0.8108413786606334
    },
    {
      "doc": 302,
      "topic": 3,
      "similarity": 0.8036733140911978
    },
    {
      "doc": 302,
      "topic": 4,
      "similarity": 0.7653408081562663
    },
    {
      "doc": 302,
      "topic": 5,
      "similarity": 0.8189014899406911
    },
    {
      "doc": 302,
      "topic": 7,
      "similarity": 0.78930039501722
    },
    {
      "doc": 302,
      "topic": 8,
      "similarity": 0.7930376329721451
    },
    {
      "doc": 302,
      "topic": 9,
      "similarity": 0.8256901849369275
    },
    {
      "doc": 302,
      "topic": 10,
      "similarity": 0.798406753288847
    },
    {
      "doc": 302,
      "topic": 11,
      "similarity": 0.8055863387888308
    },
    {
      "doc": 302,
      "topic": 13,
      "similarity": 0.7811698751592644
    },
    {
      "doc": 302,
      "topic": 14,
      "similarity": 0.8036473803114743
    },
    {
      "doc": 302,
      "topic": 15,
      "similarity": 0.8087586175376881
    },
    {
      "doc": 302,
      "topic": 16,
      "similarity": 0.8128103133381793
    },
    {
      "doc": 302,
      "topic": 17,
      "similarity": 0.8317712372656314
    },
    {
      "doc": 302,
      "topic": 18,
      "similarity": 0.7770910389660741
    },
    {
      "doc": 302,
      "topic": 19,
      "similarity": 0.8243132481677623
    },
    {
      "doc": 302,
      "topic": 20,
      "similarity": 0.7863602191225549
    },
    {
      "doc": 302,
      "topic": 21,
      "similarity": 0.8191025398436171
    },
    {
      "doc": 302,
      "topic": 23,
      "similarity": 0.764926878123894
    },
    {
      "doc": 303,
      "topic": 1,
      "similarity": 0.7807685466995021
    },
    {
      "doc": 303,
      "topic": 2,
      "similarity": 0.7686426335806805
    },
    {
      "doc": 303,
      "topic": 3,
      "similarity": 0.8057515737691965
    },
    {
      "doc": 303,
      "topic": 4,
      "similarity": 0.7610104489333426
    },
    {
      "doc": 303,
      "topic": 5,
      "similarity": 0.7792289182539445
    },
    {
      "doc": 303,
      "topic": 7,
      "similarity": 0.7820436209282853
    },
    {
      "doc": 303,
      "topic": 8,
      "similarity": 0.7820024137696848
    },
    {
      "doc": 303,
      "topic": 9,
      "similarity": 0.8260674300828507
    },
    {
      "doc": 303,
      "topic": 10,
      "similarity": 0.7872086370161236
    },
    {
      "doc": 303,
      "topic": 11,
      "similarity": 0.7858119261818489
    },
    {
      "doc": 303,
      "topic": 13,
      "similarity": 0.7694573059327628
    },
    {
      "doc": 303,
      "topic": 14,
      "similarity": 0.7681721900788632
    },
    {
      "doc": 303,
      "topic": 15,
      "similarity": 0.7704054915829393
    },
    {
      "doc": 303,
      "topic": 16,
      "similarity": 0.8012464789529486
    },
    {
      "doc": 303,
      "topic": 17,
      "similarity": 0.7887084154017528
    },
    {
      "doc": 303,
      "topic": 18,
      "similarity": 0.7560900057069386
    },
    {
      "doc": 303,
      "topic": 19,
      "similarity": 0.7955698174988292
    },
    {
      "doc": 303,
      "topic": 20,
      "similarity": 0.7690068235151631
    },
    {
      "doc": 303,
      "topic": 21,
      "similarity": 0.8080085652045752
    },
    {
      "doc": 303,
      "topic": 24,
      "similarity": 0.7595589874806137
    },
    {
      "doc": 304,
      "topic": 1,
      "similarity": 0.7570005660398058
    },
    {
      "doc": 304,
      "topic": 2,
      "similarity": 0.7999664308506172
    },
    {
      "doc": 304,
      "topic": 3,
      "similarity": 0.7929253653024778
    },
    {
      "doc": 304,
      "topic": 4,
      "similarity": 0.7528448107739776
    },
    {
      "doc": 304,
      "topic": 5,
      "similarity": 0.802324073821125
    },
    {
      "doc": 304,
      "topic": 7,
      "similarity": 0.7916466841928078
    },
    {
      "doc": 304,
      "topic": 8,
      "similarity": 0.7808520007498287
    },
    {
      "doc": 304,
      "topic": 9,
      "similarity": 0.8327138452175784
    },
    {
      "doc": 304,
      "topic": 10,
      "similarity": 0.7855935504079051
    },
    {
      "doc": 304,
      "topic": 11,
      "similarity": 0.7912479115853931
    },
    {
      "doc": 304,
      "topic": 13,
      "similarity": 0.7813541250426767
    },
    {
      "doc": 304,
      "topic": 14,
      "similarity": 0.7769389306029765
    },
    {
      "doc": 304,
      "topic": 15,
      "similarity": 0.8115665904842435
    },
    {
      "doc": 304,
      "topic": 16,
      "similarity": 0.8258432922181317
    },
    {
      "doc": 304,
      "topic": 17,
      "similarity": 0.8719829873789686
    },
    {
      "doc": 304,
      "topic": 18,
      "similarity": 0.7677841755688477
    },
    {
      "doc": 304,
      "topic": 19,
      "similarity": 0.8198995725056294
    },
    {
      "doc": 304,
      "topic": 20,
      "similarity": 0.7978523705914686
    },
    {
      "doc": 304,
      "topic": 21,
      "similarity": 0.8462217894517163
    },
    {
      "doc": 304,
      "topic": 23,
      "similarity": 0.752948259904981
    },
    {
      "doc": 304,
      "topic": 24,
      "similarity": 0.7601213977327468
    },
    {
      "doc": 305,
      "topic": 1,
      "similarity": 0.7674292515166896
    },
    {
      "doc": 305,
      "topic": 2,
      "similarity": 0.7833285045407732
    },
    {
      "doc": 305,
      "topic": 3,
      "similarity": 0.7984225323053574
    },
    {
      "doc": 305,
      "topic": 4,
      "similarity": 0.7510500397337363
    },
    {
      "doc": 305,
      "topic": 5,
      "similarity": 0.8117487491445232
    },
    {
      "doc": 305,
      "topic": 7,
      "similarity": 0.786301908987563
    },
    {
      "doc": 305,
      "topic": 8,
      "similarity": 0.7789698368541359
    },
    {
      "doc": 305,
      "topic": 9,
      "similarity": 0.8718003604051212
    },
    {
      "doc": 305,
      "topic": 10,
      "similarity": 0.7928353332583088
    },
    {
      "doc": 305,
      "topic": 11,
      "similarity": 0.7972620905767458
    },
    {
      "doc": 305,
      "topic": 12,
      "similarity": 0.7724722899490158
    },
    {
      "doc": 305,
      "topic": 13,
      "similarity": 0.7894667415247087
    },
    {
      "doc": 305,
      "topic": 14,
      "similarity": 0.7837805940587084
    },
    {
      "doc": 305,
      "topic": 15,
      "similarity": 0.7843010982723237
    },
    {
      "doc": 305,
      "topic": 16,
      "similarity": 0.8118090265144873
    },
    {
      "doc": 305,
      "topic": 17,
      "similarity": 0.8019649837110221
    },
    {
      "doc": 305,
      "topic": 18,
      "similarity": 0.7717145897237095
    },
    {
      "doc": 305,
      "topic": 19,
      "similarity": 0.816704281729781
    },
    {
      "doc": 305,
      "topic": 20,
      "similarity": 0.8053677281592768
    },
    {
      "doc": 305,
      "topic": 21,
      "similarity": 0.8039237569765881
    },
    {
      "doc": 305,
      "topic": 23,
      "similarity": 0.7606646376190593
    },
    {
      "doc": 305,
      "topic": 24,
      "similarity": 0.7864758318020825
    },
    {
      "doc": 306,
      "topic": 1,
      "similarity": 0.7734704984858878
    },
    {
      "doc": 306,
      "topic": 2,
      "similarity": 0.7933329090571526
    },
    {
      "doc": 306,
      "topic": 3,
      "similarity": 0.818374536346087
    },
    {
      "doc": 306,
      "topic": 4,
      "similarity": 0.7811965140749464
    },
    {
      "doc": 306,
      "topic": 5,
      "similarity": 0.8120777635558991
    },
    {
      "doc": 306,
      "topic": 6,
      "similarity": 0.750391264344973
    },
    {
      "doc": 306,
      "topic": 7,
      "similarity": 0.7990476399166746
    },
    {
      "doc": 306,
      "topic": 8,
      "similarity": 0.791983879306219
    },
    {
      "doc": 306,
      "topic": 9,
      "similarity": 0.8221392028550429
    },
    {
      "doc": 306,
      "topic": 10,
      "similarity": 0.8113801482458874
    },
    {
      "doc": 306,
      "topic": 11,
      "similarity": 0.8120075446047064
    },
    {
      "doc": 306,
      "topic": 13,
      "similarity": 0.7839571595815054
    },
    {
      "doc": 306,
      "topic": 14,
      "similarity": 0.804634078202083
    },
    {
      "doc": 306,
      "topic": 15,
      "similarity": 0.8151427267163907
    },
    {
      "doc": 306,
      "topic": 16,
      "similarity": 0.8268270744482978
    },
    {
      "doc": 306,
      "topic": 17,
      "similarity": 0.8213825849796903
    },
    {
      "doc": 306,
      "topic": 18,
      "similarity": 0.8066324111442554
    },
    {
      "doc": 306,
      "topic": 19,
      "similarity": 0.8652853279522235
    },
    {
      "doc": 306,
      "topic": 20,
      "similarity": 0.7906846335021823
    },
    {
      "doc": 306,
      "topic": 21,
      "similarity": 0.8025739094639119
    },
    {
      "doc": 306,
      "topic": 22,
      "similarity": 0.7529685595554108
    },
    {
      "doc": 306,
      "topic": 23,
      "similarity": 0.7500010713288303
    },
    {
      "doc": 307,
      "topic": 2,
      "similarity": 0.7655522381217921
    },
    {
      "doc": 307,
      "topic": 3,
      "similarity": 0.8096594236726732
    },
    {
      "doc": 307,
      "topic": 4,
      "similarity": 0.7604460854340932
    },
    {
      "doc": 307,
      "topic": 5,
      "similarity": 0.8090290145761247
    },
    {
      "doc": 307,
      "topic": 7,
      "similarity": 0.79083252827041
    },
    {
      "doc": 307,
      "topic": 8,
      "similarity": 0.7884356088851088
    },
    {
      "doc": 307,
      "topic": 9,
      "similarity": 0.8419061459964592
    },
    {
      "doc": 307,
      "topic": 10,
      "similarity": 0.7854644665287198
    },
    {
      "doc": 307,
      "topic": 11,
      "similarity": 0.7969766188404346
    },
    {
      "doc": 307,
      "topic": 12,
      "similarity": 0.755356701103939
    },
    {
      "doc": 307,
      "topic": 13,
      "similarity": 0.7763815023613482
    },
    {
      "doc": 307,
      "topic": 14,
      "similarity": 0.7616413039943221
    },
    {
      "doc": 307,
      "topic": 15,
      "similarity": 0.7960198690557125
    },
    {
      "doc": 307,
      "topic": 16,
      "similarity": 0.8173116711036593
    },
    {
      "doc": 307,
      "topic": 17,
      "similarity": 0.8265528304862603
    },
    {
      "doc": 307,
      "topic": 18,
      "similarity": 0.7932776698647601
    },
    {
      "doc": 307,
      "topic": 19,
      "similarity": 0.8290976016308127
    },
    {
      "doc": 307,
      "topic": 20,
      "similarity": 0.8218623747774442
    },
    {
      "doc": 307,
      "topic": 21,
      "similarity": 0.8307402711010031
    },
    {
      "doc": 307,
      "topic": 23,
      "similarity": 0.7645526575447892
    },
    {
      "doc": 307,
      "topic": 24,
      "similarity": 0.7813049894225015
    },
    {
      "doc": 308,
      "topic": 0,
      "similarity": 0.7905233733216955
    },
    {
      "doc": 308,
      "topic": 1,
      "similarity": 0.7693095360593138
    },
    {
      "doc": 308,
      "topic": 2,
      "similarity": 0.751458495094642
    },
    {
      "doc": 308,
      "topic": 3,
      "similarity": 0.7878312428070775
    },
    {
      "doc": 308,
      "topic": 5,
      "similarity": 0.7756218808955462
    },
    {
      "doc": 308,
      "topic": 7,
      "similarity": 0.7679257592180337
    },
    {
      "doc": 308,
      "topic": 8,
      "similarity": 0.7601325061193676
    },
    {
      "doc": 308,
      "topic": 9,
      "similarity": 0.8174984686290695
    },
    {
      "doc": 308,
      "topic": 10,
      "similarity": 0.7579201629862479
    },
    {
      "doc": 308,
      "topic": 11,
      "similarity": 0.8095562064449061
    },
    {
      "doc": 308,
      "topic": 13,
      "similarity": 0.7576914956482312
    },
    {
      "doc": 308,
      "topic": 15,
      "similarity": 0.7729292990942523
    },
    {
      "doc": 308,
      "topic": 16,
      "similarity": 0.7838176391325852
    },
    {
      "doc": 308,
      "topic": 17,
      "similarity": 0.7697904913935083
    },
    {
      "doc": 308,
      "topic": 19,
      "similarity": 0.7944342915259832
    },
    {
      "doc": 308,
      "topic": 20,
      "similarity": 0.7717421455575125
    },
    {
      "doc": 308,
      "topic": 21,
      "similarity": 0.7858232428821361
    },
    {
      "doc": 308,
      "topic": 23,
      "similarity": 0.758537378650532
    },
    {
      "doc": 309,
      "topic": 2,
      "similarity": 0.7718942581742623
    },
    {
      "doc": 309,
      "topic": 3,
      "similarity": 0.787112339594155
    },
    {
      "doc": 309,
      "topic": 5,
      "similarity": 0.7741788199985371
    },
    {
      "doc": 309,
      "topic": 7,
      "similarity": 0.7512581665786422
    },
    {
      "doc": 309,
      "topic": 8,
      "similarity": 0.7660376964793528
    },
    {
      "doc": 309,
      "topic": 9,
      "similarity": 0.8040283477020844
    },
    {
      "doc": 309,
      "topic": 11,
      "similarity": 0.7818180210995553
    },
    {
      "doc": 309,
      "topic": 12,
      "similarity": 0.7861772159586056
    },
    {
      "doc": 309,
      "topic": 14,
      "similarity": 0.7604755737955253
    },
    {
      "doc": 309,
      "topic": 15,
      "similarity": 0.7937239114199448
    },
    {
      "doc": 309,
      "topic": 16,
      "similarity": 0.7777647995038378
    },
    {
      "doc": 309,
      "topic": 17,
      "similarity": 0.7955366235218144
    },
    {
      "doc": 309,
      "topic": 18,
      "similarity": 0.7709607995751458
    },
    {
      "doc": 309,
      "topic": 19,
      "similarity": 0.7915356154794441
    },
    {
      "doc": 309,
      "topic": 20,
      "similarity": 0.7793231462189607
    },
    {
      "doc": 309,
      "topic": 21,
      "similarity": 0.7878573348830579
    },
    {
      "doc": 310,
      "topic": 3,
      "similarity": 0.7550905049981848
    },
    {
      "doc": 310,
      "topic": 5,
      "similarity": 0.782514676935657
    },
    {
      "doc": 310,
      "topic": 8,
      "similarity": 0.7895311772276664
    },
    {
      "doc": 310,
      "topic": 9,
      "similarity": 0.7972047221533856
    },
    {
      "doc": 310,
      "topic": 12,
      "similarity": 0.7703635744450585
    },
    {
      "doc": 310,
      "topic": 15,
      "similarity": 0.7695143165515096
    },
    {
      "doc": 310,
      "topic": 16,
      "similarity": 0.7802918617957821
    },
    {
      "doc": 310,
      "topic": 17,
      "similarity": 0.7727262397957595
    },
    {
      "doc": 310,
      "topic": 19,
      "similarity": 0.7547741732649975
    },
    {
      "doc": 310,
      "topic": 20,
      "similarity": 0.75283276239418
    },
    {
      "doc": 310,
      "topic": 21,
      "similarity": 0.7609985373063536
    },
    {
      "doc": 311,
      "topic": 1,
      "similarity": 0.758544762086905
    },
    {
      "doc": 311,
      "topic": 2,
      "similarity": 0.7790843846556175
    },
    {
      "doc": 311,
      "topic": 3,
      "similarity": 0.799670827624552
    },
    {
      "doc": 311,
      "topic": 4,
      "similarity": 0.7661967672624664
    },
    {
      "doc": 311,
      "topic": 5,
      "similarity": 0.7929199894608655
    },
    {
      "doc": 311,
      "topic": 7,
      "similarity": 0.7978234224716004
    },
    {
      "doc": 311,
      "topic": 8,
      "similarity": 0.7880145582636658
    },
    {
      "doc": 311,
      "topic": 9,
      "similarity": 0.8489839583247946
    },
    {
      "doc": 311,
      "topic": 10,
      "similarity": 0.7915224116987024
    },
    {
      "doc": 311,
      "topic": 11,
      "similarity": 0.8159861325334343
    },
    {
      "doc": 311,
      "topic": 12,
      "similarity": 0.7620816029351769
    },
    {
      "doc": 311,
      "topic": 13,
      "similarity": 0.774170392746862
    },
    {
      "doc": 311,
      "topic": 14,
      "similarity": 0.7747661096746529
    },
    {
      "doc": 311,
      "topic": 15,
      "similarity": 0.8170267138123594
    },
    {
      "doc": 311,
      "topic": 16,
      "similarity": 0.8189396556097129
    },
    {
      "doc": 311,
      "topic": 17,
      "similarity": 0.8325642449229012
    },
    {
      "doc": 311,
      "topic": 18,
      "similarity": 0.7720445530609056
    },
    {
      "doc": 311,
      "topic": 19,
      "similarity": 0.8056191243735443
    },
    {
      "doc": 311,
      "topic": 20,
      "similarity": 0.7963810174461395
    },
    {
      "doc": 311,
      "topic": 21,
      "similarity": 0.8222712000289966
    },
    {
      "doc": 311,
      "topic": 23,
      "similarity": 0.7521582007016565
    },
    {
      "doc": 311,
      "topic": 24,
      "similarity": 0.7521104525541119
    },
    {
      "doc": 312,
      "topic": 3,
      "similarity": 0.7905420713586112
    },
    {
      "doc": 312,
      "topic": 5,
      "similarity": 0.7708966024692873
    },
    {
      "doc": 312,
      "topic": 7,
      "similarity": 0.756211385746478
    },
    {
      "doc": 312,
      "topic": 9,
      "similarity": 0.805265048738532
    },
    {
      "doc": 312,
      "topic": 10,
      "similarity": 0.7776677495752832
    },
    {
      "doc": 312,
      "topic": 11,
      "similarity": 0.7616069945903633
    },
    {
      "doc": 312,
      "topic": 13,
      "similarity": 0.7557399894034443
    },
    {
      "doc": 312,
      "topic": 14,
      "similarity": 0.7611120847548504
    },
    {
      "doc": 312,
      "topic": 15,
      "similarity": 0.7548541839821543
    },
    {
      "doc": 312,
      "topic": 16,
      "similarity": 0.7927520515347265
    },
    {
      "doc": 312,
      "topic": 17,
      "similarity": 0.7570441832795911
    },
    {
      "doc": 312,
      "topic": 19,
      "similarity": 0.8038957326414411
    },
    {
      "doc": 312,
      "topic": 20,
      "similarity": 0.7807859012998567
    },
    {
      "doc": 312,
      "topic": 21,
      "similarity": 0.789076474420375
    },
    {
      "doc": 312,
      "topic": 23,
      "similarity": 0.7545387694306808
    },
    {
      "doc": 313,
      "topic": 2,
      "similarity": 0.75666844950579
    },
    {
      "doc": 313,
      "topic": 3,
      "similarity": 0.78773086362829
    },
    {
      "doc": 313,
      "topic": 4,
      "similarity": 0.7624578510220184
    },
    {
      "doc": 313,
      "topic": 5,
      "similarity": 0.7805474035749127
    },
    {
      "doc": 313,
      "topic": 6,
      "similarity": 0.75895872653685
    },
    {
      "doc": 313,
      "topic": 7,
      "similarity": 0.7692974873400217
    },
    {
      "doc": 313,
      "topic": 8,
      "similarity": 0.7776961034560368
    },
    {
      "doc": 313,
      "topic": 9,
      "similarity": 0.8159985665763659
    },
    {
      "doc": 313,
      "topic": 10,
      "similarity": 0.7844163915151615
    },
    {
      "doc": 313,
      "topic": 11,
      "similarity": 0.761353801642728
    },
    {
      "doc": 313,
      "topic": 12,
      "similarity": 0.7634765036888038
    },
    {
      "doc": 313,
      "topic": 13,
      "similarity": 0.76799311302957
    },
    {
      "doc": 313,
      "topic": 14,
      "similarity": 0.7623020924571189
    },
    {
      "doc": 313,
      "topic": 15,
      "similarity": 0.7950908491841322
    },
    {
      "doc": 313,
      "topic": 16,
      "similarity": 0.7830019971758114
    },
    {
      "doc": 313,
      "topic": 17,
      "similarity": 0.7970791146500263
    },
    {
      "doc": 313,
      "topic": 18,
      "similarity": 0.7691075126809617
    },
    {
      "doc": 313,
      "topic": 19,
      "similarity": 0.798249397743352
    },
    {
      "doc": 313,
      "topic": 20,
      "similarity": 0.7719756740736995
    },
    {
      "doc": 313,
      "topic": 21,
      "similarity": 0.7968939296089986
    },
    {
      "doc": 314,
      "topic": 2,
      "similarity": 0.7650766088669347
    },
    {
      "doc": 314,
      "topic": 3,
      "similarity": 0.798209428658675
    },
    {
      "doc": 314,
      "topic": 5,
      "similarity": 0.8085890224564961
    },
    {
      "doc": 314,
      "topic": 7,
      "similarity": 0.779586381764535
    },
    {
      "doc": 314,
      "topic": 8,
      "similarity": 0.7706513259626442
    },
    {
      "doc": 314,
      "topic": 9,
      "similarity": 0.8408088089471389
    },
    {
      "doc": 314,
      "topic": 10,
      "similarity": 0.784326294002446
    },
    {
      "doc": 314,
      "topic": 11,
      "similarity": 0.786035929635924
    },
    {
      "doc": 314,
      "topic": 13,
      "similarity": 0.7598300794148021
    },
    {
      "doc": 314,
      "topic": 14,
      "similarity": 0.7614748903069147
    },
    {
      "doc": 314,
      "topic": 15,
      "similarity": 0.7700056646876179
    },
    {
      "doc": 314,
      "topic": 16,
      "similarity": 0.802502471669658
    },
    {
      "doc": 314,
      "topic": 17,
      "similarity": 0.7945174602556526
    },
    {
      "doc": 314,
      "topic": 18,
      "similarity": 0.7743816050728813
    },
    {
      "doc": 314,
      "topic": 19,
      "similarity": 0.8031867937665195
    },
    {
      "doc": 314,
      "topic": 20,
      "similarity": 0.7744323320173814
    },
    {
      "doc": 314,
      "topic": 21,
      "similarity": 0.8101541065987298
    },
    {
      "doc": 314,
      "topic": 23,
      "similarity": 0.7527052039846741
    },
    {
      "doc": 314,
      "topic": 24,
      "similarity": 0.7758595446166888
    },
    {
      "doc": 315,
      "topic": 2,
      "similarity": 0.7545981292513892
    },
    {
      "doc": 315,
      "topic": 3,
      "similarity": 0.785336215056575
    },
    {
      "doc": 315,
      "topic": 5,
      "similarity": 0.7580692366393931
    },
    {
      "doc": 315,
      "topic": 7,
      "similarity": 0.7556317405474645
    },
    {
      "doc": 315,
      "topic": 8,
      "similarity": 0.7771307368815558
    },
    {
      "doc": 315,
      "topic": 9,
      "similarity": 0.7914550219589005
    },
    {
      "doc": 315,
      "topic": 10,
      "similarity": 0.7576314982101752
    },
    {
      "doc": 315,
      "topic": 11,
      "similarity": 0.7615482460857677
    },
    {
      "doc": 315,
      "topic": 14,
      "similarity": 0.7523229385319405
    },
    {
      "doc": 315,
      "topic": 15,
      "similarity": 0.7882659639050392
    },
    {
      "doc": 315,
      "topic": 16,
      "similarity": 0.7644269290009166
    },
    {
      "doc": 315,
      "topic": 17,
      "similarity": 0.7875790317784198
    },
    {
      "doc": 315,
      "topic": 18,
      "similarity": 0.7883507602474127
    },
    {
      "doc": 315,
      "topic": 19,
      "similarity": 0.8133284418724023
    },
    {
      "doc": 315,
      "topic": 21,
      "similarity": 0.7750429466924428
    },
    {
      "doc": 316,
      "topic": 1,
      "similarity": 0.7531334171648262
    },
    {
      "doc": 316,
      "topic": 2,
      "similarity": 0.7621856288713877
    },
    {
      "doc": 316,
      "topic": 3,
      "similarity": 0.796854334353664
    },
    {
      "doc": 316,
      "topic": 4,
      "similarity": 0.7601852865924785
    },
    {
      "doc": 316,
      "topic": 5,
      "similarity": 0.8011690122152875
    },
    {
      "doc": 316,
      "topic": 7,
      "similarity": 0.7984055923017453
    },
    {
      "doc": 316,
      "topic": 8,
      "similarity": 0.783145599097978
    },
    {
      "doc": 316,
      "topic": 9,
      "similarity": 0.8557850685676452
    },
    {
      "doc": 316,
      "topic": 10,
      "similarity": 0.7811149097922442
    },
    {
      "doc": 316,
      "topic": 11,
      "similarity": 0.7884766137130487
    },
    {
      "doc": 316,
      "topic": 12,
      "similarity": 0.7610826603883224
    },
    {
      "doc": 316,
      "topic": 13,
      "similarity": 0.7851866098895486
    },
    {
      "doc": 316,
      "topic": 14,
      "similarity": 0.7750527727796558
    },
    {
      "doc": 316,
      "topic": 15,
      "similarity": 0.7791781380478324
    },
    {
      "doc": 316,
      "topic": 16,
      "similarity": 0.8123329383834957
    },
    {
      "doc": 316,
      "topic": 17,
      "similarity": 0.8016273347895608
    },
    {
      "doc": 316,
      "topic": 18,
      "similarity": 0.7520045478617323
    },
    {
      "doc": 316,
      "topic": 19,
      "similarity": 0.7950931492688409
    },
    {
      "doc": 316,
      "topic": 20,
      "similarity": 0.7891477314895189
    },
    {
      "doc": 316,
      "topic": 21,
      "similarity": 0.8186353815953431
    },
    {
      "doc": 316,
      "topic": 24,
      "similarity": 0.7588101678574425
    },
    {
      "doc": 317,
      "topic": 2,
      "similarity": 0.7660463739221388
    },
    {
      "doc": 317,
      "topic": 3,
      "similarity": 0.8040538361339871
    },
    {
      "doc": 317,
      "topic": 4,
      "similarity": 0.7633841619791321
    },
    {
      "doc": 317,
      "topic": 5,
      "similarity": 0.8114931058215804
    },
    {
      "doc": 317,
      "topic": 7,
      "similarity": 0.7892042501000964
    },
    {
      "doc": 317,
      "topic": 8,
      "similarity": 0.7845274404383055
    },
    {
      "doc": 317,
      "topic": 9,
      "similarity": 0.834802741446131
    },
    {
      "doc": 317,
      "topic": 10,
      "similarity": 0.7816953869554936
    },
    {
      "doc": 317,
      "topic": 11,
      "similarity": 0.7836998618890483
    },
    {
      "doc": 317,
      "topic": 13,
      "similarity": 0.7631103850364231
    },
    {
      "doc": 317,
      "topic": 14,
      "similarity": 0.7849433424028502
    },
    {
      "doc": 317,
      "topic": 15,
      "similarity": 0.7754696726275209
    },
    {
      "doc": 317,
      "topic": 16,
      "similarity": 0.8059544731871049
    },
    {
      "doc": 317,
      "topic": 17,
      "similarity": 0.7957344138542385
    },
    {
      "doc": 317,
      "topic": 18,
      "similarity": 0.810217273931396
    },
    {
      "doc": 317,
      "topic": 19,
      "similarity": 0.8482807725678542
    },
    {
      "doc": 317,
      "topic": 20,
      "similarity": 0.777392725248641
    },
    {
      "doc": 317,
      "topic": 21,
      "similarity": 0.7886850045945148
    },
    {
      "doc": 317,
      "topic": 23,
      "similarity": 0.7612114167095129
    },
    {
      "doc": 317,
      "topic": 24,
      "similarity": 0.7610013290797518
    },
    {
      "doc": 318,
      "topic": 1,
      "similarity": 0.7648691494078376
    },
    {
      "doc": 318,
      "topic": 2,
      "similarity": 0.7771339904353194
    },
    {
      "doc": 318,
      "topic": 3,
      "similarity": 0.8084994924870991
    },
    {
      "doc": 318,
      "topic": 4,
      "similarity": 0.7774575040744406
    },
    {
      "doc": 318,
      "topic": 5,
      "similarity": 0.8008373328606326
    },
    {
      "doc": 318,
      "topic": 7,
      "similarity": 0.8128514327971089
    },
    {
      "doc": 318,
      "topic": 8,
      "similarity": 0.7919927067185217
    },
    {
      "doc": 318,
      "topic": 9,
      "similarity": 0.8439175295286414
    },
    {
      "doc": 318,
      "topic": 10,
      "similarity": 0.7825721025716076
    },
    {
      "doc": 318,
      "topic": 11,
      "similarity": 0.7862806130084772
    },
    {
      "doc": 318,
      "topic": 13,
      "similarity": 0.7849983963037197
    },
    {
      "doc": 318,
      "topic": 14,
      "similarity": 0.776391352736534
    },
    {
      "doc": 318,
      "topic": 15,
      "similarity": 0.7838733222979586
    },
    {
      "doc": 318,
      "topic": 16,
      "similarity": 0.8037883537213478
    },
    {
      "doc": 318,
      "topic": 17,
      "similarity": 0.7903848591580152
    },
    {
      "doc": 318,
      "topic": 19,
      "similarity": 0.8049086965965382
    },
    {
      "doc": 318,
      "topic": 20,
      "similarity": 0.7815789111552579
    },
    {
      "doc": 318,
      "topic": 21,
      "similarity": 0.8146339865560231
    },
    {
      "doc": 319,
      "topic": 1,
      "similarity": 0.7524978263834781
    },
    {
      "doc": 319,
      "topic": 2,
      "similarity": 0.776606864958781
    },
    {
      "doc": 319,
      "topic": 3,
      "similarity": 0.8000733998949582
    },
    {
      "doc": 319,
      "topic": 4,
      "similarity": 0.7567278311249965
    },
    {
      "doc": 319,
      "topic": 5,
      "similarity": 0.799476320681351
    },
    {
      "doc": 319,
      "topic": 7,
      "similarity": 0.7758077873691522
    },
    {
      "doc": 319,
      "topic": 8,
      "similarity": 0.7655850278930013
    },
    {
      "doc": 319,
      "topic": 9,
      "similarity": 0.8412636440508873
    },
    {
      "doc": 319,
      "topic": 10,
      "similarity": 0.7829538992370516
    },
    {
      "doc": 319,
      "topic": 11,
      "similarity": 0.7848044920830421
    },
    {
      "doc": 319,
      "topic": 12,
      "similarity": 0.7525393502270245
    },
    {
      "doc": 319,
      "topic": 13,
      "similarity": 0.7665172548655373
    },
    {
      "doc": 319,
      "topic": 14,
      "similarity": 0.770817757671508
    },
    {
      "doc": 319,
      "topic": 15,
      "similarity": 0.7803327353710836
    },
    {
      "doc": 319,
      "topic": 16,
      "similarity": 0.8176887854028009
    },
    {
      "doc": 319,
      "topic": 17,
      "similarity": 0.8020253153611493
    },
    {
      "doc": 319,
      "topic": 18,
      "similarity": 0.7526718975875716
    },
    {
      "doc": 319,
      "topic": 19,
      "similarity": 0.8171354710359218
    },
    {
      "doc": 319,
      "topic": 20,
      "similarity": 0.7873500683420757
    },
    {
      "doc": 319,
      "topic": 21,
      "similarity": 0.8067242397081746
    },
    {
      "doc": 319,
      "topic": 23,
      "similarity": 0.7719954082469396
    },
    {
      "doc": 319,
      "topic": 24,
      "similarity": 0.7649514557157366
    },
    {
      "doc": 320,
      "topic": 1,
      "similarity": 0.7698452914317195
    },
    {
      "doc": 320,
      "topic": 2,
      "similarity": 0.7755636091293017
    },
    {
      "doc": 320,
      "topic": 3,
      "similarity": 0.8207613889615251
    },
    {
      "doc": 320,
      "topic": 4,
      "similarity": 0.7513492182473375
    },
    {
      "doc": 320,
      "topic": 5,
      "similarity": 0.7996376966959883
    },
    {
      "doc": 320,
      "topic": 7,
      "similarity": 0.8010122350633394
    },
    {
      "doc": 320,
      "topic": 8,
      "similarity": 0.772021773241917
    },
    {
      "doc": 320,
      "topic": 9,
      "similarity": 0.8415461030706058
    },
    {
      "doc": 320,
      "topic": 10,
      "similarity": 0.7898430623342927
    },
    {
      "doc": 320,
      "topic": 11,
      "similarity": 0.791439851535648
    },
    {
      "doc": 320,
      "topic": 13,
      "similarity": 0.787275406672942
    },
    {
      "doc": 320,
      "topic": 14,
      "similarity": 0.7765854965247715
    },
    {
      "doc": 320,
      "topic": 15,
      "similarity": 0.7873437536288777
    },
    {
      "doc": 320,
      "topic": 16,
      "similarity": 0.7951028556104638
    },
    {
      "doc": 320,
      "topic": 17,
      "similarity": 0.7979665617892672
    },
    {
      "doc": 320,
      "topic": 18,
      "similarity": 0.7547942265940274
    },
    {
      "doc": 320,
      "topic": 19,
      "similarity": 0.8076073589686957
    },
    {
      "doc": 320,
      "topic": 20,
      "similarity": 0.7758043859976418
    },
    {
      "doc": 320,
      "topic": 21,
      "similarity": 0.810458609157745
    },
    {
      "doc": 320,
      "topic": 24,
      "similarity": 0.7642855964265898
    },
    {
      "doc": 321,
      "topic": 1,
      "similarity": 0.7814229248091992
    },
    {
      "doc": 321,
      "topic": 2,
      "similarity": 0.778134624876434
    },
    {
      "doc": 321,
      "topic": 3,
      "similarity": 0.7986138499949161
    },
    {
      "doc": 321,
      "topic": 4,
      "similarity": 0.7555697727986732
    },
    {
      "doc": 321,
      "topic": 5,
      "similarity": 0.7946199655092246
    },
    {
      "doc": 321,
      "topic": 6,
      "similarity": 0.7622362366291978
    },
    {
      "doc": 321,
      "topic": 7,
      "similarity": 0.7777324301755185
    },
    {
      "doc": 321,
      "topic": 8,
      "similarity": 0.7634022775137097
    },
    {
      "doc": 321,
      "topic": 9,
      "similarity": 0.8446396600024701
    },
    {
      "doc": 321,
      "topic": 10,
      "similarity": 0.7933646716309567
    },
    {
      "doc": 321,
      "topic": 11,
      "similarity": 0.7806398202319239
    },
    {
      "doc": 321,
      "topic": 13,
      "similarity": 0.7621894909499467
    },
    {
      "doc": 321,
      "topic": 14,
      "similarity": 0.7949580057590185
    },
    {
      "doc": 321,
      "topic": 15,
      "similarity": 0.7771612555858196
    },
    {
      "doc": 321,
      "topic": 16,
      "similarity": 0.8112354302012028
    },
    {
      "doc": 321,
      "topic": 17,
      "similarity": 0.7942277226319101
    },
    {
      "doc": 321,
      "topic": 18,
      "similarity": 0.7579104102922781
    },
    {
      "doc": 321,
      "topic": 19,
      "similarity": 0.8131467817643574
    },
    {
      "doc": 321,
      "topic": 20,
      "similarity": 0.7751383281327031
    },
    {
      "doc": 321,
      "topic": 21,
      "similarity": 0.8053321191623192
    },
    {
      "doc": 321,
      "topic": 23,
      "similarity": 0.8586230224957813
    },
    {
      "doc": 321,
      "topic": 24,
      "similarity": 0.7576603294729395
    },
    {
      "doc": 322,
      "topic": 3,
      "similarity": 0.7724254074498198
    },
    {
      "doc": 322,
      "topic": 4,
      "similarity": 0.7725232795223561
    },
    {
      "doc": 322,
      "topic": 5,
      "similarity": 0.7648925810494089
    },
    {
      "doc": 322,
      "topic": 7,
      "similarity": 0.7586103571285217
    },
    {
      "doc": 322,
      "topic": 9,
      "similarity": 0.7755565373030154
    },
    {
      "doc": 322,
      "topic": 16,
      "similarity": 0.7849782723076695
    },
    {
      "doc": 322,
      "topic": 17,
      "similarity": 0.7550256194773033
    },
    {
      "doc": 322,
      "topic": 19,
      "similarity": 0.7617321302577199
    },
    {
      "doc": 322,
      "topic": 21,
      "similarity": 0.7675929795216018
    },
    {
      "doc": 322,
      "topic": 24,
      "similarity": 0.7602239161053167
    },
    {
      "doc": 323,
      "topic": 1,
      "similarity": 0.7813851139416476
    },
    {
      "doc": 323,
      "topic": 2,
      "similarity": 0.7859343529936117
    },
    {
      "doc": 323,
      "topic": 3,
      "similarity": 0.839695806527555
    },
    {
      "doc": 323,
      "topic": 4,
      "similarity": 0.7647457669403247
    },
    {
      "doc": 323,
      "topic": 5,
      "similarity": 0.7913835682068316
    },
    {
      "doc": 323,
      "topic": 7,
      "similarity": 0.7896602193085104
    },
    {
      "doc": 323,
      "topic": 8,
      "similarity": 0.7717465591831031
    },
    {
      "doc": 323,
      "topic": 9,
      "similarity": 0.8128163132896016
    },
    {
      "doc": 323,
      "topic": 10,
      "similarity": 0.7823948200886951
    },
    {
      "doc": 323,
      "topic": 11,
      "similarity": 0.788182194795486
    },
    {
      "doc": 323,
      "topic": 13,
      "similarity": 0.7621165905225702
    },
    {
      "doc": 323,
      "topic": 14,
      "similarity": 0.7829517066197432
    },
    {
      "doc": 323,
      "topic": 15,
      "similarity": 0.7795665039464873
    },
    {
      "doc": 323,
      "topic": 16,
      "similarity": 0.8161437545261228
    },
    {
      "doc": 323,
      "topic": 17,
      "similarity": 0.7980164931677777
    },
    {
      "doc": 323,
      "topic": 19,
      "similarity": 0.8265478388472609
    },
    {
      "doc": 323,
      "topic": 20,
      "similarity": 0.7808823894273238
    },
    {
      "doc": 323,
      "topic": 21,
      "similarity": 0.8196901881754433
    },
    {
      "doc": 323,
      "topic": 23,
      "similarity": 0.7643306044332318
    },
    {
      "doc": 324,
      "topic": 1,
      "similarity": 0.7566997356330122
    },
    {
      "doc": 324,
      "topic": 2,
      "similarity": 0.7714711880632628
    },
    {
      "doc": 324,
      "topic": 3,
      "similarity": 0.7921162316908489
    },
    {
      "doc": 324,
      "topic": 4,
      "similarity": 0.7786724537149229
    },
    {
      "doc": 324,
      "topic": 5,
      "similarity": 0.7818343696541415
    },
    {
      "doc": 324,
      "topic": 7,
      "similarity": 0.7895072115176303
    },
    {
      "doc": 324,
      "topic": 8,
      "similarity": 0.7672338867383115
    },
    {
      "doc": 324,
      "topic": 9,
      "similarity": 0.8082353586786616
    },
    {
      "doc": 324,
      "topic": 10,
      "similarity": 0.7599954863494454
    },
    {
      "doc": 324,
      "topic": 11,
      "similarity": 0.7645115483032034
    },
    {
      "doc": 324,
      "topic": 13,
      "similarity": 0.7530953311564863
    },
    {
      "doc": 324,
      "topic": 14,
      "similarity": 0.7576676581265688
    },
    {
      "doc": 324,
      "topic": 15,
      "similarity": 0.753920553285939
    },
    {
      "doc": 324,
      "topic": 16,
      "similarity": 0.8069802075866582
    },
    {
      "doc": 324,
      "topic": 17,
      "similarity": 0.8011123733698351
    },
    {
      "doc": 324,
      "topic": 19,
      "similarity": 0.7814119000644211
    },
    {
      "doc": 324,
      "topic": 20,
      "similarity": 0.7871073546703747
    },
    {
      "doc": 324,
      "topic": 21,
      "similarity": 0.7911036752212501
    },
    {
      "doc": 325,
      "topic": 2,
      "similarity": 0.7674077478242722
    },
    {
      "doc": 325,
      "topic": 3,
      "similarity": 0.7803657161994428
    },
    {
      "doc": 325,
      "topic": 5,
      "similarity": 0.7858505750890594
    },
    {
      "doc": 325,
      "topic": 7,
      "similarity": 0.7571000294354366
    },
    {
      "doc": 325,
      "topic": 8,
      "similarity": 0.7695437778703353
    },
    {
      "doc": 325,
      "topic": 9,
      "similarity": 0.8185048146994647
    },
    {
      "doc": 325,
      "topic": 10,
      "similarity": 0.7816434991321923
    },
    {
      "doc": 325,
      "topic": 11,
      "similarity": 0.7779748885501057
    },
    {
      "doc": 325,
      "topic": 12,
      "similarity": 0.7628509232363093
    },
    {
      "doc": 325,
      "topic": 13,
      "similarity": 0.7579186295876269
    },
    {
      "doc": 325,
      "topic": 14,
      "similarity": 0.7670531307411944
    },
    {
      "doc": 325,
      "topic": 15,
      "similarity": 0.782757976159786
    },
    {
      "doc": 325,
      "topic": 16,
      "similarity": 0.8139882330064405
    },
    {
      "doc": 325,
      "topic": 17,
      "similarity": 0.8056241336742237
    },
    {
      "doc": 325,
      "topic": 18,
      "similarity": 0.791779090311664
    },
    {
      "doc": 325,
      "topic": 19,
      "similarity": 0.8508417723121826
    },
    {
      "doc": 325,
      "topic": 20,
      "similarity": 0.8083726031527696
    },
    {
      "doc": 325,
      "topic": 21,
      "similarity": 0.7964300380302846
    },
    {
      "doc": 325,
      "topic": 23,
      "similarity": 0.7583069931545457
    },
    {
      "doc": 325,
      "topic": 24,
      "similarity": 0.7644620645705188
    },
    {
      "doc": 326,
      "topic": 2,
      "similarity": 0.7514630603294209
    },
    {
      "doc": 326,
      "topic": 3,
      "similarity": 0.7684791908351317
    },
    {
      "doc": 326,
      "topic": 5,
      "similarity": 0.767616703729246
    },
    {
      "doc": 326,
      "topic": 7,
      "similarity": 0.7673591365145483
    },
    {
      "doc": 326,
      "topic": 9,
      "similarity": 0.8121171639563646
    },
    {
      "doc": 326,
      "topic": 11,
      "similarity": 0.7648619354933927
    },
    {
      "doc": 326,
      "topic": 15,
      "similarity": 0.7744964459950239
    },
    {
      "doc": 326,
      "topic": 16,
      "similarity": 0.7828724014587147
    },
    {
      "doc": 326,
      "topic": 17,
      "similarity": 0.7921957884470451
    },
    {
      "doc": 326,
      "topic": 19,
      "similarity": 0.7823871282470237
    },
    {
      "doc": 326,
      "topic": 20,
      "similarity": 0.7602639607096765
    },
    {
      "doc": 326,
      "topic": 21,
      "similarity": 0.7836614871984278
    },
    {
      "doc": 326,
      "topic": 24,
      "similarity": 0.7556645540491354
    },
    {
      "doc": 327,
      "topic": 1,
      "similarity": 0.7764569030569471
    },
    {
      "doc": 327,
      "topic": 2,
      "similarity": 0.7750196862940659
    },
    {
      "doc": 327,
      "topic": 3,
      "similarity": 0.8039041691918446
    },
    {
      "doc": 327,
      "topic": 5,
      "similarity": 0.8001174816852755
    },
    {
      "doc": 327,
      "topic": 7,
      "similarity": 0.7832287790014036
    },
    {
      "doc": 327,
      "topic": 8,
      "similarity": 0.7843562370000576
    },
    {
      "doc": 327,
      "topic": 9,
      "similarity": 0.8413130605578915
    },
    {
      "doc": 327,
      "topic": 10,
      "similarity": 0.793547186851458
    },
    {
      "doc": 327,
      "topic": 11,
      "similarity": 0.8108668630784217
    },
    {
      "doc": 327,
      "topic": 12,
      "similarity": 0.7623346128837537
    },
    {
      "doc": 327,
      "topic": 13,
      "similarity": 0.756008618442317
    },
    {
      "doc": 327,
      "topic": 14,
      "similarity": 0.7676318562030987
    },
    {
      "doc": 327,
      "topic": 15,
      "similarity": 0.8186686510641494
    },
    {
      "doc": 327,
      "topic": 16,
      "similarity": 0.8239090734269804
    },
    {
      "doc": 327,
      "topic": 17,
      "similarity": 0.8209197352874302
    },
    {
      "doc": 327,
      "topic": 18,
      "similarity": 0.7596507487285385
    },
    {
      "doc": 327,
      "topic": 19,
      "similarity": 0.8090419723409279
    },
    {
      "doc": 327,
      "topic": 20,
      "similarity": 0.7768812761588154
    },
    {
      "doc": 327,
      "topic": 21,
      "similarity": 0.8137799889545019
    },
    {
      "doc": 327,
      "topic": 23,
      "similarity": 0.7670936444556714
    },
    {
      "doc": 327,
      "topic": 24,
      "similarity": 0.7821703398817125
    },
    {
      "doc": 328,
      "topic": 2,
      "similarity": 0.7644394903639462
    },
    {
      "doc": 328,
      "topic": 3,
      "similarity": 0.7867008827696479
    },
    {
      "doc": 328,
      "topic": 4,
      "similarity": 0.7677614917783177
    },
    {
      "doc": 328,
      "topic": 5,
      "similarity": 0.7994254161093162
    },
    {
      "doc": 328,
      "topic": 6,
      "similarity": 0.7753001957148061
    },
    {
      "doc": 328,
      "topic": 7,
      "similarity": 0.7801887608580872
    },
    {
      "doc": 328,
      "topic": 8,
      "similarity": 0.7836680285472823
    },
    {
      "doc": 328,
      "topic": 9,
      "similarity": 0.8274351039777363
    },
    {
      "doc": 328,
      "topic": 10,
      "similarity": 0.7791073055732851
    },
    {
      "doc": 328,
      "topic": 11,
      "similarity": 0.7803024810248376
    },
    {
      "doc": 328,
      "topic": 12,
      "similarity": 0.7539075119729387
    },
    {
      "doc": 328,
      "topic": 13,
      "similarity": 0.7898705733319996
    },
    {
      "doc": 328,
      "topic": 14,
      "similarity": 0.7643399973712507
    },
    {
      "doc": 328,
      "topic": 15,
      "similarity": 0.7723183042172513
    },
    {
      "doc": 328,
      "topic": 16,
      "similarity": 0.8026345622744466
    },
    {
      "doc": 328,
      "topic": 17,
      "similarity": 0.8172836802928228
    },
    {
      "doc": 328,
      "topic": 18,
      "similarity": 0.7789766612334893
    },
    {
      "doc": 328,
      "topic": 19,
      "similarity": 0.806727736974502
    },
    {
      "doc": 328,
      "topic": 20,
      "similarity": 0.7719402169179505
    },
    {
      "doc": 328,
      "topic": 21,
      "similarity": 0.8141763139367337
    },
    {
      "doc": 328,
      "topic": 22,
      "similarity": 0.754931655046773
    },
    {
      "doc": 328,
      "topic": 23,
      "similarity": 0.7606166339412943
    },
    {
      "doc": 328,
      "topic": 24,
      "similarity": 0.7764247137464706
    },
    {
      "doc": 329,
      "topic": 1,
      "similarity": 0.753529208952513
    },
    {
      "doc": 329,
      "topic": 2,
      "similarity": 0.758778911723881
    },
    {
      "doc": 329,
      "topic": 3,
      "similarity": 0.7780077305758776
    },
    {
      "doc": 329,
      "topic": 5,
      "similarity": 0.7851736060070963
    },
    {
      "doc": 329,
      "topic": 6,
      "similarity": 0.7680803816095301
    },
    {
      "doc": 329,
      "topic": 7,
      "similarity": 0.7500636744069175
    },
    {
      "doc": 329,
      "topic": 8,
      "similarity": 0.7622959361398258
    },
    {
      "doc": 329,
      "topic": 9,
      "similarity": 0.7980463236027202
    },
    {
      "doc": 329,
      "topic": 10,
      "similarity": 0.7898494069412078
    },
    {
      "doc": 329,
      "topic": 11,
      "similarity": 0.7741464018417815
    },
    {
      "doc": 329,
      "topic": 13,
      "similarity": 0.7536614351668212
    },
    {
      "doc": 329,
      "topic": 14,
      "similarity": 0.7665329237592684
    },
    {
      "doc": 329,
      "topic": 15,
      "similarity": 0.7715034485545874
    },
    {
      "doc": 329,
      "topic": 16,
      "similarity": 0.8022329394151358
    },
    {
      "doc": 329,
      "topic": 17,
      "similarity": 0.7737116980227059
    },
    {
      "doc": 329,
      "topic": 18,
      "similarity": 0.7642709087168043
    },
    {
      "doc": 329,
      "topic": 19,
      "similarity": 0.8156861548880243
    },
    {
      "doc": 329,
      "topic": 20,
      "similarity": 0.7553783279810232
    },
    {
      "doc": 329,
      "topic": 21,
      "similarity": 0.7736378303759293
    },
    {
      "doc": 329,
      "topic": 23,
      "similarity": 0.8111256717691163
    },
    {
      "doc": 330,
      "topic": 2,
      "similarity": 0.7797071067411274
    },
    {
      "doc": 330,
      "topic": 3,
      "similarity": 0.7964898922223113
    },
    {
      "doc": 330,
      "topic": 5,
      "similarity": 0.7984482180325493
    },
    {
      "doc": 330,
      "topic": 6,
      "similarity": 0.7670052194180561
    },
    {
      "doc": 330,
      "topic": 7,
      "similarity": 0.7672872489617808
    },
    {
      "doc": 330,
      "topic": 8,
      "similarity": 0.7520378370515126
    },
    {
      "doc": 330,
      "topic": 9,
      "similarity": 0.8376312393536038
    },
    {
      "doc": 330,
      "topic": 10,
      "similarity": 0.770303238173431
    },
    {
      "doc": 330,
      "topic": 11,
      "similarity": 0.7697598841416888
    },
    {
      "doc": 330,
      "topic": 13,
      "similarity": 0.7517781289030658
    },
    {
      "doc": 330,
      "topic": 14,
      "similarity": 0.7693822421226112
    },
    {
      "doc": 330,
      "topic": 15,
      "similarity": 0.7864408073189687
    },
    {
      "doc": 330,
      "topic": 16,
      "similarity": 0.8054891804347404
    },
    {
      "doc": 330,
      "topic": 17,
      "similarity": 0.8033597699334246
    },
    {
      "doc": 330,
      "topic": 18,
      "similarity": 0.7586135223762954
    },
    {
      "doc": 330,
      "topic": 19,
      "similarity": 0.8218554501867302
    },
    {
      "doc": 330,
      "topic": 20,
      "similarity": 0.7856732180906694
    },
    {
      "doc": 330,
      "topic": 21,
      "similarity": 0.8032187666432458
    },
    {
      "doc": 330,
      "topic": 23,
      "similarity": 0.8587743255433025
    },
    {
      "doc": 330,
      "topic": 24,
      "similarity": 0.7645177257396897
    },
    {
      "doc": 331,
      "topic": 2,
      "similarity": 0.764213433845928
    },
    {
      "doc": 331,
      "topic": 3,
      "similarity": 0.7896990408686726
    },
    {
      "doc": 331,
      "topic": 5,
      "similarity": 0.7973365898730573
    },
    {
      "doc": 331,
      "topic": 7,
      "similarity": 0.7636321848776302
    },
    {
      "doc": 331,
      "topic": 8,
      "similarity": 0.7643563820449064
    },
    {
      "doc": 331,
      "topic": 9,
      "similarity": 0.8168217972766716
    },
    {
      "doc": 331,
      "topic": 10,
      "similarity": 0.7725209472371821
    },
    {
      "doc": 331,
      "topic": 11,
      "similarity": 0.7780348860684525
    },
    {
      "doc": 331,
      "topic": 13,
      "similarity": 0.755256719938518
    },
    {
      "doc": 331,
      "topic": 14,
      "similarity": 0.7693025834332267
    },
    {
      "doc": 331,
      "topic": 15,
      "similarity": 0.7754362658282753
    },
    {
      "doc": 331,
      "topic": 16,
      "similarity": 0.8166551755017194
    },
    {
      "doc": 331,
      "topic": 17,
      "similarity": 0.7898686134762529
    },
    {
      "doc": 331,
      "topic": 18,
      "similarity": 0.8028664384227313
    },
    {
      "doc": 331,
      "topic": 19,
      "similarity": 0.8272419065336065
    },
    {
      "doc": 331,
      "topic": 20,
      "similarity": 0.7618547543788715
    },
    {
      "doc": 331,
      "topic": 21,
      "similarity": 0.7876710214611194
    },
    {
      "doc": 331,
      "topic": 24,
      "similarity": 0.7547275038444438
    },
    {
      "doc": 332,
      "topic": 1,
      "similarity": 0.7524318584045362
    },
    {
      "doc": 332,
      "topic": 2,
      "similarity": 0.7672486736675518
    },
    {
      "doc": 332,
      "topic": 3,
      "similarity": 0.8001516714901062
    },
    {
      "doc": 332,
      "topic": 5,
      "similarity": 0.8007405064345388
    },
    {
      "doc": 332,
      "topic": 6,
      "similarity": 0.81399680344585
    },
    {
      "doc": 332,
      "topic": 7,
      "similarity": 0.7557876475267391
    },
    {
      "doc": 332,
      "topic": 8,
      "similarity": 0.7560554258177732
    },
    {
      "doc": 332,
      "topic": 9,
      "similarity": 0.8004915272412032
    },
    {
      "doc": 332,
      "topic": 10,
      "similarity": 0.7993284349384582
    },
    {
      "doc": 332,
      "topic": 11,
      "similarity": 0.7743112602709068
    },
    {
      "doc": 332,
      "topic": 13,
      "similarity": 0.7738562777640201
    },
    {
      "doc": 332,
      "topic": 14,
      "similarity": 0.7696943800299807
    },
    {
      "doc": 332,
      "topic": 15,
      "similarity": 0.7676098816535754
    },
    {
      "doc": 332,
      "topic": 16,
      "similarity": 0.8078011425769208
    },
    {
      "doc": 332,
      "topic": 17,
      "similarity": 0.8123805190810759
    },
    {
      "doc": 332,
      "topic": 18,
      "similarity": 0.802405264852704
    },
    {
      "doc": 332,
      "topic": 19,
      "similarity": 0.8191549428988003
    },
    {
      "doc": 332,
      "topic": 20,
      "similarity": 0.8012898223410653
    },
    {
      "doc": 332,
      "topic": 21,
      "similarity": 0.816380625675265
    },
    {
      "doc": 332,
      "topic": 23,
      "similarity": 0.7923933064159593
    },
    {
      "doc": 333,
      "topic": 3,
      "similarity": 0.7671658382319868
    },
    {
      "doc": 333,
      "topic": 8,
      "similarity": 0.7593681342438813
    },
    {
      "doc": 333,
      "topic": 9,
      "similarity": 0.7863405893126062
    },
    {
      "doc": 333,
      "topic": 11,
      "similarity": 0.7501856694528369
    },
    {
      "doc": 333,
      "topic": 12,
      "similarity": 0.7605357212790061
    },
    {
      "doc": 333,
      "topic": 13,
      "similarity": 0.7524103389949778
    },
    {
      "doc": 333,
      "topic": 15,
      "similarity": 0.7645999210959445
    },
    {
      "doc": 333,
      "topic": 16,
      "similarity": 0.7779098335415223
    },
    {
      "doc": 333,
      "topic": 17,
      "similarity": 0.7848811856129697
    },
    {
      "doc": 333,
      "topic": 18,
      "similarity": 0.7616223858466107
    },
    {
      "doc": 333,
      "topic": 19,
      "similarity": 0.7837966084889005
    },
    {
      "doc": 333,
      "topic": 20,
      "similarity": 0.773183590425998
    },
    {
      "doc": 333,
      "topic": 21,
      "similarity": 0.7836818238323053
    },
    {
      "doc": 334,
      "topic": 2,
      "similarity": 0.7703903200090345
    },
    {
      "doc": 334,
      "topic": 3,
      "similarity": 0.7938485837914677
    },
    {
      "doc": 334,
      "topic": 4,
      "similarity": 0.7570481267229282
    },
    {
      "doc": 334,
      "topic": 5,
      "similarity": 0.7978417442390483
    },
    {
      "doc": 334,
      "topic": 7,
      "similarity": 0.805370308554433
    },
    {
      "doc": 334,
      "topic": 8,
      "similarity": 0.8096237871428233
    },
    {
      "doc": 334,
      "topic": 9,
      "similarity": 0.836695421441762
    },
    {
      "doc": 334,
      "topic": 10,
      "similarity": 0.8001265373727271
    },
    {
      "doc": 334,
      "topic": 11,
      "similarity": 0.7882896607176013
    },
    {
      "doc": 334,
      "topic": 13,
      "similarity": 0.7707112393662628
    },
    {
      "doc": 334,
      "topic": 14,
      "similarity": 0.77115482522013
    },
    {
      "doc": 334,
      "topic": 15,
      "similarity": 0.7896942922920104
    },
    {
      "doc": 334,
      "topic": 16,
      "similarity": 0.8211115708079965
    },
    {
      "doc": 334,
      "topic": 17,
      "similarity": 0.8063561898355827
    },
    {
      "doc": 334,
      "topic": 18,
      "similarity": 0.7818817646931003
    },
    {
      "doc": 334,
      "topic": 19,
      "similarity": 0.8151920959091125
    },
    {
      "doc": 334,
      "topic": 20,
      "similarity": 0.7910822844910664
    },
    {
      "doc": 334,
      "topic": 21,
      "similarity": 0.8033859209235032
    },
    {
      "doc": 334,
      "topic": 24,
      "similarity": 0.7638612903543809
    },
    {
      "doc": 335,
      "topic": 1,
      "similarity": 0.787035842603232
    },
    {
      "doc": 335,
      "topic": 2,
      "similarity": 0.813076860479603
    },
    {
      "doc": 335,
      "topic": 3,
      "similarity": 0.8272647117893737
    },
    {
      "doc": 335,
      "topic": 4,
      "similarity": 0.7842453332019721
    },
    {
      "doc": 335,
      "topic": 5,
      "similarity": 0.8245242088144371
    },
    {
      "doc": 335,
      "topic": 7,
      "similarity": 0.8273819317764156
    },
    {
      "doc": 335,
      "topic": 8,
      "similarity": 0.8247819229725116
    },
    {
      "doc": 335,
      "topic": 9,
      "similarity": 0.8684037816747693
    },
    {
      "doc": 335,
      "topic": 10,
      "similarity": 0.8153032790400665
    },
    {
      "doc": 335,
      "topic": 11,
      "similarity": 0.8210548877734947
    },
    {
      "doc": 335,
      "topic": 12,
      "similarity": 0.7532622651609324
    },
    {
      "doc": 335,
      "topic": 13,
      "similarity": 0.8174777804488123
    },
    {
      "doc": 335,
      "topic": 14,
      "similarity": 0.8265922495016961
    },
    {
      "doc": 335,
      "topic": 15,
      "similarity": 0.8475711903677273
    },
    {
      "doc": 335,
      "topic": 16,
      "similarity": 0.8272302284337535
    },
    {
      "doc": 335,
      "topic": 17,
      "similarity": 0.8493883548291153
    },
    {
      "doc": 335,
      "topic": 18,
      "similarity": 0.7934768921719239
    },
    {
      "doc": 335,
      "topic": 19,
      "similarity": 0.8428931207365479
    },
    {
      "doc": 335,
      "topic": 20,
      "similarity": 0.7920068614385651
    },
    {
      "doc": 335,
      "topic": 21,
      "similarity": 0.8200426379982129
    },
    {
      "doc": 335,
      "topic": 22,
      "similarity": 0.7713401347239712
    },
    {
      "doc": 336,
      "topic": 0,
      "similarity": 0.752034731636704
    },
    {
      "doc": 336,
      "topic": 1,
      "similarity": 0.7513338162478788
    },
    {
      "doc": 336,
      "topic": 2,
      "similarity": 0.7592757472838703
    },
    {
      "doc": 336,
      "topic": 3,
      "similarity": 0.7874274987091969
    },
    {
      "doc": 336,
      "topic": 4,
      "similarity": 0.7544689195372015
    },
    {
      "doc": 336,
      "topic": 5,
      "similarity": 0.7809061809564093
    },
    {
      "doc": 336,
      "topic": 9,
      "similarity": 0.786715855937327
    },
    {
      "doc": 336,
      "topic": 10,
      "similarity": 0.7552028072125107
    },
    {
      "doc": 336,
      "topic": 14,
      "similarity": 0.7545422728092084
    },
    {
      "doc": 336,
      "topic": 16,
      "similarity": 0.7703730238129263
    },
    {
      "doc": 336,
      "topic": 17,
      "similarity": 0.758376663416266
    },
    {
      "doc": 336,
      "topic": 18,
      "similarity": 0.7579993965085906
    },
    {
      "doc": 336,
      "topic": 19,
      "similarity": 0.7847365196277906
    },
    {
      "doc": 336,
      "topic": 20,
      "similarity": 0.7859570227575748
    },
    {
      "doc": 336,
      "topic": 21,
      "similarity": 0.7786563590177851
    },
    {
      "doc": 336,
      "topic": 23,
      "similarity": 0.8060990002961913
    },
    {
      "doc": 336,
      "topic": 24,
      "similarity": 0.7667041943615778
    },
    {
      "doc": 337,
      "topic": 1,
      "similarity": 0.754091043681439
    },
    {
      "doc": 337,
      "topic": 2,
      "similarity": 0.7751798973180153
    },
    {
      "doc": 337,
      "topic": 3,
      "similarity": 0.8270658486323609
    },
    {
      "doc": 337,
      "topic": 5,
      "similarity": 0.8022578215050842
    },
    {
      "doc": 337,
      "topic": 7,
      "similarity": 0.7621304429006216
    },
    {
      "doc": 337,
      "topic": 8,
      "similarity": 0.7669571578419936
    },
    {
      "doc": 337,
      "topic": 9,
      "similarity": 0.7891248589987421
    },
    {
      "doc": 337,
      "topic": 10,
      "similarity": 0.7768696454483794
    },
    {
      "doc": 337,
      "topic": 11,
      "similarity": 0.8217450776972124
    },
    {
      "doc": 337,
      "topic": 13,
      "similarity": 0.7768889779073916
    },
    {
      "doc": 337,
      "topic": 14,
      "similarity": 0.7852344885784476
    },
    {
      "doc": 337,
      "topic": 15,
      "similarity": 0.7779277502617417
    },
    {
      "doc": 337,
      "topic": 16,
      "similarity": 0.7895340893196269
    },
    {
      "doc": 337,
      "topic": 17,
      "similarity": 0.7890566702791445
    },
    {
      "doc": 337,
      "topic": 18,
      "similarity": 0.7838716446899551
    },
    {
      "doc": 337,
      "topic": 19,
      "similarity": 0.8353226205869564
    },
    {
      "doc": 337,
      "topic": 20,
      "similarity": 0.7785574485426001
    },
    {
      "doc": 337,
      "topic": 21,
      "similarity": 0.782164050279794
    },
    {
      "doc": 337,
      "topic": 23,
      "similarity": 0.7520327706680243
    },
    {
      "doc": 338,
      "topic": 2,
      "similarity": 0.7626501136646692
    },
    {
      "doc": 338,
      "topic": 3,
      "similarity": 0.7709850449427174
    },
    {
      "doc": 338,
      "topic": 5,
      "similarity": 0.767939379468959
    },
    {
      "doc": 338,
      "topic": 7,
      "similarity": 0.7685736733256837
    },
    {
      "doc": 338,
      "topic": 8,
      "similarity": 0.7543321847380452
    },
    {
      "doc": 338,
      "topic": 9,
      "similarity": 0.7945414305348477
    },
    {
      "doc": 338,
      "topic": 10,
      "similarity": 0.7542418548907508
    },
    {
      "doc": 338,
      "topic": 11,
      "similarity": 0.7598224613480189
    },
    {
      "doc": 338,
      "topic": 15,
      "similarity": 0.7602945469354735
    },
    {
      "doc": 338,
      "topic": 16,
      "similarity": 0.8094328260968487
    },
    {
      "doc": 338,
      "topic": 17,
      "similarity": 0.7847206171919557
    },
    {
      "doc": 338,
      "topic": 18,
      "similarity": 0.7529514628310747
    },
    {
      "doc": 338,
      "topic": 19,
      "similarity": 0.7812298179714319
    },
    {
      "doc": 338,
      "topic": 20,
      "similarity": 0.7795227608153534
    },
    {
      "doc": 338,
      "topic": 21,
      "similarity": 0.7885404461010471
    },
    {
      "doc": 339,
      "topic": 2,
      "similarity": 0.7631385653988836
    },
    {
      "doc": 339,
      "topic": 3,
      "similarity": 0.7878706172985892
    },
    {
      "doc": 339,
      "topic": 5,
      "similarity": 0.8017197012616424
    },
    {
      "doc": 339,
      "topic": 6,
      "similarity": 0.7528324002002745
    },
    {
      "doc": 339,
      "topic": 7,
      "similarity": 0.7795591042189868
    },
    {
      "doc": 339,
      "topic": 8,
      "similarity": 0.7675859146563211
    },
    {
      "doc": 339,
      "topic": 9,
      "similarity": 0.8348222314054414
    },
    {
      "doc": 339,
      "topic": 10,
      "similarity": 0.7635665709454234
    },
    {
      "doc": 339,
      "topic": 11,
      "similarity": 0.7862296951025395
    },
    {
      "doc": 339,
      "topic": 13,
      "similarity": 0.7646607413981849
    },
    {
      "doc": 339,
      "topic": 14,
      "similarity": 0.766101455940135
    },
    {
      "doc": 339,
      "topic": 15,
      "similarity": 0.7966424471365746
    },
    {
      "doc": 339,
      "topic": 16,
      "similarity": 0.7978755208152439
    },
    {
      "doc": 339,
      "topic": 17,
      "similarity": 0.8110068859979292
    },
    {
      "doc": 339,
      "topic": 19,
      "similarity": 0.8105691231544295
    },
    {
      "doc": 339,
      "topic": 20,
      "similarity": 0.7875834275635156
    },
    {
      "doc": 339,
      "topic": 21,
      "similarity": 0.8010303714629934
    },
    {
      "doc": 339,
      "topic": 23,
      "similarity": 0.7541793753996832
    },
    {
      "doc": 339,
      "topic": 24,
      "similarity": 0.7550813292477014
    },
    {
      "doc": 340,
      "topic": 3,
      "similarity": 0.7879204839013261
    },
    {
      "doc": 340,
      "topic": 4,
      "similarity": 0.8295553492796168
    },
    {
      "doc": 340,
      "topic": 5,
      "similarity": 0.7709388801117648
    },
    {
      "doc": 340,
      "topic": 7,
      "similarity": 0.768088934854779
    },
    {
      "doc": 340,
      "topic": 8,
      "similarity": 0.7549508731805519
    },
    {
      "doc": 340,
      "topic": 9,
      "similarity": 0.7845963653036541
    },
    {
      "doc": 340,
      "topic": 10,
      "similarity": 0.7639597175693649
    },
    {
      "doc": 340,
      "topic": 11,
      "similarity": 0.7536147784157958
    },
    {
      "doc": 340,
      "topic": 13,
      "similarity": 0.7520258798916544
    },
    {
      "doc": 340,
      "topic": 16,
      "similarity": 0.791984373142792
    },
    {
      "doc": 340,
      "topic": 17,
      "similarity": 0.7770594302886119
    },
    {
      "doc": 340,
      "topic": 19,
      "similarity": 0.7805277082375054
    },
    {
      "doc": 340,
      "topic": 20,
      "similarity": 0.7572639366770434
    },
    {
      "doc": 340,
      "topic": 21,
      "similarity": 0.7800259027764981
    },
    {
      "doc": 340,
      "topic": 24,
      "similarity": 0.7628022310181073
    },
    {
      "doc": 341,
      "topic": 3,
      "similarity": 0.7833650939822261
    },
    {
      "doc": 341,
      "topic": 5,
      "similarity": 0.7581520937379399
    },
    {
      "doc": 341,
      "topic": 9,
      "similarity": 0.7653692964437323
    },
    {
      "doc": 341,
      "topic": 11,
      "similarity": 0.7545785330452358
    },
    {
      "doc": 341,
      "topic": 14,
      "similarity": 0.7510938469617864
    },
    {
      "doc": 341,
      "topic": 15,
      "similarity": 0.761251794679576
    },
    {
      "doc": 341,
      "topic": 16,
      "similarity": 0.7595004737781854
    },
    {
      "doc": 341,
      "topic": 17,
      "similarity": 0.7552105175694207
    },
    {
      "doc": 341,
      "topic": 19,
      "similarity": 0.781464545873297
    },
    {
      "doc": 341,
      "topic": 20,
      "similarity": 0.7677886837074207
    },
    {
      "doc": 341,
      "topic": 21,
      "similarity": 0.7656685698848931
    },
    {
      "doc": 342,
      "topic": 1,
      "similarity": 0.7534689800315804
    },
    {
      "doc": 342,
      "topic": 2,
      "similarity": 0.7666298126174904
    },
    {
      "doc": 342,
      "topic": 3,
      "similarity": 0.8010098187445339
    },
    {
      "doc": 342,
      "topic": 5,
      "similarity": 0.8072639467373864
    },
    {
      "doc": 342,
      "topic": 6,
      "similarity": 0.7766050267949607
    },
    {
      "doc": 342,
      "topic": 7,
      "similarity": 0.7734593737665144
    },
    {
      "doc": 342,
      "topic": 8,
      "similarity": 0.7630463987139127
    },
    {
      "doc": 342,
      "topic": 9,
      "similarity": 0.8173108601785521
    },
    {
      "doc": 342,
      "topic": 10,
      "similarity": 0.7855298193716715
    },
    {
      "doc": 342,
      "topic": 11,
      "similarity": 0.8129116700490593
    },
    {
      "doc": 342,
      "topic": 13,
      "similarity": 0.7555340926650178
    },
    {
      "doc": 342,
      "topic": 14,
      "similarity": 0.7720889438292887
    },
    {
      "doc": 342,
      "topic": 15,
      "similarity": 0.7844571439319353
    },
    {
      "doc": 342,
      "topic": 16,
      "similarity": 0.8013740681741042
    },
    {
      "doc": 342,
      "topic": 17,
      "similarity": 0.8045922782587644
    },
    {
      "doc": 342,
      "topic": 18,
      "similarity": 0.7757665342784708
    },
    {
      "doc": 342,
      "topic": 19,
      "similarity": 0.8124570853765598
    },
    {
      "doc": 342,
      "topic": 20,
      "similarity": 0.7822508525567299
    },
    {
      "doc": 342,
      "topic": 21,
      "similarity": 0.7979209357779847
    },
    {
      "doc": 342,
      "topic": 23,
      "similarity": 0.8190545493112659
    },
    {
      "doc": 342,
      "topic": 24,
      "similarity": 0.772931500094516
    },
    {
      "doc": 343,
      "topic": 2,
      "similarity": 0.7530471152005911
    },
    {
      "doc": 343,
      "topic": 3,
      "similarity": 0.7842505930850172
    },
    {
      "doc": 343,
      "topic": 5,
      "similarity": 0.7798129721285642
    },
    {
      "doc": 343,
      "topic": 7,
      "similarity": 0.7560735270976255
    },
    {
      "doc": 343,
      "topic": 8,
      "similarity": 0.7530871651182262
    },
    {
      "doc": 343,
      "topic": 9,
      "similarity": 0.8002190082499078
    },
    {
      "doc": 343,
      "topic": 10,
      "similarity": 0.752989751639909
    },
    {
      "doc": 343,
      "topic": 11,
      "similarity": 0.7703012166856719
    },
    {
      "doc": 343,
      "topic": 13,
      "similarity": 0.7516683155075807
    },
    {
      "doc": 343,
      "topic": 15,
      "similarity": 0.7637417022975553
    },
    {
      "doc": 343,
      "topic": 16,
      "similarity": 0.7998447634963061
    },
    {
      "doc": 343,
      "topic": 17,
      "similarity": 0.7799915646887363
    },
    {
      "doc": 343,
      "topic": 19,
      "similarity": 0.7976094329790552
    },
    {
      "doc": 343,
      "topic": 20,
      "similarity": 0.798770398968786
    },
    {
      "doc": 343,
      "topic": 21,
      "similarity": 0.7910442596296766
    },
    {
      "doc": 343,
      "topic": 22,
      "similarity": 0.7509188192130718
    },
    {
      "doc": 344,
      "topic": 3,
      "similarity": 0.7831775311278629
    },
    {
      "doc": 344,
      "topic": 4,
      "similarity": 0.7645679813646334
    },
    {
      "doc": 344,
      "topic": 5,
      "similarity": 0.7658921700946647
    },
    {
      "doc": 344,
      "topic": 7,
      "similarity": 0.7697419605260256
    },
    {
      "doc": 344,
      "topic": 9,
      "similarity": 0.8018561282549671
    },
    {
      "doc": 344,
      "topic": 11,
      "similarity": 0.7681830120799089
    },
    {
      "doc": 344,
      "topic": 16,
      "similarity": 0.7859019732237883
    },
    {
      "doc": 344,
      "topic": 17,
      "similarity": 0.7666121707128198
    },
    {
      "doc": 344,
      "topic": 19,
      "similarity": 0.7662510004903822
    },
    {
      "doc": 344,
      "topic": 20,
      "similarity": 0.7543564236737841
    },
    {
      "doc": 344,
      "topic": 21,
      "similarity": 0.7929146332943126
    },
    {
      "doc": 344,
      "topic": 24,
      "similarity": 0.7577333686363019
    },
    {
      "doc": 345,
      "topic": 2,
      "similarity": 0.7667648390975164
    },
    {
      "doc": 345,
      "topic": 3,
      "similarity": 0.7907789552146904
    },
    {
      "doc": 345,
      "topic": 5,
      "similarity": 0.7821828284560578
    },
    {
      "doc": 345,
      "topic": 7,
      "similarity": 0.7662748091642182
    },
    {
      "doc": 345,
      "topic": 9,
      "similarity": 0.809859741449127
    },
    {
      "doc": 345,
      "topic": 10,
      "similarity": 0.7613774390476813
    },
    {
      "doc": 345,
      "topic": 11,
      "similarity": 0.7713027756779589
    },
    {
      "doc": 345,
      "topic": 14,
      "similarity": 0.7541148943874452
    },
    {
      "doc": 345,
      "topic": 15,
      "similarity": 0.7548819204724071
    },
    {
      "doc": 345,
      "topic": 16,
      "similarity": 0.7927281032355253
    },
    {
      "doc": 345,
      "topic": 17,
      "similarity": 0.7684238038370259
    },
    {
      "doc": 345,
      "topic": 19,
      "similarity": 0.8115767380605109
    },
    {
      "doc": 345,
      "topic": 20,
      "similarity": 0.7860396440468682
    },
    {
      "doc": 345,
      "topic": 21,
      "similarity": 0.7886354708771333
    },
    {
      "doc": 345,
      "topic": 23,
      "similarity": 0.7647684912242265
    },
    {
      "doc": 345,
      "topic": 24,
      "similarity": 0.7504097810964614
    },
    {
      "doc": 346,
      "topic": 2,
      "similarity": 0.7653363456182287
    },
    {
      "doc": 346,
      "topic": 3,
      "similarity": 0.8081979799810681
    },
    {
      "doc": 346,
      "topic": 5,
      "similarity": 0.7842158265787951
    },
    {
      "doc": 346,
      "topic": 7,
      "similarity": 0.7998022352834948
    },
    {
      "doc": 346,
      "topic": 8,
      "similarity": 0.794140036025144
    },
    {
      "doc": 346,
      "topic": 9,
      "similarity": 0.8255635153973557
    },
    {
      "doc": 346,
      "topic": 10,
      "similarity": 0.771288918693491
    },
    {
      "doc": 346,
      "topic": 11,
      "similarity": 0.7882627254610589
    },
    {
      "doc": 346,
      "topic": 13,
      "similarity": 0.7677143424113902
    },
    {
      "doc": 346,
      "topic": 14,
      "similarity": 0.764731340561714
    },
    {
      "doc": 346,
      "topic": 15,
      "similarity": 0.807671181971403
    },
    {
      "doc": 346,
      "topic": 16,
      "similarity": 0.798136738162061
    },
    {
      "doc": 346,
      "topic": 17,
      "similarity": 0.7947772794740328
    },
    {
      "doc": 346,
      "topic": 18,
      "similarity": 0.7726923941049667
    },
    {
      "doc": 346,
      "topic": 19,
      "similarity": 0.7955669104848487
    },
    {
      "doc": 346,
      "topic": 20,
      "similarity": 0.7974749553623783
    },
    {
      "doc": 346,
      "topic": 21,
      "similarity": 0.8096606462994481
    },
    {
      "doc": 347,
      "topic": 3,
      "similarity": 0.7738708953749277
    },
    {
      "doc": 347,
      "topic": 5,
      "similarity": 0.7626594907785669
    },
    {
      "doc": 347,
      "topic": 7,
      "similarity": 0.7674834943051081
    },
    {
      "doc": 347,
      "topic": 8,
      "similarity": 0.7778531092083818
    },
    {
      "doc": 347,
      "topic": 9,
      "similarity": 0.8121007335389253
    },
    {
      "doc": 347,
      "topic": 11,
      "similarity": 0.7644925407866858
    },
    {
      "doc": 347,
      "topic": 15,
      "similarity": 0.7539664213107856
    },
    {
      "doc": 347,
      "topic": 16,
      "similarity": 0.7668061292576079
    },
    {
      "doc": 347,
      "topic": 17,
      "similarity": 0.7669169269565514
    },
    {
      "doc": 347,
      "topic": 19,
      "similarity": 0.7602050466725224
    },
    {
      "doc": 347,
      "topic": 21,
      "similarity": 0.7580411724073935
    },
    {
      "doc": 347,
      "topic": 24,
      "similarity": 0.7796909509992511
    },
    {
      "doc": 348,
      "topic": 3,
      "similarity": 0.7746740927183184
    },
    {
      "doc": 348,
      "topic": 5,
      "similarity": 0.7952163536480689
    },
    {
      "doc": 348,
      "topic": 7,
      "similarity": 0.7505573901231377
    },
    {
      "doc": 348,
      "topic": 8,
      "similarity": 0.7696989657805067
    },
    {
      "doc": 348,
      "topic": 9,
      "similarity": 0.8052728223368456
    },
    {
      "doc": 348,
      "topic": 10,
      "similarity": 0.764609235213161
    },
    {
      "doc": 348,
      "topic": 11,
      "similarity": 0.7541762482047664
    },
    {
      "doc": 348,
      "topic": 15,
      "similarity": 0.7580768494796075
    },
    {
      "doc": 348,
      "topic": 16,
      "similarity": 0.7957502586782614
    },
    {
      "doc": 348,
      "topic": 17,
      "similarity": 0.7975784971784706
    },
    {
      "doc": 348,
      "topic": 19,
      "similarity": 0.7900729332832356
    },
    {
      "doc": 348,
      "topic": 20,
      "similarity": 0.7806350854069529
    },
    {
      "doc": 348,
      "topic": 21,
      "similarity": 0.7809353100824301
    },
    {
      "doc": 348,
      "topic": 24,
      "similarity": 0.7741387105169865
    },
    {
      "doc": 349,
      "topic": 0,
      "similarity": 0.7734603673626161
    },
    {
      "doc": 349,
      "topic": 3,
      "similarity": 0.7800407710842425
    },
    {
      "doc": 349,
      "topic": 5,
      "similarity": 0.7811706286202645
    },
    {
      "doc": 349,
      "topic": 7,
      "similarity": 0.7510560239691484
    },
    {
      "doc": 349,
      "topic": 9,
      "similarity": 0.7882876503026512
    },
    {
      "doc": 349,
      "topic": 11,
      "similarity": 0.8283773088078923
    },
    {
      "doc": 349,
      "topic": 15,
      "similarity": 0.766665105597394
    },
    {
      "doc": 349,
      "topic": 16,
      "similarity": 0.7858686925267302
    },
    {
      "doc": 349,
      "topic": 17,
      "similarity": 0.7841660139163273
    },
    {
      "doc": 349,
      "topic": 18,
      "similarity": 0.75633104380001
    },
    {
      "doc": 349,
      "topic": 19,
      "similarity": 0.7927715497258212
    },
    {
      "doc": 349,
      "topic": 20,
      "similarity": 0.7659209086374282
    },
    {
      "doc": 349,
      "topic": 21,
      "similarity": 0.7840648333227718
    },
    {
      "doc": 349,
      "topic": 23,
      "similarity": 0.7528269332939854
    },
    {
      "doc": 349,
      "topic": 24,
      "similarity": 0.7857897034169732
    },
    {
      "doc": 350,
      "topic": 1,
      "similarity": 0.754109845778516
    },
    {
      "doc": 350,
      "topic": 2,
      "similarity": 0.7657341355584799
    },
    {
      "doc": 350,
      "topic": 3,
      "similarity": 0.8119493119848699
    },
    {
      "doc": 350,
      "topic": 4,
      "similarity": 0.7714781145300764
    },
    {
      "doc": 350,
      "topic": 5,
      "similarity": 0.8053705711749318
    },
    {
      "doc": 350,
      "topic": 7,
      "similarity": 0.7856410981069233
    },
    {
      "doc": 350,
      "topic": 8,
      "similarity": 0.7880051251282923
    },
    {
      "doc": 350,
      "topic": 9,
      "similarity": 0.8271757699659992
    },
    {
      "doc": 350,
      "topic": 10,
      "similarity": 0.7852553820135312
    },
    {
      "doc": 350,
      "topic": 11,
      "similarity": 0.7942461732781576
    },
    {
      "doc": 350,
      "topic": 12,
      "similarity": 0.7714369369690233
    },
    {
      "doc": 350,
      "topic": 13,
      "similarity": 0.7853532787406193
    },
    {
      "doc": 350,
      "topic": 14,
      "similarity": 0.7698748595000711
    },
    {
      "doc": 350,
      "topic": 15,
      "similarity": 0.7932629865461077
    },
    {
      "doc": 350,
      "topic": 16,
      "similarity": 0.8194140641525509
    },
    {
      "doc": 350,
      "topic": 17,
      "similarity": 0.8105274006169226
    },
    {
      "doc": 350,
      "topic": 18,
      "similarity": 0.7529698675348212
    },
    {
      "doc": 350,
      "topic": 19,
      "similarity": 0.8007302932989424
    },
    {
      "doc": 350,
      "topic": 20,
      "similarity": 0.8189096529210018
    },
    {
      "doc": 350,
      "topic": 21,
      "similarity": 0.8083076850052572
    },
    {
      "doc": 350,
      "topic": 22,
      "similarity": 0.76240354110415
    },
    {
      "doc": 350,
      "topic": 23,
      "similarity": 0.7646285293711201
    },
    {
      "doc": 350,
      "topic": 24,
      "similarity": 0.760950269004042
    },
    {
      "doc": 351,
      "topic": 0,
      "similarity": 0.7515131384391408
    },
    {
      "doc": 351,
      "topic": 3,
      "similarity": 0.7709764709435666
    },
    {
      "doc": 351,
      "topic": 5,
      "similarity": 0.7641657347197922
    },
    {
      "doc": 351,
      "topic": 7,
      "similarity": 0.7571937879255649
    },
    {
      "doc": 351,
      "topic": 9,
      "similarity": 0.8240677709714156
    },
    {
      "doc": 351,
      "topic": 11,
      "similarity": 0.8126488239727597
    },
    {
      "doc": 351,
      "topic": 15,
      "similarity": 0.7679350192553595
    },
    {
      "doc": 351,
      "topic": 16,
      "similarity": 0.7823501294142678
    },
    {
      "doc": 351,
      "topic": 17,
      "similarity": 0.7783118888070117
    },
    {
      "doc": 351,
      "topic": 18,
      "similarity": 0.7582899597353542
    },
    {
      "doc": 351,
      "topic": 19,
      "similarity": 0.7877268169529948
    },
    {
      "doc": 351,
      "topic": 20,
      "similarity": 0.7790190868062237
    },
    {
      "doc": 351,
      "topic": 21,
      "similarity": 0.7916408017309223
    },
    {
      "doc": 351,
      "topic": 24,
      "similarity": 0.7742966051494655
    },
    {
      "doc": 352,
      "topic": 2,
      "similarity": 0.7534199500863898
    },
    {
      "doc": 352,
      "topic": 3,
      "similarity": 0.795859769335042
    },
    {
      "doc": 352,
      "topic": 4,
      "similarity": 0.8099966589669911
    },
    {
      "doc": 352,
      "topic": 5,
      "similarity": 0.7932075847840211
    },
    {
      "doc": 352,
      "topic": 7,
      "similarity": 0.7853396173385496
    },
    {
      "doc": 352,
      "topic": 8,
      "similarity": 0.7837242939013576
    },
    {
      "doc": 352,
      "topic": 9,
      "similarity": 0.8036566287556426
    },
    {
      "doc": 352,
      "topic": 10,
      "similarity": 0.7851841725023673
    },
    {
      "doc": 352,
      "topic": 11,
      "similarity": 0.7735421020521714
    },
    {
      "doc": 352,
      "topic": 13,
      "similarity": 0.7522791798039175
    },
    {
      "doc": 352,
      "topic": 14,
      "similarity": 0.7614693764020205
    },
    {
      "doc": 352,
      "topic": 15,
      "similarity": 0.7522412226527333
    },
    {
      "doc": 352,
      "topic": 16,
      "similarity": 0.8108646911788321
    },
    {
      "doc": 352,
      "topic": 17,
      "similarity": 0.7894738751923572
    },
    {
      "doc": 352,
      "topic": 19,
      "similarity": 0.7944100655905662
    },
    {
      "doc": 352,
      "topic": 20,
      "similarity": 0.7598173868551686
    },
    {
      "doc": 352,
      "topic": 21,
      "similarity": 0.7864685833205627
    },
    {
      "doc": 352,
      "topic": 24,
      "similarity": 0.7980682051307454
    },
    {
      "doc": 353,
      "topic": 2,
      "similarity": 0.7751246504196403
    },
    {
      "doc": 353,
      "topic": 3,
      "similarity": 0.7834348376291564
    },
    {
      "doc": 353,
      "topic": 4,
      "similarity": 0.7517111030312557
    },
    {
      "doc": 353,
      "topic": 5,
      "similarity": 0.7857776171603722
    },
    {
      "doc": 353,
      "topic": 7,
      "similarity": 0.8222696967179152
    },
    {
      "doc": 353,
      "topic": 8,
      "similarity": 0.8345339533713649
    },
    {
      "doc": 353,
      "topic": 9,
      "similarity": 0.8313872360444294
    },
    {
      "doc": 353,
      "topic": 10,
      "similarity": 0.775597791476453
    },
    {
      "doc": 353,
      "topic": 11,
      "similarity": 0.7752477606305633
    },
    {
      "doc": 353,
      "topic": 12,
      "similarity": 0.7773670493273843
    },
    {
      "doc": 353,
      "topic": 13,
      "similarity": 0.8245840291137458
    },
    {
      "doc": 353,
      "topic": 14,
      "similarity": 0.7751588929164027
    },
    {
      "doc": 353,
      "topic": 15,
      "similarity": 0.7737659133235081
    },
    {
      "doc": 353,
      "topic": 16,
      "similarity": 0.7973007147620811
    },
    {
      "doc": 353,
      "topic": 17,
      "similarity": 0.7856882377484845
    },
    {
      "doc": 353,
      "topic": 18,
      "similarity": 0.7573932561615344
    },
    {
      "doc": 353,
      "topic": 19,
      "similarity": 0.7885004635342278
    },
    {
      "doc": 353,
      "topic": 20,
      "similarity": 0.7822925485855724
    },
    {
      "doc": 353,
      "topic": 21,
      "similarity": 0.7886770196747789
    },
    {
      "doc": 353,
      "topic": 22,
      "similarity": 0.7654852094678408
    },
    {
      "doc": 353,
      "topic": 24,
      "similarity": 0.7553749263433309
    },
    {
      "doc": 354,
      "topic": 3,
      "similarity": 0.7738602460679392
    },
    {
      "doc": 354,
      "topic": 5,
      "similarity": 0.765255109494292
    },
    {
      "doc": 354,
      "topic": 7,
      "similarity": 0.764565265668274
    },
    {
      "doc": 354,
      "topic": 8,
      "similarity": 0.759012032542635
    },
    {
      "doc": 354,
      "topic": 9,
      "similarity": 0.8278914951477404
    },
    {
      "doc": 354,
      "topic": 10,
      "similarity": 0.7627799241210139
    },
    {
      "doc": 354,
      "topic": 11,
      "similarity": 0.7711555430987904
    },
    {
      "doc": 354,
      "topic": 12,
      "similarity": 0.7805843079359885
    },
    {
      "doc": 354,
      "topic": 15,
      "similarity": 0.7690860737375266
    },
    {
      "doc": 354,
      "topic": 16,
      "similarity": 0.7845791297597176
    },
    {
      "doc": 354,
      "topic": 17,
      "similarity": 0.7963364862367237
    },
    {
      "doc": 354,
      "topic": 18,
      "similarity": 0.7612773220231179
    },
    {
      "doc": 354,
      "topic": 19,
      "similarity": 0.7937950372123652
    },
    {
      "doc": 354,
      "topic": 20,
      "similarity": 0.783049767703971
    },
    {
      "doc": 354,
      "topic": 21,
      "similarity": 0.7797741390158041
    },
    {
      "doc": 354,
      "topic": 23,
      "similarity": 0.75535946540301
    },
    {
      "doc": 354,
      "topic": 24,
      "similarity": 0.7840361663614877
    },
    {
      "doc": 355,
      "topic": 1,
      "similarity": 0.7518156109131237
    },
    {
      "doc": 355,
      "topic": 2,
      "similarity": 0.7685838830982913
    },
    {
      "doc": 355,
      "topic": 3,
      "similarity": 0.799652868635203
    },
    {
      "doc": 355,
      "topic": 4,
      "similarity": 0.7503001120460292
    },
    {
      "doc": 355,
      "topic": 5,
      "similarity": 0.7917116175131119
    },
    {
      "doc": 355,
      "topic": 7,
      "similarity": 0.7750021697340136
    },
    {
      "doc": 355,
      "topic": 8,
      "similarity": 0.7938326018487667
    },
    {
      "doc": 355,
      "topic": 9,
      "similarity": 0.8302035362138049
    },
    {
      "doc": 355,
      "topic": 10,
      "similarity": 0.7815550667291269
    },
    {
      "doc": 355,
      "topic": 11,
      "similarity": 0.7906055587366896
    },
    {
      "doc": 355,
      "topic": 12,
      "similarity": 0.765238169830339
    },
    {
      "doc": 355,
      "topic": 13,
      "similarity": 0.7695547540246455
    },
    {
      "doc": 355,
      "topic": 14,
      "similarity": 0.7708660674136394
    },
    {
      "doc": 355,
      "topic": 15,
      "similarity": 0.8124996746360789
    },
    {
      "doc": 355,
      "topic": 16,
      "similarity": 0.8169779649673935
    },
    {
      "doc": 355,
      "topic": 17,
      "similarity": 0.8220471420886044
    },
    {
      "doc": 355,
      "topic": 18,
      "similarity": 0.7782045114563784
    },
    {
      "doc": 355,
      "topic": 19,
      "similarity": 0.838938746337103
    },
    {
      "doc": 355,
      "topic": 20,
      "similarity": 0.800038893391412
    },
    {
      "doc": 355,
      "topic": 21,
      "similarity": 0.8115478736805878
    },
    {
      "doc": 355,
      "topic": 22,
      "similarity": 0.7607303027897675
    },
    {
      "doc": 355,
      "topic": 23,
      "similarity": 0.7561975008735432
    },
    {
      "doc": 355,
      "topic": 24,
      "similarity": 0.768118155798862
    },
    {
      "doc": 356,
      "topic": 2,
      "similarity": 0.7745358578933909
    },
    {
      "doc": 356,
      "topic": 3,
      "similarity": 0.8018785932856124
    },
    {
      "doc": 356,
      "topic": 4,
      "similarity": 0.7521084361740937
    },
    {
      "doc": 356,
      "topic": 5,
      "similarity": 0.7975671232292417
    },
    {
      "doc": 356,
      "topic": 6,
      "similarity": 0.7632939486134116
    },
    {
      "doc": 356,
      "topic": 7,
      "similarity": 0.7825007734203406
    },
    {
      "doc": 356,
      "topic": 8,
      "similarity": 0.7938736252293506
    },
    {
      "doc": 356,
      "topic": 9,
      "similarity": 0.8214907315763302
    },
    {
      "doc": 356,
      "topic": 10,
      "similarity": 0.795387460763929
    },
    {
      "doc": 356,
      "topic": 11,
      "similarity": 0.7883033540569027
    },
    {
      "doc": 356,
      "topic": 12,
      "similarity": 0.7585289213100623
    },
    {
      "doc": 356,
      "topic": 13,
      "similarity": 0.7811165843510616
    },
    {
      "doc": 356,
      "topic": 14,
      "similarity": 0.7924353672708099
    },
    {
      "doc": 356,
      "topic": 15,
      "similarity": 0.8051767287526707
    },
    {
      "doc": 356,
      "topic": 16,
      "similarity": 0.8081054947557493
    },
    {
      "doc": 356,
      "topic": 17,
      "similarity": 0.7959853278292409
    },
    {
      "doc": 356,
      "topic": 18,
      "similarity": 0.8074733865519019
    },
    {
      "doc": 356,
      "topic": 19,
      "similarity": 0.8502604825599426
    },
    {
      "doc": 356,
      "topic": 20,
      "similarity": 0.7884865898987425
    },
    {
      "doc": 356,
      "topic": 21,
      "similarity": 0.8003661765367339
    },
    {
      "doc": 356,
      "topic": 22,
      "similarity": 0.7757016795524283
    },
    {
      "doc": 356,
      "topic": 23,
      "similarity": 0.7624962789556972
    },
    {
      "doc": 357,
      "topic": 3,
      "similarity": 0.7730377469681894
    },
    {
      "doc": 357,
      "topic": 5,
      "similarity": 0.7607335906639383
    },
    {
      "doc": 357,
      "topic": 6,
      "similarity": 0.8314082500927414
    },
    {
      "doc": 357,
      "topic": 7,
      "similarity": 0.7555664670401372
    },
    {
      "doc": 357,
      "topic": 9,
      "similarity": 0.7981737376186928
    },
    {
      "doc": 357,
      "topic": 10,
      "similarity": 0.7838488910792738
    },
    {
      "doc": 357,
      "topic": 11,
      "similarity": 0.7779018990943419
    },
    {
      "doc": 357,
      "topic": 13,
      "similarity": 0.7643582059194661
    },
    {
      "doc": 357,
      "topic": 14,
      "similarity": 0.7736315136839849
    },
    {
      "doc": 357,
      "topic": 15,
      "similarity": 0.7771307792223232
    },
    {
      "doc": 357,
      "topic": 16,
      "similarity": 0.7877869882343682
    },
    {
      "doc": 357,
      "topic": 17,
      "similarity": 0.7743121372528037
    },
    {
      "doc": 357,
      "topic": 18,
      "similarity": 0.7581934272958054
    },
    {
      "doc": 357,
      "topic": 19,
      "similarity": 0.8170108725783729
    },
    {
      "doc": 357,
      "topic": 21,
      "similarity": 0.7672773978708699
    },
    {
      "doc": 357,
      "topic": 23,
      "similarity": 0.7848912124107721
    },
    {
      "doc": 358,
      "topic": 3,
      "similarity": 0.7591308235501452
    },
    {
      "doc": 358,
      "topic": 5,
      "similarity": 0.7529609639653757
    },
    {
      "doc": 358,
      "topic": 7,
      "similarity": 0.7611369832470146
    },
    {
      "doc": 358,
      "topic": 9,
      "similarity": 0.7843170412411709
    },
    {
      "doc": 358,
      "topic": 11,
      "similarity": 0.751995264891704
    },
    {
      "doc": 358,
      "topic": 16,
      "similarity": 0.7510830903394662
    },
    {
      "doc": 358,
      "topic": 19,
      "similarity": 0.7905601333201983
    },
    {
      "doc": 358,
      "topic": 20,
      "similarity": 0.75167318860845
    },
    {
      "doc": 358,
      "topic": 24,
      "similarity": 0.7925090063256033
    },
    {
      "doc": 359,
      "topic": 1,
      "similarity": 0.7679048892432693
    },
    {
      "doc": 359,
      "topic": 2,
      "similarity": 0.7665889675237519
    },
    {
      "doc": 359,
      "topic": 3,
      "similarity": 0.7987375870225771
    },
    {
      "doc": 359,
      "topic": 4,
      "similarity": 0.7808354935061811
    },
    {
      "doc": 359,
      "topic": 5,
      "similarity": 0.801661972654812
    },
    {
      "doc": 359,
      "topic": 7,
      "similarity": 0.7790281505480068
    },
    {
      "doc": 359,
      "topic": 8,
      "similarity": 0.7702904261746814
    },
    {
      "doc": 359,
      "topic": 9,
      "similarity": 0.819959337881085
    },
    {
      "doc": 359,
      "topic": 10,
      "similarity": 0.7976752678289225
    },
    {
      "doc": 359,
      "topic": 11,
      "similarity": 0.7776433706313916
    },
    {
      "doc": 359,
      "topic": 13,
      "similarity": 0.7662504549668112
    },
    {
      "doc": 359,
      "topic": 14,
      "similarity": 0.76405748963988
    },
    {
      "doc": 359,
      "topic": 15,
      "similarity": 0.7883896529644934
    },
    {
      "doc": 359,
      "topic": 16,
      "similarity": 0.8169041924838789
    },
    {
      "doc": 359,
      "topic": 17,
      "similarity": 0.8405473269209646
    },
    {
      "doc": 359,
      "topic": 18,
      "similarity": 0.7611911956593499
    },
    {
      "doc": 359,
      "topic": 19,
      "similarity": 0.8152520148778231
    },
    {
      "doc": 359,
      "topic": 20,
      "similarity": 0.7890439570183282
    },
    {
      "doc": 359,
      "topic": 21,
      "similarity": 0.8079669072275328
    },
    {
      "doc": 359,
      "topic": 24,
      "similarity": 0.7518342478719569
    },
    {
      "doc": 360,
      "topic": 2,
      "similarity": 0.7640242426955186
    },
    {
      "doc": 360,
      "topic": 3,
      "similarity": 0.800007767795918
    },
    {
      "doc": 360,
      "topic": 4,
      "similarity": 0.7578195999361298
    },
    {
      "doc": 360,
      "topic": 5,
      "similarity": 0.7875916727338119
    },
    {
      "doc": 360,
      "topic": 7,
      "similarity": 0.7769141152082134
    },
    {
      "doc": 360,
      "topic": 8,
      "similarity": 0.7616731558749902
    },
    {
      "doc": 360,
      "topic": 9,
      "similarity": 0.80170073567946
    },
    {
      "doc": 360,
      "topic": 10,
      "similarity": 0.7807522736915963
    },
    {
      "doc": 360,
      "topic": 11,
      "similarity": 0.7820275977785176
    },
    {
      "doc": 360,
      "topic": 13,
      "similarity": 0.7635880005299783
    },
    {
      "doc": 360,
      "topic": 14,
      "similarity": 0.76319239767075
    },
    {
      "doc": 360,
      "topic": 15,
      "similarity": 0.7670218131331669
    },
    {
      "doc": 360,
      "topic": 16,
      "similarity": 0.8105350995280035
    },
    {
      "doc": 360,
      "topic": 17,
      "similarity": 0.7853511102582617
    },
    {
      "doc": 360,
      "topic": 18,
      "similarity": 0.8169047708474881
    },
    {
      "doc": 360,
      "topic": 19,
      "similarity": 0.8489210735031902
    },
    {
      "doc": 360,
      "topic": 20,
      "similarity": 0.764578818161543
    },
    {
      "doc": 360,
      "topic": 21,
      "similarity": 0.80039126206537
    },
    {
      "doc": 361,
      "topic": 2,
      "similarity": 0.7755753781387603
    },
    {
      "doc": 361,
      "topic": 3,
      "similarity": 0.7849819473365569
    },
    {
      "doc": 361,
      "topic": 5,
      "similarity": 0.774963212265688
    },
    {
      "doc": 361,
      "topic": 7,
      "similarity": 0.7522072308167506
    },
    {
      "doc": 361,
      "topic": 8,
      "similarity": 0.8140259158071307
    },
    {
      "doc": 361,
      "topic": 9,
      "similarity": 0.8041809210582433
    },
    {
      "doc": 361,
      "topic": 10,
      "similarity": 0.759978318950018
    },
    {
      "doc": 361,
      "topic": 11,
      "similarity": 0.7698120991500463
    },
    {
      "doc": 361,
      "topic": 12,
      "similarity": 0.7648982880826867
    },
    {
      "doc": 361,
      "topic": 13,
      "similarity": 0.7790872943613479
    },
    {
      "doc": 361,
      "topic": 14,
      "similarity": 0.764610153629752
    },
    {
      "doc": 361,
      "topic": 15,
      "similarity": 0.7974070507510416
    },
    {
      "doc": 361,
      "topic": 16,
      "similarity": 0.7938230502756408
    },
    {
      "doc": 361,
      "topic": 17,
      "similarity": 0.8115245514749287
    },
    {
      "doc": 361,
      "topic": 18,
      "similarity": 0.767771695050173
    },
    {
      "doc": 361,
      "topic": 19,
      "similarity": 0.8080405672237884
    },
    {
      "doc": 361,
      "topic": 20,
      "similarity": 0.7797513514605443
    },
    {
      "doc": 361,
      "topic": 21,
      "similarity": 0.7914690694450135
    },
    {
      "doc": 361,
      "topic": 22,
      "similarity": 0.792956658612589
    },
    {
      "doc": 362,
      "topic": 2,
      "similarity": 0.7557822248957905
    },
    {
      "doc": 362,
      "topic": 3,
      "similarity": 0.7822938488949038
    },
    {
      "doc": 362,
      "topic": 5,
      "similarity": 0.7864195518693621
    },
    {
      "doc": 362,
      "topic": 7,
      "similarity": 0.7659487283691414
    },
    {
      "doc": 362,
      "topic": 8,
      "similarity": 0.797017311138481
    },
    {
      "doc": 362,
      "topic": 9,
      "similarity": 0.8170111396616027
    },
    {
      "doc": 362,
      "topic": 10,
      "similarity": 0.7695030521187882
    },
    {
      "doc": 362,
      "topic": 11,
      "similarity": 0.7636596586045872
    },
    {
      "doc": 362,
      "topic": 12,
      "similarity": 0.7539637424470572
    },
    {
      "doc": 362,
      "topic": 13,
      "similarity": 0.7863924929496887
    },
    {
      "doc": 362,
      "topic": 14,
      "similarity": 0.7559307152952581
    },
    {
      "doc": 362,
      "topic": 15,
      "similarity": 0.768742138436806
    },
    {
      "doc": 362,
      "topic": 16,
      "similarity": 0.7744648533192571
    },
    {
      "doc": 362,
      "topic": 17,
      "similarity": 0.7709414401492559
    },
    {
      "doc": 362,
      "topic": 18,
      "similarity": 0.7667156200670859
    },
    {
      "doc": 362,
      "topic": 19,
      "similarity": 0.791421211491315
    },
    {
      "doc": 362,
      "topic": 20,
      "similarity": 0.785512582037538
    },
    {
      "doc": 362,
      "topic": 21,
      "similarity": 0.788024438951122
    },
    {
      "doc": 362,
      "topic": 22,
      "similarity": 0.7564718249907079
    },
    {
      "doc": 363,
      "topic": 2,
      "similarity": 0.762599180485758
    },
    {
      "doc": 363,
      "topic": 3,
      "similarity": 0.7849171487300564
    },
    {
      "doc": 363,
      "topic": 5,
      "similarity": 0.780384630642637
    },
    {
      "doc": 363,
      "topic": 7,
      "similarity": 0.7673063452959134
    },
    {
      "doc": 363,
      "topic": 8,
      "similarity": 0.7776422415871092
    },
    {
      "doc": 363,
      "topic": 9,
      "similarity": 0.7996867188399474
    },
    {
      "doc": 363,
      "topic": 10,
      "similarity": 0.7600592715488634
    },
    {
      "doc": 363,
      "topic": 11,
      "similarity": 0.7693728068200169
    },
    {
      "doc": 363,
      "topic": 15,
      "similarity": 0.7874327741457136
    },
    {
      "doc": 363,
      "topic": 16,
      "similarity": 0.7827071177443399
    },
    {
      "doc": 363,
      "topic": 17,
      "similarity": 0.8072995134744411
    },
    {
      "doc": 363,
      "topic": 19,
      "similarity": 0.8028657141135301
    },
    {
      "doc": 363,
      "topic": 20,
      "similarity": 0.7703468473115174
    },
    {
      "doc": 363,
      "topic": 21,
      "similarity": 0.7922738540653033
    },
    {
      "doc": 363,
      "topic": 22,
      "similarity": 0.7610702618816313
    },
    {
      "doc": 364,
      "topic": 2,
      "similarity": 0.7648718047649812
    },
    {
      "doc": 364,
      "topic": 3,
      "similarity": 0.7713746072042849
    },
    {
      "doc": 364,
      "topic": 4,
      "similarity": 0.7509066467026649
    },
    {
      "doc": 364,
      "topic": 5,
      "similarity": 0.7810895859855502
    },
    {
      "doc": 364,
      "topic": 7,
      "similarity": 0.7563644884332937
    },
    {
      "doc": 364,
      "topic": 9,
      "similarity": 0.8113782017323955
    },
    {
      "doc": 364,
      "topic": 10,
      "similarity": 0.7679773248398534
    },
    {
      "doc": 364,
      "topic": 11,
      "similarity": 0.7572298173178211
    },
    {
      "doc": 364,
      "topic": 14,
      "similarity": 0.7572227346045488
    },
    {
      "doc": 364,
      "topic": 15,
      "similarity": 0.7508414148891681
    },
    {
      "doc": 364,
      "topic": 16,
      "similarity": 0.8002545053504712
    },
    {
      "doc": 364,
      "topic": 17,
      "similarity": 0.7740694497945029
    },
    {
      "doc": 364,
      "topic": 19,
      "similarity": 0.7833411730273673
    },
    {
      "doc": 364,
      "topic": 20,
      "similarity": 0.792229178466685
    },
    {
      "doc": 364,
      "topic": 21,
      "similarity": 0.7808238694370873
    },
    {
      "doc": 364,
      "topic": 23,
      "similarity": 0.7507720788713509
    },
    {
      "doc": 365,
      "topic": 2,
      "similarity": 0.7620296472200008
    },
    {
      "doc": 365,
      "topic": 3,
      "similarity": 0.7675389502510114
    },
    {
      "doc": 365,
      "topic": 5,
      "similarity": 0.7613523463861152
    },
    {
      "doc": 365,
      "topic": 8,
      "similarity": 0.762864860959598
    },
    {
      "doc": 365,
      "topic": 9,
      "similarity": 0.7830265036062205
    },
    {
      "doc": 365,
      "topic": 10,
      "similarity": 0.7681826240432482
    },
    {
      "doc": 365,
      "topic": 11,
      "similarity": 0.7648774862508304
    },
    {
      "doc": 365,
      "topic": 12,
      "similarity": 0.8112288608748723
    },
    {
      "doc": 365,
      "topic": 13,
      "similarity": 0.7646855539744027
    },
    {
      "doc": 365,
      "topic": 15,
      "similarity": 0.7793362483045351
    },
    {
      "doc": 365,
      "topic": 16,
      "similarity": 0.7790136049799156
    },
    {
      "doc": 365,
      "topic": 17,
      "similarity": 0.7909676806435738
    },
    {
      "doc": 365,
      "topic": 18,
      "similarity": 0.7728641115613761
    },
    {
      "doc": 365,
      "topic": 19,
      "similarity": 0.802442903671742
    },
    {
      "doc": 365,
      "topic": 20,
      "similarity": 0.7989346661898917
    },
    {
      "doc": 365,
      "topic": 21,
      "similarity": 0.7767448836016975
    },
    {
      "doc": 365,
      "topic": 22,
      "similarity": 0.7683898853821954
    },
    {
      "doc": 366,
      "topic": 1,
      "similarity": 0.7817972406879189
    },
    {
      "doc": 366,
      "topic": 2,
      "similarity": 0.779647208101685
    },
    {
      "doc": 366,
      "topic": 3,
      "similarity": 0.820440872572688
    },
    {
      "doc": 366,
      "topic": 4,
      "similarity": 0.7600553850784709
    },
    {
      "doc": 366,
      "topic": 5,
      "similarity": 0.8073084777265672
    },
    {
      "doc": 366,
      "topic": 7,
      "similarity": 0.7827092449770774
    },
    {
      "doc": 366,
      "topic": 8,
      "similarity": 0.7964674503694797
    },
    {
      "doc": 366,
      "topic": 9,
      "similarity": 0.830459797407807
    },
    {
      "doc": 366,
      "topic": 10,
      "similarity": 0.8112477201123833
    },
    {
      "doc": 366,
      "topic": 11,
      "similarity": 0.7872006386183557
    },
    {
      "doc": 366,
      "topic": 12,
      "similarity": 0.7627270157430075
    },
    {
      "doc": 366,
      "topic": 13,
      "similarity": 0.7773737487073459
    },
    {
      "doc": 366,
      "topic": 14,
      "similarity": 0.7918303106563528
    },
    {
      "doc": 366,
      "topic": 15,
      "similarity": 0.7717102178944004
    },
    {
      "doc": 366,
      "topic": 16,
      "similarity": 0.8069716103297369
    },
    {
      "doc": 366,
      "topic": 17,
      "similarity": 0.796295379566625
    },
    {
      "doc": 366,
      "topic": 18,
      "similarity": 0.7759431973860766
    },
    {
      "doc": 366,
      "topic": 19,
      "similarity": 0.8222799155884015
    },
    {
      "doc": 366,
      "topic": 20,
      "similarity": 0.8017268547303444
    },
    {
      "doc": 366,
      "topic": 21,
      "similarity": 0.7998573177137503
    },
    {
      "doc": 366,
      "topic": 23,
      "similarity": 0.8163279751903731
    },
    {
      "doc": 366,
      "topic": 24,
      "similarity": 0.7523800013111369
    },
    {
      "doc": 367,
      "topic": 3,
      "similarity": 0.7619068844781187
    },
    {
      "doc": 367,
      "topic": 5,
      "similarity": 0.7696606655509339
    },
    {
      "doc": 367,
      "topic": 9,
      "similarity": 0.773147930586738
    },
    {
      "doc": 367,
      "topic": 11,
      "similarity": 0.7941440630245059
    },
    {
      "doc": 367,
      "topic": 15,
      "similarity": 0.7600198396757516
    },
    {
      "doc": 367,
      "topic": 16,
      "similarity": 0.7756975567821967
    },
    {
      "doc": 367,
      "topic": 17,
      "similarity": 0.794873359759175
    },
    {
      "doc": 367,
      "topic": 18,
      "similarity": 0.756535170347226
    },
    {
      "doc": 367,
      "topic": 19,
      "similarity": 0.7762558185291969
    },
    {
      "doc": 367,
      "topic": 20,
      "similarity": 0.7961551224325873
    },
    {
      "doc": 367,
      "topic": 21,
      "similarity": 0.7706993943677073
    },
    {
      "doc": 367,
      "topic": 23,
      "similarity": 0.75565772104391
    },
    {
      "doc": 368,
      "topic": 5,
      "similarity": 0.7689571938395227
    },
    {
      "doc": 368,
      "topic": 9,
      "similarity": 0.7835424309942146
    },
    {
      "doc": 368,
      "topic": 16,
      "similarity": 0.7834745631206274
    },
    {
      "doc": 368,
      "topic": 17,
      "similarity": 0.7652635618326507
    },
    {
      "doc": 368,
      "topic": 19,
      "similarity": 0.7921400639584003
    },
    {
      "doc": 368,
      "topic": 20,
      "similarity": 0.7815945373709932
    },
    {
      "doc": 368,
      "topic": 21,
      "similarity": 0.760091512361522
    },
    {
      "doc": 369,
      "topic": 3,
      "similarity": 0.7838813172087634
    },
    {
      "doc": 369,
      "topic": 5,
      "similarity": 0.7792570260441083
    },
    {
      "doc": 369,
      "topic": 7,
      "similarity": 0.7777920145948131
    },
    {
      "doc": 369,
      "topic": 8,
      "similarity": 0.7511220491558328
    },
    {
      "doc": 369,
      "topic": 9,
      "similarity": 0.8259743928359855
    },
    {
      "doc": 369,
      "topic": 10,
      "similarity": 0.8025446224306451
    },
    {
      "doc": 369,
      "topic": 11,
      "similarity": 0.7878713655173604
    },
    {
      "doc": 369,
      "topic": 15,
      "similarity": 0.7538804334988165
    },
    {
      "doc": 369,
      "topic": 16,
      "similarity": 0.7738545587950221
    },
    {
      "doc": 369,
      "topic": 17,
      "similarity": 0.7759867578422236
    },
    {
      "doc": 369,
      "topic": 18,
      "similarity": 0.7615107469931853
    },
    {
      "doc": 369,
      "topic": 19,
      "similarity": 0.7872066571918759
    },
    {
      "doc": 369,
      "topic": 20,
      "similarity": 0.7747929347568768
    },
    {
      "doc": 369,
      "topic": 21,
      "similarity": 0.7952659481636672
    },
    {
      "doc": 369,
      "topic": 23,
      "similarity": 0.7527414476297121
    },
    {
      "doc": 370,
      "topic": 1,
      "similarity": 0.7510983478952413
    },
    {
      "doc": 370,
      "topic": 2,
      "similarity": 0.7610885328976772
    },
    {
      "doc": 370,
      "topic": 3,
      "similarity": 0.8088359976771843
    },
    {
      "doc": 370,
      "topic": 4,
      "similarity": 0.7611043497363217
    },
    {
      "doc": 370,
      "topic": 5,
      "similarity": 0.7985721980857451
    },
    {
      "doc": 370,
      "topic": 7,
      "similarity": 0.7907983601499553
    },
    {
      "doc": 370,
      "topic": 8,
      "similarity": 0.7768344347706424
    },
    {
      "doc": 370,
      "topic": 9,
      "similarity": 0.8309669782901504
    },
    {
      "doc": 370,
      "topic": 10,
      "similarity": 0.7830441040667824
    },
    {
      "doc": 370,
      "topic": 11,
      "similarity": 0.7985110235011389
    },
    {
      "doc": 370,
      "topic": 13,
      "similarity": 0.7664602435207507
    },
    {
      "doc": 370,
      "topic": 14,
      "similarity": 0.7798545023483134
    },
    {
      "doc": 370,
      "topic": 15,
      "similarity": 0.7971847236069802
    },
    {
      "doc": 370,
      "topic": 16,
      "similarity": 0.7861886844000547
    },
    {
      "doc": 370,
      "topic": 17,
      "similarity": 0.8200027095180028
    },
    {
      "doc": 370,
      "topic": 19,
      "similarity": 0.7950192756152697
    },
    {
      "doc": 370,
      "topic": 21,
      "similarity": 0.8038674144943025
    },
    {
      "doc": 370,
      "topic": 23,
      "similarity": 0.7614875299912245
    },
    {
      "doc": 370,
      "topic": 24,
      "similarity": 0.7825255669903625
    },
    {
      "doc": 371,
      "topic": 9,
      "similarity": 0.7813747617134187
    },
    {
      "doc": 371,
      "topic": 16,
      "similarity": 0.7963046136661516
    },
    {
      "doc": 371,
      "topic": 17,
      "similarity": 0.7641834068467188
    },
    {
      "doc": 371,
      "topic": 18,
      "similarity": 0.7736405272486235
    },
    {
      "doc": 371,
      "topic": 19,
      "similarity": 0.784483647276364
    },
    {
      "doc": 371,
      "topic": 20,
      "similarity": 0.7640465473046885
    },
    {
      "doc": 371,
      "topic": 21,
      "similarity": 0.758592084599476
    },
    {
      "doc": 372,
      "topic": 1,
      "similarity": 0.7516405362157184
    },
    {
      "doc": 372,
      "topic": 2,
      "similarity": 0.7560152843545246
    },
    {
      "doc": 372,
      "topic": 3,
      "similarity": 0.8007306718882035
    },
    {
      "doc": 372,
      "topic": 4,
      "similarity": 0.7705246151284861
    },
    {
      "doc": 372,
      "topic": 5,
      "similarity": 0.7905996986701141
    },
    {
      "doc": 372,
      "topic": 7,
      "similarity": 0.8022082475347481
    },
    {
      "doc": 372,
      "topic": 8,
      "similarity": 0.7708358065038573
    },
    {
      "doc": 372,
      "topic": 9,
      "similarity": 0.8228294002981081
    },
    {
      "doc": 372,
      "topic": 10,
      "similarity": 0.7679731409050082
    },
    {
      "doc": 372,
      "topic": 11,
      "similarity": 0.770192647387511
    },
    {
      "doc": 372,
      "topic": 13,
      "similarity": 0.7723688116016865
    },
    {
      "doc": 372,
      "topic": 14,
      "similarity": 0.751980659261503
    },
    {
      "doc": 372,
      "topic": 15,
      "similarity": 0.7648388807168995
    },
    {
      "doc": 372,
      "topic": 16,
      "similarity": 0.7907873362750996
    },
    {
      "doc": 372,
      "topic": 17,
      "similarity": 0.7818113148854284
    },
    {
      "doc": 372,
      "topic": 19,
      "similarity": 0.7854154672593581
    },
    {
      "doc": 372,
      "topic": 20,
      "similarity": 0.7644901036794981
    },
    {
      "doc": 372,
      "topic": 21,
      "similarity": 0.7972540195773961
    },
    {
      "doc": 372,
      "topic": 24,
      "similarity": 0.7643440865144915
    },
    {
      "doc": 373,
      "topic": 0,
      "similarity": 0.7539127539541036
    },
    {
      "doc": 373,
      "topic": 3,
      "similarity": 0.7649466425053971
    },
    {
      "doc": 373,
      "topic": 5,
      "similarity": 0.7649026760403241
    },
    {
      "doc": 373,
      "topic": 9,
      "similarity": 0.7940624899138813
    },
    {
      "doc": 373,
      "topic": 11,
      "similarity": 0.8142231128843643
    },
    {
      "doc": 373,
      "topic": 15,
      "similarity": 0.7671235450967964
    },
    {
      "doc": 373,
      "topic": 16,
      "similarity": 0.7853776025304933
    },
    {
      "doc": 373,
      "topic": 17,
      "similarity": 0.7640891595404906
    },
    {
      "doc": 373,
      "topic": 19,
      "similarity": 0.7748426674234347
    },
    {
      "doc": 373,
      "topic": 21,
      "similarity": 0.7694167922167381
    },
    {
      "doc": 374,
      "topic": 3,
      "similarity": 0.7919137316399706
    },
    {
      "doc": 374,
      "topic": 4,
      "similarity": 0.7627710862381192
    },
    {
      "doc": 374,
      "topic": 5,
      "similarity": 0.8102491861728642
    },
    {
      "doc": 374,
      "topic": 7,
      "similarity": 0.7697983035494066
    },
    {
      "doc": 374,
      "topic": 8,
      "similarity": 0.7665410868584728
    },
    {
      "doc": 374,
      "topic": 9,
      "similarity": 0.807752135784822
    },
    {
      "doc": 374,
      "topic": 10,
      "similarity": 0.7644138835657246
    },
    {
      "doc": 374,
      "topic": 11,
      "similarity": 0.760294903317816
    },
    {
      "doc": 374,
      "topic": 13,
      "similarity": 0.7533830899257722
    },
    {
      "doc": 374,
      "topic": 14,
      "similarity": 0.7513801782113146
    },
    {
      "doc": 374,
      "topic": 15,
      "similarity": 0.7595280703600947
    },
    {
      "doc": 374,
      "topic": 16,
      "similarity": 0.8087851825470818
    },
    {
      "doc": 374,
      "topic": 17,
      "similarity": 0.7931428099153369
    },
    {
      "doc": 374,
      "topic": 19,
      "similarity": 0.7938141021282381
    },
    {
      "doc": 374,
      "topic": 20,
      "similarity": 0.7720779248594923
    },
    {
      "doc": 374,
      "topic": 21,
      "similarity": 0.8017789326173346
    },
    {
      "doc": 374,
      "topic": 23,
      "similarity": 0.7549910766988687
    },
    {
      "doc": 374,
      "topic": 24,
      "similarity": 0.7634037913639005
    },
    {
      "doc": 375,
      "topic": 3,
      "similarity": 0.775241266427514
    },
    {
      "doc": 375,
      "topic": 5,
      "similarity": 0.7712973133087535
    },
    {
      "doc": 375,
      "topic": 8,
      "similarity": 0.7687548202029347
    },
    {
      "doc": 375,
      "topic": 9,
      "similarity": 0.8125653814891957
    },
    {
      "doc": 375,
      "topic": 10,
      "similarity": 0.7556415024193086
    },
    {
      "doc": 375,
      "topic": 11,
      "similarity": 0.7588670913049547
    },
    {
      "doc": 375,
      "topic": 12,
      "similarity": 0.7954556094838126
    },
    {
      "doc": 375,
      "topic": 15,
      "similarity": 0.7670232623379498
    },
    {
      "doc": 375,
      "topic": 16,
      "similarity": 0.7829512725443067
    },
    {
      "doc": 375,
      "topic": 17,
      "similarity": 0.7855587318580178
    },
    {
      "doc": 375,
      "topic": 19,
      "similarity": 0.777013398987036
    },
    {
      "doc": 375,
      "topic": 20,
      "similarity": 0.7708573048076234
    },
    {
      "doc": 375,
      "topic": 21,
      "similarity": 0.7991049958115483
    },
    {
      "doc": 375,
      "topic": 24,
      "similarity": 0.7782670185000471
    },
    {
      "doc": 376,
      "topic": 1,
      "similarity": 0.7677666485630309
    },
    {
      "doc": 376,
      "topic": 2,
      "similarity": 0.8005956006116948
    },
    {
      "doc": 376,
      "topic": 3,
      "similarity": 0.8151321249611418
    },
    {
      "doc": 376,
      "topic": 4,
      "similarity": 0.7785781205038466
    },
    {
      "doc": 376,
      "topic": 5,
      "similarity": 0.8365946023867098
    },
    {
      "doc": 376,
      "topic": 7,
      "similarity": 0.8003757178002043
    },
    {
      "doc": 376,
      "topic": 8,
      "similarity": 0.7974767977141962
    },
    {
      "doc": 376,
      "topic": 9,
      "similarity": 0.8328222706005269
    },
    {
      "doc": 376,
      "topic": 10,
      "similarity": 0.7929438447359478
    },
    {
      "doc": 376,
      "topic": 11,
      "similarity": 0.7948091608052135
    },
    {
      "doc": 376,
      "topic": 13,
      "similarity": 0.7935787032355142
    },
    {
      "doc": 376,
      "topic": 14,
      "similarity": 0.7858412721915616
    },
    {
      "doc": 376,
      "topic": 15,
      "similarity": 0.7951769490577382
    },
    {
      "doc": 376,
      "topic": 16,
      "similarity": 0.8222404100817166
    },
    {
      "doc": 376,
      "topic": 17,
      "similarity": 0.8323707757913176
    },
    {
      "doc": 376,
      "topic": 18,
      "similarity": 0.7762626217744937
    },
    {
      "doc": 376,
      "topic": 19,
      "similarity": 0.8205203383339903
    },
    {
      "doc": 376,
      "topic": 20,
      "similarity": 0.7997782708570194
    },
    {
      "doc": 376,
      "topic": 21,
      "similarity": 0.8251853502338986
    },
    {
      "doc": 376,
      "topic": 23,
      "similarity": 0.7501480344046066
    },
    {
      "doc": 376,
      "topic": 24,
      "similarity": 0.7526812230445864
    },
    {
      "doc": 377,
      "topic": 1,
      "similarity": 0.7511060809847115
    },
    {
      "doc": 377,
      "topic": 2,
      "similarity": 0.7730202089720616
    },
    {
      "doc": 377,
      "topic": 3,
      "similarity": 0.7871499317315328
    },
    {
      "doc": 377,
      "topic": 4,
      "similarity": 0.7515798323900217
    },
    {
      "doc": 377,
      "topic": 5,
      "similarity": 0.7917607115675017
    },
    {
      "doc": 377,
      "topic": 7,
      "similarity": 0.7985341706608682
    },
    {
      "doc": 377,
      "topic": 8,
      "similarity": 0.8039671889216208
    },
    {
      "doc": 377,
      "topic": 9,
      "similarity": 0.8235758802081771
    },
    {
      "doc": 377,
      "topic": 10,
      "similarity": 0.7743950450004005
    },
    {
      "doc": 377,
      "topic": 11,
      "similarity": 0.7649914803874128
    },
    {
      "doc": 377,
      "topic": 12,
      "similarity": 0.7551958220090136
    },
    {
      "doc": 377,
      "topic": 13,
      "similarity": 0.7747759455650098
    },
    {
      "doc": 377,
      "topic": 14,
      "similarity": 0.7818992966179023
    },
    {
      "doc": 377,
      "topic": 15,
      "similarity": 0.7900340405197026
    },
    {
      "doc": 377,
      "topic": 16,
      "similarity": 0.8048087508016369
    },
    {
      "doc": 377,
      "topic": 17,
      "similarity": 0.785856883161835
    },
    {
      "doc": 377,
      "topic": 18,
      "similarity": 0.7595289928895106
    },
    {
      "doc": 377,
      "topic": 19,
      "similarity": 0.8184788727511851
    },
    {
      "doc": 377,
      "topic": 20,
      "similarity": 0.7705834306097304
    },
    {
      "doc": 377,
      "topic": 21,
      "similarity": 0.7844946269782531
    },
    {
      "doc": 377,
      "topic": 22,
      "similarity": 0.7780456301610472
    },
    {
      "doc": 378,
      "topic": 1,
      "similarity": 0.762369093563438
    },
    {
      "doc": 378,
      "topic": 2,
      "similarity": 0.7797317332823073
    },
    {
      "doc": 378,
      "topic": 3,
      "similarity": 0.8090395978861677
    },
    {
      "doc": 378,
      "topic": 4,
      "similarity": 0.7634855637058544
    },
    {
      "doc": 378,
      "topic": 5,
      "similarity": 0.8050075690753902
    },
    {
      "doc": 378,
      "topic": 6,
      "similarity": 0.7645047387361443
    },
    {
      "doc": 378,
      "topic": 7,
      "similarity": 0.7862917156878714
    },
    {
      "doc": 378,
      "topic": 8,
      "similarity": 0.7852969942122322
    },
    {
      "doc": 378,
      "topic": 9,
      "similarity": 0.833749937597541
    },
    {
      "doc": 378,
      "topic": 10,
      "similarity": 0.8041287068288075
    },
    {
      "doc": 378,
      "topic": 11,
      "similarity": 0.8029893205752621
    },
    {
      "doc": 378,
      "topic": 12,
      "similarity": 0.769288080301707
    },
    {
      "doc": 378,
      "topic": 13,
      "similarity": 0.7753318589725582
    },
    {
      "doc": 378,
      "topic": 14,
      "similarity": 0.7776577562373777
    },
    {
      "doc": 378,
      "topic": 15,
      "similarity": 0.805002164653509
    },
    {
      "doc": 378,
      "topic": 16,
      "similarity": 0.8254500039906696
    },
    {
      "doc": 378,
      "topic": 17,
      "similarity": 0.8245111884999181
    },
    {
      "doc": 378,
      "topic": 18,
      "similarity": 0.7870162272854536
    },
    {
      "doc": 378,
      "topic": 19,
      "similarity": 0.8356677914464061
    },
    {
      "doc": 378,
      "topic": 20,
      "similarity": 0.7966649686899708
    },
    {
      "doc": 378,
      "topic": 21,
      "similarity": 0.8053567284109718
    },
    {
      "doc": 378,
      "topic": 22,
      "similarity": 0.763580886743729
    },
    {
      "doc": 378,
      "topic": 23,
      "similarity": 0.7521055616758293
    },
    {
      "doc": 378,
      "topic": 24,
      "similarity": 0.7703997361539534
    },
    {
      "doc": 379,
      "topic": 2,
      "similarity": 0.7622792175924636
    },
    {
      "doc": 379,
      "topic": 3,
      "similarity": 0.8047697828354247
    },
    {
      "doc": 379,
      "topic": 4,
      "similarity": 0.7611474496960988
    },
    {
      "doc": 379,
      "topic": 5,
      "similarity": 0.7968809376113609
    },
    {
      "doc": 379,
      "topic": 7,
      "similarity": 0.7898340435563407
    },
    {
      "doc": 379,
      "topic": 8,
      "similarity": 0.8039062916156443
    },
    {
      "doc": 379,
      "topic": 9,
      "similarity": 0.8339435922371584
    },
    {
      "doc": 379,
      "topic": 10,
      "similarity": 0.7785323074380582
    },
    {
      "doc": 379,
      "topic": 11,
      "similarity": 0.777328187389771
    },
    {
      "doc": 379,
      "topic": 13,
      "similarity": 0.7636685574114569
    },
    {
      "doc": 379,
      "topic": 14,
      "similarity": 0.7751176113357925
    },
    {
      "doc": 379,
      "topic": 15,
      "similarity": 0.7796049390922236
    },
    {
      "doc": 379,
      "topic": 16,
      "similarity": 0.809046694781185
    },
    {
      "doc": 379,
      "topic": 17,
      "similarity": 0.7938383047776459
    },
    {
      "doc": 379,
      "topic": 18,
      "similarity": 0.7561356972063972
    },
    {
      "doc": 379,
      "topic": 19,
      "similarity": 0.81501940267494
    },
    {
      "doc": 379,
      "topic": 20,
      "similarity": 0.7601612351274042
    },
    {
      "doc": 379,
      "topic": 21,
      "similarity": 0.8080491970289554
    },
    {
      "doc": 379,
      "topic": 24,
      "similarity": 0.7580272458987951
    },
    {
      "doc": 380,
      "topic": 2,
      "similarity": 0.7734882987939652
    },
    {
      "doc": 380,
      "topic": 3,
      "similarity": 0.8039323347705565
    },
    {
      "doc": 380,
      "topic": 5,
      "similarity": 0.7964483464513806
    },
    {
      "doc": 380,
      "topic": 7,
      "similarity": 0.7679515878423677
    },
    {
      "doc": 380,
      "topic": 8,
      "similarity": 0.7587677304347923
    },
    {
      "doc": 380,
      "topic": 9,
      "similarity": 0.8148601683918361
    },
    {
      "doc": 380,
      "topic": 10,
      "similarity": 0.7589327118419553
    },
    {
      "doc": 380,
      "topic": 11,
      "similarity": 0.7734873278064897
    },
    {
      "doc": 380,
      "topic": 13,
      "similarity": 0.7631002077507643
    },
    {
      "doc": 380,
      "topic": 14,
      "similarity": 0.7541996486479114
    },
    {
      "doc": 380,
      "topic": 15,
      "similarity": 0.7741216911086417
    },
    {
      "doc": 380,
      "topic": 16,
      "similarity": 0.7807103891008198
    },
    {
      "doc": 380,
      "topic": 17,
      "similarity": 0.7906019054125546
    },
    {
      "doc": 380,
      "topic": 18,
      "similarity": 0.7731042374234945
    },
    {
      "doc": 380,
      "topic": 19,
      "similarity": 0.8017012862806964
    },
    {
      "doc": 380,
      "topic": 20,
      "similarity": 0.7716759831347869
    },
    {
      "doc": 380,
      "topic": 21,
      "similarity": 0.8059226209861319
    },
    {
      "doc": 380,
      "topic": 23,
      "similarity": 0.7773179155661173
    },
    {
      "doc": 380,
      "topic": 24,
      "similarity": 0.7717000347474441
    },
    {
      "doc": 381,
      "topic": 3,
      "similarity": 0.7730113959730531
    },
    {
      "doc": 381,
      "topic": 4,
      "similarity": 0.7571079117322246
    },
    {
      "doc": 381,
      "topic": 5,
      "similarity": 0.7680979758510194
    },
    {
      "doc": 381,
      "topic": 7,
      "similarity": 0.7628237088989432
    },
    {
      "doc": 381,
      "topic": 8,
      "similarity": 0.7503341510876067
    },
    {
      "doc": 381,
      "topic": 9,
      "similarity": 0.8095714274698143
    },
    {
      "doc": 381,
      "topic": 11,
      "similarity": 0.8206052743887802
    },
    {
      "doc": 381,
      "topic": 15,
      "similarity": 0.7634775616845462
    },
    {
      "doc": 381,
      "topic": 16,
      "similarity": 0.7910302883354206
    },
    {
      "doc": 381,
      "topic": 17,
      "similarity": 0.7814089179551854
    },
    {
      "doc": 381,
      "topic": 19,
      "similarity": 0.7709173579821006
    },
    {
      "doc": 381,
      "topic": 20,
      "similarity": 0.751373267886062
    },
    {
      "doc": 381,
      "topic": 21,
      "similarity": 0.7980665697266904
    },
    {
      "doc": 381,
      "topic": 24,
      "similarity": 0.8028525509593446
    },
    {
      "doc": 382,
      "topic": 1,
      "similarity": 0.7683059581842613
    },
    {
      "doc": 382,
      "topic": 2,
      "similarity": 0.7914282746092288
    },
    {
      "doc": 382,
      "topic": 3,
      "similarity": 0.8015113428699443
    },
    {
      "doc": 382,
      "topic": 4,
      "similarity": 0.7513691149314645
    },
    {
      "doc": 382,
      "topic": 5,
      "similarity": 0.8135113857359512
    },
    {
      "doc": 382,
      "topic": 7,
      "similarity": 0.8062210401101368
    },
    {
      "doc": 382,
      "topic": 8,
      "similarity": 0.8052092869087298
    },
    {
      "doc": 382,
      "topic": 9,
      "similarity": 0.8296498940373195
    },
    {
      "doc": 382,
      "topic": 10,
      "similarity": 0.7932426739291455
    },
    {
      "doc": 382,
      "topic": 11,
      "similarity": 0.8056801102941373
    },
    {
      "doc": 382,
      "topic": 12,
      "similarity": 0.7567614609172277
    },
    {
      "doc": 382,
      "topic": 13,
      "similarity": 0.8042000645465072
    },
    {
      "doc": 382,
      "topic": 14,
      "similarity": 0.7848278791841166
    },
    {
      "doc": 382,
      "topic": 15,
      "similarity": 0.8050008287493475
    },
    {
      "doc": 382,
      "topic": 16,
      "similarity": 0.8174772554008328
    },
    {
      "doc": 382,
      "topic": 17,
      "similarity": 0.8155352596932233
    },
    {
      "doc": 382,
      "topic": 18,
      "similarity": 0.785262952087971
    },
    {
      "doc": 382,
      "topic": 19,
      "similarity": 0.8277195119311707
    },
    {
      "doc": 382,
      "topic": 20,
      "similarity": 0.7888427672348062
    },
    {
      "doc": 382,
      "topic": 21,
      "similarity": 0.8030671083284133
    },
    {
      "doc": 382,
      "topic": 22,
      "similarity": 0.7877689387479994
    },
    {
      "doc": 383,
      "topic": 1,
      "similarity": 0.7543055665843865
    },
    {
      "doc": 383,
      "topic": 2,
      "similarity": 0.763982858325245
    },
    {
      "doc": 383,
      "topic": 3,
      "similarity": 0.8157265563155656
    },
    {
      "doc": 383,
      "topic": 4,
      "similarity": 0.7702037684894115
    },
    {
      "doc": 383,
      "topic": 5,
      "similarity": 0.7981781370769234
    },
    {
      "doc": 383,
      "topic": 7,
      "similarity": 0.7944012696018634
    },
    {
      "doc": 383,
      "topic": 8,
      "similarity": 0.7895238099464817
    },
    {
      "doc": 383,
      "topic": 9,
      "similarity": 0.834669552272143
    },
    {
      "doc": 383,
      "topic": 10,
      "similarity": 0.7912083577866588
    },
    {
      "doc": 383,
      "topic": 11,
      "similarity": 0.7909787901974702
    },
    {
      "doc": 383,
      "topic": 13,
      "similarity": 0.7691063918605447
    },
    {
      "doc": 383,
      "topic": 14,
      "similarity": 0.7691315579263159
    },
    {
      "doc": 383,
      "topic": 15,
      "similarity": 0.7858225595515755
    },
    {
      "doc": 383,
      "topic": 16,
      "similarity": 0.8194828804090625
    },
    {
      "doc": 383,
      "topic": 17,
      "similarity": 0.803476399819394
    },
    {
      "doc": 383,
      "topic": 18,
      "similarity": 0.82589751985737
    },
    {
      "doc": 383,
      "topic": 19,
      "similarity": 0.8583404971217823
    },
    {
      "doc": 383,
      "topic": 20,
      "similarity": 0.7820132980986977
    },
    {
      "doc": 383,
      "topic": 21,
      "similarity": 0.8182775242780166
    },
    {
      "doc": 383,
      "topic": 23,
      "similarity": 0.7618600129458173
    },
    {
      "doc": 383,
      "topic": 24,
      "similarity": 0.7741892659646644
    },
    {
      "doc": 384,
      "topic": 1,
      "similarity": 0.7930261932241368
    },
    {
      "doc": 384,
      "topic": 2,
      "similarity": 0.7860816164318457
    },
    {
      "doc": 384,
      "topic": 3,
      "similarity": 0.8242253672067162
    },
    {
      "doc": 384,
      "topic": 4,
      "similarity": 0.7722592275639196
    },
    {
      "doc": 384,
      "topic": 5,
      "similarity": 0.7932837794364758
    },
    {
      "doc": 384,
      "topic": 7,
      "similarity": 0.7925279253949761
    },
    {
      "doc": 384,
      "topic": 8,
      "similarity": 0.7912520914612201
    },
    {
      "doc": 384,
      "topic": 9,
      "similarity": 0.8240537216240876
    },
    {
      "doc": 384,
      "topic": 10,
      "similarity": 0.7844918427072589
    },
    {
      "doc": 384,
      "topic": 11,
      "similarity": 0.7822059553324149
    },
    {
      "doc": 384,
      "topic": 13,
      "similarity": 0.7839997149448028
    },
    {
      "doc": 384,
      "topic": 14,
      "similarity": 0.7813271552018238
    },
    {
      "doc": 384,
      "topic": 15,
      "similarity": 0.7981487915331879
    },
    {
      "doc": 384,
      "topic": 16,
      "similarity": 0.8261962308579088
    },
    {
      "doc": 384,
      "topic": 17,
      "similarity": 0.8157969501317001
    },
    {
      "doc": 384,
      "topic": 18,
      "similarity": 0.7636968460533418
    },
    {
      "doc": 384,
      "topic": 19,
      "similarity": 0.8198300495198152
    },
    {
      "doc": 384,
      "topic": 20,
      "similarity": 0.7849764441733487
    },
    {
      "doc": 384,
      "topic": 21,
      "similarity": 0.8054130544483822
    },
    {
      "doc": 384,
      "topic": 23,
      "similarity": 0.7597377131580063
    },
    {
      "doc": 385,
      "topic": 3,
      "similarity": 0.7892193243848388
    },
    {
      "doc": 385,
      "topic": 4,
      "similarity": 0.7574493227531492
    },
    {
      "doc": 385,
      "topic": 5,
      "similarity": 0.7954525240152623
    },
    {
      "doc": 385,
      "topic": 7,
      "similarity": 0.7691846780900158
    },
    {
      "doc": 385,
      "topic": 8,
      "similarity": 0.7626359913339106
    },
    {
      "doc": 385,
      "topic": 9,
      "similarity": 0.8149820380362405
    },
    {
      "doc": 385,
      "topic": 10,
      "similarity": 0.7865006293577064
    },
    {
      "doc": 385,
      "topic": 11,
      "similarity": 0.7723295883989314
    },
    {
      "doc": 385,
      "topic": 13,
      "similarity": 0.7529042925656112
    },
    {
      "doc": 385,
      "topic": 14,
      "similarity": 0.7543064842830218
    },
    {
      "doc": 385,
      "topic": 15,
      "similarity": 0.7633845643689745
    },
    {
      "doc": 385,
      "topic": 16,
      "similarity": 0.7986092451364272
    },
    {
      "doc": 385,
      "topic": 17,
      "similarity": 0.7910367945060224
    },
    {
      "doc": 385,
      "topic": 19,
      "similarity": 0.7870100960349716
    },
    {
      "doc": 385,
      "topic": 20,
      "similarity": 0.7778709680991807
    },
    {
      "doc": 385,
      "topic": 21,
      "similarity": 0.7988097594324219
    },
    {
      "doc": 385,
      "topic": 23,
      "similarity": 0.759755933851607
    },
    {
      "doc": 385,
      "topic": 24,
      "similarity": 0.7780527507970942
    },
    {
      "doc": 386,
      "topic": 2,
      "similarity": 0.7639443121022628
    },
    {
      "doc": 386,
      "topic": 3,
      "similarity": 0.7839990115342119
    },
    {
      "doc": 386,
      "topic": 5,
      "similarity": 0.7901840333211962
    },
    {
      "doc": 386,
      "topic": 7,
      "similarity": 0.7739889523987991
    },
    {
      "doc": 386,
      "topic": 8,
      "similarity": 0.7819682329843852
    },
    {
      "doc": 386,
      "topic": 9,
      "similarity": 0.8136382245704381
    },
    {
      "doc": 386,
      "topic": 10,
      "similarity": 0.7789903550999896
    },
    {
      "doc": 386,
      "topic": 11,
      "similarity": 0.7896754251630548
    },
    {
      "doc": 386,
      "topic": 13,
      "similarity": 0.7731690894003277
    },
    {
      "doc": 386,
      "topic": 14,
      "similarity": 0.7555196357429299
    },
    {
      "doc": 386,
      "topic": 15,
      "similarity": 0.7839158396754784
    },
    {
      "doc": 386,
      "topic": 16,
      "similarity": 0.7915149098181912
    },
    {
      "doc": 386,
      "topic": 17,
      "similarity": 0.7920838276942225
    },
    {
      "doc": 386,
      "topic": 18,
      "similarity": 0.7775246944580964
    },
    {
      "doc": 386,
      "topic": 19,
      "similarity": 0.7914185672178944
    },
    {
      "doc": 386,
      "topic": 20,
      "similarity": 0.7840177320571377
    },
    {
      "doc": 386,
      "topic": 21,
      "similarity": 0.7925720283623102
    },
    {
      "doc": 386,
      "topic": 22,
      "similarity": 0.7568895187040505
    },
    {
      "doc": 386,
      "topic": 24,
      "similarity": 0.7553471525550022
    },
    {
      "doc": 387,
      "topic": 2,
      "similarity": 0.7537161939864232
    },
    {
      "doc": 387,
      "topic": 3,
      "similarity": 0.7970358107552995
    },
    {
      "doc": 387,
      "topic": 5,
      "similarity": 0.7925563273472733
    },
    {
      "doc": 387,
      "topic": 7,
      "similarity": 0.7626922124819017
    },
    {
      "doc": 387,
      "topic": 8,
      "similarity": 0.7636681105340376
    },
    {
      "doc": 387,
      "topic": 9,
      "similarity": 0.8082704157776692
    },
    {
      "doc": 387,
      "topic": 10,
      "similarity": 0.7579933643353377
    },
    {
      "doc": 387,
      "topic": 11,
      "similarity": 0.7502024399338286
    },
    {
      "doc": 387,
      "topic": 14,
      "similarity": 0.7577731240048314
    },
    {
      "doc": 387,
      "topic": 15,
      "similarity": 0.7615128687533412
    },
    {
      "doc": 387,
      "topic": 16,
      "similarity": 0.7830592554942803
    },
    {
      "doc": 387,
      "topic": 17,
      "similarity": 0.7808891379523094
    },
    {
      "doc": 387,
      "topic": 19,
      "similarity": 0.8009738522721032
    },
    {
      "doc": 387,
      "topic": 20,
      "similarity": 0.7750560473464047
    },
    {
      "doc": 387,
      "topic": 21,
      "similarity": 0.7776209381595488
    },
    {
      "doc": 388,
      "topic": 2,
      "similarity": 0.7563384407467107
    },
    {
      "doc": 388,
      "topic": 3,
      "similarity": 0.8362789446282245
    },
    {
      "doc": 388,
      "topic": 4,
      "similarity": 0.7602761060198393
    },
    {
      "doc": 388,
      "topic": 5,
      "similarity": 0.7903190611711499
    },
    {
      "doc": 388,
      "topic": 7,
      "similarity": 0.779725802005233
    },
    {
      "doc": 388,
      "topic": 8,
      "similarity": 0.7690989647162523
    },
    {
      "doc": 388,
      "topic": 9,
      "similarity": 0.818338474917173
    },
    {
      "doc": 388,
      "topic": 10,
      "similarity": 0.7622324767384009
    },
    {
      "doc": 388,
      "topic": 11,
      "similarity": 0.771680032250613
    },
    {
      "doc": 388,
      "topic": 13,
      "similarity": 0.7590782477496923
    },
    {
      "doc": 388,
      "topic": 15,
      "similarity": 0.7638587859905643
    },
    {
      "doc": 388,
      "topic": 16,
      "similarity": 0.7790470318735052
    },
    {
      "doc": 388,
      "topic": 17,
      "similarity": 0.7763787783379389
    },
    {
      "doc": 388,
      "topic": 19,
      "similarity": 0.7956568911480504
    },
    {
      "doc": 388,
      "topic": 20,
      "similarity": 0.7632260951103025
    },
    {
      "doc": 388,
      "topic": 21,
      "similarity": 0.789391663575607
    },
    {
      "doc": 389,
      "topic": 3,
      "similarity": 0.7820546859570348
    },
    {
      "doc": 389,
      "topic": 4,
      "similarity": 0.7584147218699429
    },
    {
      "doc": 389,
      "topic": 5,
      "similarity": 0.7780534250374962
    },
    {
      "doc": 389,
      "topic": 7,
      "similarity": 0.7771663830918134
    },
    {
      "doc": 389,
      "topic": 8,
      "similarity": 0.7724908889520549
    },
    {
      "doc": 389,
      "topic": 9,
      "similarity": 0.8227730726107229
    },
    {
      "doc": 389,
      "topic": 10,
      "similarity": 0.7716739962913141
    },
    {
      "doc": 389,
      "topic": 11,
      "similarity": 0.8195349396936304
    },
    {
      "doc": 389,
      "topic": 15,
      "similarity": 0.7738300429782121
    },
    {
      "doc": 389,
      "topic": 16,
      "similarity": 0.7935927627252917
    },
    {
      "doc": 389,
      "topic": 17,
      "similarity": 0.7970005639816303
    },
    {
      "doc": 389,
      "topic": 19,
      "similarity": 0.7777137010256565
    },
    {
      "doc": 389,
      "topic": 20,
      "similarity": 0.7534013068056061
    },
    {
      "doc": 389,
      "topic": 21,
      "similarity": 0.8305242646316273
    },
    {
      "doc": 389,
      "topic": 24,
      "similarity": 0.80174586596438
    },
    {
      "doc": 390,
      "topic": 2,
      "similarity": 0.7587840188376901
    },
    {
      "doc": 390,
      "topic": 3,
      "similarity": 0.7802002914050908
    },
    {
      "doc": 390,
      "topic": 5,
      "similarity": 0.7618470177707523
    },
    {
      "doc": 390,
      "topic": 7,
      "similarity": 0.7955297011509685
    },
    {
      "doc": 390,
      "topic": 8,
      "similarity": 0.75047328979652
    },
    {
      "doc": 390,
      "topic": 9,
      "similarity": 0.82624800922512
    },
    {
      "doc": 390,
      "topic": 10,
      "similarity": 0.7563671774967925
    },
    {
      "doc": 390,
      "topic": 11,
      "similarity": 0.7572940438594459
    },
    {
      "doc": 390,
      "topic": 15,
      "similarity": 0.7591421489373616
    },
    {
      "doc": 390,
      "topic": 16,
      "similarity": 0.7842789717029051
    },
    {
      "doc": 390,
      "topic": 17,
      "similarity": 0.7664535225212725
    },
    {
      "doc": 390,
      "topic": 19,
      "similarity": 0.7868699622322789
    },
    {
      "doc": 390,
      "topic": 20,
      "similarity": 0.7664897109173499
    },
    {
      "doc": 390,
      "topic": 21,
      "similarity": 0.7859024573213792
    },
    {
      "doc": 390,
      "topic": 23,
      "similarity": 0.7729806747679357
    },
    {
      "doc": 391,
      "topic": 3,
      "similarity": 0.7586983624970258
    },
    {
      "doc": 391,
      "topic": 9,
      "similarity": 0.7783189887525849
    },
    {
      "doc": 391,
      "topic": 16,
      "similarity": 0.7693054516558678
    },
    {
      "doc": 391,
      "topic": 19,
      "similarity": 0.7637493720278268
    },
    {
      "doc": 392,
      "topic": 2,
      "similarity": 0.7592633936344141
    },
    {
      "doc": 392,
      "topic": 3,
      "similarity": 0.7974911511369507
    },
    {
      "doc": 392,
      "topic": 4,
      "similarity": 0.8190134301548992
    },
    {
      "doc": 392,
      "topic": 5,
      "similarity": 0.7910866362496413
    },
    {
      "doc": 392,
      "topic": 7,
      "similarity": 0.7761205305042476
    },
    {
      "doc": 392,
      "topic": 8,
      "similarity": 0.7544167921689091
    },
    {
      "doc": 392,
      "topic": 9,
      "similarity": 0.7928007976018242
    },
    {
      "doc": 392,
      "topic": 10,
      "similarity": 0.7709800624467867
    },
    {
      "doc": 392,
      "topic": 11,
      "similarity": 0.7764525371689794
    },
    {
      "doc": 392,
      "topic": 15,
      "similarity": 0.7568628164856319
    },
    {
      "doc": 392,
      "topic": 16,
      "similarity": 0.80389302452157
    },
    {
      "doc": 392,
      "topic": 17,
      "similarity": 0.7917865562848294
    },
    {
      "doc": 392,
      "topic": 19,
      "similarity": 0.7930301314121909
    },
    {
      "doc": 392,
      "topic": 20,
      "similarity": 0.7682008264631158
    },
    {
      "doc": 392,
      "topic": 21,
      "similarity": 0.7815271443153107
    },
    {
      "doc": 392,
      "topic": 24,
      "similarity": 0.7521815453502321
    },
    {
      "doc": 393,
      "topic": 1,
      "similarity": 0.7777257847870698
    },
    {
      "doc": 393,
      "topic": 2,
      "similarity": 0.8733815804315851
    },
    {
      "doc": 393,
      "topic": 3,
      "similarity": 0.8003735461076696
    },
    {
      "doc": 393,
      "topic": 4,
      "similarity": 0.7522891562940839
    },
    {
      "doc": 393,
      "topic": 5,
      "similarity": 0.7973897444301569
    },
    {
      "doc": 393,
      "topic": 7,
      "similarity": 0.778174296893966
    },
    {
      "doc": 393,
      "topic": 8,
      "similarity": 0.7795887406828153
    },
    {
      "doc": 393,
      "topic": 9,
      "similarity": 0.8095748986209613
    },
    {
      "doc": 393,
      "topic": 10,
      "similarity": 0.7868987072617258
    },
    {
      "doc": 393,
      "topic": 11,
      "similarity": 0.7933827374033644
    },
    {
      "doc": 393,
      "topic": 13,
      "similarity": 0.7890312045427327
    },
    {
      "doc": 393,
      "topic": 14,
      "similarity": 0.8473679280555434
    },
    {
      "doc": 393,
      "topic": 15,
      "similarity": 0.7953075183530188
    },
    {
      "doc": 393,
      "topic": 16,
      "similarity": 0.8084508571984272
    },
    {
      "doc": 393,
      "topic": 17,
      "similarity": 0.8170686638271881
    },
    {
      "doc": 393,
      "topic": 18,
      "similarity": 0.8019079277879465
    },
    {
      "doc": 393,
      "topic": 19,
      "similarity": 0.8523187583117667
    },
    {
      "doc": 393,
      "topic": 20,
      "similarity": 0.7801814580036069
    },
    {
      "doc": 393,
      "topic": 21,
      "similarity": 0.8089753334166575
    },
    {
      "doc": 394,
      "topic": 1,
      "similarity": 0.7605464090039478
    },
    {
      "doc": 394,
      "topic": 2,
      "similarity": 0.7944438546049095
    },
    {
      "doc": 394,
      "topic": 3,
      "similarity": 0.8304548896569339
    },
    {
      "doc": 394,
      "topic": 4,
      "similarity": 0.7575919163794973
    },
    {
      "doc": 394,
      "topic": 5,
      "similarity": 0.8075275878020084
    },
    {
      "doc": 394,
      "topic": 7,
      "similarity": 0.8040261509044893
    },
    {
      "doc": 394,
      "topic": 8,
      "similarity": 0.8072136376148233
    },
    {
      "doc": 394,
      "topic": 9,
      "similarity": 0.8225153916859071
    },
    {
      "doc": 394,
      "topic": 10,
      "similarity": 0.7835752328080625
    },
    {
      "doc": 394,
      "topic": 11,
      "similarity": 0.7854067849311618
    },
    {
      "doc": 394,
      "topic": 13,
      "similarity": 0.7599987746354876
    },
    {
      "doc": 394,
      "topic": 14,
      "similarity": 0.7706405774866535
    },
    {
      "doc": 394,
      "topic": 15,
      "similarity": 0.7802646105549428
    },
    {
      "doc": 394,
      "topic": 16,
      "similarity": 0.805835283519117
    },
    {
      "doc": 394,
      "topic": 17,
      "similarity": 0.8007468947896565
    },
    {
      "doc": 394,
      "topic": 18,
      "similarity": 0.7658931111114976
    },
    {
      "doc": 394,
      "topic": 19,
      "similarity": 0.811318817720347
    },
    {
      "doc": 394,
      "topic": 20,
      "similarity": 0.8004593057206748
    },
    {
      "doc": 394,
      "topic": 21,
      "similarity": 0.8062800183504231
    },
    {
      "doc": 394,
      "topic": 23,
      "similarity": 0.7683620740917498
    },
    {
      "doc": 394,
      "topic": 24,
      "similarity": 0.7747181784513106
    },
    {
      "doc": 395,
      "topic": 3,
      "similarity": 0.7696763100676599
    },
    {
      "doc": 395,
      "topic": 5,
      "similarity": 0.7795514551762202
    },
    {
      "doc": 395,
      "topic": 6,
      "similarity": 0.7623589156756843
    },
    {
      "doc": 395,
      "topic": 7,
      "similarity": 0.7598720251097467
    },
    {
      "doc": 395,
      "topic": 8,
      "similarity": 0.7630173021712578
    },
    {
      "doc": 395,
      "topic": 9,
      "similarity": 0.8186121073319635
    },
    {
      "doc": 395,
      "topic": 10,
      "similarity": 0.7582348177594656
    },
    {
      "doc": 395,
      "topic": 11,
      "similarity": 0.772874310648232
    },
    {
      "doc": 395,
      "topic": 12,
      "similarity": 0.758688143040971
    },
    {
      "doc": 395,
      "topic": 15,
      "similarity": 0.7704433088162695
    },
    {
      "doc": 395,
      "topic": 16,
      "similarity": 0.7702154495261386
    },
    {
      "doc": 395,
      "topic": 17,
      "similarity": 0.7732844449080518
    },
    {
      "doc": 395,
      "topic": 18,
      "similarity": 0.7550266174797964
    },
    {
      "doc": 395,
      "topic": 19,
      "similarity": 0.7809180877360211
    },
    {
      "doc": 395,
      "topic": 20,
      "similarity": 0.7592003733503666
    },
    {
      "doc": 395,
      "topic": 21,
      "similarity": 0.7686018719319365
    },
    {
      "doc": 395,
      "topic": 24,
      "similarity": 0.7663134250026198
    },
    {
      "doc": 396,
      "topic": 3,
      "similarity": 0.7950365615231615
    },
    {
      "doc": 396,
      "topic": 4,
      "similarity": 0.8394901966818635
    },
    {
      "doc": 396,
      "topic": 5,
      "similarity": 0.7882495349888481
    },
    {
      "doc": 396,
      "topic": 7,
      "similarity": 0.7828880822845313
    },
    {
      "doc": 396,
      "topic": 8,
      "similarity": 0.7585737961843432
    },
    {
      "doc": 396,
      "topic": 9,
      "similarity": 0.7912719952270788
    },
    {
      "doc": 396,
      "topic": 10,
      "similarity": 0.7866966860946939
    },
    {
      "doc": 396,
      "topic": 11,
      "similarity": 0.7741377835487679
    },
    {
      "doc": 396,
      "topic": 13,
      "similarity": 0.7555281608451695
    },
    {
      "doc": 396,
      "topic": 16,
      "similarity": 0.8061082731154646
    },
    {
      "doc": 396,
      "topic": 17,
      "similarity": 0.7844331917526703
    },
    {
      "doc": 396,
      "topic": 19,
      "similarity": 0.7917277029814833
    },
    {
      "doc": 396,
      "topic": 20,
      "similarity": 0.7545198568020228
    },
    {
      "doc": 396,
      "topic": 21,
      "similarity": 0.7806148585745721
    },
    {
      "doc": 397,
      "topic": 1,
      "similarity": 0.7770573889681115
    },
    {
      "doc": 397,
      "topic": 2,
      "similarity": 0.810470668015205
    },
    {
      "doc": 397,
      "topic": 3,
      "similarity": 0.8131493860793381
    },
    {
      "doc": 397,
      "topic": 4,
      "similarity": 0.7598136892882242
    },
    {
      "doc": 397,
      "topic": 5,
      "similarity": 0.8118632806820736
    },
    {
      "doc": 397,
      "topic": 7,
      "similarity": 0.8000164731476048
    },
    {
      "doc": 397,
      "topic": 8,
      "similarity": 0.8034051187500509
    },
    {
      "doc": 397,
      "topic": 9,
      "similarity": 0.8522298900479143
    },
    {
      "doc": 397,
      "topic": 10,
      "similarity": 0.7994821002268199
    },
    {
      "doc": 397,
      "topic": 11,
      "similarity": 0.8187370730070063
    },
    {
      "doc": 397,
      "topic": 13,
      "similarity": 0.8006956704695729
    },
    {
      "doc": 397,
      "topic": 14,
      "similarity": 0.8133910172204056
    },
    {
      "doc": 397,
      "topic": 15,
      "similarity": 0.814670397502795
    },
    {
      "doc": 397,
      "topic": 16,
      "similarity": 0.8341050433117635
    },
    {
      "doc": 397,
      "topic": 17,
      "similarity": 0.834237718824046
    },
    {
      "doc": 397,
      "topic": 18,
      "similarity": 0.7660347387857003
    },
    {
      "doc": 397,
      "topic": 19,
      "similarity": 0.8197616497287202
    },
    {
      "doc": 397,
      "topic": 20,
      "similarity": 0.7978910102857822
    },
    {
      "doc": 397,
      "topic": 21,
      "similarity": 0.8128853015429922
    },
    {
      "doc": 397,
      "topic": 24,
      "similarity": 0.7711050980875859
    },
    {
      "doc": 398,
      "topic": 1,
      "similarity": 0.7570112588579363
    },
    {
      "doc": 398,
      "topic": 2,
      "similarity": 0.7693143434175256
    },
    {
      "doc": 398,
      "topic": 3,
      "similarity": 0.8094796141056461
    },
    {
      "doc": 398,
      "topic": 4,
      "similarity": 0.7626426110999275
    },
    {
      "doc": 398,
      "topic": 5,
      "similarity": 0.8165935814194144
    },
    {
      "doc": 398,
      "topic": 7,
      "similarity": 0.7858646195115635
    },
    {
      "doc": 398,
      "topic": 8,
      "similarity": 0.7948132857020491
    },
    {
      "doc": 398,
      "topic": 9,
      "similarity": 0.847202531585548
    },
    {
      "doc": 398,
      "topic": 10,
      "similarity": 0.7898778766037471
    },
    {
      "doc": 398,
      "topic": 11,
      "similarity": 0.7994081844410652
    },
    {
      "doc": 398,
      "topic": 12,
      "similarity": 0.760101175783181
    },
    {
      "doc": 398,
      "topic": 13,
      "similarity": 0.7807826270753376
    },
    {
      "doc": 398,
      "topic": 14,
      "similarity": 0.7786908484814035
    },
    {
      "doc": 398,
      "topic": 15,
      "similarity": 0.7908087245351837
    },
    {
      "doc": 398,
      "topic": 16,
      "similarity": 0.8092993297797653
    },
    {
      "doc": 398,
      "topic": 17,
      "similarity": 0.8052910147055126
    },
    {
      "doc": 398,
      "topic": 18,
      "similarity": 0.7724051515314873
    },
    {
      "doc": 398,
      "topic": 19,
      "similarity": 0.8186375344472567
    },
    {
      "doc": 398,
      "topic": 20,
      "similarity": 0.7963960676126864
    },
    {
      "doc": 398,
      "topic": 21,
      "similarity": 0.8193769941091207
    },
    {
      "doc": 398,
      "topic": 22,
      "similarity": 0.7521074045030185
    },
    {
      "doc": 398,
      "topic": 23,
      "similarity": 0.7525529362985404
    },
    {
      "doc": 398,
      "topic": 24,
      "similarity": 0.7752830892890479
    },
    {
      "doc": 399,
      "topic": 1,
      "similarity": 0.7582857916662427
    },
    {
      "doc": 399,
      "topic": 2,
      "similarity": 0.7903539330665895
    },
    {
      "doc": 399,
      "topic": 3,
      "similarity": 0.7949504257789708
    },
    {
      "doc": 399,
      "topic": 4,
      "similarity": 0.7620419839043301
    },
    {
      "doc": 399,
      "topic": 5,
      "similarity": 0.7846394025563864
    },
    {
      "doc": 399,
      "topic": 7,
      "similarity": 0.7798037302668516
    },
    {
      "doc": 399,
      "topic": 8,
      "similarity": 0.7676673994581126
    },
    {
      "doc": 399,
      "topic": 9,
      "similarity": 0.7975942975933926
    },
    {
      "doc": 399,
      "topic": 10,
      "similarity": 0.7706216971599822
    },
    {
      "doc": 399,
      "topic": 11,
      "similarity": 0.7879608646279148
    },
    {
      "doc": 399,
      "topic": 13,
      "similarity": 0.7690026382980054
    },
    {
      "doc": 399,
      "topic": 14,
      "similarity": 0.7736571461048246
    },
    {
      "doc": 399,
      "topic": 15,
      "similarity": 0.7922630921347543
    },
    {
      "doc": 399,
      "topic": 16,
      "similarity": 0.8155409660570421
    },
    {
      "doc": 399,
      "topic": 17,
      "similarity": 0.7985622559123036
    },
    {
      "doc": 399,
      "topic": 18,
      "similarity": 0.7599651957772668
    },
    {
      "doc": 399,
      "topic": 19,
      "similarity": 0.8218828152253705
    },
    {
      "doc": 399,
      "topic": 20,
      "similarity": 0.7622035851446768
    },
    {
      "doc": 399,
      "topic": 21,
      "similarity": 0.7748891724224487
    },
    {
      "doc": 399,
      "topic": 22,
      "similarity": 0.7818299949124609
    },
    {
      "doc": 400,
      "topic": 3,
      "similarity": 0.7665784719453301
    },
    {
      "doc": 400,
      "topic": 5,
      "similarity": 0.7680279889324948
    },
    {
      "doc": 400,
      "topic": 7,
      "similarity": 0.7731321211957226
    },
    {
      "doc": 400,
      "topic": 9,
      "similarity": 0.817918319880711
    },
    {
      "doc": 400,
      "topic": 11,
      "similarity": 0.7601732450508748
    },
    {
      "doc": 400,
      "topic": 15,
      "similarity": 0.7618221944777013
    },
    {
      "doc": 400,
      "topic": 16,
      "similarity": 0.7664980410061012
    },
    {
      "doc": 400,
      "topic": 17,
      "similarity": 0.8111570962589167
    },
    {
      "doc": 400,
      "topic": 19,
      "similarity": 0.7886854290095875
    },
    {
      "doc": 400,
      "topic": 20,
      "similarity": 0.7673747283887612
    },
    {
      "doc": 400,
      "topic": 21,
      "similarity": 0.7918920761152759
    },
    {
      "doc": 400,
      "topic": 24,
      "similarity": 0.7506207460467212
    },
    {
      "doc": 401,
      "topic": 1,
      "similarity": 0.7677061740148797
    },
    {
      "doc": 401,
      "topic": 2,
      "similarity": 0.7746699313737755
    },
    {
      "doc": 401,
      "topic": 3,
      "similarity": 0.7983900272771793
    },
    {
      "doc": 401,
      "topic": 5,
      "similarity": 0.7795855213172094
    },
    {
      "doc": 401,
      "topic": 7,
      "similarity": 0.7877143550015732
    },
    {
      "doc": 401,
      "topic": 8,
      "similarity": 0.7876326721570556
    },
    {
      "doc": 401,
      "topic": 9,
      "similarity": 0.8278903388868711
    },
    {
      "doc": 401,
      "topic": 10,
      "similarity": 0.7781002973577319
    },
    {
      "doc": 401,
      "topic": 11,
      "similarity": 0.7801897235549475
    },
    {
      "doc": 401,
      "topic": 13,
      "similarity": 0.7741241905722788
    },
    {
      "doc": 401,
      "topic": 14,
      "similarity": 0.7985562563656196
    },
    {
      "doc": 401,
      "topic": 15,
      "similarity": 0.7760616949050994
    },
    {
      "doc": 401,
      "topic": 16,
      "similarity": 0.7858408162173588
    },
    {
      "doc": 401,
      "topic": 17,
      "similarity": 0.7666333704466782
    },
    {
      "doc": 401,
      "topic": 18,
      "similarity": 0.7574626181597581
    },
    {
      "doc": 401,
      "topic": 19,
      "similarity": 0.7923543889266198
    },
    {
      "doc": 401,
      "topic": 20,
      "similarity": 0.7757092848085824
    },
    {
      "doc": 401,
      "topic": 21,
      "similarity": 0.7834464195605714
    },
    {
      "doc": 402,
      "topic": 2,
      "similarity": 0.7751278071836151
    },
    {
      "doc": 402,
      "topic": 3,
      "similarity": 0.7853154517486973
    },
    {
      "doc": 402,
      "topic": 5,
      "similarity": 0.7935262763221695
    },
    {
      "doc": 402,
      "topic": 7,
      "similarity": 0.7679592685773489
    },
    {
      "doc": 402,
      "topic": 8,
      "similarity": 0.760828266198294
    },
    {
      "doc": 402,
      "topic": 9,
      "similarity": 0.7974009611331192
    },
    {
      "doc": 402,
      "topic": 10,
      "similarity": 0.7760875714837246
    },
    {
      "doc": 402,
      "topic": 11,
      "similarity": 0.7887067650467592
    },
    {
      "doc": 402,
      "topic": 13,
      "similarity": 0.7693893052698626
    },
    {
      "doc": 402,
      "topic": 14,
      "similarity": 0.8033824373870099
    },
    {
      "doc": 402,
      "topic": 15,
      "similarity": 0.7707918430069369
    },
    {
      "doc": 402,
      "topic": 16,
      "similarity": 0.7959042346154559
    },
    {
      "doc": 402,
      "topic": 17,
      "similarity": 0.7957513431990234
    },
    {
      "doc": 402,
      "topic": 18,
      "similarity": 0.7578097386222807
    },
    {
      "doc": 402,
      "topic": 19,
      "similarity": 0.8039135477052818
    },
    {
      "doc": 402,
      "topic": 20,
      "similarity": 0.7946354633192735
    },
    {
      "doc": 402,
      "topic": 21,
      "similarity": 0.7961288144007399
    },
    {
      "doc": 403,
      "topic": 3,
      "similarity": 0.7971737910200598
    },
    {
      "doc": 403,
      "topic": 4,
      "similarity": 0.7648496975277476
    },
    {
      "doc": 403,
      "topic": 5,
      "similarity": 0.7669835666093779
    },
    {
      "doc": 403,
      "topic": 7,
      "similarity": 0.7958297204140368
    },
    {
      "doc": 403,
      "topic": 8,
      "similarity": 0.7612106819886441
    },
    {
      "doc": 403,
      "topic": 9,
      "similarity": 0.8021996837112425
    },
    {
      "doc": 403,
      "topic": 10,
      "similarity": 0.7523513917289443
    },
    {
      "doc": 403,
      "topic": 11,
      "similarity": 0.7592461315336423
    },
    {
      "doc": 403,
      "topic": 16,
      "similarity": 0.7917872777429378
    },
    {
      "doc": 403,
      "topic": 17,
      "similarity": 0.750638043910199
    },
    {
      "doc": 403,
      "topic": 19,
      "similarity": 0.7839088983610257
    },
    {
      "doc": 403,
      "topic": 21,
      "similarity": 0.7622215803571524
    },
    {
      "doc": 404,
      "topic": 1,
      "similarity": 0.7571109012768347
    },
    {
      "doc": 404,
      "topic": 2,
      "similarity": 0.770887553503491
    },
    {
      "doc": 404,
      "topic": 3,
      "similarity": 0.8011331755990809
    },
    {
      "doc": 404,
      "topic": 4,
      "similarity": 0.7614455735724737
    },
    {
      "doc": 404,
      "topic": 5,
      "similarity": 0.8041207781408372
    },
    {
      "doc": 404,
      "topic": 7,
      "similarity": 0.7859746766285014
    },
    {
      "doc": 404,
      "topic": 8,
      "similarity": 0.7835278650381424
    },
    {
      "doc": 404,
      "topic": 9,
      "similarity": 0.8232319511355496
    },
    {
      "doc": 404,
      "topic": 10,
      "similarity": 0.8190991782618645
    },
    {
      "doc": 404,
      "topic": 11,
      "similarity": 0.7790504699674168
    },
    {
      "doc": 404,
      "topic": 12,
      "similarity": 0.7547942890269784
    },
    {
      "doc": 404,
      "topic": 13,
      "similarity": 0.7740118288269483
    },
    {
      "doc": 404,
      "topic": 14,
      "similarity": 0.7714402786686828
    },
    {
      "doc": 404,
      "topic": 15,
      "similarity": 0.774787070263664
    },
    {
      "doc": 404,
      "topic": 16,
      "similarity": 0.8075860912719194
    },
    {
      "doc": 404,
      "topic": 17,
      "similarity": 0.7964278662953292
    },
    {
      "doc": 404,
      "topic": 18,
      "similarity": 0.7765227831422827
    },
    {
      "doc": 404,
      "topic": 19,
      "similarity": 0.8127227753939206
    },
    {
      "doc": 404,
      "topic": 20,
      "similarity": 0.789197529377569
    },
    {
      "doc": 404,
      "topic": 21,
      "similarity": 0.7912545569871919
    },
    {
      "doc": 404,
      "topic": 22,
      "similarity": 0.7613965942383122
    },
    {
      "doc": 404,
      "topic": 23,
      "similarity": 0.7670495096126747
    },
    {
      "doc": 405,
      "topic": 0,
      "similarity": 0.7746980937056117
    },
    {
      "doc": 405,
      "topic": 1,
      "similarity": 0.7608804906561808
    },
    {
      "doc": 405,
      "topic": 2,
      "similarity": 0.7553897232107772
    },
    {
      "doc": 405,
      "topic": 3,
      "similarity": 0.7635694652362939
    },
    {
      "doc": 405,
      "topic": 5,
      "similarity": 0.7503914232279418
    },
    {
      "doc": 405,
      "topic": 9,
      "similarity": 0.7894793278459957
    },
    {
      "doc": 405,
      "topic": 10,
      "similarity": 0.7552903556254696
    },
    {
      "doc": 405,
      "topic": 11,
      "similarity": 0.7854686426170312
    },
    {
      "doc": 405,
      "topic": 14,
      "similarity": 0.7553536016580594
    },
    {
      "doc": 405,
      "topic": 15,
      "similarity": 0.7725392107078369
    },
    {
      "doc": 405,
      "topic": 16,
      "similarity": 0.7867563481477846
    },
    {
      "doc": 405,
      "topic": 17,
      "similarity": 0.7851319892086626
    },
    {
      "doc": 405,
      "topic": 18,
      "similarity": 0.7553909030854725
    },
    {
      "doc": 405,
      "topic": 19,
      "similarity": 0.7807442805722562
    },
    {
      "doc": 405,
      "topic": 20,
      "similarity": 0.7646612062705469
    },
    {
      "doc": 405,
      "topic": 21,
      "similarity": 0.7902983300392755
    },
    {
      "doc": 405,
      "topic": 23,
      "similarity": 0.7854510131347202
    },
    {
      "doc": 405,
      "topic": 24,
      "similarity": 0.7700131790262063
    },
    {
      "doc": 406,
      "topic": 1,
      "similarity": 0.7606101857273414
    },
    {
      "doc": 406,
      "topic": 2,
      "similarity": 0.7710148130387439
    },
    {
      "doc": 406,
      "topic": 3,
      "similarity": 0.7902880640860954
    },
    {
      "doc": 406,
      "topic": 4,
      "similarity": 0.755979784182759
    },
    {
      "doc": 406,
      "topic": 5,
      "similarity": 0.8013687987526564
    },
    {
      "doc": 406,
      "topic": 7,
      "similarity": 0.7807363028330342
    },
    {
      "doc": 406,
      "topic": 8,
      "similarity": 0.7873341960847824
    },
    {
      "doc": 406,
      "topic": 9,
      "similarity": 0.8277318125764812
    },
    {
      "doc": 406,
      "topic": 10,
      "similarity": 0.7955130413002064
    },
    {
      "doc": 406,
      "topic": 11,
      "similarity": 0.7868782196659984
    },
    {
      "doc": 406,
      "topic": 12,
      "similarity": 0.7832911211147678
    },
    {
      "doc": 406,
      "topic": 13,
      "similarity": 0.7896135651623664
    },
    {
      "doc": 406,
      "topic": 14,
      "similarity": 0.785888165034219
    },
    {
      "doc": 406,
      "topic": 15,
      "similarity": 0.8160624514515205
    },
    {
      "doc": 406,
      "topic": 16,
      "similarity": 0.821573121725766
    },
    {
      "doc": 406,
      "topic": 17,
      "similarity": 0.8073455894163991
    },
    {
      "doc": 406,
      "topic": 18,
      "similarity": 0.7673020646776227
    },
    {
      "doc": 406,
      "topic": 19,
      "similarity": 0.8144664519648797
    },
    {
      "doc": 406,
      "topic": 20,
      "similarity": 0.8125677627387418
    },
    {
      "doc": 406,
      "topic": 21,
      "similarity": 0.793431764836594
    },
    {
      "doc": 406,
      "topic": 22,
      "similarity": 0.779648761512834
    },
    {
      "doc": 407,
      "topic": 3,
      "similarity": 0.786051588628942
    },
    {
      "doc": 407,
      "topic": 4,
      "similarity": 0.7577562764081286
    },
    {
      "doc": 407,
      "topic": 5,
      "similarity": 0.7925510013982329
    },
    {
      "doc": 407,
      "topic": 7,
      "similarity": 0.7585420318597117
    },
    {
      "doc": 407,
      "topic": 8,
      "similarity": 0.7655492531276352
    },
    {
      "doc": 407,
      "topic": 9,
      "similarity": 0.7950489769767126
    },
    {
      "doc": 407,
      "topic": 10,
      "similarity": 0.7654446408664421
    },
    {
      "doc": 407,
      "topic": 11,
      "similarity": 0.7768115005296742
    },
    {
      "doc": 407,
      "topic": 15,
      "similarity": 0.762314455218808
    },
    {
      "doc": 407,
      "topic": 16,
      "similarity": 0.7894790088513686
    },
    {
      "doc": 407,
      "topic": 17,
      "similarity": 0.7737900988970966
    },
    {
      "doc": 407,
      "topic": 19,
      "similarity": 0.7901188175383033
    },
    {
      "doc": 407,
      "topic": 20,
      "similarity": 0.7631597159167054
    },
    {
      "doc": 407,
      "topic": 21,
      "similarity": 0.7703507765354072
    },
    {
      "doc": 407,
      "topic": 24,
      "similarity": 0.7583485183539178
    },
    {
      "doc": 408,
      "topic": 2,
      "similarity": 0.7699357602534643
    },
    {
      "doc": 408,
      "topic": 3,
      "similarity": 0.7796851955019021
    },
    {
      "doc": 408,
      "topic": 5,
      "similarity": 0.7785399605054504
    },
    {
      "doc": 408,
      "topic": 6,
      "similarity": 0.7618995858564466
    },
    {
      "doc": 408,
      "topic": 8,
      "similarity": 0.762865290504422
    },
    {
      "doc": 408,
      "topic": 9,
      "similarity": 0.803806711029218
    },
    {
      "doc": 408,
      "topic": 10,
      "similarity": 0.7557866970442864
    },
    {
      "doc": 408,
      "topic": 11,
      "similarity": 0.7509952720650204
    },
    {
      "doc": 408,
      "topic": 13,
      "similarity": 0.7522416649380451
    },
    {
      "doc": 408,
      "topic": 14,
      "similarity": 0.7701788503551508
    },
    {
      "doc": 408,
      "topic": 15,
      "similarity": 0.786245733778779
    },
    {
      "doc": 408,
      "topic": 16,
      "similarity": 0.7763361346979253
    },
    {
      "doc": 408,
      "topic": 17,
      "similarity": 0.8003388489297552
    },
    {
      "doc": 408,
      "topic": 19,
      "similarity": 0.8117084125928428
    },
    {
      "doc": 408,
      "topic": 20,
      "similarity": 0.7698839209276388
    },
    {
      "doc": 408,
      "topic": 21,
      "similarity": 0.7873366161346099
    },
    {
      "doc": 408,
      "topic": 22,
      "similarity": 0.7632921633052273
    },
    {
      "doc": 408,
      "topic": 23,
      "similarity": 0.7858797663683132
    },
    {
      "doc": 409,
      "topic": 1,
      "similarity": 0.7603579374986117
    },
    {
      "doc": 409,
      "topic": 2,
      "similarity": 0.7710191231953928
    },
    {
      "doc": 409,
      "topic": 3,
      "similarity": 0.7661258025233593
    },
    {
      "doc": 409,
      "topic": 5,
      "similarity": 0.7603109295927175
    },
    {
      "doc": 409,
      "topic": 9,
      "similarity": 0.768237241098702
    },
    {
      "doc": 409,
      "topic": 15,
      "similarity": 0.765307758017187
    },
    {
      "doc": 409,
      "topic": 16,
      "similarity": 0.7955486087867241
    },
    {
      "doc": 409,
      "topic": 17,
      "similarity": 0.7798066562298854
    },
    {
      "doc": 409,
      "topic": 19,
      "similarity": 0.7738582292890719
    },
    {
      "doc": 409,
      "topic": 20,
      "similarity": 0.7650980933129343
    },
    {
      "doc": 409,
      "topic": 21,
      "similarity": 0.7626123540614177
    },
    {
      "doc": 410,
      "topic": 1,
      "similarity": 0.7712230825735785
    },
    {
      "doc": 410,
      "topic": 2,
      "similarity": 0.7861374366455213
    },
    {
      "doc": 410,
      "topic": 3,
      "similarity": 0.8154778982456261
    },
    {
      "doc": 410,
      "topic": 4,
      "similarity": 0.7585163081439831
    },
    {
      "doc": 410,
      "topic": 5,
      "similarity": 0.7977296732272061
    },
    {
      "doc": 410,
      "topic": 7,
      "similarity": 0.8130422883071032
    },
    {
      "doc": 410,
      "topic": 8,
      "similarity": 0.7913987364995102
    },
    {
      "doc": 410,
      "topic": 9,
      "similarity": 0.842641351832079
    },
    {
      "doc": 410,
      "topic": 10,
      "similarity": 0.776434017312677
    },
    {
      "doc": 410,
      "topic": 11,
      "similarity": 0.7948955908321096
    },
    {
      "doc": 410,
      "topic": 13,
      "similarity": 0.7897039839176426
    },
    {
      "doc": 410,
      "topic": 14,
      "similarity": 0.7746269742749605
    },
    {
      "doc": 410,
      "topic": 15,
      "similarity": 0.7909267269672428
    },
    {
      "doc": 410,
      "topic": 16,
      "similarity": 0.8298461422087815
    },
    {
      "doc": 410,
      "topic": 17,
      "similarity": 0.7972012992852185
    },
    {
      "doc": 410,
      "topic": 18,
      "similarity": 0.7555044174168575
    },
    {
      "doc": 410,
      "topic": 19,
      "similarity": 0.8104359364169808
    },
    {
      "doc": 410,
      "topic": 20,
      "similarity": 0.7863349000914323
    },
    {
      "doc": 410,
      "topic": 21,
      "similarity": 0.8019056898471417
    },
    {
      "doc": 410,
      "topic": 22,
      "similarity": 0.7531935826048909
    },
    {
      "doc": 411,
      "topic": 1,
      "similarity": 0.7535859722439998
    },
    {
      "doc": 411,
      "topic": 2,
      "similarity": 0.7639063342567392
    },
    {
      "doc": 411,
      "topic": 3,
      "similarity": 0.7918925648502757
    },
    {
      "doc": 411,
      "topic": 4,
      "similarity": 0.7701149240751641
    },
    {
      "doc": 411,
      "topic": 5,
      "similarity": 0.8032095413686418
    },
    {
      "doc": 411,
      "topic": 6,
      "similarity": 0.7628995706401841
    },
    {
      "doc": 411,
      "topic": 7,
      "similarity": 0.7671642814089998
    },
    {
      "doc": 411,
      "topic": 8,
      "similarity": 0.7777418227118603
    },
    {
      "doc": 411,
      "topic": 9,
      "similarity": 0.8391528125757761
    },
    {
      "doc": 411,
      "topic": 10,
      "similarity": 0.7717637799852733
    },
    {
      "doc": 411,
      "topic": 11,
      "similarity": 0.7935398176111026
    },
    {
      "doc": 411,
      "topic": 12,
      "similarity": 0.7857405943041407
    },
    {
      "doc": 411,
      "topic": 13,
      "similarity": 0.7879391075763479
    },
    {
      "doc": 411,
      "topic": 14,
      "similarity": 0.783374696170433
    },
    {
      "doc": 411,
      "topic": 15,
      "similarity": 0.7800715273709715
    },
    {
      "doc": 411,
      "topic": 16,
      "similarity": 0.8161287002985012
    },
    {
      "doc": 411,
      "topic": 17,
      "similarity": 0.8068359494689592
    },
    {
      "doc": 411,
      "topic": 18,
      "similarity": 0.765271422925225
    },
    {
      "doc": 411,
      "topic": 19,
      "similarity": 0.810794084510737
    },
    {
      "doc": 411,
      "topic": 20,
      "similarity": 0.7855558864983866
    },
    {
      "doc": 411,
      "topic": 21,
      "similarity": 0.8028223167351036
    },
    {
      "doc": 411,
      "topic": 22,
      "similarity": 0.7661668414268726
    },
    {
      "doc": 411,
      "topic": 23,
      "similarity": 0.7651146604181844
    },
    {
      "doc": 411,
      "topic": 24,
      "similarity": 0.7902971644356642
    },
    {
      "doc": 412,
      "topic": 1,
      "similarity": 0.7604811420641066
    },
    {
      "doc": 412,
      "topic": 2,
      "similarity": 0.784381795394656
    },
    {
      "doc": 412,
      "topic": 3,
      "similarity": 0.8079331088412909
    },
    {
      "doc": 412,
      "topic": 4,
      "similarity": 0.7561661931988085
    },
    {
      "doc": 412,
      "topic": 5,
      "similarity": 0.8132986142411918
    },
    {
      "doc": 412,
      "topic": 7,
      "similarity": 0.7935393815157474
    },
    {
      "doc": 412,
      "topic": 8,
      "similarity": 0.7982833093105443
    },
    {
      "doc": 412,
      "topic": 9,
      "similarity": 0.8312682650425105
    },
    {
      "doc": 412,
      "topic": 10,
      "similarity": 0.7847900135199476
    },
    {
      "doc": 412,
      "topic": 11,
      "similarity": 0.799397951679297
    },
    {
      "doc": 412,
      "topic": 12,
      "similarity": 0.7532346036157266
    },
    {
      "doc": 412,
      "topic": 13,
      "similarity": 0.7941007898289717
    },
    {
      "doc": 412,
      "topic": 14,
      "similarity": 0.7827322217882797
    },
    {
      "doc": 412,
      "topic": 15,
      "similarity": 0.8025388785944501
    },
    {
      "doc": 412,
      "topic": 16,
      "similarity": 0.8377432104802727
    },
    {
      "doc": 412,
      "topic": 17,
      "similarity": 0.8288095750508006
    },
    {
      "doc": 412,
      "topic": 18,
      "similarity": 0.8018648249904282
    },
    {
      "doc": 412,
      "topic": 19,
      "similarity": 0.8226713521618941
    },
    {
      "doc": 412,
      "topic": 20,
      "similarity": 0.819544684761449
    },
    {
      "doc": 412,
      "topic": 21,
      "similarity": 0.8083471773886747
    },
    {
      "doc": 412,
      "topic": 23,
      "similarity": 0.7543474731160588
    },
    {
      "doc": 413,
      "topic": 2,
      "similarity": 0.7548256201729538
    },
    {
      "doc": 413,
      "topic": 3,
      "similarity": 0.760446858896501
    },
    {
      "doc": 413,
      "topic": 5,
      "similarity": 0.7580440264093946
    },
    {
      "doc": 413,
      "topic": 7,
      "similarity": 0.750382044717211
    },
    {
      "doc": 413,
      "topic": 8,
      "similarity": 0.7540930662448884
    },
    {
      "doc": 413,
      "topic": 9,
      "similarity": 0.8006960714263612
    },
    {
      "doc": 413,
      "topic": 11,
      "similarity": 0.7707861962190873
    },
    {
      "doc": 413,
      "topic": 15,
      "similarity": 0.7674124264536085
    },
    {
      "doc": 413,
      "topic": 16,
      "similarity": 0.7918789807813824
    },
    {
      "doc": 413,
      "topic": 17,
      "similarity": 0.780443954269535
    },
    {
      "doc": 413,
      "topic": 18,
      "similarity": 0.7632182102894902
    },
    {
      "doc": 413,
      "topic": 19,
      "similarity": 0.7850474713564496
    },
    {
      "doc": 413,
      "topic": 20,
      "similarity": 0.7606665564017021
    },
    {
      "doc": 413,
      "topic": 21,
      "similarity": 0.780630383946388
    },
    {
      "doc": 413,
      "topic": 24,
      "similarity": 0.7640213211227158
    },
    {
      "doc": 414,
      "topic": 1,
      "similarity": 0.758515114856747
    },
    {
      "doc": 414,
      "topic": 2,
      "similarity": 0.7820417270229714
    },
    {
      "doc": 414,
      "topic": 3,
      "similarity": 0.8027749214639316
    },
    {
      "doc": 414,
      "topic": 5,
      "similarity": 0.8118670125889909
    },
    {
      "doc": 414,
      "topic": 7,
      "similarity": 0.7936083432563326
    },
    {
      "doc": 414,
      "topic": 8,
      "similarity": 0.778920466455874
    },
    {
      "doc": 414,
      "topic": 9,
      "similarity": 0.8589231034398179
    },
    {
      "doc": 414,
      "topic": 10,
      "similarity": 0.795803352230501
    },
    {
      "doc": 414,
      "topic": 11,
      "similarity": 0.8023141768063302
    },
    {
      "doc": 414,
      "topic": 12,
      "similarity": 0.7610194571043644
    },
    {
      "doc": 414,
      "topic": 13,
      "similarity": 0.7711803591962118
    },
    {
      "doc": 414,
      "topic": 14,
      "similarity": 0.7825555670444654
    },
    {
      "doc": 414,
      "topic": 15,
      "similarity": 0.7998303725381923
    },
    {
      "doc": 414,
      "topic": 16,
      "similarity": 0.823156390475424
    },
    {
      "doc": 414,
      "topic": 17,
      "similarity": 0.8217101584246463
    },
    {
      "doc": 414,
      "topic": 18,
      "similarity": 0.7821831771151366
    },
    {
      "doc": 414,
      "topic": 19,
      "similarity": 0.8134872293346265
    },
    {
      "doc": 414,
      "topic": 20,
      "similarity": 0.7880693836636607
    },
    {
      "doc": 414,
      "topic": 21,
      "similarity": 0.8128689294019602
    },
    {
      "doc": 414,
      "topic": 23,
      "similarity": 0.7766550993557606
    },
    {
      "doc": 414,
      "topic": 24,
      "similarity": 0.7770626368806425
    },
    {
      "doc": 415,
      "topic": 1,
      "similarity": 0.7553819992681559
    },
    {
      "doc": 415,
      "topic": 2,
      "similarity": 0.772499423963051
    },
    {
      "doc": 415,
      "topic": 3,
      "similarity": 0.814178337639761
    },
    {
      "doc": 415,
      "topic": 5,
      "similarity": 0.7803060120558495
    },
    {
      "doc": 415,
      "topic": 6,
      "similarity": 0.7517416786349798
    },
    {
      "doc": 415,
      "topic": 7,
      "similarity": 0.7667146294962022
    },
    {
      "doc": 415,
      "topic": 8,
      "similarity": 0.7656856608276201
    },
    {
      "doc": 415,
      "topic": 9,
      "similarity": 0.814653797620378
    },
    {
      "doc": 415,
      "topic": 10,
      "similarity": 0.7719563122642894
    },
    {
      "doc": 415,
      "topic": 11,
      "similarity": 0.7704283761180561
    },
    {
      "doc": 415,
      "topic": 14,
      "similarity": 0.764296750917846
    },
    {
      "doc": 415,
      "topic": 15,
      "similarity": 0.7666458904463282
    },
    {
      "doc": 415,
      "topic": 16,
      "similarity": 0.7819410147816044
    },
    {
      "doc": 415,
      "topic": 17,
      "similarity": 0.7800568586429086
    },
    {
      "doc": 415,
      "topic": 19,
      "similarity": 0.8080109538070692
    },
    {
      "doc": 415,
      "topic": 20,
      "similarity": 0.7614912466307097
    },
    {
      "doc": 415,
      "topic": 21,
      "similarity": 0.7881531917963763
    },
    {
      "doc": 415,
      "topic": 23,
      "similarity": 0.8190956800394651
    },
    {
      "doc": 415,
      "topic": 24,
      "similarity": 0.7590854106487547
    },
    {
      "doc": 416,
      "topic": 3,
      "similarity": 0.756284918789731
    },
    {
      "doc": 416,
      "topic": 4,
      "similarity": 0.8107334257671945
    },
    {
      "doc": 416,
      "topic": 9,
      "similarity": 0.771977431057905
    },
    {
      "doc": 416,
      "topic": 10,
      "similarity": 0.7534012377697522
    },
    {
      "doc": 416,
      "topic": 16,
      "similarity": 0.7661928016281976
    },
    {
      "doc": 416,
      "topic": 21,
      "similarity": 0.7597586219972214
    },
    {
      "doc": 417,
      "topic": 3,
      "similarity": 0.7567059435106113
    },
    {
      "doc": 417,
      "topic": 5,
      "similarity": 0.7676497786066129
    },
    {
      "doc": 417,
      "topic": 7,
      "similarity": 0.7579913425216986
    },
    {
      "doc": 417,
      "topic": 9,
      "similarity": 0.8406241759510408
    },
    {
      "doc": 417,
      "topic": 16,
      "similarity": 0.7581553515923748
    },
    {
      "doc": 417,
      "topic": 17,
      "similarity": 0.7507874762730512
    },
    {
      "doc": 417,
      "topic": 19,
      "similarity": 0.7677080911639953
    },
    {
      "doc": 417,
      "topic": 20,
      "similarity": 0.7561517939765687
    },
    {
      "doc": 417,
      "topic": 21,
      "similarity": 0.7727469842836169
    },
    {
      "doc": 417,
      "topic": 24,
      "similarity": 0.757557185604499
    },
    {
      "doc": 418,
      "topic": 3,
      "similarity": 0.7736713798954865
    },
    {
      "doc": 418,
      "topic": 5,
      "similarity": 0.7739347776663411
    },
    {
      "doc": 418,
      "topic": 7,
      "similarity": 0.757534623116518
    },
    {
      "doc": 418,
      "topic": 8,
      "similarity": 0.7610374740311625
    },
    {
      "doc": 418,
      "topic": 9,
      "similarity": 0.8289701530758543
    },
    {
      "doc": 418,
      "topic": 10,
      "similarity": 0.7668135635803912
    },
    {
      "doc": 418,
      "topic": 11,
      "similarity": 0.76823092558226
    },
    {
      "doc": 418,
      "topic": 12,
      "similarity": 0.7640001206780701
    },
    {
      "doc": 418,
      "topic": 14,
      "similarity": 0.7556739643186442
    },
    {
      "doc": 418,
      "topic": 15,
      "similarity": 0.7637665318324016
    },
    {
      "doc": 418,
      "topic": 16,
      "similarity": 0.7922055213116128
    },
    {
      "doc": 418,
      "topic": 17,
      "similarity": 0.7725218461608782
    },
    {
      "doc": 418,
      "topic": 18,
      "similarity": 0.7653396821656098
    },
    {
      "doc": 418,
      "topic": 19,
      "similarity": 0.8077080953341781
    },
    {
      "doc": 418,
      "topic": 20,
      "similarity": 0.787387083682168
    },
    {
      "doc": 418,
      "topic": 21,
      "similarity": 0.7893814332350086
    },
    {
      "doc": 418,
      "topic": 24,
      "similarity": 0.7566761925338831
    },
    {
      "doc": 419,
      "topic": 1,
      "similarity": 0.7531611094255224
    },
    {
      "doc": 419,
      "topic": 2,
      "similarity": 0.7576587500858263
    },
    {
      "doc": 419,
      "topic": 3,
      "similarity": 0.7895348899549193
    },
    {
      "doc": 419,
      "topic": 4,
      "similarity": 0.8351229140868812
    },
    {
      "doc": 419,
      "topic": 5,
      "similarity": 0.7581943493374922
    },
    {
      "doc": 419,
      "topic": 7,
      "similarity": 0.7589624744119658
    },
    {
      "doc": 419,
      "topic": 9,
      "similarity": 0.7889610843084625
    },
    {
      "doc": 419,
      "topic": 10,
      "similarity": 0.753492703481391
    },
    {
      "doc": 419,
      "topic": 16,
      "similarity": 0.7941866770890391
    },
    {
      "doc": 419,
      "topic": 17,
      "similarity": 0.7778241362707888
    },
    {
      "doc": 419,
      "topic": 19,
      "similarity": 0.7686904512680146
    },
    {
      "doc": 419,
      "topic": 20,
      "similarity": 0.7605831508613865
    },
    {
      "doc": 419,
      "topic": 21,
      "similarity": 0.7815240437026404
    },
    {
      "doc": 419,
      "topic": 23,
      "similarity": 0.7605193138006229
    },
    {
      "doc": 419,
      "topic": 24,
      "similarity": 0.7669421755674372
    },
    {
      "doc": 420,
      "topic": 1,
      "similarity": 0.7538752074466558
    },
    {
      "doc": 420,
      "topic": 2,
      "similarity": 0.7897885309237466
    },
    {
      "doc": 420,
      "topic": 3,
      "similarity": 0.8069476388787851
    },
    {
      "doc": 420,
      "topic": 4,
      "similarity": 0.7668917602652567
    },
    {
      "doc": 420,
      "topic": 5,
      "similarity": 0.8173006862436042
    },
    {
      "doc": 420,
      "topic": 6,
      "similarity": 0.754052495536586
    },
    {
      "doc": 420,
      "topic": 7,
      "similarity": 0.7815174782316003
    },
    {
      "doc": 420,
      "topic": 8,
      "similarity": 0.7967873844508931
    },
    {
      "doc": 420,
      "topic": 9,
      "similarity": 0.8364905409127736
    },
    {
      "doc": 420,
      "topic": 10,
      "similarity": 0.8012183977260594
    },
    {
      "doc": 420,
      "topic": 11,
      "similarity": 0.7982855523007634
    },
    {
      "doc": 420,
      "topic": 12,
      "similarity": 0.7955423111355411
    },
    {
      "doc": 420,
      "topic": 13,
      "similarity": 0.7821669495597308
    },
    {
      "doc": 420,
      "topic": 14,
      "similarity": 0.7976306403251351
    },
    {
      "doc": 420,
      "topic": 15,
      "similarity": 0.8339797275257632
    },
    {
      "doc": 420,
      "topic": 16,
      "similarity": 0.8163383474749777
    },
    {
      "doc": 420,
      "topic": 17,
      "similarity": 0.8450378461342278
    },
    {
      "doc": 420,
      "topic": 18,
      "similarity": 0.7711404420516919
    },
    {
      "doc": 420,
      "topic": 19,
      "similarity": 0.8302452316481919
    },
    {
      "doc": 420,
      "topic": 20,
      "similarity": 0.8060797269346195
    },
    {
      "doc": 420,
      "topic": 21,
      "similarity": 0.827419123331001
    },
    {
      "doc": 420,
      "topic": 23,
      "similarity": 0.760714656200399
    },
    {
      "doc": 420,
      "topic": 24,
      "similarity": 0.7613700485183812
    },
    {
      "doc": 421,
      "topic": 1,
      "similarity": 0.751412565577565
    },
    {
      "doc": 421,
      "topic": 2,
      "similarity": 0.7804780959225996
    },
    {
      "doc": 421,
      "topic": 3,
      "similarity": 0.7874397987978147
    },
    {
      "doc": 421,
      "topic": 4,
      "similarity": 0.7554156118330819
    },
    {
      "doc": 421,
      "topic": 5,
      "similarity": 0.7822197008795414
    },
    {
      "doc": 421,
      "topic": 7,
      "similarity": 0.7711599957252808
    },
    {
      "doc": 421,
      "topic": 8,
      "similarity": 0.7998200831502624
    },
    {
      "doc": 421,
      "topic": 9,
      "similarity": 0.8131192026042402
    },
    {
      "doc": 421,
      "topic": 10,
      "similarity": 0.7614059220156719
    },
    {
      "doc": 421,
      "topic": 11,
      "similarity": 0.7605243499292207
    },
    {
      "doc": 421,
      "topic": 13,
      "similarity": 0.7724511282216318
    },
    {
      "doc": 421,
      "topic": 14,
      "similarity": 0.7771450375786014
    },
    {
      "doc": 421,
      "topic": 15,
      "similarity": 0.7749869175530305
    },
    {
      "doc": 421,
      "topic": 16,
      "similarity": 0.7841882367251676
    },
    {
      "doc": 421,
      "topic": 17,
      "similarity": 0.7930827026719235
    },
    {
      "doc": 421,
      "topic": 19,
      "similarity": 0.7949606744610892
    },
    {
      "doc": 421,
      "topic": 20,
      "similarity": 0.7830310135071155
    },
    {
      "doc": 421,
      "topic": 21,
      "similarity": 0.7781982398145827
    },
    {
      "doc": 421,
      "topic": 23,
      "similarity": 0.7530381471625346
    },
    {
      "doc": 421,
      "topic": 24,
      "similarity": 0.75523754930044
    },
    {
      "doc": 422,
      "topic": 2,
      "similarity": 0.7614327817835111
    },
    {
      "doc": 422,
      "topic": 3,
      "similarity": 0.8000416328420592
    },
    {
      "doc": 422,
      "topic": 5,
      "similarity": 0.8289787671667995
    },
    {
      "doc": 422,
      "topic": 7,
      "similarity": 0.76832464217261
    },
    {
      "doc": 422,
      "topic": 8,
      "similarity": 0.762267707712408
    },
    {
      "doc": 422,
      "topic": 9,
      "similarity": 0.8145322035327659
    },
    {
      "doc": 422,
      "topic": 10,
      "similarity": 0.7761552143747066
    },
    {
      "doc": 422,
      "topic": 11,
      "similarity": 0.7765237641429187
    },
    {
      "doc": 422,
      "topic": 13,
      "similarity": 0.777882640093728
    },
    {
      "doc": 422,
      "topic": 14,
      "similarity": 0.7645909870691564
    },
    {
      "doc": 422,
      "topic": 15,
      "similarity": 0.7583110390123242
    },
    {
      "doc": 422,
      "topic": 16,
      "similarity": 0.7850804149169537
    },
    {
      "doc": 422,
      "topic": 17,
      "similarity": 0.7786032671591279
    },
    {
      "doc": 422,
      "topic": 18,
      "similarity": 0.767493503726449
    },
    {
      "doc": 422,
      "topic": 19,
      "similarity": 0.8396957694468192
    },
    {
      "doc": 422,
      "topic": 20,
      "similarity": 0.7938691926971063
    },
    {
      "doc": 422,
      "topic": 21,
      "similarity": 0.7953861107845025
    },
    {
      "doc": 422,
      "topic": 23,
      "similarity": 0.760287588796661
    },
    {
      "doc": 423,
      "topic": 1,
      "similarity": 0.750509881027285
    },
    {
      "doc": 423,
      "topic": 2,
      "similarity": 0.7677064417773666
    },
    {
      "doc": 423,
      "topic": 3,
      "similarity": 0.8215200115313144
    },
    {
      "doc": 423,
      "topic": 4,
      "similarity": 0.7574977294672621
    },
    {
      "doc": 423,
      "topic": 5,
      "similarity": 0.8085622849058788
    },
    {
      "doc": 423,
      "topic": 6,
      "similarity": 0.7525852834621347
    },
    {
      "doc": 423,
      "topic": 7,
      "similarity": 0.7889474722262424
    },
    {
      "doc": 423,
      "topic": 8,
      "similarity": 0.7624803203517583
    },
    {
      "doc": 423,
      "topic": 9,
      "similarity": 0.8154707813769524
    },
    {
      "doc": 423,
      "topic": 10,
      "similarity": 0.7666616773295801
    },
    {
      "doc": 423,
      "topic": 11,
      "similarity": 0.7923344195708413
    },
    {
      "doc": 423,
      "topic": 13,
      "similarity": 0.7502171813898698
    },
    {
      "doc": 423,
      "topic": 14,
      "similarity": 0.7749600817252805
    },
    {
      "doc": 423,
      "topic": 15,
      "similarity": 0.7872502358563288
    },
    {
      "doc": 423,
      "topic": 16,
      "similarity": 0.809308171675872
    },
    {
      "doc": 423,
      "topic": 17,
      "similarity": 0.796470780355884
    },
    {
      "doc": 423,
      "topic": 18,
      "similarity": 0.7689757603630566
    },
    {
      "doc": 423,
      "topic": 19,
      "similarity": 0.8120306070168709
    },
    {
      "doc": 423,
      "topic": 20,
      "similarity": 0.7968336490346193
    },
    {
      "doc": 423,
      "topic": 21,
      "similarity": 0.803063945187271
    },
    {
      "doc": 423,
      "topic": 23,
      "similarity": 0.7701760572759335
    },
    {
      "doc": 424,
      "topic": 3,
      "similarity": 0.7550515872333333
    },
    {
      "doc": 424,
      "topic": 5,
      "similarity": 0.7613405077009143
    },
    {
      "doc": 424,
      "topic": 7,
      "similarity": 0.77195549834791
    },
    {
      "doc": 424,
      "topic": 9,
      "similarity": 0.8135377224627462
    },
    {
      "doc": 424,
      "topic": 15,
      "similarity": 0.7694164097031875
    },
    {
      "doc": 424,
      "topic": 16,
      "similarity": 0.7881218030858635
    },
    {
      "doc": 424,
      "topic": 17,
      "similarity": 0.7726935020983348
    },
    {
      "doc": 424,
      "topic": 19,
      "similarity": 0.7763975586296052
    },
    {
      "doc": 424,
      "topic": 20,
      "similarity": 0.7534326151551147
    },
    {
      "doc": 424,
      "topic": 21,
      "similarity": 0.7661796037736703
    },
    {
      "doc": 424,
      "topic": 24,
      "similarity": 0.7703758140257323
    },
    {
      "doc": 425,
      "topic": 2,
      "similarity": 0.7771779734776145
    },
    {
      "doc": 425,
      "topic": 3,
      "similarity": 0.7812779075515852
    },
    {
      "doc": 425,
      "topic": 5,
      "similarity": 0.7755937128780066
    },
    {
      "doc": 425,
      "topic": 7,
      "similarity": 0.7617046453516362
    },
    {
      "doc": 425,
      "topic": 8,
      "similarity": 0.7550796341318922
    },
    {
      "doc": 425,
      "topic": 9,
      "similarity": 0.7854539431426136
    },
    {
      "doc": 425,
      "topic": 10,
      "similarity": 0.7584711239972374
    },
    {
      "doc": 425,
      "topic": 11,
      "similarity": 0.8001391870296725
    },
    {
      "doc": 425,
      "topic": 13,
      "similarity": 0.7668247730330789
    },
    {
      "doc": 425,
      "topic": 14,
      "similarity": 0.7665394213311743
    },
    {
      "doc": 425,
      "topic": 15,
      "similarity": 0.77658857830735
    },
    {
      "doc": 425,
      "topic": 16,
      "similarity": 0.8036955589501354
    },
    {
      "doc": 425,
      "topic": 17,
      "similarity": 0.7919034670784425
    },
    {
      "doc": 425,
      "topic": 18,
      "similarity": 0.8315267872655582
    },
    {
      "doc": 425,
      "topic": 19,
      "similarity": 0.8346066530644184
    },
    {
      "doc": 425,
      "topic": 20,
      "similarity": 0.7858488132009043
    },
    {
      "doc": 425,
      "topic": 21,
      "similarity": 0.7929016679803628
    },
    {
      "doc": 425,
      "topic": 22,
      "similarity": 0.7509450903949072
    },
    {
      "doc": 425,
      "topic": 23,
      "similarity": 0.7538355310883407
    },
    {
      "doc": 426,
      "topic": 1,
      "similarity": 0.7539094928118089
    },
    {
      "doc": 426,
      "topic": 2,
      "similarity": 0.776188909012079
    },
    {
      "doc": 426,
      "topic": 3,
      "similarity": 0.8140033542592231
    },
    {
      "doc": 426,
      "topic": 4,
      "similarity": 0.7680872357093839
    },
    {
      "doc": 426,
      "topic": 5,
      "similarity": 0.7772562386836711
    },
    {
      "doc": 426,
      "topic": 7,
      "similarity": 0.8024879316560893
    },
    {
      "doc": 426,
      "topic": 8,
      "similarity": 0.7645717741055497
    },
    {
      "doc": 426,
      "topic": 9,
      "similarity": 0.8299455582239026
    },
    {
      "doc": 426,
      "topic": 10,
      "similarity": 0.7578109881673663
    },
    {
      "doc": 426,
      "topic": 11,
      "similarity": 0.7835830802059291
    },
    {
      "doc": 426,
      "topic": 14,
      "similarity": 0.7568606654438144
    },
    {
      "doc": 426,
      "topic": 15,
      "similarity": 0.7700833468517656
    },
    {
      "doc": 426,
      "topic": 16,
      "similarity": 0.8043484154573238
    },
    {
      "doc": 426,
      "topic": 17,
      "similarity": 0.8024290001734844
    },
    {
      "doc": 426,
      "topic": 19,
      "similarity": 0.8000065502194732
    },
    {
      "doc": 426,
      "topic": 20,
      "similarity": 0.7625870839784522
    },
    {
      "doc": 426,
      "topic": 21,
      "similarity": 0.7962386317736927
    },
    {
      "doc": 426,
      "topic": 23,
      "similarity": 0.7518348142560085
    },
    {
      "doc": 426,
      "topic": 24,
      "similarity": 0.7686585354540042
    },
    {
      "doc": 427,
      "topic": 1,
      "similarity": 0.7858696369545386
    },
    {
      "doc": 427,
      "topic": 2,
      "similarity": 0.8092699888457107
    },
    {
      "doc": 427,
      "topic": 3,
      "similarity": 0.8309216165122312
    },
    {
      "doc": 427,
      "topic": 4,
      "similarity": 0.7735881858243472
    },
    {
      "doc": 427,
      "topic": 5,
      "similarity": 0.8314105185613111
    },
    {
      "doc": 427,
      "topic": 7,
      "similarity": 0.8127338889540311
    },
    {
      "doc": 427,
      "topic": 8,
      "similarity": 0.7997127751595053
    },
    {
      "doc": 427,
      "topic": 9,
      "similarity": 0.8528517474323012
    },
    {
      "doc": 427,
      "topic": 10,
      "similarity": 0.8234985971758422
    },
    {
      "doc": 427,
      "topic": 11,
      "similarity": 0.8082769898808804
    },
    {
      "doc": 427,
      "topic": 13,
      "similarity": 0.7859032661493219
    },
    {
      "doc": 427,
      "topic": 14,
      "similarity": 0.8059515614031538
    },
    {
      "doc": 427,
      "topic": 15,
      "similarity": 0.796072327571019
    },
    {
      "doc": 427,
      "topic": 16,
      "similarity": 0.8230824266614554
    },
    {
      "doc": 427,
      "topic": 17,
      "similarity": 0.8256016797041773
    },
    {
      "doc": 427,
      "topic": 18,
      "similarity": 0.7853607980277292
    },
    {
      "doc": 427,
      "topic": 19,
      "similarity": 0.83780717314192
    },
    {
      "doc": 427,
      "topic": 20,
      "similarity": 0.7957625935905737
    },
    {
      "doc": 427,
      "topic": 21,
      "similarity": 0.8232161562118794
    },
    {
      "doc": 427,
      "topic": 23,
      "similarity": 0.7816871125337583
    },
    {
      "doc": 428,
      "topic": 3,
      "similarity": 0.7656444946865383
    },
    {
      "doc": 428,
      "topic": 5,
      "similarity": 0.7615231456665112
    },
    {
      "doc": 428,
      "topic": 6,
      "similarity": 0.7740265498385116
    },
    {
      "doc": 428,
      "topic": 9,
      "similarity": 0.8205028524204818
    },
    {
      "doc": 428,
      "topic": 10,
      "similarity": 0.7516951966422569
    },
    {
      "doc": 428,
      "topic": 11,
      "similarity": 0.7613998136157425
    },
    {
      "doc": 428,
      "topic": 15,
      "similarity": 0.769306511057478
    },
    {
      "doc": 428,
      "topic": 16,
      "similarity": 0.7700651014665848
    },
    {
      "doc": 428,
      "topic": 17,
      "similarity": 0.8093774332469718
    },
    {
      "doc": 428,
      "topic": 18,
      "similarity": 0.7686897192699833
    },
    {
      "doc": 428,
      "topic": 19,
      "similarity": 0.7835988162926331
    },
    {
      "doc": 428,
      "topic": 20,
      "similarity": 0.7880691738219345
    },
    {
      "doc": 428,
      "topic": 21,
      "similarity": 0.8066970716652836
    },
    {
      "doc": 428,
      "topic": 23,
      "similarity": 0.7620254388656397
    },
    {
      "doc": 428,
      "topic": 24,
      "similarity": 0.7741233204131376
    },
    {
      "doc": 429,
      "topic": 0,
      "similarity": 0.8392768818214247
    },
    {
      "doc": 429,
      "topic": 1,
      "similarity": 0.784522874434024
    },
    {
      "doc": 429,
      "topic": 2,
      "similarity": 0.7936106570516822
    },
    {
      "doc": 429,
      "topic": 3,
      "similarity": 0.801624004955065
    },
    {
      "doc": 429,
      "topic": 5,
      "similarity": 0.7806785343615031
    },
    {
      "doc": 429,
      "topic": 7,
      "similarity": 0.7663134670009436
    },
    {
      "doc": 429,
      "topic": 8,
      "similarity": 0.7522987990890446
    },
    {
      "doc": 429,
      "topic": 9,
      "similarity": 0.7962489604618536
    },
    {
      "doc": 429,
      "topic": 10,
      "similarity": 0.7696881341008395
    },
    {
      "doc": 429,
      "topic": 11,
      "similarity": 0.818793520936159
    },
    {
      "doc": 429,
      "topic": 13,
      "similarity": 0.7575860726338077
    },
    {
      "doc": 429,
      "topic": 14,
      "similarity": 0.7606308896594774
    },
    {
      "doc": 429,
      "topic": 15,
      "similarity": 0.7769685730562393
    },
    {
      "doc": 429,
      "topic": 16,
      "similarity": 0.8183857385897454
    },
    {
      "doc": 429,
      "topic": 17,
      "similarity": 0.8119214912124793
    },
    {
      "doc": 429,
      "topic": 19,
      "similarity": 0.7910395134716882
    },
    {
      "doc": 429,
      "topic": 20,
      "similarity": 0.7652916513672344
    },
    {
      "doc": 429,
      "topic": 21,
      "similarity": 0.7985632499563831
    },
    {
      "doc": 429,
      "topic": 23,
      "similarity": 0.7530444951951167
    },
    {
      "doc": 430,
      "topic": 3,
      "similarity": 0.7950411921379709
    },
    {
      "doc": 430,
      "topic": 5,
      "similarity": 0.7879706749584563
    },
    {
      "doc": 430,
      "topic": 7,
      "similarity": 0.7810030590671548
    },
    {
      "doc": 430,
      "topic": 8,
      "similarity": 0.7675147187370222
    },
    {
      "doc": 430,
      "topic": 9,
      "similarity": 0.817956891662529
    },
    {
      "doc": 430,
      "topic": 10,
      "similarity": 0.7710728898631265
    },
    {
      "doc": 430,
      "topic": 11,
      "similarity": 0.7888927602466379
    },
    {
      "doc": 430,
      "topic": 13,
      "similarity": 0.758192346750173
    },
    {
      "doc": 430,
      "topic": 14,
      "similarity": 0.759480014475523
    },
    {
      "doc": 430,
      "topic": 15,
      "similarity": 0.7793010833317403
    },
    {
      "doc": 430,
      "topic": 16,
      "similarity": 0.8076106482208079
    },
    {
      "doc": 430,
      "topic": 17,
      "similarity": 0.7994188358960095
    },
    {
      "doc": 430,
      "topic": 18,
      "similarity": 0.7614228707984857
    },
    {
      "doc": 430,
      "topic": 19,
      "similarity": 0.8078343344705301
    },
    {
      "doc": 430,
      "topic": 20,
      "similarity": 0.7815448661710406
    },
    {
      "doc": 430,
      "topic": 21,
      "similarity": 0.7959313432703524
    },
    {
      "doc": 430,
      "topic": 22,
      "similarity": 0.7559317177847069
    },
    {
      "doc": 431,
      "topic": 0,
      "similarity": 0.7629127759093762
    },
    {
      "doc": 431,
      "topic": 2,
      "similarity": 0.7631074062168279
    },
    {
      "doc": 431,
      "topic": 3,
      "similarity": 0.7917876460228226
    },
    {
      "doc": 431,
      "topic": 5,
      "similarity": 0.77862037689023
    },
    {
      "doc": 431,
      "topic": 6,
      "similarity": 0.7689219616210548
    },
    {
      "doc": 431,
      "topic": 7,
      "similarity": 0.7511033403599401
    },
    {
      "doc": 431,
      "topic": 8,
      "similarity": 0.7662545157586009
    },
    {
      "doc": 431,
      "topic": 9,
      "similarity": 0.8196515220223554
    },
    {
      "doc": 431,
      "topic": 10,
      "similarity": 0.7598035491343691
    },
    {
      "doc": 431,
      "topic": 11,
      "similarity": 0.7658880468989576
    },
    {
      "doc": 431,
      "topic": 12,
      "similarity": 0.7642414502616803
    },
    {
      "doc": 431,
      "topic": 14,
      "similarity": 0.757770834702906
    },
    {
      "doc": 431,
      "topic": 15,
      "similarity": 0.7561065869740788
    },
    {
      "doc": 431,
      "topic": 16,
      "similarity": 0.7759939392492895
    },
    {
      "doc": 431,
      "topic": 17,
      "similarity": 0.7737290610535432
    },
    {
      "doc": 431,
      "topic": 18,
      "similarity": 0.7789747313883989
    },
    {
      "doc": 431,
      "topic": 19,
      "similarity": 0.797257285571886
    },
    {
      "doc": 431,
      "topic": 20,
      "similarity": 0.7882606929901491
    },
    {
      "doc": 431,
      "topic": 21,
      "similarity": 0.7962207213008758
    },
    {
      "doc": 431,
      "topic": 23,
      "similarity": 0.814316593507122
    },
    {
      "doc": 431,
      "topic": 24,
      "similarity": 0.776003681614963
    },
    {
      "doc": 432,
      "topic": 1,
      "similarity": 0.7720746376751815
    },
    {
      "doc": 432,
      "topic": 2,
      "similarity": 0.7949077483042627
    },
    {
      "doc": 432,
      "topic": 3,
      "similarity": 0.8115056165994238
    },
    {
      "doc": 432,
      "topic": 4,
      "similarity": 0.75735131935952
    },
    {
      "doc": 432,
      "topic": 5,
      "similarity": 0.8069206350985312
    },
    {
      "doc": 432,
      "topic": 7,
      "similarity": 0.7825617885426072
    },
    {
      "doc": 432,
      "topic": 8,
      "similarity": 0.781667760998914
    },
    {
      "doc": 432,
      "topic": 9,
      "similarity": 0.8357603893840512
    },
    {
      "doc": 432,
      "topic": 10,
      "similarity": 0.8097276573992083
    },
    {
      "doc": 432,
      "topic": 11,
      "similarity": 0.7962801346442193
    },
    {
      "doc": 432,
      "topic": 13,
      "similarity": 0.7760420461483742
    },
    {
      "doc": 432,
      "topic": 14,
      "similarity": 0.7864207970403574
    },
    {
      "doc": 432,
      "topic": 15,
      "similarity": 0.7857556321389616
    },
    {
      "doc": 432,
      "topic": 16,
      "similarity": 0.822351540003458
    },
    {
      "doc": 432,
      "topic": 17,
      "similarity": 0.8192937813486121
    },
    {
      "doc": 432,
      "topic": 18,
      "similarity": 0.7906737137981626
    },
    {
      "doc": 432,
      "topic": 19,
      "similarity": 0.8258226290679841
    },
    {
      "doc": 432,
      "topic": 20,
      "similarity": 0.8222291225719326
    },
    {
      "doc": 432,
      "topic": 21,
      "similarity": 0.8218396045627531
    },
    {
      "doc": 432,
      "topic": 23,
      "similarity": 0.7767930986700904
    },
    {
      "doc": 432,
      "topic": 24,
      "similarity": 0.7615753020842997
    },
    {
      "doc": 433,
      "topic": 3,
      "similarity": 0.7908594879745485
    },
    {
      "doc": 433,
      "topic": 4,
      "similarity": 0.7813946880357794
    },
    {
      "doc": 433,
      "topic": 5,
      "similarity": 0.7824523621139353
    },
    {
      "doc": 433,
      "topic": 7,
      "similarity": 0.7704550281912059
    },
    {
      "doc": 433,
      "topic": 8,
      "similarity": 0.7912327115276752
    },
    {
      "doc": 433,
      "topic": 9,
      "similarity": 0.810997489653063
    },
    {
      "doc": 433,
      "topic": 10,
      "similarity": 0.7999047329281557
    },
    {
      "doc": 433,
      "topic": 11,
      "similarity": 0.7697648074348389
    },
    {
      "doc": 433,
      "topic": 13,
      "similarity": 0.7721447794205883
    },
    {
      "doc": 433,
      "topic": 14,
      "similarity": 0.7612739374762083
    },
    {
      "doc": 433,
      "topic": 15,
      "similarity": 0.7726074484210774
    },
    {
      "doc": 433,
      "topic": 16,
      "similarity": 0.7812814992877309
    },
    {
      "doc": 433,
      "topic": 17,
      "similarity": 0.7787934184006753
    },
    {
      "doc": 433,
      "topic": 18,
      "similarity": 0.7691116207603026
    },
    {
      "doc": 433,
      "topic": 19,
      "similarity": 0.7986280365714269
    },
    {
      "doc": 433,
      "topic": 20,
      "similarity": 0.7774711774773104
    },
    {
      "doc": 433,
      "topic": 21,
      "similarity": 0.7991180698654318
    },
    {
      "doc": 433,
      "topic": 24,
      "similarity": 0.7628622384718348
    },
    {
      "doc": 434,
      "topic": 2,
      "similarity": 0.7576749665567234
    },
    {
      "doc": 434,
      "topic": 3,
      "similarity": 0.7835932575901928
    },
    {
      "doc": 434,
      "topic": 5,
      "similarity": 0.7779423300771202
    },
    {
      "doc": 434,
      "topic": 7,
      "similarity": 0.7770871786058655
    },
    {
      "doc": 434,
      "topic": 8,
      "similarity": 0.7909433897724072
    },
    {
      "doc": 434,
      "topic": 9,
      "similarity": 0.8474890253223208
    },
    {
      "doc": 434,
      "topic": 10,
      "similarity": 0.7707359515342338
    },
    {
      "doc": 434,
      "topic": 11,
      "similarity": 0.7795698533498139
    },
    {
      "doc": 434,
      "topic": 12,
      "similarity": 0.798530316066783
    },
    {
      "doc": 434,
      "topic": 13,
      "similarity": 0.7598445300520686
    },
    {
      "doc": 434,
      "topic": 14,
      "similarity": 0.758017670254752
    },
    {
      "doc": 434,
      "topic": 15,
      "similarity": 0.7991752750265813
    },
    {
      "doc": 434,
      "topic": 16,
      "similarity": 0.7868644109410657
    },
    {
      "doc": 434,
      "topic": 17,
      "similarity": 0.8076510620582967
    },
    {
      "doc": 434,
      "topic": 18,
      "similarity": 0.7555985680010556
    },
    {
      "doc": 434,
      "topic": 19,
      "similarity": 0.7942078483781596
    },
    {
      "doc": 434,
      "topic": 20,
      "similarity": 0.7672012762880985
    },
    {
      "doc": 434,
      "topic": 21,
      "similarity": 0.7915017865208985
    },
    {
      "doc": 435,
      "topic": 2,
      "similarity": 0.7756089548399399
    },
    {
      "doc": 435,
      "topic": 3,
      "similarity": 0.7809781161257641
    },
    {
      "doc": 435,
      "topic": 5,
      "similarity": 0.7740506790785557
    },
    {
      "doc": 435,
      "topic": 7,
      "similarity": 0.7763176289216841
    },
    {
      "doc": 435,
      "topic": 8,
      "similarity": 0.7672882649438639
    },
    {
      "doc": 435,
      "topic": 9,
      "similarity": 0.8193504782388554
    },
    {
      "doc": 435,
      "topic": 10,
      "similarity": 0.7732069688073766
    },
    {
      "doc": 435,
      "topic": 11,
      "similarity": 0.823669478859524
    },
    {
      "doc": 435,
      "topic": 14,
      "similarity": 0.765831065432045
    },
    {
      "doc": 435,
      "topic": 15,
      "similarity": 0.7888325932953212
    },
    {
      "doc": 435,
      "topic": 16,
      "similarity": 0.7855032143272804
    },
    {
      "doc": 435,
      "topic": 17,
      "similarity": 0.7883169485278743
    },
    {
      "doc": 435,
      "topic": 18,
      "similarity": 0.7964538726240414
    },
    {
      "doc": 435,
      "topic": 19,
      "similarity": 0.8079398366988807
    },
    {
      "doc": 435,
      "topic": 20,
      "similarity": 0.7728963409684294
    },
    {
      "doc": 435,
      "topic": 21,
      "similarity": 0.7959029417149821
    },
    {
      "doc": 436,
      "topic": 1,
      "similarity": 0.7532191123813223
    },
    {
      "doc": 436,
      "topic": 2,
      "similarity": 0.7540209677736436
    },
    {
      "doc": 436,
      "topic": 3,
      "similarity": 0.7883421597766741
    },
    {
      "doc": 436,
      "topic": 4,
      "similarity": 0.7770812691529082
    },
    {
      "doc": 436,
      "topic": 5,
      "similarity": 0.7999215785724607
    },
    {
      "doc": 436,
      "topic": 7,
      "similarity": 0.7736463976554845
    },
    {
      "doc": 436,
      "topic": 8,
      "similarity": 0.7652853884395314
    },
    {
      "doc": 436,
      "topic": 9,
      "similarity": 0.8292241320211503
    },
    {
      "doc": 436,
      "topic": 10,
      "similarity": 0.7639774808166852
    },
    {
      "doc": 436,
      "topic": 11,
      "similarity": 0.7754178965973803
    },
    {
      "doc": 436,
      "topic": 13,
      "similarity": 0.7662947697948281
    },
    {
      "doc": 436,
      "topic": 14,
      "similarity": 0.7628204979596467
    },
    {
      "doc": 436,
      "topic": 15,
      "similarity": 0.7598545985546876
    },
    {
      "doc": 436,
      "topic": 16,
      "similarity": 0.7869725875147779
    },
    {
      "doc": 436,
      "topic": 17,
      "similarity": 0.7787565327164114
    },
    {
      "doc": 436,
      "topic": 19,
      "similarity": 0.780803187242531
    },
    {
      "doc": 436,
      "topic": 20,
      "similarity": 0.7550963588743315
    },
    {
      "doc": 436,
      "topic": 21,
      "similarity": 0.7815512010874744
    },
    {
      "doc": 436,
      "topic": 24,
      "similarity": 0.7672318096553208
    },
    {
      "doc": 437,
      "topic": 2,
      "similarity": 0.7536710679484865
    },
    {
      "doc": 437,
      "topic": 3,
      "similarity": 0.7777593008545243
    },
    {
      "doc": 437,
      "topic": 5,
      "similarity": 0.79267586317292
    },
    {
      "doc": 437,
      "topic": 7,
      "similarity": 0.7693843359382203
    },
    {
      "doc": 437,
      "topic": 8,
      "similarity": 0.7543180848600217
    },
    {
      "doc": 437,
      "topic": 9,
      "similarity": 0.8096972128257889
    },
    {
      "doc": 437,
      "topic": 11,
      "similarity": 0.8004058576001027
    },
    {
      "doc": 437,
      "topic": 15,
      "similarity": 0.7503134489867582
    },
    {
      "doc": 437,
      "topic": 16,
      "similarity": 0.7567080575462567
    },
    {
      "doc": 437,
      "topic": 17,
      "similarity": 0.760433484133883
    },
    {
      "doc": 437,
      "topic": 18,
      "similarity": 0.7745459089600022
    },
    {
      "doc": 437,
      "topic": 19,
      "similarity": 0.7715849180577623
    },
    {
      "doc": 437,
      "topic": 20,
      "similarity": 0.7834522123898446
    },
    {
      "doc": 437,
      "topic": 21,
      "similarity": 0.7758996707111753
    },
    {
      "doc": 437,
      "topic": 23,
      "similarity": 0.7558705966204122
    },
    {
      "doc": 437,
      "topic": 24,
      "similarity": 0.757246303104476
    },
    {
      "doc": 438,
      "topic": 2,
      "similarity": 0.7552967781617921
    },
    {
      "doc": 438,
      "topic": 3,
      "similarity": 0.7722645619489684
    },
    {
      "doc": 438,
      "topic": 5,
      "similarity": 0.778867248914763
    },
    {
      "doc": 438,
      "topic": 7,
      "similarity": 0.7824135352271843
    },
    {
      "doc": 438,
      "topic": 8,
      "similarity": 0.8098500976798789
    },
    {
      "doc": 438,
      "topic": 9,
      "similarity": 0.804766797430403
    },
    {
      "doc": 438,
      "topic": 10,
      "similarity": 0.7631669668897053
    },
    {
      "doc": 438,
      "topic": 11,
      "similarity": 0.7627457428118923
    },
    {
      "doc": 438,
      "topic": 13,
      "similarity": 0.752805401634403
    },
    {
      "doc": 438,
      "topic": 14,
      "similarity": 0.7550060013950355
    },
    {
      "doc": 438,
      "topic": 15,
      "similarity": 0.7701851754092988
    },
    {
      "doc": 438,
      "topic": 16,
      "similarity": 0.7938011883281882
    },
    {
      "doc": 438,
      "topic": 17,
      "similarity": 0.7812420188338237
    },
    {
      "doc": 438,
      "topic": 18,
      "similarity": 0.7508255152343726
    },
    {
      "doc": 438,
      "topic": 19,
      "similarity": 0.790022560420311
    },
    {
      "doc": 438,
      "topic": 20,
      "similarity": 0.7788843483148025
    },
    {
      "doc": 438,
      "topic": 21,
      "similarity": 0.8017186460012561
    },
    {
      "doc": 438,
      "topic": 24,
      "similarity": 0.755392831308511
    },
    {
      "doc": 439,
      "topic": 2,
      "similarity": 0.781166101392002
    },
    {
      "doc": 439,
      "topic": 3,
      "similarity": 0.7979589091607792
    },
    {
      "doc": 439,
      "topic": 5,
      "similarity": 0.8072689689053574
    },
    {
      "doc": 439,
      "topic": 7,
      "similarity": 0.7709374651245525
    },
    {
      "doc": 439,
      "topic": 8,
      "similarity": 0.7696174051001496
    },
    {
      "doc": 439,
      "topic": 9,
      "similarity": 0.8113684954014891
    },
    {
      "doc": 439,
      "topic": 10,
      "similarity": 0.7858238570223627
    },
    {
      "doc": 439,
      "topic": 11,
      "similarity": 0.7654407529977217
    },
    {
      "doc": 439,
      "topic": 13,
      "similarity": 0.7576955984313677
    },
    {
      "doc": 439,
      "topic": 15,
      "similarity": 0.7643461757221476
    },
    {
      "doc": 439,
      "topic": 16,
      "similarity": 0.8045274376558619
    },
    {
      "doc": 439,
      "topic": 17,
      "similarity": 0.8008271887114816
    },
    {
      "doc": 439,
      "topic": 18,
      "similarity": 0.7628380087962905
    },
    {
      "doc": 439,
      "topic": 19,
      "similarity": 0.8194056764332127
    },
    {
      "doc": 439,
      "topic": 20,
      "similarity": 0.7936764419281221
    },
    {
      "doc": 439,
      "topic": 21,
      "similarity": 0.7976488320452376
    },
    {
      "doc": 439,
      "topic": 23,
      "similarity": 0.7503965759785642
    },
    {
      "doc": 439,
      "topic": 24,
      "similarity": 0.7533838950931978
    },
    {
      "doc": 440,
      "topic": 1,
      "similarity": 0.7525389236838602
    },
    {
      "doc": 440,
      "topic": 2,
      "similarity": 0.7695018855290245
    },
    {
      "doc": 440,
      "topic": 3,
      "similarity": 0.7894392894855798
    },
    {
      "doc": 440,
      "topic": 5,
      "similarity": 0.8012773460355069
    },
    {
      "doc": 440,
      "topic": 6,
      "similarity": 0.7680635501887694
    },
    {
      "doc": 440,
      "topic": 7,
      "similarity": 0.7868911824573145
    },
    {
      "doc": 440,
      "topic": 8,
      "similarity": 0.7549081393962985
    },
    {
      "doc": 440,
      "topic": 9,
      "similarity": 0.8621311197956557
    },
    {
      "doc": 440,
      "topic": 10,
      "similarity": 0.7690121190572451
    },
    {
      "doc": 440,
      "topic": 11,
      "similarity": 0.7786126687830321
    },
    {
      "doc": 440,
      "topic": 13,
      "similarity": 0.7588129852209216
    },
    {
      "doc": 440,
      "topic": 14,
      "similarity": 0.7664261865690065
    },
    {
      "doc": 440,
      "topic": 15,
      "similarity": 0.7690190439081561
    },
    {
      "doc": 440,
      "topic": 16,
      "similarity": 0.8031740219548626
    },
    {
      "doc": 440,
      "topic": 17,
      "similarity": 0.795839637228561
    },
    {
      "doc": 440,
      "topic": 19,
      "similarity": 0.7975501748127566
    },
    {
      "doc": 440,
      "topic": 20,
      "similarity": 0.7573815949625313
    },
    {
      "doc": 440,
      "topic": 21,
      "similarity": 0.7999620329356677
    },
    {
      "doc": 440,
      "topic": 23,
      "similarity": 0.796416992716271
    },
    {
      "doc": 440,
      "topic": 24,
      "similarity": 0.7811657763009188
    },
    {
      "doc": 441,
      "topic": 1,
      "similarity": 0.7613159863050838
    },
    {
      "doc": 441,
      "topic": 2,
      "similarity": 0.7761286658354358
    },
    {
      "doc": 441,
      "topic": 3,
      "similarity": 0.802319874638203
    },
    {
      "doc": 441,
      "topic": 4,
      "similarity": 0.7560099489323334
    },
    {
      "doc": 441,
      "topic": 5,
      "similarity": 0.8027232059462991
    },
    {
      "doc": 441,
      "topic": 7,
      "similarity": 0.8134445905076227
    },
    {
      "doc": 441,
      "topic": 8,
      "similarity": 0.8013988782000107
    },
    {
      "doc": 441,
      "topic": 9,
      "similarity": 0.8486753630460223
    },
    {
      "doc": 441,
      "topic": 10,
      "similarity": 0.7927783011104304
    },
    {
      "doc": 441,
      "topic": 11,
      "similarity": 0.7953724121673132
    },
    {
      "doc": 441,
      "topic": 13,
      "similarity": 0.7968908013580049
    },
    {
      "doc": 441,
      "topic": 14,
      "similarity": 0.7830767614581458
    },
    {
      "doc": 441,
      "topic": 15,
      "similarity": 0.7858865158408523
    },
    {
      "doc": 441,
      "topic": 16,
      "similarity": 0.8148169583760333
    },
    {
      "doc": 441,
      "topic": 17,
      "similarity": 0.8226583579362983
    },
    {
      "doc": 441,
      "topic": 18,
      "similarity": 0.7605857802215311
    },
    {
      "doc": 441,
      "topic": 19,
      "similarity": 0.7993100208693311
    },
    {
      "doc": 441,
      "topic": 20,
      "similarity": 0.7898846841397679
    },
    {
      "doc": 441,
      "topic": 21,
      "similarity": 0.8437155820418296
    },
    {
      "doc": 441,
      "topic": 24,
      "similarity": 0.7505850647241953
    },
    {
      "doc": 442,
      "topic": 3,
      "similarity": 0.7672755186707015
    },
    {
      "doc": 442,
      "topic": 5,
      "similarity": 0.777488299861602
    },
    {
      "doc": 442,
      "topic": 7,
      "similarity": 0.7515088360118426
    },
    {
      "doc": 442,
      "topic": 8,
      "similarity": 0.7618786619379285
    },
    {
      "doc": 442,
      "topic": 9,
      "similarity": 0.8164165187112165
    },
    {
      "doc": 442,
      "topic": 10,
      "similarity": 0.752565659431652
    },
    {
      "doc": 442,
      "topic": 11,
      "similarity": 0.7685848413087047
    },
    {
      "doc": 442,
      "topic": 12,
      "similarity": 0.7563787158290587
    },
    {
      "doc": 442,
      "topic": 15,
      "similarity": 0.7972386756910161
    },
    {
      "doc": 442,
      "topic": 16,
      "similarity": 0.7765658386791106
    },
    {
      "doc": 442,
      "topic": 17,
      "similarity": 0.7835777018802337
    },
    {
      "doc": 442,
      "topic": 19,
      "similarity": 0.7801111115453832
    },
    {
      "doc": 442,
      "topic": 20,
      "similarity": 0.7723846886345762
    },
    {
      "doc": 442,
      "topic": 21,
      "similarity": 0.7826493115400804
    },
    {
      "doc": 442,
      "topic": 24,
      "similarity": 0.7595403840492767
    },
    {
      "doc": 443,
      "topic": 1,
      "similarity": 0.7655969902293811
    },
    {
      "doc": 443,
      "topic": 2,
      "similarity": 0.8004149098572565
    },
    {
      "doc": 443,
      "topic": 3,
      "similarity": 0.7901981244438362
    },
    {
      "doc": 443,
      "topic": 5,
      "similarity": 0.783010507799976
    },
    {
      "doc": 443,
      "topic": 7,
      "similarity": 0.7764396875227674
    },
    {
      "doc": 443,
      "topic": 8,
      "similarity": 0.8216513094492679
    },
    {
      "doc": 443,
      "topic": 9,
      "similarity": 0.8126113588623569
    },
    {
      "doc": 443,
      "topic": 10,
      "similarity": 0.7847671704485286
    },
    {
      "doc": 443,
      "topic": 11,
      "similarity": 0.7898375051978216
    },
    {
      "doc": 443,
      "topic": 13,
      "similarity": 0.7892391968099797
    },
    {
      "doc": 443,
      "topic": 14,
      "similarity": 0.7909803393331546
    },
    {
      "doc": 443,
      "topic": 15,
      "similarity": 0.8204467063514165
    },
    {
      "doc": 443,
      "topic": 16,
      "similarity": 0.7950892645092028
    },
    {
      "doc": 443,
      "topic": 17,
      "similarity": 0.8149984943632742
    },
    {
      "doc": 443,
      "topic": 18,
      "similarity": 0.762270127724946
    },
    {
      "doc": 443,
      "topic": 19,
      "similarity": 0.8181602914135576
    },
    {
      "doc": 443,
      "topic": 20,
      "similarity": 0.7665810008034959
    },
    {
      "doc": 443,
      "topic": 21,
      "similarity": 0.7915630095932589
    },
    {
      "doc": 443,
      "topic": 22,
      "similarity": 0.8032273074954542
    },
    {
      "doc": 444,
      "topic": 1,
      "similarity": 0.7613233647075962
    },
    {
      "doc": 444,
      "topic": 2,
      "similarity": 0.7913408576162169
    },
    {
      "doc": 444,
      "topic": 3,
      "similarity": 0.8087721512858798
    },
    {
      "doc": 444,
      "topic": 4,
      "similarity": 0.7565395107187962
    },
    {
      "doc": 444,
      "topic": 5,
      "similarity": 0.8148168002796846
    },
    {
      "doc": 444,
      "topic": 7,
      "similarity": 0.7903096401407058
    },
    {
      "doc": 444,
      "topic": 8,
      "similarity": 0.7872986652972187
    },
    {
      "doc": 444,
      "topic": 9,
      "similarity": 0.8436238663137894
    },
    {
      "doc": 444,
      "topic": 10,
      "similarity": 0.8006779357080874
    },
    {
      "doc": 444,
      "topic": 11,
      "similarity": 0.8035344015820697
    },
    {
      "doc": 444,
      "topic": 13,
      "similarity": 0.7924784372114033
    },
    {
      "doc": 444,
      "topic": 14,
      "similarity": 0.7808057093967277
    },
    {
      "doc": 444,
      "topic": 15,
      "similarity": 0.8024002801449124
    },
    {
      "doc": 444,
      "topic": 16,
      "similarity": 0.8294927665078422
    },
    {
      "doc": 444,
      "topic": 17,
      "similarity": 0.8237069616721688
    },
    {
      "doc": 444,
      "topic": 18,
      "similarity": 0.77021563460444
    },
    {
      "doc": 444,
      "topic": 19,
      "similarity": 0.8159930940072423
    },
    {
      "doc": 444,
      "topic": 20,
      "similarity": 0.7906804914090464
    },
    {
      "doc": 444,
      "topic": 21,
      "similarity": 0.8222893605178145
    },
    {
      "doc": 444,
      "topic": 22,
      "similarity": 0.7653773502054517
    },
    {
      "doc": 444,
      "topic": 23,
      "similarity": 0.7541550353736494
    },
    {
      "doc": 444,
      "topic": 24,
      "similarity": 0.7920284171509656
    },
    {
      "doc": 445,
      "topic": 2,
      "similarity": 0.7554013275189342
    },
    {
      "doc": 445,
      "topic": 3,
      "similarity": 0.7849477501220292
    },
    {
      "doc": 445,
      "topic": 5,
      "similarity": 0.7967748582244609
    },
    {
      "doc": 445,
      "topic": 7,
      "similarity": 0.7540264346850709
    },
    {
      "doc": 445,
      "topic": 8,
      "similarity": 0.7571725503190629
    },
    {
      "doc": 445,
      "topic": 9,
      "similarity": 0.7870725914231622
    },
    {
      "doc": 445,
      "topic": 10,
      "similarity": 0.7541771249711158
    },
    {
      "doc": 445,
      "topic": 11,
      "similarity": 0.7708908986764192
    },
    {
      "doc": 445,
      "topic": 15,
      "similarity": 0.7647519425683232
    },
    {
      "doc": 445,
      "topic": 16,
      "similarity": 0.7809513482384047
    },
    {
      "doc": 445,
      "topic": 17,
      "similarity": 0.7672999856085257
    },
    {
      "doc": 445,
      "topic": 18,
      "similarity": 0.7603618772256006
    },
    {
      "doc": 445,
      "topic": 19,
      "similarity": 0.8056845137862974
    },
    {
      "doc": 445,
      "topic": 20,
      "similarity": 0.7690717285240471
    },
    {
      "doc": 445,
      "topic": 21,
      "similarity": 0.7795748875855681
    },
    {
      "doc": 445,
      "topic": 23,
      "similarity": 0.7587042177264173
    },
    {
      "doc": 446,
      "topic": 2,
      "similarity": 0.7611519033968073
    },
    {
      "doc": 446,
      "topic": 3,
      "similarity": 0.7764729083717142
    },
    {
      "doc": 446,
      "topic": 5,
      "similarity": 0.791734954131324
    },
    {
      "doc": 446,
      "topic": 7,
      "similarity": 0.7677782764275367
    },
    {
      "doc": 446,
      "topic": 8,
      "similarity": 0.7579810331850132
    },
    {
      "doc": 446,
      "topic": 9,
      "similarity": 0.7998511075710538
    },
    {
      "doc": 446,
      "topic": 10,
      "similarity": 0.7664005012300782
    },
    {
      "doc": 446,
      "topic": 11,
      "similarity": 0.7610431388393278
    },
    {
      "doc": 446,
      "topic": 15,
      "similarity": 0.7733973104462838
    },
    {
      "doc": 446,
      "topic": 16,
      "similarity": 0.7801386719798582
    },
    {
      "doc": 446,
      "topic": 17,
      "similarity": 0.794186628386061
    },
    {
      "doc": 446,
      "topic": 19,
      "similarity": 0.7878812341901082
    },
    {
      "doc": 446,
      "topic": 20,
      "similarity": 0.774085796952675
    },
    {
      "doc": 446,
      "topic": 21,
      "similarity": 0.7961136176678772
    },
    {
      "doc": 447,
      "topic": 3,
      "similarity": 0.7663736471685114
    },
    {
      "doc": 447,
      "topic": 5,
      "similarity": 0.7749479223151824
    },
    {
      "doc": 447,
      "topic": 7,
      "similarity": 0.7521994706326178
    },
    {
      "doc": 447,
      "topic": 9,
      "similarity": 0.7891956100326094
    },
    {
      "doc": 447,
      "topic": 11,
      "similarity": 0.7501455747041199
    },
    {
      "doc": 447,
      "topic": 15,
      "similarity": 0.7591708787492404
    },
    {
      "doc": 447,
      "topic": 16,
      "similarity": 0.7713890958789561
    },
    {
      "doc": 447,
      "topic": 17,
      "similarity": 0.7885852543285814
    },
    {
      "doc": 447,
      "topic": 19,
      "similarity": 0.7792160167894014
    },
    {
      "doc": 447,
      "topic": 20,
      "similarity": 0.7699264397064275
    },
    {
      "doc": 447,
      "topic": 21,
      "similarity": 0.7867955467466606
    },
    {
      "doc": 447,
      "topic": 23,
      "similarity": 0.7540979815699483
    },
    {
      "doc": 447,
      "topic": 24,
      "similarity": 0.7711403688374057
    },
    {
      "doc": 448,
      "topic": 1,
      "similarity": 0.7558226395487194
    },
    {
      "doc": 448,
      "topic": 2,
      "similarity": 0.7781073336901969
    },
    {
      "doc": 448,
      "topic": 3,
      "similarity": 0.7866664721459596
    },
    {
      "doc": 448,
      "topic": 5,
      "similarity": 0.7929204052600167
    },
    {
      "doc": 448,
      "topic": 6,
      "similarity": 0.807528831645149
    },
    {
      "doc": 448,
      "topic": 7,
      "similarity": 0.762990797332083
    },
    {
      "doc": 448,
      "topic": 8,
      "similarity": 0.7602901826298288
    },
    {
      "doc": 448,
      "topic": 9,
      "similarity": 0.8179167722836523
    },
    {
      "doc": 448,
      "topic": 10,
      "similarity": 0.7830174430728332
    },
    {
      "doc": 448,
      "topic": 11,
      "similarity": 0.7897394157033426
    },
    {
      "doc": 448,
      "topic": 13,
      "similarity": 0.7894099420945115
    },
    {
      "doc": 448,
      "topic": 14,
      "similarity": 0.7915405652025788
    },
    {
      "doc": 448,
      "topic": 15,
      "similarity": 0.7897252436165395
    },
    {
      "doc": 448,
      "topic": 16,
      "similarity": 0.815360926051125
    },
    {
      "doc": 448,
      "topic": 17,
      "similarity": 0.8198986947971967
    },
    {
      "doc": 448,
      "topic": 18,
      "similarity": 0.7693198375542512
    },
    {
      "doc": 448,
      "topic": 19,
      "similarity": 0.8205879555982253
    },
    {
      "doc": 448,
      "topic": 20,
      "similarity": 0.7700766799576704
    },
    {
      "doc": 448,
      "topic": 21,
      "similarity": 0.8129962660782011
    },
    {
      "doc": 448,
      "topic": 23,
      "similarity": 0.7611305806941957
    },
    {
      "doc": 449,
      "topic": 2,
      "similarity": 0.7640239179030688
    },
    {
      "doc": 449,
      "topic": 3,
      "similarity": 0.7925783082314971
    },
    {
      "doc": 449,
      "topic": 4,
      "similarity": 0.7565908855318989
    },
    {
      "doc": 449,
      "topic": 5,
      "similarity": 0.8088602187813418
    },
    {
      "doc": 449,
      "topic": 7,
      "similarity": 0.7906968995588971
    },
    {
      "doc": 449,
      "topic": 8,
      "similarity": 0.803286815784482
    },
    {
      "doc": 449,
      "topic": 9,
      "similarity": 0.82586392647282
    },
    {
      "doc": 449,
      "topic": 10,
      "similarity": 0.8030521935204166
    },
    {
      "doc": 449,
      "topic": 11,
      "similarity": 0.7766934503401485
    },
    {
      "doc": 449,
      "topic": 13,
      "similarity": 0.7734281696694962
    },
    {
      "doc": 449,
      "topic": 14,
      "similarity": 0.7824837471337504
    },
    {
      "doc": 449,
      "topic": 15,
      "similarity": 0.7637606995821148
    },
    {
      "doc": 449,
      "topic": 16,
      "similarity": 0.7932361604217627
    },
    {
      "doc": 449,
      "topic": 17,
      "similarity": 0.7811738558985013
    },
    {
      "doc": 449,
      "topic": 18,
      "similarity": 0.7641496970587638
    },
    {
      "doc": 449,
      "topic": 19,
      "similarity": 0.7990652440624003
    },
    {
      "doc": 449,
      "topic": 20,
      "similarity": 0.768708284510224
    },
    {
      "doc": 449,
      "topic": 21,
      "similarity": 0.7816736640834349
    },
    {
      "doc": 449,
      "topic": 23,
      "similarity": 0.7502888582974331
    },
    {
      "doc": 449,
      "topic": 24,
      "similarity": 0.754987418796809
    },
    {
      "doc": 450,
      "topic": 2,
      "similarity": 0.7715542515938402
    },
    {
      "doc": 450,
      "topic": 3,
      "similarity": 0.7847910728432176
    },
    {
      "doc": 450,
      "topic": 5,
      "similarity": 0.777815049144786
    },
    {
      "doc": 450,
      "topic": 7,
      "similarity": 0.7620601801281957
    },
    {
      "doc": 450,
      "topic": 9,
      "similarity": 0.8256401442534484
    },
    {
      "doc": 450,
      "topic": 10,
      "similarity": 0.7633432506893302
    },
    {
      "doc": 450,
      "topic": 11,
      "similarity": 0.7749759422625135
    },
    {
      "doc": 450,
      "topic": 14,
      "similarity": 0.758751263490446
    },
    {
      "doc": 450,
      "topic": 15,
      "similarity": 0.7841062473159566
    },
    {
      "doc": 450,
      "topic": 16,
      "similarity": 0.7938878453492723
    },
    {
      "doc": 450,
      "topic": 17,
      "similarity": 0.7971261551504475
    },
    {
      "doc": 450,
      "topic": 18,
      "similarity": 0.7661938182443283
    },
    {
      "doc": 450,
      "topic": 19,
      "similarity": 0.8099110266506767
    },
    {
      "doc": 450,
      "topic": 20,
      "similarity": 0.7653527810688917
    },
    {
      "doc": 450,
      "topic": 21,
      "similarity": 0.7973185114477026
    },
    {
      "doc": 450,
      "topic": 23,
      "similarity": 0.8300211403472283
    },
    {
      "doc": 450,
      "topic": 24,
      "similarity": 0.7633954101594506
    },
    {
      "doc": 451,
      "topic": 16,
      "similarity": 0.7581027275743456
    },
    {
      "doc": 452,
      "topic": 3,
      "similarity": 0.7861739374846687
    },
    {
      "doc": 452,
      "topic": 4,
      "similarity": 0.7597427313317897
    },
    {
      "doc": 452,
      "topic": 5,
      "similarity": 0.7753933848307715
    },
    {
      "doc": 452,
      "topic": 7,
      "similarity": 0.7565495302268613
    },
    {
      "doc": 452,
      "topic": 8,
      "similarity": 0.7645875844729295
    },
    {
      "doc": 452,
      "topic": 9,
      "similarity": 0.8183678419292847
    },
    {
      "doc": 452,
      "topic": 10,
      "similarity": 0.7612478889737913
    },
    {
      "doc": 452,
      "topic": 11,
      "similarity": 0.7653841895551035
    },
    {
      "doc": 452,
      "topic": 15,
      "similarity": 0.7521188561872728
    },
    {
      "doc": 452,
      "topic": 16,
      "similarity": 0.764642993472025
    },
    {
      "doc": 452,
      "topic": 17,
      "similarity": 0.7834645442965094
    },
    {
      "doc": 452,
      "topic": 18,
      "similarity": 0.7566349462875838
    },
    {
      "doc": 452,
      "topic": 19,
      "similarity": 0.7835277238380237
    },
    {
      "doc": 452,
      "topic": 20,
      "similarity": 0.7530882149323972
    },
    {
      "doc": 452,
      "topic": 21,
      "similarity": 0.7894413983179925
    },
    {
      "doc": 452,
      "topic": 23,
      "similarity": 0.7507447437549014
    },
    {
      "doc": 452,
      "topic": 24,
      "similarity": 0.8199636225878589
    },
    {
      "doc": 453,
      "topic": 1,
      "similarity": 0.7591356882820777
    },
    {
      "doc": 453,
      "topic": 2,
      "similarity": 0.7807706434328605
    },
    {
      "doc": 453,
      "topic": 3,
      "similarity": 0.7958870289905025
    },
    {
      "doc": 453,
      "topic": 5,
      "similarity": 0.804655877653096
    },
    {
      "doc": 453,
      "topic": 6,
      "similarity": 0.773700666203937
    },
    {
      "doc": 453,
      "topic": 7,
      "similarity": 0.7952669114379398
    },
    {
      "doc": 453,
      "topic": 8,
      "similarity": 0.7729328608138839
    },
    {
      "doc": 453,
      "topic": 9,
      "similarity": 0.8507435362425619
    },
    {
      "doc": 453,
      "topic": 10,
      "similarity": 0.7818462119224823
    },
    {
      "doc": 453,
      "topic": 11,
      "similarity": 0.7746348184657488
    },
    {
      "doc": 453,
      "topic": 13,
      "similarity": 0.7641162567786983
    },
    {
      "doc": 453,
      "topic": 14,
      "similarity": 0.768028827770369
    },
    {
      "doc": 453,
      "topic": 15,
      "similarity": 0.780201541330883
    },
    {
      "doc": 453,
      "topic": 16,
      "similarity": 0.8118081609080936
    },
    {
      "doc": 453,
      "topic": 17,
      "similarity": 0.8072531333502214
    },
    {
      "doc": 453,
      "topic": 19,
      "similarity": 0.8053850450184525
    },
    {
      "doc": 453,
      "topic": 20,
      "similarity": 0.7686682918260547
    },
    {
      "doc": 453,
      "topic": 21,
      "similarity": 0.8092099029138844
    },
    {
      "doc": 453,
      "topic": 23,
      "similarity": 0.8199781953413241
    },
    {
      "doc": 453,
      "topic": 24,
      "similarity": 0.7651739438053827
    },
    {
      "doc": 454,
      "topic": 1,
      "similarity": 0.7767874706762405
    },
    {
      "doc": 454,
      "topic": 2,
      "similarity": 0.7840659245043673
    },
    {
      "doc": 454,
      "topic": 3,
      "similarity": 0.8130767008798906
    },
    {
      "doc": 454,
      "topic": 4,
      "similarity": 0.7922470776074455
    },
    {
      "doc": 454,
      "topic": 5,
      "similarity": 0.8089091551436763
    },
    {
      "doc": 454,
      "topic": 7,
      "similarity": 0.8173521060319693
    },
    {
      "doc": 454,
      "topic": 8,
      "similarity": 0.7916190107348562
    },
    {
      "doc": 454,
      "topic": 9,
      "similarity": 0.8442053545777298
    },
    {
      "doc": 454,
      "topic": 10,
      "similarity": 0.7985448699252246
    },
    {
      "doc": 454,
      "topic": 11,
      "similarity": 0.796255739855582
    },
    {
      "doc": 454,
      "topic": 13,
      "similarity": 0.7975330748091287
    },
    {
      "doc": 454,
      "topic": 14,
      "similarity": 0.7835231371853871
    },
    {
      "doc": 454,
      "topic": 15,
      "similarity": 0.7888750613473241
    },
    {
      "doc": 454,
      "topic": 16,
      "similarity": 0.8400371332932901
    },
    {
      "doc": 454,
      "topic": 17,
      "similarity": 0.8130918168476005
    },
    {
      "doc": 454,
      "topic": 18,
      "similarity": 0.762815815015965
    },
    {
      "doc": 454,
      "topic": 19,
      "similarity": 0.8123379023131299
    },
    {
      "doc": 454,
      "topic": 20,
      "similarity": 0.8104745600246146
    },
    {
      "doc": 454,
      "topic": 21,
      "similarity": 0.8284788844364113
    },
    {
      "doc": 454,
      "topic": 23,
      "similarity": 0.7502565201168045
    },
    {
      "doc": 454,
      "topic": 24,
      "similarity": 0.7561202318793143
    },
    {
      "doc": 455,
      "topic": 0,
      "similarity": 0.766435658774971
    },
    {
      "doc": 455,
      "topic": 1,
      "similarity": 0.8116824795853304
    },
    {
      "doc": 455,
      "topic": 2,
      "similarity": 0.777419565694571
    },
    {
      "doc": 455,
      "topic": 3,
      "similarity": 0.8195734332365056
    },
    {
      "doc": 455,
      "topic": 5,
      "similarity": 0.7774892623338471
    },
    {
      "doc": 455,
      "topic": 7,
      "similarity": 0.7714091483885647
    },
    {
      "doc": 455,
      "topic": 8,
      "similarity": 0.7665564973533867
    },
    {
      "doc": 455,
      "topic": 9,
      "similarity": 0.8004514005279902
    },
    {
      "doc": 455,
      "topic": 10,
      "similarity": 0.7762448933175431
    },
    {
      "doc": 455,
      "topic": 11,
      "similarity": 0.7715879241075303
    },
    {
      "doc": 455,
      "topic": 13,
      "similarity": 0.7634132835742267
    },
    {
      "doc": 455,
      "topic": 14,
      "similarity": 0.7784673880717257
    },
    {
      "doc": 455,
      "topic": 15,
      "similarity": 0.7642135171093443
    },
    {
      "doc": 455,
      "topic": 16,
      "similarity": 0.7913816416885862
    },
    {
      "doc": 455,
      "topic": 17,
      "similarity": 0.7742143801767998
    },
    {
      "doc": 455,
      "topic": 18,
      "similarity": 0.7505911353522985
    },
    {
      "doc": 455,
      "topic": 19,
      "similarity": 0.8019748766858561
    },
    {
      "doc": 455,
      "topic": 20,
      "similarity": 0.7864382366708337
    },
    {
      "doc": 455,
      "topic": 21,
      "similarity": 0.8050535223992311
    },
    {
      "doc": 455,
      "topic": 23,
      "similarity": 0.776635396479179
    },
    {
      "doc": 456,
      "topic": 1,
      "similarity": 0.7503501227627235
    },
    {
      "doc": 456,
      "topic": 2,
      "similarity": 0.7520914673768866
    },
    {
      "doc": 456,
      "topic": 3,
      "similarity": 0.7993453694161685
    },
    {
      "doc": 456,
      "topic": 4,
      "similarity": 0.7629402047532545
    },
    {
      "doc": 456,
      "topic": 5,
      "similarity": 0.7830611025333706
    },
    {
      "doc": 456,
      "topic": 7,
      "similarity": 0.7689253631707826
    },
    {
      "doc": 456,
      "topic": 8,
      "similarity": 0.7726765322696121
    },
    {
      "doc": 456,
      "topic": 9,
      "similarity": 0.8155575028614828
    },
    {
      "doc": 456,
      "topic": 10,
      "similarity": 0.8018061364701551
    },
    {
      "doc": 456,
      "topic": 11,
      "similarity": 0.786683511966707
    },
    {
      "doc": 456,
      "topic": 12,
      "similarity": 0.7766283887396613
    },
    {
      "doc": 456,
      "topic": 13,
      "similarity": 0.7602096801468866
    },
    {
      "doc": 456,
      "topic": 14,
      "similarity": 0.7567337182325952
    },
    {
      "doc": 456,
      "topic": 15,
      "similarity": 0.7910479208673362
    },
    {
      "doc": 456,
      "topic": 16,
      "similarity": 0.7812812174905742
    },
    {
      "doc": 456,
      "topic": 17,
      "similarity": 0.7975288262268894
    },
    {
      "doc": 456,
      "topic": 18,
      "similarity": 0.7766751189753056
    },
    {
      "doc": 456,
      "topic": 19,
      "similarity": 0.7948724503804034
    },
    {
      "doc": 456,
      "topic": 20,
      "similarity": 0.773053978451298
    },
    {
      "doc": 456,
      "topic": 21,
      "similarity": 0.8030985164742133
    },
    {
      "doc": 456,
      "topic": 23,
      "similarity": 0.7531605735654587
    },
    {
      "doc": 456,
      "topic": 24,
      "similarity": 0.7546833115915119
    },
    {
      "doc": 457,
      "topic": 3,
      "similarity": 0.7616587932996303
    },
    {
      "doc": 457,
      "topic": 5,
      "similarity": 0.7650026078196703
    },
    {
      "doc": 457,
      "topic": 9,
      "similarity": 0.7774923812406734
    },
    {
      "doc": 457,
      "topic": 16,
      "similarity": 0.7607164503688512
    },
    {
      "doc": 457,
      "topic": 17,
      "similarity": 0.7823239215646676
    },
    {
      "doc": 457,
      "topic": 19,
      "similarity": 0.7676958064577086
    },
    {
      "doc": 457,
      "topic": 20,
      "similarity": 0.7641586253637181
    },
    {
      "doc": 457,
      "topic": 21,
      "similarity": 0.787688775567398
    },
    {
      "doc": 458,
      "topic": 2,
      "similarity": 0.753382296961813
    },
    {
      "doc": 458,
      "topic": 3,
      "similarity": 0.7661480435487026
    },
    {
      "doc": 458,
      "topic": 5,
      "similarity": 0.7600628283315232
    },
    {
      "doc": 458,
      "topic": 9,
      "similarity": 0.7836199234554391
    },
    {
      "doc": 458,
      "topic": 15,
      "similarity": 0.7596043841891237
    },
    {
      "doc": 458,
      "topic": 16,
      "similarity": 0.7633383663312518
    },
    {
      "doc": 458,
      "topic": 17,
      "similarity": 0.7688791544918652
    },
    {
      "doc": 458,
      "topic": 19,
      "similarity": 0.7758400539970016
    },
    {
      "doc": 458,
      "topic": 20,
      "similarity": 0.7809560025486233
    },
    {
      "doc": 458,
      "topic": 21,
      "similarity": 0.7831533116480718
    },
    {
      "doc": 458,
      "topic": 23,
      "similarity": 0.7505198362194243
    },
    {
      "doc": 458,
      "topic": 24,
      "similarity": 0.7564509063220437
    },
    {
      "doc": 459,
      "topic": 3,
      "similarity": 0.771107454240103
    },
    {
      "doc": 459,
      "topic": 5,
      "similarity": 0.7759212774513392
    },
    {
      "doc": 459,
      "topic": 9,
      "similarity": 0.8036720354494503
    },
    {
      "doc": 459,
      "topic": 10,
      "similarity": 0.7507242850480549
    },
    {
      "doc": 459,
      "topic": 11,
      "similarity": 0.7733334131430671
    },
    {
      "doc": 459,
      "topic": 15,
      "similarity": 0.7565047609951756
    },
    {
      "doc": 459,
      "topic": 16,
      "similarity": 0.7758585241042679
    },
    {
      "doc": 459,
      "topic": 17,
      "similarity": 0.7732102118629743
    },
    {
      "doc": 459,
      "topic": 18,
      "similarity": 0.7596957375163378
    },
    {
      "doc": 459,
      "topic": 19,
      "similarity": 0.7722700311156437
    },
    {
      "doc": 459,
      "topic": 20,
      "similarity": 0.779487908678596
    },
    {
      "doc": 459,
      "topic": 21,
      "similarity": 0.7832074341806807
    },
    {
      "doc": 459,
      "topic": 24,
      "similarity": 0.771154549160511
    },
    {
      "doc": 460,
      "topic": 3,
      "similarity": 0.7916230552598204
    },
    {
      "doc": 460,
      "topic": 4,
      "similarity": 0.7539909490549875
    },
    {
      "doc": 460,
      "topic": 5,
      "similarity": 0.7863186280961629
    },
    {
      "doc": 460,
      "topic": 7,
      "similarity": 0.7873101293715542
    },
    {
      "doc": 460,
      "topic": 8,
      "similarity": 0.7746214406238722
    },
    {
      "doc": 460,
      "topic": 9,
      "similarity": 0.7986314047098936
    },
    {
      "doc": 460,
      "topic": 10,
      "similarity": 0.7621359064806371
    },
    {
      "doc": 460,
      "topic": 11,
      "similarity": 0.7889342357747461
    },
    {
      "doc": 460,
      "topic": 13,
      "similarity": 0.7761742681531939
    },
    {
      "doc": 460,
      "topic": 14,
      "similarity": 0.7556826926750531
    },
    {
      "doc": 460,
      "topic": 15,
      "similarity": 0.7617429656989312
    },
    {
      "doc": 460,
      "topic": 16,
      "similarity": 0.7913907093359047
    },
    {
      "doc": 460,
      "topic": 17,
      "similarity": 0.7768158633428819
    },
    {
      "doc": 460,
      "topic": 19,
      "similarity": 0.7960023054728564
    },
    {
      "doc": 460,
      "topic": 20,
      "similarity": 0.7795116677384125
    },
    {
      "doc": 460,
      "topic": 21,
      "similarity": 0.7745928108632585
    },
    {
      "doc": 461,
      "topic": 3,
      "similarity": 0.7612841233143243
    },
    {
      "doc": 461,
      "topic": 4,
      "similarity": 0.8246028209643058
    },
    {
      "doc": 461,
      "topic": 5,
      "similarity": 0.7510212137962454
    },
    {
      "doc": 461,
      "topic": 9,
      "similarity": 0.7642364604236785
    },
    {
      "doc": 461,
      "topic": 16,
      "similarity": 0.7755005668011458
    },
    {
      "doc": 461,
      "topic": 17,
      "similarity": 0.7540578396187801
    },
    {
      "doc": 461,
      "topic": 19,
      "similarity": 0.7571384593954062
    },
    {
      "doc": 462,
      "topic": 2,
      "similarity": 0.75482612652524
    },
    {
      "doc": 462,
      "topic": 3,
      "similarity": 0.7823623745455106
    },
    {
      "doc": 462,
      "topic": 5,
      "similarity": 0.7752583902729779
    },
    {
      "doc": 462,
      "topic": 7,
      "similarity": 0.760000704716381
    },
    {
      "doc": 462,
      "topic": 8,
      "similarity": 0.7647788449185455
    },
    {
      "doc": 462,
      "topic": 9,
      "similarity": 0.8257851116812576
    },
    {
      "doc": 462,
      "topic": 10,
      "similarity": 0.7707951248348563
    },
    {
      "doc": 462,
      "topic": 11,
      "similarity": 0.7714793283299701
    },
    {
      "doc": 462,
      "topic": 13,
      "similarity": 0.7697170382715288
    },
    {
      "doc": 462,
      "topic": 14,
      "similarity": 0.776017178240145
    },
    {
      "doc": 462,
      "topic": 15,
      "similarity": 0.7798212418036661
    },
    {
      "doc": 462,
      "topic": 16,
      "similarity": 0.7939184666578771
    },
    {
      "doc": 462,
      "topic": 17,
      "similarity": 0.7888076540711065
    },
    {
      "doc": 462,
      "topic": 18,
      "similarity": 0.7688503803189205
    },
    {
      "doc": 462,
      "topic": 19,
      "similarity": 0.8064654773507377
    },
    {
      "doc": 462,
      "topic": 20,
      "similarity": 0.7729026094594832
    },
    {
      "doc": 462,
      "topic": 21,
      "similarity": 0.7924863396332589
    },
    {
      "doc": 462,
      "topic": 22,
      "similarity": 0.7549568266672236
    },
    {
      "doc": 462,
      "topic": 23,
      "similarity": 0.7536782672076967
    },
    {
      "doc": 462,
      "topic": 24,
      "similarity": 0.7571438734569161
    },
    {
      "doc": 463,
      "topic": 1,
      "similarity": 0.780575261291973
    },
    {
      "doc": 463,
      "topic": 2,
      "similarity": 0.7969523184592271
    },
    {
      "doc": 463,
      "topic": 3,
      "similarity": 0.8055275137007947
    },
    {
      "doc": 463,
      "topic": 4,
      "similarity": 0.7515288647319612
    },
    {
      "doc": 463,
      "topic": 5,
      "similarity": 0.7981837603331485
    },
    {
      "doc": 463,
      "topic": 7,
      "similarity": 0.7771277738343146
    },
    {
      "doc": 463,
      "topic": 8,
      "similarity": 0.7891032817321255
    },
    {
      "doc": 463,
      "topic": 9,
      "similarity": 0.8290302349623203
    },
    {
      "doc": 463,
      "topic": 10,
      "similarity": 0.7925725333661827
    },
    {
      "doc": 463,
      "topic": 11,
      "similarity": 0.7890616045926055
    },
    {
      "doc": 463,
      "topic": 12,
      "similarity": 0.7590950141013297
    },
    {
      "doc": 463,
      "topic": 13,
      "similarity": 0.7746882799693533
    },
    {
      "doc": 463,
      "topic": 14,
      "similarity": 0.7955840814115751
    },
    {
      "doc": 463,
      "topic": 15,
      "similarity": 0.7860839401170978
    },
    {
      "doc": 463,
      "topic": 16,
      "similarity": 0.815694027102634
    },
    {
      "doc": 463,
      "topic": 17,
      "similarity": 0.8085668749832299
    },
    {
      "doc": 463,
      "topic": 18,
      "similarity": 0.777451911135156
    },
    {
      "doc": 463,
      "topic": 19,
      "similarity": 0.8305742412895586
    },
    {
      "doc": 463,
      "topic": 20,
      "similarity": 0.7989643736920911
    },
    {
      "doc": 463,
      "topic": 21,
      "similarity": 0.8135455407064377
    },
    {
      "doc": 463,
      "topic": 23,
      "similarity": 0.7851589748894374
    },
    {
      "doc": 463,
      "topic": 24,
      "similarity": 0.7609966712301405
    },
    {
      "doc": 464,
      "topic": 1,
      "similarity": 0.7643857195563444
    },
    {
      "doc": 464,
      "topic": 2,
      "similarity": 0.7703812300713765
    },
    {
      "doc": 464,
      "topic": 3,
      "similarity": 0.8061712582760925
    },
    {
      "doc": 464,
      "topic": 4,
      "similarity": 0.7612079128417123
    },
    {
      "doc": 464,
      "topic": 5,
      "similarity": 0.8150037167623134
    },
    {
      "doc": 464,
      "topic": 6,
      "similarity": 0.7849412236300233
    },
    {
      "doc": 464,
      "topic": 7,
      "similarity": 0.778696565310269
    },
    {
      "doc": 464,
      "topic": 8,
      "similarity": 0.768737838156886
    },
    {
      "doc": 464,
      "topic": 9,
      "similarity": 0.8386969200183404
    },
    {
      "doc": 464,
      "topic": 10,
      "similarity": 0.8153422421217754
    },
    {
      "doc": 464,
      "topic": 11,
      "similarity": 0.7979293337577548
    },
    {
      "doc": 464,
      "topic": 13,
      "similarity": 0.7693564794115072
    },
    {
      "doc": 464,
      "topic": 14,
      "similarity": 0.7789162874252052
    },
    {
      "doc": 464,
      "topic": 15,
      "similarity": 0.7806546646188877
    },
    {
      "doc": 464,
      "topic": 16,
      "similarity": 0.8147529320300232
    },
    {
      "doc": 464,
      "topic": 17,
      "similarity": 0.7954677038047788
    },
    {
      "doc": 464,
      "topic": 18,
      "similarity": 0.7769194795859546
    },
    {
      "doc": 464,
      "topic": 19,
      "similarity": 0.8285953431048165
    },
    {
      "doc": 464,
      "topic": 20,
      "similarity": 0.7938556842805045
    },
    {
      "doc": 464,
      "topic": 21,
      "similarity": 0.8088116892785882
    },
    {
      "doc": 464,
      "topic": 23,
      "similarity": 0.8212256767954681
    },
    {
      "doc": 464,
      "topic": 24,
      "similarity": 0.7555641141863959
    },
    {
      "doc": 465,
      "topic": 1,
      "similarity": 0.75070856354015
    },
    {
      "doc": 465,
      "topic": 2,
      "similarity": 0.7583039037379781
    },
    {
      "doc": 465,
      "topic": 3,
      "similarity": 0.7953468125309087
    },
    {
      "doc": 465,
      "topic": 4,
      "similarity": 0.7523419899367649
    },
    {
      "doc": 465,
      "topic": 5,
      "similarity": 0.7885498411218893
    },
    {
      "doc": 465,
      "topic": 7,
      "similarity": 0.7919019843686045
    },
    {
      "doc": 465,
      "topic": 8,
      "similarity": 0.7748585238564429
    },
    {
      "doc": 465,
      "topic": 9,
      "similarity": 0.8123857819550905
    },
    {
      "doc": 465,
      "topic": 10,
      "similarity": 0.760453698682799
    },
    {
      "doc": 465,
      "topic": 11,
      "similarity": 0.7599054775416626
    },
    {
      "doc": 465,
      "topic": 13,
      "similarity": 0.7677769382145403
    },
    {
      "doc": 465,
      "topic": 14,
      "similarity": 0.7668348935213503
    },
    {
      "doc": 465,
      "topic": 15,
      "similarity": 0.7526471558591716
    },
    {
      "doc": 465,
      "topic": 16,
      "similarity": 0.7790614759849471
    },
    {
      "doc": 465,
      "topic": 17,
      "similarity": 0.7585691132125151
    },
    {
      "doc": 465,
      "topic": 19,
      "similarity": 0.7667165835263292
    },
    {
      "doc": 465,
      "topic": 20,
      "similarity": 0.7617699305577988
    },
    {
      "doc": 465,
      "topic": 21,
      "similarity": 0.7810730045019596
    },
    {
      "doc": 465,
      "topic": 23,
      "similarity": 0.782060262233823
    },
    {
      "doc": 465,
      "topic": 24,
      "similarity": 0.7549944858550908
    },
    {
      "doc": 466,
      "topic": 1,
      "similarity": 0.7653527810491084
    },
    {
      "doc": 466,
      "topic": 2,
      "similarity": 0.7980951413682683
    },
    {
      "doc": 466,
      "topic": 3,
      "similarity": 0.7928987935749982
    },
    {
      "doc": 466,
      "topic": 5,
      "similarity": 0.8107238530684338
    },
    {
      "doc": 466,
      "topic": 7,
      "similarity": 0.774002673227213
    },
    {
      "doc": 466,
      "topic": 8,
      "similarity": 0.7917282441304799
    },
    {
      "doc": 466,
      "topic": 9,
      "similarity": 0.81622696684888
    },
    {
      "doc": 466,
      "topic": 10,
      "similarity": 0.7944624141374849
    },
    {
      "doc": 466,
      "topic": 11,
      "similarity": 0.7763093528567909
    },
    {
      "doc": 466,
      "topic": 12,
      "similarity": 0.750844106552996
    },
    {
      "doc": 466,
      "topic": 13,
      "similarity": 0.7635351529694145
    },
    {
      "doc": 466,
      "topic": 14,
      "similarity": 0.8000054284038001
    },
    {
      "doc": 466,
      "topic": 15,
      "similarity": 0.7944964754523713
    },
    {
      "doc": 466,
      "topic": 16,
      "similarity": 0.7936214605556773
    },
    {
      "doc": 466,
      "topic": 17,
      "similarity": 0.7902380896196395
    },
    {
      "doc": 466,
      "topic": 18,
      "similarity": 0.7599996537479725
    },
    {
      "doc": 466,
      "topic": 19,
      "similarity": 0.7978879749082558
    },
    {
      "doc": 466,
      "topic": 20,
      "similarity": 0.7714665708550955
    },
    {
      "doc": 466,
      "topic": 21,
      "similarity": 0.7875116631420567
    },
    {
      "doc": 466,
      "topic": 23,
      "similarity": 0.7944971626425809
    },
    {
      "doc": 467,
      "topic": 2,
      "similarity": 0.7506707994715074
    },
    {
      "doc": 467,
      "topic": 3,
      "similarity": 0.7953194970049641
    },
    {
      "doc": 467,
      "topic": 5,
      "similarity": 0.7932523011923484
    },
    {
      "doc": 467,
      "topic": 6,
      "similarity": 0.8496940563991435
    },
    {
      "doc": 467,
      "topic": 7,
      "similarity": 0.7748878454091485
    },
    {
      "doc": 467,
      "topic": 8,
      "similarity": 0.7709715828250581
    },
    {
      "doc": 467,
      "topic": 9,
      "similarity": 0.8028627341786593
    },
    {
      "doc": 467,
      "topic": 10,
      "similarity": 0.7605096043914886
    },
    {
      "doc": 467,
      "topic": 11,
      "similarity": 0.7807191367220553
    },
    {
      "doc": 467,
      "topic": 13,
      "similarity": 0.7648745633551297
    },
    {
      "doc": 467,
      "topic": 14,
      "similarity": 0.7653026312358809
    },
    {
      "doc": 467,
      "topic": 15,
      "similarity": 0.7582204877463087
    },
    {
      "doc": 467,
      "topic": 16,
      "similarity": 0.7733288286816907
    },
    {
      "doc": 467,
      "topic": 17,
      "similarity": 0.7808092617007808
    },
    {
      "doc": 467,
      "topic": 19,
      "similarity": 0.7933596736089181
    },
    {
      "doc": 467,
      "topic": 21,
      "similarity": 0.777451005975835
    },
    {
      "doc": 467,
      "topic": 23,
      "similarity": 0.784703230682755
    },
    {
      "doc": 468,
      "topic": 1,
      "similarity": 0.756613834298509
    },
    {
      "doc": 468,
      "topic": 2,
      "similarity": 0.7605570328764275
    },
    {
      "doc": 468,
      "topic": 3,
      "similarity": 0.7887495052049688
    },
    {
      "doc": 468,
      "topic": 4,
      "similarity": 0.7515705257933719
    },
    {
      "doc": 468,
      "topic": 5,
      "similarity": 0.7916202243558805
    },
    {
      "doc": 468,
      "topic": 6,
      "similarity": 0.7542232595753253
    },
    {
      "doc": 468,
      "topic": 7,
      "similarity": 0.7645601603543491
    },
    {
      "doc": 468,
      "topic": 8,
      "similarity": 0.7515358510261003
    },
    {
      "doc": 468,
      "topic": 9,
      "similarity": 0.8341768836449961
    },
    {
      "doc": 468,
      "topic": 10,
      "similarity": 0.7957163226774873
    },
    {
      "doc": 468,
      "topic": 11,
      "similarity": 0.8019066046679597
    },
    {
      "doc": 468,
      "topic": 13,
      "similarity": 0.752416518120664
    },
    {
      "doc": 468,
      "topic": 14,
      "similarity": 0.7678214831127733
    },
    {
      "doc": 468,
      "topic": 15,
      "similarity": 0.7750895315710269
    },
    {
      "doc": 468,
      "topic": 16,
      "similarity": 0.8120244438718733
    },
    {
      "doc": 468,
      "topic": 17,
      "similarity": 0.7854959756103959
    },
    {
      "doc": 468,
      "topic": 18,
      "similarity": 0.7659428665003518
    },
    {
      "doc": 468,
      "topic": 19,
      "similarity": 0.8169634084954815
    },
    {
      "doc": 468,
      "topic": 20,
      "similarity": 0.7689183437064024
    },
    {
      "doc": 468,
      "topic": 21,
      "similarity": 0.7946687247239932
    },
    {
      "doc": 468,
      "topic": 23,
      "similarity": 0.7962874938066304
    },
    {
      "doc": 468,
      "topic": 24,
      "similarity": 0.7818467567258073
    },
    {
      "doc": 469,
      "topic": 1,
      "similarity": 0.7674445859966638
    },
    {
      "doc": 469,
      "topic": 2,
      "similarity": 0.7895162751890905
    },
    {
      "doc": 469,
      "topic": 3,
      "similarity": 0.8081061179967685
    },
    {
      "doc": 469,
      "topic": 4,
      "similarity": 0.7744995392580918
    },
    {
      "doc": 469,
      "topic": 5,
      "similarity": 0.8183661107778014
    },
    {
      "doc": 469,
      "topic": 7,
      "similarity": 0.8172727412090046
    },
    {
      "doc": 469,
      "topic": 8,
      "similarity": 0.7961069621352509
    },
    {
      "doc": 469,
      "topic": 9,
      "similarity": 0.8747088156808251
    },
    {
      "doc": 469,
      "topic": 10,
      "similarity": 0.8188995826747191
    },
    {
      "doc": 469,
      "topic": 11,
      "similarity": 0.8075625166577568
    },
    {
      "doc": 469,
      "topic": 13,
      "similarity": 0.7974538774602304
    },
    {
      "doc": 469,
      "topic": 14,
      "similarity": 0.7938011398724146
    },
    {
      "doc": 469,
      "topic": 15,
      "similarity": 0.7857991646061513
    },
    {
      "doc": 469,
      "topic": 16,
      "similarity": 0.8291639635898667
    },
    {
      "doc": 469,
      "topic": 17,
      "similarity": 0.8021957530580609
    },
    {
      "doc": 469,
      "topic": 18,
      "similarity": 0.7783657305803052
    },
    {
      "doc": 469,
      "topic": 19,
      "similarity": 0.8248770516110896
    },
    {
      "doc": 469,
      "topic": 20,
      "similarity": 0.7935930468976402
    },
    {
      "doc": 469,
      "topic": 21,
      "similarity": 0.8131275211868032
    },
    {
      "doc": 469,
      "topic": 23,
      "similarity": 0.7585844051420804
    },
    {
      "doc": 469,
      "topic": 24,
      "similarity": 0.7810859796576729
    },
    {
      "doc": 470,
      "topic": 3,
      "similarity": 0.7734537455813458
    },
    {
      "doc": 470,
      "topic": 5,
      "similarity": 0.774009089842864
    },
    {
      "doc": 470,
      "topic": 7,
      "similarity": 0.7697168035639257
    },
    {
      "doc": 470,
      "topic": 8,
      "similarity": 0.7508153201917243
    },
    {
      "doc": 470,
      "topic": 9,
      "similarity": 0.8107338816078987
    },
    {
      "doc": 470,
      "topic": 10,
      "similarity": 0.7563129317671016
    },
    {
      "doc": 470,
      "topic": 11,
      "similarity": 0.764612579052002
    },
    {
      "doc": 470,
      "topic": 14,
      "similarity": 0.7515147461259448
    },
    {
      "doc": 470,
      "topic": 15,
      "similarity": 0.7699420413979032
    },
    {
      "doc": 470,
      "topic": 16,
      "similarity": 0.7864484489938616
    },
    {
      "doc": 470,
      "topic": 17,
      "similarity": 0.7744888982714343
    },
    {
      "doc": 470,
      "topic": 18,
      "similarity": 0.7606843255180346
    },
    {
      "doc": 470,
      "topic": 19,
      "similarity": 0.7943084260705683
    },
    {
      "doc": 470,
      "topic": 20,
      "similarity": 0.825344411452501
    },
    {
      "doc": 470,
      "topic": 21,
      "similarity": 0.7783441839397404
    },
    {
      "doc": 470,
      "topic": 23,
      "similarity": 0.7678891423042092
    },
    {
      "doc": 470,
      "topic": 24,
      "similarity": 0.7560887318194774
    },
    {
      "doc": 471,
      "topic": 2,
      "similarity": 0.770692501688532
    },
    {
      "doc": 471,
      "topic": 3,
      "similarity": 0.7880555363113617
    },
    {
      "doc": 471,
      "topic": 4,
      "similarity": 0.7565047256827501
    },
    {
      "doc": 471,
      "topic": 5,
      "similarity": 0.7959538222651303
    },
    {
      "doc": 471,
      "topic": 7,
      "similarity": 0.7602001963789153
    },
    {
      "doc": 471,
      "topic": 8,
      "similarity": 0.7793427002895861
    },
    {
      "doc": 471,
      "topic": 9,
      "similarity": 0.8064707209746849
    },
    {
      "doc": 471,
      "topic": 10,
      "similarity": 0.7854622106932432
    },
    {
      "doc": 471,
      "topic": 11,
      "similarity": 0.7675218982069549
    },
    {
      "doc": 471,
      "topic": 13,
      "similarity": 0.780119325863528
    },
    {
      "doc": 471,
      "topic": 14,
      "similarity": 0.7704447286094662
    },
    {
      "doc": 471,
      "topic": 15,
      "similarity": 0.7738268428555218
    },
    {
      "doc": 471,
      "topic": 16,
      "similarity": 0.8206432879923776
    },
    {
      "doc": 471,
      "topic": 17,
      "similarity": 0.8013263254622639
    },
    {
      "doc": 471,
      "topic": 18,
      "similarity": 0.756883611080291
    },
    {
      "doc": 471,
      "topic": 19,
      "similarity": 0.8017222665293723
    },
    {
      "doc": 471,
      "topic": 20,
      "similarity": 0.7709916607246258
    },
    {
      "doc": 471,
      "topic": 21,
      "similarity": 0.8212768758578232
    },
    {
      "doc": 471,
      "topic": 24,
      "similarity": 0.7601865290143999
    },
    {
      "doc": 472,
      "topic": 2,
      "similarity": 0.7748208240323565
    },
    {
      "doc": 472,
      "topic": 3,
      "similarity": 0.7981171767250981
    },
    {
      "doc": 472,
      "topic": 5,
      "similarity": 0.7870212281154709
    },
    {
      "doc": 472,
      "topic": 6,
      "similarity": 0.7508237210370361
    },
    {
      "doc": 472,
      "topic": 7,
      "similarity": 0.8005006945912602
    },
    {
      "doc": 472,
      "topic": 8,
      "similarity": 0.7848746764442676
    },
    {
      "doc": 472,
      "topic": 9,
      "similarity": 0.829999050724008
    },
    {
      "doc": 472,
      "topic": 10,
      "similarity": 0.7668415769424409
    },
    {
      "doc": 472,
      "topic": 11,
      "similarity": 0.7693694190409434
    },
    {
      "doc": 472,
      "topic": 13,
      "similarity": 0.7748055890945841
    },
    {
      "doc": 472,
      "topic": 14,
      "similarity": 0.7628965801828236
    },
    {
      "doc": 472,
      "topic": 15,
      "similarity": 0.7701671081582291
    },
    {
      "doc": 472,
      "topic": 16,
      "similarity": 0.7944670311212343
    },
    {
      "doc": 472,
      "topic": 17,
      "similarity": 0.7937325840295585
    },
    {
      "doc": 472,
      "topic": 19,
      "similarity": 0.7889654432406398
    },
    {
      "doc": 472,
      "topic": 20,
      "similarity": 0.7709640358683679
    },
    {
      "doc": 472,
      "topic": 21,
      "similarity": 0.8238616969234482
    },
    {
      "doc": 472,
      "topic": 23,
      "similarity": 0.7911848561064923
    },
    {
      "doc": 473,
      "topic": 2,
      "similarity": 0.7768485215024835
    },
    {
      "doc": 473,
      "topic": 3,
      "similarity": 0.766065007094968
    },
    {
      "doc": 473,
      "topic": 5,
      "similarity": 0.76247972137024
    },
    {
      "doc": 473,
      "topic": 8,
      "similarity": 0.7806700831250296
    },
    {
      "doc": 473,
      "topic": 9,
      "similarity": 0.7863600348619258
    },
    {
      "doc": 473,
      "topic": 14,
      "similarity": 0.7672346904128793
    },
    {
      "doc": 473,
      "topic": 15,
      "similarity": 0.7516109783003947
    },
    {
      "doc": 473,
      "topic": 17,
      "similarity": 0.7554971542028914
    },
    {
      "doc": 473,
      "topic": 21,
      "similarity": 0.7883922476952526
    },
    {
      "doc": 473,
      "topic": 24,
      "similarity": 0.7597829574382442
    },
    {
      "doc": 474,
      "topic": 2,
      "similarity": 0.7665323357309776
    },
    {
      "doc": 474,
      "topic": 3,
      "similarity": 0.8056984085150977
    },
    {
      "doc": 474,
      "topic": 5,
      "similarity": 0.7893593562473762
    },
    {
      "doc": 474,
      "topic": 6,
      "similarity": 0.7894310641215784
    },
    {
      "doc": 474,
      "topic": 7,
      "similarity": 0.7630652462226429
    },
    {
      "doc": 474,
      "topic": 8,
      "similarity": 0.7653517545726272
    },
    {
      "doc": 474,
      "topic": 9,
      "similarity": 0.8274712536160244
    },
    {
      "doc": 474,
      "topic": 10,
      "similarity": 0.7682603500247734
    },
    {
      "doc": 474,
      "topic": 11,
      "similarity": 0.77976595856898
    },
    {
      "doc": 474,
      "topic": 13,
      "similarity": 0.7587755226759179
    },
    {
      "doc": 474,
      "topic": 14,
      "similarity": 0.77494510714988
    },
    {
      "doc": 474,
      "topic": 15,
      "similarity": 0.7716034758864028
    },
    {
      "doc": 474,
      "topic": 16,
      "similarity": 0.7803298571915321
    },
    {
      "doc": 474,
      "topic": 17,
      "similarity": 0.8020272460414128
    },
    {
      "doc": 474,
      "topic": 18,
      "similarity": 0.7815582738068744
    },
    {
      "doc": 474,
      "topic": 19,
      "similarity": 0.8235293977215566
    },
    {
      "doc": 474,
      "topic": 20,
      "similarity": 0.7654382048223997
    },
    {
      "doc": 474,
      "topic": 21,
      "similarity": 0.7994308905576911
    },
    {
      "doc": 474,
      "topic": 23,
      "similarity": 0.7932617417567612
    },
    {
      "doc": 474,
      "topic": 24,
      "similarity": 0.7825952975015967
    },
    {
      "doc": 475,
      "topic": 2,
      "similarity": 0.7711785718778155
    },
    {
      "doc": 475,
      "topic": 3,
      "similarity": 0.7939129868173883
    },
    {
      "doc": 475,
      "topic": 4,
      "similarity": 0.7590370373014864
    },
    {
      "doc": 475,
      "topic": 5,
      "similarity": 0.8009820747058654
    },
    {
      "doc": 475,
      "topic": 7,
      "similarity": 0.7770058698163245
    },
    {
      "doc": 475,
      "topic": 8,
      "similarity": 0.7643258217166734
    },
    {
      "doc": 475,
      "topic": 9,
      "similarity": 0.8260083699943186
    },
    {
      "doc": 475,
      "topic": 10,
      "similarity": 0.769972044418651
    },
    {
      "doc": 475,
      "topic": 11,
      "similarity": 0.7790504211370877
    },
    {
      "doc": 475,
      "topic": 13,
      "similarity": 0.7634052667181633
    },
    {
      "doc": 475,
      "topic": 14,
      "similarity": 0.7655330286662875
    },
    {
      "doc": 475,
      "topic": 15,
      "similarity": 0.7874752709447213
    },
    {
      "doc": 475,
      "topic": 16,
      "similarity": 0.8213152644838003
    },
    {
      "doc": 475,
      "topic": 17,
      "similarity": 0.7917158995128605
    },
    {
      "doc": 475,
      "topic": 19,
      "similarity": 0.8001249319047271
    },
    {
      "doc": 475,
      "topic": 20,
      "similarity": 0.7689247077640445
    },
    {
      "doc": 475,
      "topic": 21,
      "similarity": 0.7953831621185555
    },
    {
      "doc": 475,
      "topic": 23,
      "similarity": 0.754772214522548
    },
    {
      "doc": 475,
      "topic": 24,
      "similarity": 0.7735757143295705
    },
    {
      "doc": 476,
      "topic": 3,
      "similarity": 0.7904318611066253
    },
    {
      "doc": 476,
      "topic": 5,
      "similarity": 0.7963708143681792
    },
    {
      "doc": 476,
      "topic": 7,
      "similarity": 0.7677295816313772
    },
    {
      "doc": 476,
      "topic": 8,
      "similarity": 0.7649326172484933
    },
    {
      "doc": 476,
      "topic": 9,
      "similarity": 0.8121539981633635
    },
    {
      "doc": 476,
      "topic": 10,
      "similarity": 0.7809511135997053
    },
    {
      "doc": 476,
      "topic": 11,
      "similarity": 0.7683027678600921
    },
    {
      "doc": 476,
      "topic": 13,
      "similarity": 0.7555667828970092
    },
    {
      "doc": 476,
      "topic": 14,
      "similarity": 0.7557092938066424
    },
    {
      "doc": 476,
      "topic": 15,
      "similarity": 0.7646757531328124
    },
    {
      "doc": 476,
      "topic": 16,
      "similarity": 0.803820912139227
    },
    {
      "doc": 476,
      "topic": 17,
      "similarity": 0.7829419258519514
    },
    {
      "doc": 476,
      "topic": 19,
      "similarity": 0.797825740815554
    },
    {
      "doc": 476,
      "topic": 20,
      "similarity": 0.7904695681636065
    },
    {
      "doc": 476,
      "topic": 21,
      "similarity": 0.7971649782181078
    },
    {
      "doc": 476,
      "topic": 24,
      "similarity": 0.7587826785256382
    },
    {
      "doc": 477,
      "topic": 2,
      "similarity": 0.7803034528461459
    },
    {
      "doc": 477,
      "topic": 3,
      "similarity": 0.7881031252572819
    },
    {
      "doc": 477,
      "topic": 4,
      "similarity": 0.7570203430578989
    },
    {
      "doc": 477,
      "topic": 5,
      "similarity": 0.8037888570338256
    },
    {
      "doc": 477,
      "topic": 7,
      "similarity": 0.7500902427781729
    },
    {
      "doc": 477,
      "topic": 9,
      "similarity": 0.8028137020141445
    },
    {
      "doc": 477,
      "topic": 10,
      "similarity": 0.7635447945955701
    },
    {
      "doc": 477,
      "topic": 11,
      "similarity": 0.7764223149743489
    },
    {
      "doc": 477,
      "topic": 13,
      "similarity": 0.7572652631815826
    },
    {
      "doc": 477,
      "topic": 15,
      "similarity": 0.7585577040874856
    },
    {
      "doc": 477,
      "topic": 16,
      "similarity": 0.7965404160408818
    },
    {
      "doc": 477,
      "topic": 17,
      "similarity": 0.7941807623957231
    },
    {
      "doc": 477,
      "topic": 18,
      "similarity": 0.7543179656966623
    },
    {
      "doc": 477,
      "topic": 19,
      "similarity": 0.7946962607831916
    },
    {
      "doc": 477,
      "topic": 20,
      "similarity": 0.8064799452542377
    },
    {
      "doc": 477,
      "topic": 21,
      "similarity": 0.8008704226201233
    },
    {
      "doc": 477,
      "topic": 23,
      "similarity": 0.7758732322493253
    },
    {
      "doc": 477,
      "topic": 24,
      "similarity": 0.7630580848829603
    },
    {
      "doc": 478,
      "topic": 1,
      "similarity": 0.7531865509534765
    },
    {
      "doc": 478,
      "topic": 2,
      "similarity": 0.7786705324204628
    },
    {
      "doc": 478,
      "topic": 3,
      "similarity": 0.8015165435373404
    },
    {
      "doc": 478,
      "topic": 4,
      "similarity": 0.7527050386181191
    },
    {
      "doc": 478,
      "topic": 5,
      "similarity": 0.8084739597932538
    },
    {
      "doc": 478,
      "topic": 6,
      "similarity": 0.7555583680212766
    },
    {
      "doc": 478,
      "topic": 7,
      "similarity": 0.7888158865495003
    },
    {
      "doc": 478,
      "topic": 8,
      "similarity": 0.7887924115945957
    },
    {
      "doc": 478,
      "topic": 9,
      "similarity": 0.8369339545191988
    },
    {
      "doc": 478,
      "topic": 10,
      "similarity": 0.8316906551729366
    },
    {
      "doc": 478,
      "topic": 11,
      "similarity": 0.7890376985871117
    },
    {
      "doc": 478,
      "topic": 13,
      "similarity": 0.7737834148185891
    },
    {
      "doc": 478,
      "topic": 14,
      "similarity": 0.7855122266452865
    },
    {
      "doc": 478,
      "topic": 15,
      "similarity": 0.7912035582193829
    },
    {
      "doc": 478,
      "topic": 16,
      "similarity": 0.7998807211762663
    },
    {
      "doc": 478,
      "topic": 17,
      "similarity": 0.8079625916148504
    },
    {
      "doc": 478,
      "topic": 18,
      "similarity": 0.8264612951918957
    },
    {
      "doc": 478,
      "topic": 19,
      "similarity": 0.848035160433163
    },
    {
      "doc": 478,
      "topic": 20,
      "similarity": 0.7739300803401913
    },
    {
      "doc": 478,
      "topic": 21,
      "similarity": 0.8046076333306476
    },
    {
      "doc": 478,
      "topic": 23,
      "similarity": 0.768521170895647
    },
    {
      "doc": 478,
      "topic": 24,
      "similarity": 0.751633657683061
    },
    {
      "doc": 479,
      "topic": 1,
      "similarity": 0.7513869873457029
    },
    {
      "doc": 479,
      "topic": 2,
      "similarity": 0.7653210778942884
    },
    {
      "doc": 479,
      "topic": 3,
      "similarity": 0.7965156114316168
    },
    {
      "doc": 479,
      "topic": 4,
      "similarity": 0.7613853373000222
    },
    {
      "doc": 479,
      "topic": 5,
      "similarity": 0.8108498491968731
    },
    {
      "doc": 479,
      "topic": 7,
      "similarity": 0.7962297462458413
    },
    {
      "doc": 479,
      "topic": 8,
      "similarity": 0.7718672131810664
    },
    {
      "doc": 479,
      "topic": 9,
      "similarity": 0.842200495579215
    },
    {
      "doc": 479,
      "topic": 10,
      "similarity": 0.7894632516267183
    },
    {
      "doc": 479,
      "topic": 11,
      "similarity": 0.7832017586297099
    },
    {
      "doc": 479,
      "topic": 13,
      "similarity": 0.7689010620089064
    },
    {
      "doc": 479,
      "topic": 14,
      "similarity": 0.78148098110182
    },
    {
      "doc": 479,
      "topic": 15,
      "similarity": 0.7629958171338012
    },
    {
      "doc": 479,
      "topic": 16,
      "similarity": 0.8023180545446172
    },
    {
      "doc": 479,
      "topic": 17,
      "similarity": 0.797153383042981
    },
    {
      "doc": 479,
      "topic": 18,
      "similarity": 0.7564065128271676
    },
    {
      "doc": 479,
      "topic": 19,
      "similarity": 0.8128354595773161
    },
    {
      "doc": 479,
      "topic": 20,
      "similarity": 0.7789910126772985
    },
    {
      "doc": 479,
      "topic": 21,
      "similarity": 0.799004309196144
    },
    {
      "doc": 479,
      "topic": 23,
      "similarity": 0.7772565347446745
    },
    {
      "doc": 479,
      "topic": 24,
      "similarity": 0.7720180113903009
    },
    {
      "doc": 480,
      "topic": 2,
      "similarity": 0.76124866882725
    },
    {
      "doc": 480,
      "topic": 3,
      "similarity": 0.8046272077108845
    },
    {
      "doc": 480,
      "topic": 5,
      "similarity": 0.7887944912546354
    },
    {
      "doc": 480,
      "topic": 7,
      "similarity": 0.7684058660037911
    },
    {
      "doc": 480,
      "topic": 8,
      "similarity": 0.7522483538043961
    },
    {
      "doc": 480,
      "topic": 9,
      "similarity": 0.8151502089961921
    },
    {
      "doc": 480,
      "topic": 10,
      "similarity": 0.7849154729855121
    },
    {
      "doc": 480,
      "topic": 11,
      "similarity": 0.7694326961719289
    },
    {
      "doc": 480,
      "topic": 13,
      "similarity": 0.7525106100194615
    },
    {
      "doc": 480,
      "topic": 14,
      "similarity": 0.7722256330289914
    },
    {
      "doc": 480,
      "topic": 15,
      "similarity": 0.7669493803912288
    },
    {
      "doc": 480,
      "topic": 16,
      "similarity": 0.8102002285388341
    },
    {
      "doc": 480,
      "topic": 17,
      "similarity": 0.7852407584058718
    },
    {
      "doc": 480,
      "topic": 18,
      "similarity": 0.7975739667767121
    },
    {
      "doc": 480,
      "topic": 19,
      "similarity": 0.8399099711403563
    },
    {
      "doc": 480,
      "topic": 20,
      "similarity": 0.7922509055214861
    },
    {
      "doc": 480,
      "topic": 21,
      "similarity": 0.7945985365844589
    },
    {
      "doc": 480,
      "topic": 23,
      "similarity": 0.7677781984185156
    },
    {
      "doc": 481,
      "topic": 2,
      "similarity": 0.7633059794447514
    },
    {
      "doc": 481,
      "topic": 3,
      "similarity": 0.7913084623216068
    },
    {
      "doc": 481,
      "topic": 5,
      "similarity": 0.7953197813758235
    },
    {
      "doc": 481,
      "topic": 7,
      "similarity": 0.7703743891442614
    },
    {
      "doc": 481,
      "topic": 8,
      "similarity": 0.7668083267038535
    },
    {
      "doc": 481,
      "topic": 9,
      "similarity": 0.8071010734486734
    },
    {
      "doc": 481,
      "topic": 10,
      "similarity": 0.7546579620947919
    },
    {
      "doc": 481,
      "topic": 11,
      "similarity": 0.7601089782449343
    },
    {
      "doc": 481,
      "topic": 14,
      "similarity": 0.7517194094245095
    },
    {
      "doc": 481,
      "topic": 15,
      "similarity": 0.7564386587798557
    },
    {
      "doc": 481,
      "topic": 16,
      "similarity": 0.8113638287329574
    },
    {
      "doc": 481,
      "topic": 17,
      "similarity": 0.8010645203515137
    },
    {
      "doc": 481,
      "topic": 19,
      "similarity": 0.8158659777698136
    },
    {
      "doc": 481,
      "topic": 20,
      "similarity": 0.785214070092139
    },
    {
      "doc": 481,
      "topic": 21,
      "similarity": 0.8047604240248784
    },
    {
      "doc": 481,
      "topic": 24,
      "similarity": 0.7588947254817403
    },
    {
      "doc": 482,
      "topic": 1,
      "similarity": 0.7534889951653014
    },
    {
      "doc": 482,
      "topic": 2,
      "similarity": 0.789964772018795
    },
    {
      "doc": 482,
      "topic": 3,
      "similarity": 0.793730262095579
    },
    {
      "doc": 482,
      "topic": 5,
      "similarity": 0.79338036781059
    },
    {
      "doc": 482,
      "topic": 6,
      "similarity": 0.7559815447260507
    },
    {
      "doc": 482,
      "topic": 7,
      "similarity": 0.7648650475137169
    },
    {
      "doc": 482,
      "topic": 8,
      "similarity": 0.7823250723974181
    },
    {
      "doc": 482,
      "topic": 9,
      "similarity": 0.8119081904574457
    },
    {
      "doc": 482,
      "topic": 10,
      "similarity": 0.7821716676576458
    },
    {
      "doc": 482,
      "topic": 11,
      "similarity": 0.800602977128304
    },
    {
      "doc": 482,
      "topic": 13,
      "similarity": 0.7953003982160619
    },
    {
      "doc": 482,
      "topic": 14,
      "similarity": 0.790065088943799
    },
    {
      "doc": 482,
      "topic": 15,
      "similarity": 0.788438934952862
    },
    {
      "doc": 482,
      "topic": 16,
      "similarity": 0.8146260629270214
    },
    {
      "doc": 482,
      "topic": 17,
      "similarity": 0.8139857589779289
    },
    {
      "doc": 482,
      "topic": 18,
      "similarity": 0.7888732931093028
    },
    {
      "doc": 482,
      "topic": 19,
      "similarity": 0.8280600341395724
    },
    {
      "doc": 482,
      "topic": 20,
      "similarity": 0.7917080738949893
    },
    {
      "doc": 482,
      "topic": 21,
      "similarity": 0.8180219929737326
    },
    {
      "doc": 482,
      "topic": 22,
      "similarity": 0.7591935584819277
    },
    {
      "doc": 482,
      "topic": 23,
      "similarity": 0.7604709022308723
    },
    {
      "doc": 482,
      "topic": 24,
      "similarity": 0.7515930442324804
    },
    {
      "doc": 483,
      "topic": 1,
      "similarity": 0.7661983132968746
    },
    {
      "doc": 483,
      "topic": 2,
      "similarity": 0.7879983897569437
    },
    {
      "doc": 483,
      "topic": 3,
      "similarity": 0.825916413487643
    },
    {
      "doc": 483,
      "topic": 4,
      "similarity": 0.7560724779539076
    },
    {
      "doc": 483,
      "topic": 5,
      "similarity": 0.8117747349584659
    },
    {
      "doc": 483,
      "topic": 6,
      "similarity": 0.7508219735773488
    },
    {
      "doc": 483,
      "topic": 7,
      "similarity": 0.7798564220543337
    },
    {
      "doc": 483,
      "topic": 8,
      "similarity": 0.8216494605097509
    },
    {
      "doc": 483,
      "topic": 9,
      "similarity": 0.8273375126444669
    },
    {
      "doc": 483,
      "topic": 10,
      "similarity": 0.7918266545826995
    },
    {
      "doc": 483,
      "topic": 11,
      "similarity": 0.8018097499746352
    },
    {
      "doc": 483,
      "topic": 12,
      "similarity": 0.819259248737672
    },
    {
      "doc": 483,
      "topic": 13,
      "similarity": 0.7946664657838974
    },
    {
      "doc": 483,
      "topic": 14,
      "similarity": 0.7887679949352782
    },
    {
      "doc": 483,
      "topic": 15,
      "similarity": 0.8181753183180613
    },
    {
      "doc": 483,
      "topic": 16,
      "similarity": 0.8218026505156089
    },
    {
      "doc": 483,
      "topic": 17,
      "similarity": 0.8225943430246371
    },
    {
      "doc": 483,
      "topic": 18,
      "similarity": 0.7951589371253757
    },
    {
      "doc": 483,
      "topic": 19,
      "similarity": 0.8330328630122059
    },
    {
      "doc": 483,
      "topic": 20,
      "similarity": 0.7972735763001351
    },
    {
      "doc": 483,
      "topic": 21,
      "similarity": 0.8240923891819361
    },
    {
      "doc": 483,
      "topic": 23,
      "similarity": 0.7503870943700972
    },
    {
      "doc": 483,
      "topic": 24,
      "similarity": 0.7550793417275439
    },
    {
      "doc": 484,
      "topic": 1,
      "similarity": 0.7532433711334774
    },
    {
      "doc": 484,
      "topic": 2,
      "similarity": 0.7725972728336415
    },
    {
      "doc": 484,
      "topic": 3,
      "similarity": 0.79784245813233
    },
    {
      "doc": 484,
      "topic": 4,
      "similarity": 0.7544117349279236
    },
    {
      "doc": 484,
      "topic": 5,
      "similarity": 0.7861483610170044
    },
    {
      "doc": 484,
      "topic": 7,
      "similarity": 0.7916304819224539
    },
    {
      "doc": 484,
      "topic": 8,
      "similarity": 0.7636612629913101
    },
    {
      "doc": 484,
      "topic": 9,
      "similarity": 0.852637764030995
    },
    {
      "doc": 484,
      "topic": 10,
      "similarity": 0.7634058113223927
    },
    {
      "doc": 484,
      "topic": 11,
      "similarity": 0.7820427965666301
    },
    {
      "doc": 484,
      "topic": 13,
      "similarity": 0.7656151917075575
    },
    {
      "doc": 484,
      "topic": 14,
      "similarity": 0.7782596674169853
    },
    {
      "doc": 484,
      "topic": 15,
      "similarity": 0.7685681826784608
    },
    {
      "doc": 484,
      "topic": 16,
      "similarity": 0.7917787725775278
    },
    {
      "doc": 484,
      "topic": 17,
      "similarity": 0.7797801065686757
    },
    {
      "doc": 484,
      "topic": 19,
      "similarity": 0.8009175459252106
    },
    {
      "doc": 484,
      "topic": 20,
      "similarity": 0.760181524942635
    },
    {
      "doc": 484,
      "topic": 21,
      "similarity": 0.7953100236836194
    },
    {
      "doc": 484,
      "topic": 23,
      "similarity": 0.7638336981403088
    },
    {
      "doc": 484,
      "topic": 24,
      "similarity": 0.7523972619111796
    },
    {
      "doc": 485,
      "topic": 3,
      "similarity": 0.7785654997380053
    },
    {
      "doc": 485,
      "topic": 4,
      "similarity": 0.8022957554712569
    },
    {
      "doc": 485,
      "topic": 5,
      "similarity": 0.7789806996734112
    },
    {
      "doc": 485,
      "topic": 7,
      "similarity": 0.7792535622752996
    },
    {
      "doc": 485,
      "topic": 9,
      "similarity": 0.7830741361433549
    },
    {
      "doc": 485,
      "topic": 11,
      "similarity": 0.7735714288789123
    },
    {
      "doc": 485,
      "topic": 16,
      "similarity": 0.8083145198320856
    },
    {
      "doc": 485,
      "topic": 17,
      "similarity": 0.7800315184930419
    },
    {
      "doc": 485,
      "topic": 19,
      "similarity": 0.7858184597742959
    },
    {
      "doc": 485,
      "topic": 21,
      "similarity": 0.7688363054745251
    },
    {
      "doc": 486,
      "topic": 1,
      "similarity": 0.7776426480421414
    },
    {
      "doc": 486,
      "topic": 2,
      "similarity": 0.7843034997002742
    },
    {
      "doc": 486,
      "topic": 3,
      "similarity": 0.8106002850517328
    },
    {
      "doc": 486,
      "topic": 4,
      "similarity": 0.7635819960011774
    },
    {
      "doc": 486,
      "topic": 5,
      "similarity": 0.8145146691656453
    },
    {
      "doc": 486,
      "topic": 7,
      "similarity": 0.7952432851640872
    },
    {
      "doc": 486,
      "topic": 8,
      "similarity": 0.7954663025714119
    },
    {
      "doc": 486,
      "topic": 9,
      "similarity": 0.8581503276515147
    },
    {
      "doc": 486,
      "topic": 10,
      "similarity": 0.8048631053675025
    },
    {
      "doc": 486,
      "topic": 11,
      "similarity": 0.7998373244906958
    },
    {
      "doc": 486,
      "topic": 13,
      "similarity": 0.7990141547773276
    },
    {
      "doc": 486,
      "topic": 14,
      "similarity": 0.812820803711476
    },
    {
      "doc": 486,
      "topic": 15,
      "similarity": 0.7828897329987339
    },
    {
      "doc": 486,
      "topic": 16,
      "similarity": 0.8083492150830945
    },
    {
      "doc": 486,
      "topic": 17,
      "similarity": 0.8051377648167551
    },
    {
      "doc": 486,
      "topic": 18,
      "similarity": 0.7541440165012568
    },
    {
      "doc": 486,
      "topic": 19,
      "similarity": 0.8179277504302852
    },
    {
      "doc": 486,
      "topic": 20,
      "similarity": 0.7676479811478035
    },
    {
      "doc": 486,
      "topic": 21,
      "similarity": 0.8040044538101582
    },
    {
      "doc": 486,
      "topic": 23,
      "similarity": 0.7653517342412215
    },
    {
      "doc": 487,
      "topic": 2,
      "similarity": 0.769388990421787
    },
    {
      "doc": 487,
      "topic": 3,
      "similarity": 0.7904414727273261
    },
    {
      "doc": 487,
      "topic": 4,
      "similarity": 0.7637590713629313
    },
    {
      "doc": 487,
      "topic": 5,
      "similarity": 0.796194690052842
    },
    {
      "doc": 487,
      "topic": 7,
      "similarity": 0.795840772751813
    },
    {
      "doc": 487,
      "topic": 8,
      "similarity": 0.7962940864863484
    },
    {
      "doc": 487,
      "topic": 9,
      "similarity": 0.8486810858759993
    },
    {
      "doc": 487,
      "topic": 10,
      "similarity": 0.7739761293739852
    },
    {
      "doc": 487,
      "topic": 11,
      "similarity": 0.8038489693953467
    },
    {
      "doc": 487,
      "topic": 13,
      "similarity": 0.7705146877007775
    },
    {
      "doc": 487,
      "topic": 14,
      "similarity": 0.7674014296122806
    },
    {
      "doc": 487,
      "topic": 15,
      "similarity": 0.7870678432843652
    },
    {
      "doc": 487,
      "topic": 16,
      "similarity": 0.7938003835792143
    },
    {
      "doc": 487,
      "topic": 17,
      "similarity": 0.7985625495577543
    },
    {
      "doc": 487,
      "topic": 18,
      "similarity": 0.7579371431228662
    },
    {
      "doc": 487,
      "topic": 19,
      "similarity": 0.7906524318069627
    },
    {
      "doc": 487,
      "topic": 20,
      "similarity": 0.7766050842386877
    },
    {
      "doc": 487,
      "topic": 21,
      "similarity": 0.8018004128494003
    },
    {
      "doc": 487,
      "topic": 24,
      "similarity": 0.7721234459292171
    },
    {
      "doc": 488,
      "topic": 2,
      "similarity": 0.7587068254675028
    },
    {
      "doc": 488,
      "topic": 3,
      "similarity": 0.7943608271624776
    },
    {
      "doc": 488,
      "topic": 5,
      "similarity": 0.8139800576953754
    },
    {
      "doc": 488,
      "topic": 7,
      "similarity": 0.7579523367855516
    },
    {
      "doc": 488,
      "topic": 8,
      "similarity": 0.7521004345908509
    },
    {
      "doc": 488,
      "topic": 9,
      "similarity": 0.8120791840368602
    },
    {
      "doc": 488,
      "topic": 10,
      "similarity": 0.7612458520626383
    },
    {
      "doc": 488,
      "topic": 11,
      "similarity": 0.7529531867713187
    },
    {
      "doc": 488,
      "topic": 15,
      "similarity": 0.766503535009489
    },
    {
      "doc": 488,
      "topic": 16,
      "similarity": 0.78311175091536
    },
    {
      "doc": 488,
      "topic": 17,
      "similarity": 0.79589649919163
    },
    {
      "doc": 488,
      "topic": 18,
      "similarity": 0.7760880155386503
    },
    {
      "doc": 488,
      "topic": 19,
      "similarity": 0.7971027622856569
    },
    {
      "doc": 488,
      "topic": 20,
      "similarity": 0.7857803510393837
    },
    {
      "doc": 488,
      "topic": 21,
      "similarity": 0.8051547182704748
    },
    {
      "doc": 488,
      "topic": 23,
      "similarity": 0.7722003649370252
    },
    {
      "doc": 488,
      "topic": 24,
      "similarity": 0.7699130465468166
    },
    {
      "doc": 489,
      "topic": 3,
      "similarity": 0.790840693245945
    },
    {
      "doc": 489,
      "topic": 5,
      "similarity": 0.7695650016920815
    },
    {
      "doc": 489,
      "topic": 6,
      "similarity": 0.7637441997394441
    },
    {
      "doc": 489,
      "topic": 7,
      "similarity": 0.7612436896487283
    },
    {
      "doc": 489,
      "topic": 8,
      "similarity": 0.7521394394121896
    },
    {
      "doc": 489,
      "topic": 9,
      "similarity": 0.8286741221608578
    },
    {
      "doc": 489,
      "topic": 10,
      "similarity": 0.7594747854813284
    },
    {
      "doc": 489,
      "topic": 11,
      "similarity": 0.758050586326593
    },
    {
      "doc": 489,
      "topic": 12,
      "similarity": 0.7660532293366931
    },
    {
      "doc": 489,
      "topic": 14,
      "similarity": 0.751854739159979
    },
    {
      "doc": 489,
      "topic": 15,
      "similarity": 0.7517504511790837
    },
    {
      "doc": 489,
      "topic": 16,
      "similarity": 0.7783398932132694
    },
    {
      "doc": 489,
      "topic": 17,
      "similarity": 0.7699216685317951
    },
    {
      "doc": 489,
      "topic": 18,
      "similarity": 0.77320102138267
    },
    {
      "doc": 489,
      "topic": 19,
      "similarity": 0.7901252428549621
    },
    {
      "doc": 489,
      "topic": 20,
      "similarity": 0.7857834002678262
    },
    {
      "doc": 489,
      "topic": 21,
      "similarity": 0.7991433725129848
    },
    {
      "doc": 489,
      "topic": 23,
      "similarity": 0.8038721813114623
    },
    {
      "doc": 489,
      "topic": 24,
      "similarity": 0.7687372848686639
    },
    {
      "doc": 490,
      "topic": 1,
      "similarity": 0.7902092164873974
    },
    {
      "doc": 490,
      "topic": 2,
      "similarity": 0.7965675393937142
    },
    {
      "doc": 490,
      "topic": 3,
      "similarity": 0.8114651051420029
    },
    {
      "doc": 490,
      "topic": 4,
      "similarity": 0.7634716112794826
    },
    {
      "doc": 490,
      "topic": 5,
      "similarity": 0.7973886669799132
    },
    {
      "doc": 490,
      "topic": 6,
      "similarity": 0.814806235600555
    },
    {
      "doc": 490,
      "topic": 7,
      "similarity": 0.7775409415174583
    },
    {
      "doc": 490,
      "topic": 8,
      "similarity": 0.7704661465250523
    },
    {
      "doc": 490,
      "topic": 9,
      "similarity": 0.8113426041779815
    },
    {
      "doc": 490,
      "topic": 10,
      "similarity": 0.7965791569088272
    },
    {
      "doc": 490,
      "topic": 11,
      "similarity": 0.7850955735712791
    },
    {
      "doc": 490,
      "topic": 13,
      "similarity": 0.7989999710540078
    },
    {
      "doc": 490,
      "topic": 14,
      "similarity": 0.8019578700031202
    },
    {
      "doc": 490,
      "topic": 15,
      "similarity": 0.7838131088304292
    },
    {
      "doc": 490,
      "topic": 16,
      "similarity": 0.802492013150235
    },
    {
      "doc": 490,
      "topic": 17,
      "similarity": 0.8131033941347293
    },
    {
      "doc": 490,
      "topic": 18,
      "similarity": 0.7729922104925702
    },
    {
      "doc": 490,
      "topic": 19,
      "similarity": 0.8134790380382015
    },
    {
      "doc": 490,
      "topic": 20,
      "similarity": 0.7820220818979993
    },
    {
      "doc": 490,
      "topic": 21,
      "similarity": 0.7968300045235751
    },
    {
      "doc": 490,
      "topic": 23,
      "similarity": 0.7837189031122356
    },
    {
      "doc": 491,
      "topic": 3,
      "similarity": 0.7813972605661729
    },
    {
      "doc": 491,
      "topic": 4,
      "similarity": 0.7704872560194369
    },
    {
      "doc": 491,
      "topic": 5,
      "similarity": 0.7583244580958233
    },
    {
      "doc": 491,
      "topic": 7,
      "similarity": 0.7616600600192761
    },
    {
      "doc": 491,
      "topic": 9,
      "similarity": 0.786179666069305
    },
    {
      "doc": 491,
      "topic": 16,
      "similarity": 0.7551455426436015
    },
    {
      "doc": 491,
      "topic": 17,
      "similarity": 0.7629610918538514
    },
    {
      "doc": 491,
      "topic": 19,
      "similarity": 0.7706170455461571
    },
    {
      "doc": 491,
      "topic": 21,
      "similarity": 0.7537548337937504
    },
    {
      "doc": 491,
      "topic": 24,
      "similarity": 0.7676737703049122
    },
    {
      "doc": 492,
      "topic": 2,
      "similarity": 0.7892111032426128
    },
    {
      "doc": 492,
      "topic": 3,
      "similarity": 0.8014828219298317
    },
    {
      "doc": 492,
      "topic": 4,
      "similarity": 0.7516897133102299
    },
    {
      "doc": 492,
      "topic": 5,
      "similarity": 0.8051144208158384
    },
    {
      "doc": 492,
      "topic": 7,
      "similarity": 0.8122423523657663
    },
    {
      "doc": 492,
      "topic": 8,
      "similarity": 0.8232340665114689
    },
    {
      "doc": 492,
      "topic": 9,
      "similarity": 0.8455710416731748
    },
    {
      "doc": 492,
      "topic": 10,
      "similarity": 0.7797513090902252
    },
    {
      "doc": 492,
      "topic": 11,
      "similarity": 0.8076691986188641
    },
    {
      "doc": 492,
      "topic": 12,
      "similarity": 0.7773724122303038
    },
    {
      "doc": 492,
      "topic": 13,
      "similarity": 0.7879204173048627
    },
    {
      "doc": 492,
      "topic": 14,
      "similarity": 0.7905921705749841
    },
    {
      "doc": 492,
      "topic": 15,
      "similarity": 0.817758663376251
    },
    {
      "doc": 492,
      "topic": 16,
      "similarity": 0.8163386183179325
    },
    {
      "doc": 492,
      "topic": 17,
      "similarity": 0.8066606035588388
    },
    {
      "doc": 492,
      "topic": 18,
      "similarity": 0.769438298481685
    },
    {
      "doc": 492,
      "topic": 19,
      "similarity": 0.8114207972691547
    },
    {
      "doc": 492,
      "topic": 20,
      "similarity": 0.7833107758519211
    },
    {
      "doc": 492,
      "topic": 21,
      "similarity": 0.807927756215705
    },
    {
      "doc": 493,
      "topic": 3,
      "similarity": 0.7751687556878065
    },
    {
      "doc": 493,
      "topic": 4,
      "similarity": 0.8341680978819257
    },
    {
      "doc": 493,
      "topic": 5,
      "similarity": 0.7669257899772749
    },
    {
      "doc": 493,
      "topic": 7,
      "similarity": 0.7597117631225703
    },
    {
      "doc": 493,
      "topic": 9,
      "similarity": 0.7679186725091814
    },
    {
      "doc": 493,
      "topic": 10,
      "similarity": 0.7690818471501281
    },
    {
      "doc": 493,
      "topic": 16,
      "similarity": 0.7856752169865323
    },
    {
      "doc": 493,
      "topic": 17,
      "similarity": 0.7574063130211056
    },
    {
      "doc": 493,
      "topic": 19,
      "similarity": 0.7734687138733007
    },
    {
      "doc": 493,
      "topic": 21,
      "similarity": 0.7584175127491071
    },
    {
      "doc": 494,
      "topic": 1,
      "similarity": 0.7559290658644838
    },
    {
      "doc": 494,
      "topic": 2,
      "similarity": 0.8115660250800149
    },
    {
      "doc": 494,
      "topic": 3,
      "similarity": 0.819831903323024
    },
    {
      "doc": 494,
      "topic": 4,
      "similarity": 0.7663401463846535
    },
    {
      "doc": 494,
      "topic": 5,
      "similarity": 0.8140296067257458
    },
    {
      "doc": 494,
      "topic": 7,
      "similarity": 0.8043396944620539
    },
    {
      "doc": 494,
      "topic": 8,
      "similarity": 0.802350833405268
    },
    {
      "doc": 494,
      "topic": 9,
      "similarity": 0.8509532148990095
    },
    {
      "doc": 494,
      "topic": 10,
      "similarity": 0.7811659047411115
    },
    {
      "doc": 494,
      "topic": 11,
      "similarity": 0.7904302401399564
    },
    {
      "doc": 494,
      "topic": 12,
      "similarity": 0.7507866821103296
    },
    {
      "doc": 494,
      "topic": 13,
      "similarity": 0.7882946011799982
    },
    {
      "doc": 494,
      "topic": 14,
      "similarity": 0.8141394508974286
    },
    {
      "doc": 494,
      "topic": 15,
      "similarity": 0.8159473966311683
    },
    {
      "doc": 494,
      "topic": 16,
      "similarity": 0.8076346561310458
    },
    {
      "doc": 494,
      "topic": 17,
      "similarity": 0.8324446481641345
    },
    {
      "doc": 494,
      "topic": 18,
      "similarity": 0.7750597774944739
    },
    {
      "doc": 494,
      "topic": 19,
      "similarity": 0.8491196339856807
    },
    {
      "doc": 494,
      "topic": 20,
      "similarity": 0.7805361987863546
    },
    {
      "doc": 494,
      "topic": 21,
      "similarity": 0.8154150533965914
    },
    {
      "doc": 494,
      "topic": 22,
      "similarity": 0.751915031126796
    },
    {
      "doc": 494,
      "topic": 23,
      "similarity": 0.7757275892799383
    },
    {
      "doc": 494,
      "topic": 24,
      "similarity": 0.7572456323103549
    },
    {
      "doc": 495,
      "topic": 1,
      "similarity": 0.7816631157557987
    },
    {
      "doc": 495,
      "topic": 2,
      "similarity": 0.7989822403116109
    },
    {
      "doc": 495,
      "topic": 3,
      "similarity": 0.8204475013751186
    },
    {
      "doc": 495,
      "topic": 4,
      "similarity": 0.7711495385030874
    },
    {
      "doc": 495,
      "topic": 5,
      "similarity": 0.8072920680367007
    },
    {
      "doc": 495,
      "topic": 7,
      "similarity": 0.839859158059624
    },
    {
      "doc": 495,
      "topic": 8,
      "similarity": 0.8088173490715427
    },
    {
      "doc": 495,
      "topic": 9,
      "similarity": 0.8318508154661677
    },
    {
      "doc": 495,
      "topic": 10,
      "similarity": 0.7860981925836169
    },
    {
      "doc": 495,
      "topic": 11,
      "similarity": 0.7920347144955233
    },
    {
      "doc": 495,
      "topic": 13,
      "similarity": 0.7952036486246419
    },
    {
      "doc": 495,
      "topic": 14,
      "similarity": 0.7951255564625066
    },
    {
      "doc": 495,
      "topic": 15,
      "similarity": 0.7728231345529109
    },
    {
      "doc": 495,
      "topic": 16,
      "similarity": 0.8183268695768039
    },
    {
      "doc": 495,
      "topic": 17,
      "similarity": 0.7887052109010078
    },
    {
      "doc": 495,
      "topic": 18,
      "similarity": 0.755709437719513
    },
    {
      "doc": 495,
      "topic": 19,
      "similarity": 0.7977538608665712
    },
    {
      "doc": 495,
      "topic": 20,
      "similarity": 0.778547255661007
    },
    {
      "doc": 495,
      "topic": 21,
      "similarity": 0.8077927012571079
    },
    {
      "doc": 495,
      "topic": 23,
      "similarity": 0.7532091943517811
    },
    {
      "doc": 496,
      "topic": 1,
      "similarity": 0.751099473778164
    },
    {
      "doc": 496,
      "topic": 2,
      "similarity": 0.797845518357393
    },
    {
      "doc": 496,
      "topic": 3,
      "similarity": 0.8013136494820157
    },
    {
      "doc": 496,
      "topic": 4,
      "similarity": 0.7650395659083833
    },
    {
      "doc": 496,
      "topic": 5,
      "similarity": 0.8002503236224375
    },
    {
      "doc": 496,
      "topic": 7,
      "similarity": 0.8063805864172102
    },
    {
      "doc": 496,
      "topic": 8,
      "similarity": 0.7985016631456032
    },
    {
      "doc": 496,
      "topic": 9,
      "similarity": 0.8411319788399472
    },
    {
      "doc": 496,
      "topic": 10,
      "similarity": 0.7882656535195351
    },
    {
      "doc": 496,
      "topic": 11,
      "similarity": 0.7803597933809828
    },
    {
      "doc": 496,
      "topic": 13,
      "similarity": 0.8037214483687496
    },
    {
      "doc": 496,
      "topic": 14,
      "similarity": 0.7900297310754626
    },
    {
      "doc": 496,
      "topic": 15,
      "similarity": 0.7718047487037707
    },
    {
      "doc": 496,
      "topic": 16,
      "similarity": 0.8235173208894818
    },
    {
      "doc": 496,
      "topic": 17,
      "similarity": 0.803110339691748
    },
    {
      "doc": 496,
      "topic": 18,
      "similarity": 0.7571683943902124
    },
    {
      "doc": 496,
      "topic": 19,
      "similarity": 0.8091236825292446
    },
    {
      "doc": 496,
      "topic": 20,
      "similarity": 0.7905899792864715
    },
    {
      "doc": 496,
      "topic": 21,
      "similarity": 0.7962963450269933
    },
    {
      "doc": 497,
      "topic": 3,
      "similarity": 0.7733105514262265
    },
    {
      "doc": 497,
      "topic": 5,
      "similarity": 0.7824903549523633
    },
    {
      "doc": 497,
      "topic": 9,
      "similarity": 0.7942623335178484
    },
    {
      "doc": 497,
      "topic": 10,
      "similarity": 0.7516724804488866
    },
    {
      "doc": 497,
      "topic": 11,
      "similarity": 0.7693105618459485
    },
    {
      "doc": 497,
      "topic": 15,
      "similarity": 0.7609853253054105
    },
    {
      "doc": 497,
      "topic": 16,
      "similarity": 0.7726260362149391
    },
    {
      "doc": 497,
      "topic": 17,
      "similarity": 0.7842534050395243
    },
    {
      "doc": 497,
      "topic": 18,
      "similarity": 0.7567696292942832
    },
    {
      "doc": 497,
      "topic": 19,
      "similarity": 0.7797943234949085
    },
    {
      "doc": 497,
      "topic": 20,
      "similarity": 0.7925265040029847
    },
    {
      "doc": 497,
      "topic": 21,
      "similarity": 0.7777029678124563
    },
    {
      "doc": 497,
      "topic": 23,
      "similarity": 0.7503341599351966
    },
    {
      "doc": 497,
      "topic": 24,
      "similarity": 0.7594069572032083
    },
    {
      "doc": 498,
      "topic": 1,
      "similarity": 0.7873822549208083
    },
    {
      "doc": 498,
      "topic": 2,
      "similarity": 0.8456260255585145
    },
    {
      "doc": 498,
      "topic": 3,
      "similarity": 0.8018128530652873
    },
    {
      "doc": 498,
      "topic": 4,
      "similarity": 0.765061510483804
    },
    {
      "doc": 498,
      "topic": 5,
      "similarity": 0.7904460194454861
    },
    {
      "doc": 498,
      "topic": 7,
      "similarity": 0.7745596560087983
    },
    {
      "doc": 498,
      "topic": 8,
      "similarity": 0.7896218184461159
    },
    {
      "doc": 498,
      "topic": 9,
      "similarity": 0.8070389262605799
    },
    {
      "doc": 498,
      "topic": 10,
      "similarity": 0.7936167706832076
    },
    {
      "doc": 498,
      "topic": 11,
      "similarity": 0.7869960265457175
    },
    {
      "doc": 498,
      "topic": 13,
      "similarity": 0.8084373522958566
    },
    {
      "doc": 498,
      "topic": 14,
      "similarity": 0.8589071789645469
    },
    {
      "doc": 498,
      "topic": 15,
      "similarity": 0.7916131536407238
    },
    {
      "doc": 498,
      "topic": 16,
      "similarity": 0.8216773404236792
    },
    {
      "doc": 498,
      "topic": 17,
      "similarity": 0.8091529065688533
    },
    {
      "doc": 498,
      "topic": 18,
      "similarity": 0.7572230661334037
    },
    {
      "doc": 498,
      "topic": 19,
      "similarity": 0.8202669958111044
    },
    {
      "doc": 498,
      "topic": 20,
      "similarity": 0.7604405117803864
    },
    {
      "doc": 498,
      "topic": 21,
      "similarity": 0.7963342614260621
    },
    {
      "doc": 499,
      "topic": 1,
      "similarity": 0.755071696627116
    },
    {
      "doc": 499,
      "topic": 2,
      "similarity": 0.7730926650462977
    },
    {
      "doc": 499,
      "topic": 3,
      "similarity": 0.7950375564415297
    },
    {
      "doc": 499,
      "topic": 5,
      "similarity": 0.8068235168013843
    },
    {
      "doc": 499,
      "topic": 6,
      "similarity": 0.7546920977148943
    },
    {
      "doc": 499,
      "topic": 7,
      "similarity": 0.7891967362520931
    },
    {
      "doc": 499,
      "topic": 8,
      "similarity": 0.7712340359676546
    },
    {
      "doc": 499,
      "topic": 9,
      "similarity": 0.8371038538559366
    },
    {
      "doc": 499,
      "topic": 10,
      "similarity": 0.7813457035442031
    },
    {
      "doc": 499,
      "topic": 11,
      "similarity": 0.7904894459879022
    },
    {
      "doc": 499,
      "topic": 12,
      "similarity": 0.7601803873905144
    },
    {
      "doc": 499,
      "topic": 13,
      "similarity": 0.7661930627487027
    },
    {
      "doc": 499,
      "topic": 14,
      "similarity": 0.7658838345551523
    },
    {
      "doc": 499,
      "topic": 15,
      "similarity": 0.7722177676754336
    },
    {
      "doc": 499,
      "topic": 16,
      "similarity": 0.8115956462776329
    },
    {
      "doc": 499,
      "topic": 17,
      "similarity": 0.8159087241645048
    },
    {
      "doc": 499,
      "topic": 18,
      "similarity": 0.7576851689102272
    },
    {
      "doc": 499,
      "topic": 19,
      "similarity": 0.8173708670547362
    },
    {
      "doc": 499,
      "topic": 20,
      "similarity": 0.7786036935209714
    },
    {
      "doc": 499,
      "topic": 21,
      "similarity": 0.8011773312544779
    },
    {
      "doc": 499,
      "topic": 23,
      "similarity": 0.7865226214659528
    },
    {
      "doc": 499,
      "topic": 24,
      "similarity": 0.7678605643645089
    },
    {
      "doc": 500,
      "topic": 1,
      "similarity": 0.7649361906677508
    },
    {
      "doc": 500,
      "topic": 2,
      "similarity": 0.7807072363609826
    },
    {
      "doc": 500,
      "topic": 3,
      "similarity": 0.8260438450472986
    },
    {
      "doc": 500,
      "topic": 4,
      "similarity": 0.7549694387627209
    },
    {
      "doc": 500,
      "topic": 5,
      "similarity": 0.8083115944467352
    },
    {
      "doc": 500,
      "topic": 7,
      "similarity": 0.7865349906768768
    },
    {
      "doc": 500,
      "topic": 8,
      "similarity": 0.7734364141089188
    },
    {
      "doc": 500,
      "topic": 9,
      "similarity": 0.8385359525698356
    },
    {
      "doc": 500,
      "topic": 10,
      "similarity": 0.7910529100059517
    },
    {
      "doc": 500,
      "topic": 11,
      "similarity": 0.7910423077709707
    },
    {
      "doc": 500,
      "topic": 13,
      "similarity": 0.7803123741324364
    },
    {
      "doc": 500,
      "topic": 14,
      "similarity": 0.7767610368075878
    },
    {
      "doc": 500,
      "topic": 15,
      "similarity": 0.7890026054345671
    },
    {
      "doc": 500,
      "topic": 16,
      "similarity": 0.8091985602543263
    },
    {
      "doc": 500,
      "topic": 17,
      "similarity": 0.8087744158795793
    },
    {
      "doc": 500,
      "topic": 18,
      "similarity": 0.7733986936683832
    },
    {
      "doc": 500,
      "topic": 19,
      "similarity": 0.8048939349128841
    },
    {
      "doc": 500,
      "topic": 20,
      "similarity": 0.7815957418654396
    },
    {
      "doc": 500,
      "topic": 21,
      "similarity": 0.8523034906218682
    },
    {
      "doc": 500,
      "topic": 23,
      "similarity": 0.7551451482496621
    },
    {
      "doc": 500,
      "topic": 24,
      "similarity": 0.7586292168068819
    },
    {
      "doc": 501,
      "topic": 2,
      "similarity": 0.7508032396688176
    },
    {
      "doc": 501,
      "topic": 3,
      "similarity": 0.7848519150688016
    },
    {
      "doc": 501,
      "topic": 5,
      "similarity": 0.7970211580051357
    },
    {
      "doc": 501,
      "topic": 7,
      "similarity": 0.7574838973314033
    },
    {
      "doc": 501,
      "topic": 8,
      "similarity": 0.7774684211623453
    },
    {
      "doc": 501,
      "topic": 9,
      "similarity": 0.8115574718086895
    },
    {
      "doc": 501,
      "topic": 10,
      "similarity": 0.8130821067113002
    },
    {
      "doc": 501,
      "topic": 11,
      "similarity": 0.7759663805768464
    },
    {
      "doc": 501,
      "topic": 12,
      "similarity": 0.7904726636544441
    },
    {
      "doc": 501,
      "topic": 13,
      "similarity": 0.7604530061703156
    },
    {
      "doc": 501,
      "topic": 14,
      "similarity": 0.7503961331584424
    },
    {
      "doc": 501,
      "topic": 15,
      "similarity": 0.7752827002129193
    },
    {
      "doc": 501,
      "topic": 16,
      "similarity": 0.7953789497661682
    },
    {
      "doc": 501,
      "topic": 17,
      "similarity": 0.7907596077447773
    },
    {
      "doc": 501,
      "topic": 18,
      "similarity": 0.783779313001688
    },
    {
      "doc": 501,
      "topic": 19,
      "similarity": 0.8036137041702672
    },
    {
      "doc": 501,
      "topic": 20,
      "similarity": 0.7974315529698058
    },
    {
      "doc": 501,
      "topic": 21,
      "similarity": 0.7886606109085255
    },
    {
      "doc": 501,
      "topic": 23,
      "similarity": 0.7665927457429185
    },
    {
      "doc": 501,
      "topic": 24,
      "similarity": 0.7671744233023325
    },
    {
      "doc": 502,
      "topic": 2,
      "similarity": 0.7561730478205163
    },
    {
      "doc": 502,
      "topic": 3,
      "similarity": 0.7666944951718563
    },
    {
      "doc": 502,
      "topic": 5,
      "similarity": 0.7769171748266672
    },
    {
      "doc": 502,
      "topic": 7,
      "similarity": 0.7736613178228933
    },
    {
      "doc": 502,
      "topic": 8,
      "similarity": 0.7732507165596297
    },
    {
      "doc": 502,
      "topic": 9,
      "similarity": 0.838142966793512
    },
    {
      "doc": 502,
      "topic": 10,
      "similarity": 0.772916984862589
    },
    {
      "doc": 502,
      "topic": 11,
      "similarity": 0.7628669809870506
    },
    {
      "doc": 502,
      "topic": 12,
      "similarity": 0.7531183149438759
    },
    {
      "doc": 502,
      "topic": 13,
      "similarity": 0.7504695989218105
    },
    {
      "doc": 502,
      "topic": 14,
      "similarity": 0.7651950425162625
    },
    {
      "doc": 502,
      "topic": 15,
      "similarity": 0.7915034767244421
    },
    {
      "doc": 502,
      "topic": 16,
      "similarity": 0.7782487629958741
    },
    {
      "doc": 502,
      "topic": 17,
      "similarity": 0.7904737151224819
    },
    {
      "doc": 502,
      "topic": 18,
      "similarity": 0.7502032893306533
    },
    {
      "doc": 502,
      "topic": 19,
      "similarity": 0.7724742899694984
    },
    {
      "doc": 502,
      "topic": 20,
      "similarity": 0.7570566711170592
    },
    {
      "doc": 502,
      "topic": 21,
      "similarity": 0.7959560062036289
    },
    {
      "doc": 502,
      "topic": 23,
      "similarity": 0.7618700310117973
    },
    {
      "doc": 502,
      "topic": 24,
      "similarity": 0.7830138476073789
    },
    {
      "doc": 503,
      "topic": 1,
      "similarity": 0.7557225883786152
    },
    {
      "doc": 503,
      "topic": 3,
      "similarity": 0.7653319077481614
    },
    {
      "doc": 503,
      "topic": 7,
      "similarity": 0.7623780307938215
    },
    {
      "doc": 503,
      "topic": 8,
      "similarity": 0.7542302738468901
    },
    {
      "doc": 503,
      "topic": 9,
      "similarity": 0.7804461204923281
    },
    {
      "doc": 503,
      "topic": 16,
      "similarity": 0.7785674284884496
    },
    {
      "doc": 503,
      "topic": 17,
      "similarity": 0.7634707309769462
    },
    {
      "doc": 503,
      "topic": 18,
      "similarity": 0.7504146952825256
    },
    {
      "doc": 503,
      "topic": 19,
      "similarity": 0.7647853454660032
    },
    {
      "doc": 503,
      "topic": 20,
      "similarity": 0.7504769426260895
    },
    {
      "doc": 503,
      "topic": 21,
      "similarity": 0.779631698977868
    },
    {
      "doc": 504,
      "topic": 2,
      "similarity": 0.7577096264500488
    },
    {
      "doc": 504,
      "topic": 3,
      "similarity": 0.7814730658969761
    },
    {
      "doc": 504,
      "topic": 5,
      "similarity": 0.7976374851720178
    },
    {
      "doc": 504,
      "topic": 7,
      "similarity": 0.7609923084579503
    },
    {
      "doc": 504,
      "topic": 8,
      "similarity": 0.7599926153904603
    },
    {
      "doc": 504,
      "topic": 9,
      "similarity": 0.8032920770064956
    },
    {
      "doc": 504,
      "topic": 10,
      "similarity": 0.7655977256167744
    },
    {
      "doc": 504,
      "topic": 11,
      "similarity": 0.7607336486405505
    },
    {
      "doc": 504,
      "topic": 15,
      "similarity": 0.7607475653729392
    },
    {
      "doc": 504,
      "topic": 16,
      "similarity": 0.7911882407663577
    },
    {
      "doc": 504,
      "topic": 17,
      "similarity": 0.7904455231543872
    },
    {
      "doc": 504,
      "topic": 18,
      "similarity": 0.7522316690792694
    },
    {
      "doc": 504,
      "topic": 19,
      "similarity": 0.7948528367709844
    },
    {
      "doc": 504,
      "topic": 20,
      "similarity": 0.7964023149852185
    },
    {
      "doc": 504,
      "topic": 21,
      "similarity": 0.7969431766094606
    },
    {
      "doc": 504,
      "topic": 23,
      "similarity": 0.7610429431615856
    },
    {
      "doc": 504,
      "topic": 24,
      "similarity": 0.7743676353022063
    },
    {
      "doc": 505,
      "topic": 1,
      "similarity": 0.7687259286198196
    },
    {
      "doc": 505,
      "topic": 2,
      "similarity": 0.7773898870557469
    },
    {
      "doc": 505,
      "topic": 3,
      "similarity": 0.8000702811105002
    },
    {
      "doc": 505,
      "topic": 4,
      "similarity": 0.7513843686482601
    },
    {
      "doc": 505,
      "topic": 5,
      "similarity": 0.8100500718468941
    },
    {
      "doc": 505,
      "topic": 7,
      "similarity": 0.7921088243815252
    },
    {
      "doc": 505,
      "topic": 8,
      "similarity": 0.7739058838801909
    },
    {
      "doc": 505,
      "topic": 9,
      "similarity": 0.85353400133576
    },
    {
      "doc": 505,
      "topic": 10,
      "similarity": 0.794905894115706
    },
    {
      "doc": 505,
      "topic": 11,
      "similarity": 0.8135065593607065
    },
    {
      "doc": 505,
      "topic": 13,
      "similarity": 0.781776077141318
    },
    {
      "doc": 505,
      "topic": 14,
      "similarity": 0.7866401843550743
    },
    {
      "doc": 505,
      "topic": 15,
      "similarity": 0.816103104698905
    },
    {
      "doc": 505,
      "topic": 16,
      "similarity": 0.8313532968950079
    },
    {
      "doc": 505,
      "topic": 17,
      "similarity": 0.8369672289548759
    },
    {
      "doc": 505,
      "topic": 18,
      "similarity": 0.7678292458977829
    },
    {
      "doc": 505,
      "topic": 19,
      "similarity": 0.8196901545996752
    },
    {
      "doc": 505,
      "topic": 20,
      "similarity": 0.7806758551030468
    },
    {
      "doc": 505,
      "topic": 21,
      "similarity": 0.8216000554128374
    },
    {
      "doc": 505,
      "topic": 23,
      "similarity": 0.7598862553064946
    },
    {
      "doc": 505,
      "topic": 24,
      "similarity": 0.7675682316788254
    },
    {
      "doc": 506,
      "topic": 2,
      "similarity": 0.7667833708910498
    },
    {
      "doc": 506,
      "topic": 3,
      "similarity": 0.7851252375088783
    },
    {
      "doc": 506,
      "topic": 5,
      "similarity": 0.8008076021660236
    },
    {
      "doc": 506,
      "topic": 7,
      "similarity": 0.7911911815650222
    },
    {
      "doc": 506,
      "topic": 8,
      "similarity": 0.7821984631295111
    },
    {
      "doc": 506,
      "topic": 9,
      "similarity": 0.8252577849702761
    },
    {
      "doc": 506,
      "topic": 10,
      "similarity": 0.7665768744428744
    },
    {
      "doc": 506,
      "topic": 11,
      "similarity": 0.8209806798286541
    },
    {
      "doc": 506,
      "topic": 13,
      "similarity": 0.7524219557365184
    },
    {
      "doc": 506,
      "topic": 14,
      "similarity": 0.7517596774381669
    },
    {
      "doc": 506,
      "topic": 15,
      "similarity": 0.7865138712632367
    },
    {
      "doc": 506,
      "topic": 16,
      "similarity": 0.7884560273400563
    },
    {
      "doc": 506,
      "topic": 17,
      "similarity": 0.7867912405177471
    },
    {
      "doc": 506,
      "topic": 18,
      "similarity": 0.7750307025705148
    },
    {
      "doc": 506,
      "topic": 19,
      "similarity": 0.7943818243773073
    },
    {
      "doc": 506,
      "topic": 20,
      "similarity": 0.7736255798390342
    },
    {
      "doc": 506,
      "topic": 21,
      "similarity": 0.7887921202173
    },
    {
      "doc": 506,
      "topic": 22,
      "similarity": 0.7518698318744483
    },
    {
      "doc": 506,
      "topic": 24,
      "similarity": 0.7622149206230494
    },
    {
      "doc": 507,
      "topic": 1,
      "similarity": 0.7722430106508821
    },
    {
      "doc": 507,
      "topic": 2,
      "similarity": 0.7885682206895088
    },
    {
      "doc": 507,
      "topic": 3,
      "similarity": 0.8094285267131501
    },
    {
      "doc": 507,
      "topic": 4,
      "similarity": 0.7576178504737788
    },
    {
      "doc": 507,
      "topic": 5,
      "similarity": 0.8169860241326143
    },
    {
      "doc": 507,
      "topic": 7,
      "similarity": 0.7898058543988604
    },
    {
      "doc": 507,
      "topic": 8,
      "similarity": 0.8014095071066966
    },
    {
      "doc": 507,
      "topic": 9,
      "similarity": 0.8220710593322961
    },
    {
      "doc": 507,
      "topic": 10,
      "similarity": 0.7872683409382744
    },
    {
      "doc": 507,
      "topic": 11,
      "similarity": 0.8088223139615554
    },
    {
      "doc": 507,
      "topic": 13,
      "similarity": 0.7924688852180537
    },
    {
      "doc": 507,
      "topic": 14,
      "similarity": 0.8007051977951476
    },
    {
      "doc": 507,
      "topic": 15,
      "similarity": 0.7763008689011491
    },
    {
      "doc": 507,
      "topic": 16,
      "similarity": 0.8198622307480353
    },
    {
      "doc": 507,
      "topic": 17,
      "similarity": 0.792677412722739
    },
    {
      "doc": 507,
      "topic": 18,
      "similarity": 0.7504630622159982
    },
    {
      "doc": 507,
      "topic": 19,
      "similarity": 0.813216156662735
    },
    {
      "doc": 507,
      "topic": 20,
      "similarity": 0.7766273479683946
    },
    {
      "doc": 507,
      "topic": 21,
      "similarity": 0.7943095706649048
    },
    {
      "doc": 507,
      "topic": 22,
      "similarity": 0.7679988670525697
    },
    {
      "doc": 507,
      "topic": 23,
      "similarity": 0.7578663294032082
    },
    {
      "doc": 508,
      "topic": 2,
      "similarity": 0.7718075167038473
    },
    {
      "doc": 508,
      "topic": 3,
      "similarity": 0.8061548990740026
    },
    {
      "doc": 508,
      "topic": 5,
      "similarity": 0.8111897121524589
    },
    {
      "doc": 508,
      "topic": 7,
      "similarity": 0.7666024434529634
    },
    {
      "doc": 508,
      "topic": 8,
      "similarity": 0.7731224897482771
    },
    {
      "doc": 508,
      "topic": 9,
      "similarity": 0.8297656729779374
    },
    {
      "doc": 508,
      "topic": 10,
      "similarity": 0.763482832486758
    },
    {
      "doc": 508,
      "topic": 11,
      "similarity": 0.7544249583463207
    },
    {
      "doc": 508,
      "topic": 12,
      "similarity": 0.7540477109969105
    },
    {
      "doc": 508,
      "topic": 13,
      "similarity": 0.772978349940601
    },
    {
      "doc": 508,
      "topic": 14,
      "similarity": 0.7726848444414206
    },
    {
      "doc": 508,
      "topic": 15,
      "similarity": 0.7567367401469665
    },
    {
      "doc": 508,
      "topic": 16,
      "similarity": 0.8075942867305634
    },
    {
      "doc": 508,
      "topic": 17,
      "similarity": 0.8008412582230318
    },
    {
      "doc": 508,
      "topic": 18,
      "similarity": 0.7506986520322863
    },
    {
      "doc": 508,
      "topic": 19,
      "similarity": 0.7941950019652178
    },
    {
      "doc": 508,
      "topic": 20,
      "similarity": 0.774003936161063
    },
    {
      "doc": 508,
      "topic": 21,
      "similarity": 0.8077106586709231
    },
    {
      "doc": 508,
      "topic": 23,
      "similarity": 0.767638351432399
    },
    {
      "doc": 508,
      "topic": 24,
      "similarity": 0.7710161940479542
    },
    {
      "doc": 509,
      "topic": 2,
      "similarity": 0.7545602717516315
    },
    {
      "doc": 509,
      "topic": 3,
      "similarity": 0.8142829382101638
    },
    {
      "doc": 509,
      "topic": 4,
      "similarity": 0.7519572516207428
    },
    {
      "doc": 509,
      "topic": 5,
      "similarity": 0.793455466537159
    },
    {
      "doc": 509,
      "topic": 7,
      "similarity": 0.7907476088767554
    },
    {
      "doc": 509,
      "topic": 8,
      "similarity": 0.7611708456011189
    },
    {
      "doc": 509,
      "topic": 9,
      "similarity": 0.8131721256553858
    },
    {
      "doc": 509,
      "topic": 10,
      "similarity": 0.7793492861584023
    },
    {
      "doc": 509,
      "topic": 11,
      "similarity": 0.7798054621074926
    },
    {
      "doc": 509,
      "topic": 13,
      "similarity": 0.7590053514658611
    },
    {
      "doc": 509,
      "topic": 14,
      "similarity": 0.7661235617253652
    },
    {
      "doc": 509,
      "topic": 15,
      "similarity": 0.7718159279113996
    },
    {
      "doc": 509,
      "topic": 16,
      "similarity": 0.7962500678363645
    },
    {
      "doc": 509,
      "topic": 17,
      "similarity": 0.7910901198974574
    },
    {
      "doc": 509,
      "topic": 18,
      "similarity": 0.8123274116959247
    },
    {
      "doc": 509,
      "topic": 19,
      "similarity": 0.8360732024786883
    },
    {
      "doc": 509,
      "topic": 20,
      "similarity": 0.781626571250558
    },
    {
      "doc": 509,
      "topic": 21,
      "similarity": 0.7933775490142874
    },
    {
      "doc": 509,
      "topic": 23,
      "similarity": 0.7533902502469586
    },
    {
      "doc": 510,
      "topic": 1,
      "similarity": 0.7785381261481678
    },
    {
      "doc": 510,
      "topic": 2,
      "similarity": 0.7977385469240679
    },
    {
      "doc": 510,
      "topic": 3,
      "similarity": 0.8184928755225549
    },
    {
      "doc": 510,
      "topic": 4,
      "similarity": 0.7576836614464355
    },
    {
      "doc": 510,
      "topic": 5,
      "similarity": 0.8016255685548697
    },
    {
      "doc": 510,
      "topic": 7,
      "similarity": 0.7886852160595902
    },
    {
      "doc": 510,
      "topic": 8,
      "similarity": 0.7920391185561738
    },
    {
      "doc": 510,
      "topic": 9,
      "similarity": 0.8430691878283838
    },
    {
      "doc": 510,
      "topic": 10,
      "similarity": 0.7889722800211124
    },
    {
      "doc": 510,
      "topic": 11,
      "similarity": 0.7846505328783918
    },
    {
      "doc": 510,
      "topic": 13,
      "similarity": 0.7749069232669916
    },
    {
      "doc": 510,
      "topic": 14,
      "similarity": 0.7970488101185182
    },
    {
      "doc": 510,
      "topic": 15,
      "similarity": 0.789942707933595
    },
    {
      "doc": 510,
      "topic": 16,
      "similarity": 0.8258846375330128
    },
    {
      "doc": 510,
      "topic": 17,
      "similarity": 0.7904288358972741
    },
    {
      "doc": 510,
      "topic": 18,
      "similarity": 0.7626890286782424
    },
    {
      "doc": 510,
      "topic": 19,
      "similarity": 0.8176613256457405
    },
    {
      "doc": 510,
      "topic": 20,
      "similarity": 0.7898108523865636
    },
    {
      "doc": 510,
      "topic": 21,
      "similarity": 0.7939848891103843
    },
    {
      "doc": 510,
      "topic": 22,
      "similarity": 0.7549809393344526
    },
    {
      "doc": 510,
      "topic": 23,
      "similarity": 0.7525284673799205
    },
    {
      "doc": 511,
      "topic": 2,
      "similarity": 0.7710704521326338
    },
    {
      "doc": 511,
      "topic": 3,
      "similarity": 0.7946476329369405
    },
    {
      "doc": 511,
      "topic": 5,
      "similarity": 0.8009103128293988
    },
    {
      "doc": 511,
      "topic": 7,
      "similarity": 0.7782295389954226
    },
    {
      "doc": 511,
      "topic": 8,
      "similarity": 0.7741180854401266
    },
    {
      "doc": 511,
      "topic": 9,
      "similarity": 0.82353366059245
    },
    {
      "doc": 511,
      "topic": 10,
      "similarity": 0.7756518729951917
    },
    {
      "doc": 511,
      "topic": 11,
      "similarity": 0.7706601077257118
    },
    {
      "doc": 511,
      "topic": 13,
      "similarity": 0.7684980465544881
    },
    {
      "doc": 511,
      "topic": 14,
      "similarity": 0.7636603569725562
    },
    {
      "doc": 511,
      "topic": 15,
      "similarity": 0.7676855705135615
    },
    {
      "doc": 511,
      "topic": 16,
      "similarity": 0.8148301855325847
    },
    {
      "doc": 511,
      "topic": 17,
      "similarity": 0.7826744679419007
    },
    {
      "doc": 511,
      "topic": 19,
      "similarity": 0.794880415946088
    },
    {
      "doc": 511,
      "topic": 20,
      "similarity": 0.7791494682070583
    },
    {
      "doc": 511,
      "topic": 21,
      "similarity": 0.8018155989779886
    },
    {
      "doc": 511,
      "topic": 24,
      "similarity": 0.7602602463906809
    },
    {
      "doc": 512,
      "topic": 2,
      "similarity": 0.7731579783340237
    },
    {
      "doc": 512,
      "topic": 3,
      "similarity": 0.7844794813785751
    },
    {
      "doc": 512,
      "topic": 5,
      "similarity": 0.783971053445055
    },
    {
      "doc": 512,
      "topic": 7,
      "similarity": 0.76470504140562
    },
    {
      "doc": 512,
      "topic": 8,
      "similarity": 0.7849153811825657
    },
    {
      "doc": 512,
      "topic": 9,
      "similarity": 0.8067558711827894
    },
    {
      "doc": 512,
      "topic": 10,
      "similarity": 0.7740207392250701
    },
    {
      "doc": 512,
      "topic": 11,
      "similarity": 0.7716089576147354
    },
    {
      "doc": 512,
      "topic": 12,
      "similarity": 0.7712736943354598
    },
    {
      "doc": 512,
      "topic": 13,
      "similarity": 0.7778938417012189
    },
    {
      "doc": 512,
      "topic": 14,
      "similarity": 0.7879777300836663
    },
    {
      "doc": 512,
      "topic": 15,
      "similarity": 0.7722965227265749
    },
    {
      "doc": 512,
      "topic": 16,
      "similarity": 0.7964583681569871
    },
    {
      "doc": 512,
      "topic": 17,
      "similarity": 0.7761323762918094
    },
    {
      "doc": 512,
      "topic": 18,
      "similarity": 0.800875491636002
    },
    {
      "doc": 512,
      "topic": 19,
      "similarity": 0.8354151031639212
    },
    {
      "doc": 512,
      "topic": 20,
      "similarity": 0.7932656525318298
    },
    {
      "doc": 512,
      "topic": 21,
      "similarity": 0.7802704686349742
    },
    {
      "doc": 512,
      "topic": 22,
      "similarity": 0.7822324118864522
    },
    {
      "doc": 512,
      "topic": 23,
      "similarity": 0.7738348260930749
    },
    {
      "doc": 513,
      "topic": 2,
      "similarity": 0.776959362336354
    },
    {
      "doc": 513,
      "topic": 3,
      "similarity": 0.7850450729411728
    },
    {
      "doc": 513,
      "topic": 4,
      "similarity": 0.7571694921097584
    },
    {
      "doc": 513,
      "topic": 5,
      "similarity": 0.7870986456232226
    },
    {
      "doc": 513,
      "topic": 7,
      "similarity": 0.800646739479779
    },
    {
      "doc": 513,
      "topic": 8,
      "similarity": 0.7973307215837526
    },
    {
      "doc": 513,
      "topic": 9,
      "similarity": 0.8465468622607434
    },
    {
      "doc": 513,
      "topic": 10,
      "similarity": 0.7802439678623189
    },
    {
      "doc": 513,
      "topic": 11,
      "similarity": 0.7790316636710101
    },
    {
      "doc": 513,
      "topic": 13,
      "similarity": 0.7853239977902848
    },
    {
      "doc": 513,
      "topic": 14,
      "similarity": 0.7758190875985049
    },
    {
      "doc": 513,
      "topic": 15,
      "similarity": 0.7762336410661934
    },
    {
      "doc": 513,
      "topic": 16,
      "similarity": 0.7896413817853195
    },
    {
      "doc": 513,
      "topic": 17,
      "similarity": 0.7965533736836165
    },
    {
      "doc": 513,
      "topic": 18,
      "similarity": 0.7583323255173195
    },
    {
      "doc": 513,
      "topic": 19,
      "similarity": 0.8038681343677667
    },
    {
      "doc": 513,
      "topic": 20,
      "similarity": 0.7774320178917167
    },
    {
      "doc": 513,
      "topic": 21,
      "similarity": 0.8075037052895631
    },
    {
      "doc": 513,
      "topic": 24,
      "similarity": 0.7525317472295817
    },
    {
      "doc": 514,
      "topic": 2,
      "similarity": 0.7599209396184797
    },
    {
      "doc": 514,
      "topic": 3,
      "similarity": 0.7692022252145521
    },
    {
      "doc": 514,
      "topic": 5,
      "similarity": 0.7771872074715847
    },
    {
      "doc": 514,
      "topic": 8,
      "similarity": 0.7529134798085687
    },
    {
      "doc": 514,
      "topic": 9,
      "similarity": 0.7826479174163123
    },
    {
      "doc": 514,
      "topic": 10,
      "similarity": 0.7588803926742173
    },
    {
      "doc": 514,
      "topic": 11,
      "similarity": 0.7542444978802019
    },
    {
      "doc": 514,
      "topic": 15,
      "similarity": 0.7748531865255789
    },
    {
      "doc": 514,
      "topic": 16,
      "similarity": 0.7935020241491434
    },
    {
      "doc": 514,
      "topic": 17,
      "similarity": 0.8086513855021398
    },
    {
      "doc": 514,
      "topic": 18,
      "similarity": 0.7569434486975293
    },
    {
      "doc": 514,
      "topic": 19,
      "similarity": 0.7880300026586498
    },
    {
      "doc": 514,
      "topic": 20,
      "similarity": 0.784674292210854
    },
    {
      "doc": 514,
      "topic": 21,
      "similarity": 0.7820773446417529
    },
    {
      "doc": 515,
      "topic": 3,
      "similarity": 0.7842570344355664
    },
    {
      "doc": 515,
      "topic": 5,
      "similarity": 0.799113770393455
    },
    {
      "doc": 515,
      "topic": 7,
      "similarity": 0.7694061164142839
    },
    {
      "doc": 515,
      "topic": 8,
      "similarity": 0.7653640366825433
    },
    {
      "doc": 515,
      "topic": 9,
      "similarity": 0.8109183857694742
    },
    {
      "doc": 515,
      "topic": 10,
      "similarity": 0.7585698116814292
    },
    {
      "doc": 515,
      "topic": 11,
      "similarity": 0.7759577766771563
    },
    {
      "doc": 515,
      "topic": 15,
      "similarity": 0.7692633197160426
    },
    {
      "doc": 515,
      "topic": 16,
      "similarity": 0.7966848671944436
    },
    {
      "doc": 515,
      "topic": 17,
      "similarity": 0.8047014611935164
    },
    {
      "doc": 515,
      "topic": 18,
      "similarity": 0.7743073719930195
    },
    {
      "doc": 515,
      "topic": 19,
      "similarity": 0.7933756337895025
    },
    {
      "doc": 515,
      "topic": 20,
      "similarity": 0.7789196014869895
    },
    {
      "doc": 515,
      "topic": 21,
      "similarity": 0.8271430470363151
    },
    {
      "doc": 515,
      "topic": 23,
      "similarity": 0.7595689325350494
    },
    {
      "doc": 515,
      "topic": 24,
      "similarity": 0.7670120209072149
    },
    {
      "doc": 516,
      "topic": 0,
      "similarity": 0.7501263354405359
    },
    {
      "doc": 516,
      "topic": 1,
      "similarity": 0.8287715559177007
    },
    {
      "doc": 516,
      "topic": 2,
      "similarity": 0.7992880780676443
    },
    {
      "doc": 516,
      "topic": 3,
      "similarity": 0.8153414292770303
    },
    {
      "doc": 516,
      "topic": 4,
      "similarity": 0.7713798742150445
    },
    {
      "doc": 516,
      "topic": 5,
      "similarity": 0.8021130018226997
    },
    {
      "doc": 516,
      "topic": 7,
      "similarity": 0.7964067706219854
    },
    {
      "doc": 516,
      "topic": 8,
      "similarity": 0.7964454681886679
    },
    {
      "doc": 516,
      "topic": 9,
      "similarity": 0.8312798045387655
    },
    {
      "doc": 516,
      "topic": 10,
      "similarity": 0.8008550836638554
    },
    {
      "doc": 516,
      "topic": 11,
      "similarity": 0.7919054277371977
    },
    {
      "doc": 516,
      "topic": 13,
      "similarity": 0.7870913980345654
    },
    {
      "doc": 516,
      "topic": 14,
      "similarity": 0.7898589241711583
    },
    {
      "doc": 516,
      "topic": 15,
      "similarity": 0.7803953111023841
    },
    {
      "doc": 516,
      "topic": 16,
      "similarity": 0.8028219985017392
    },
    {
      "doc": 516,
      "topic": 17,
      "similarity": 0.789172401380253
    },
    {
      "doc": 516,
      "topic": 18,
      "similarity": 0.7546625108048302
    },
    {
      "doc": 516,
      "topic": 19,
      "similarity": 0.8114566800546751
    },
    {
      "doc": 516,
      "topic": 20,
      "similarity": 0.7791470594283937
    },
    {
      "doc": 516,
      "topic": 21,
      "similarity": 0.8141404724380333
    },
    {
      "doc": 516,
      "topic": 23,
      "similarity": 0.7762262345095783
    },
    {
      "doc": 517,
      "topic": 2,
      "similarity": 0.7675022096405055
    },
    {
      "doc": 517,
      "topic": 3,
      "similarity": 0.8022398520798192
    },
    {
      "doc": 517,
      "topic": 4,
      "similarity": 0.759447499664072
    },
    {
      "doc": 517,
      "topic": 5,
      "similarity": 0.8199470281728796
    },
    {
      "doc": 517,
      "topic": 7,
      "similarity": 0.7908414655857252
    },
    {
      "doc": 517,
      "topic": 8,
      "similarity": 0.7849376336670786
    },
    {
      "doc": 517,
      "topic": 9,
      "similarity": 0.8424664794659715
    },
    {
      "doc": 517,
      "topic": 10,
      "similarity": 0.7847765086457753
    },
    {
      "doc": 517,
      "topic": 11,
      "similarity": 0.7946745046028613
    },
    {
      "doc": 517,
      "topic": 12,
      "similarity": 0.7543345568241995
    },
    {
      "doc": 517,
      "topic": 13,
      "similarity": 0.7637935492370742
    },
    {
      "doc": 517,
      "topic": 14,
      "similarity": 0.7723577533728666
    },
    {
      "doc": 517,
      "topic": 15,
      "similarity": 0.7737605134411547
    },
    {
      "doc": 517,
      "topic": 16,
      "similarity": 0.8118316179465285
    },
    {
      "doc": 517,
      "topic": 17,
      "similarity": 0.8009768942640445
    },
    {
      "doc": 517,
      "topic": 18,
      "similarity": 0.7679852862937472
    },
    {
      "doc": 517,
      "topic": 19,
      "similarity": 0.8079498351322076
    },
    {
      "doc": 517,
      "topic": 20,
      "similarity": 0.7981818600881857
    },
    {
      "doc": 517,
      "topic": 21,
      "similarity": 0.7969836444923357
    },
    {
      "doc": 517,
      "topic": 23,
      "similarity": 0.7626050769725878
    },
    {
      "doc": 517,
      "topic": 24,
      "similarity": 0.7826921607513481
    },
    {
      "doc": 518,
      "topic": 1,
      "similarity": 0.7572887323854979
    },
    {
      "doc": 518,
      "topic": 2,
      "similarity": 0.766086282842104
    },
    {
      "doc": 518,
      "topic": 3,
      "similarity": 0.7927879118976757
    },
    {
      "doc": 518,
      "topic": 5,
      "similarity": 0.7713331045286096
    },
    {
      "doc": 518,
      "topic": 7,
      "similarity": 0.7700093085938428
    },
    {
      "doc": 518,
      "topic": 8,
      "similarity": 0.77020275517069
    },
    {
      "doc": 518,
      "topic": 9,
      "similarity": 0.8077494183713609
    },
    {
      "doc": 518,
      "topic": 10,
      "similarity": 0.7656528715033449
    },
    {
      "doc": 518,
      "topic": 11,
      "similarity": 0.762988068009485
    },
    {
      "doc": 518,
      "topic": 13,
      "similarity": 0.7692698445445224
    },
    {
      "doc": 518,
      "topic": 14,
      "similarity": 0.7706882996308824
    },
    {
      "doc": 518,
      "topic": 15,
      "similarity": 0.77922206998751
    },
    {
      "doc": 518,
      "topic": 16,
      "similarity": 0.7960848942268625
    },
    {
      "doc": 518,
      "topic": 17,
      "similarity": 0.7900007028258749
    },
    {
      "doc": 518,
      "topic": 18,
      "similarity": 0.7563252750081749
    },
    {
      "doc": 518,
      "topic": 19,
      "similarity": 0.804603538196422
    },
    {
      "doc": 518,
      "topic": 20,
      "similarity": 0.7606578959261459
    },
    {
      "doc": 518,
      "topic": 21,
      "similarity": 0.7784083203104666
    },
    {
      "doc": 518,
      "topic": 22,
      "similarity": 0.7720596463365191
    },
    {
      "doc": 518,
      "topic": 23,
      "similarity": 0.762335971191536
    },
    {
      "doc": 519,
      "topic": 1,
      "similarity": 0.7566317520391552
    },
    {
      "doc": 519,
      "topic": 2,
      "similarity": 0.7688220439983303
    },
    {
      "doc": 519,
      "topic": 3,
      "similarity": 0.787150915576072
    },
    {
      "doc": 519,
      "topic": 4,
      "similarity": 0.7515518143124917
    },
    {
      "doc": 519,
      "topic": 5,
      "similarity": 0.8016394785018982
    },
    {
      "doc": 519,
      "topic": 7,
      "similarity": 0.7733958073674974
    },
    {
      "doc": 519,
      "topic": 8,
      "similarity": 0.7776058103406371
    },
    {
      "doc": 519,
      "topic": 9,
      "similarity": 0.8013034424865288
    },
    {
      "doc": 519,
      "topic": 10,
      "similarity": 0.830884886421533
    },
    {
      "doc": 519,
      "topic": 11,
      "similarity": 0.7812367053101313
    },
    {
      "doc": 519,
      "topic": 13,
      "similarity": 0.7735692993042174
    },
    {
      "doc": 519,
      "topic": 14,
      "similarity": 0.7716883191827397
    },
    {
      "doc": 519,
      "topic": 15,
      "similarity": 0.7727744302383259
    },
    {
      "doc": 519,
      "topic": 16,
      "similarity": 0.7908388708098003
    },
    {
      "doc": 519,
      "topic": 17,
      "similarity": 0.7879554872007912
    },
    {
      "doc": 519,
      "topic": 18,
      "similarity": 0.766773184372466
    },
    {
      "doc": 519,
      "topic": 19,
      "similarity": 0.8127283790893788
    },
    {
      "doc": 519,
      "topic": 20,
      "similarity": 0.7902524511509622
    },
    {
      "doc": 519,
      "topic": 21,
      "similarity": 0.8013085527165649
    },
    {
      "doc": 520,
      "topic": 3,
      "similarity": 0.7677606116654958
    },
    {
      "doc": 520,
      "topic": 5,
      "similarity": 0.7605058163327235
    },
    {
      "doc": 520,
      "topic": 6,
      "similarity": 0.7832564503145635
    },
    {
      "doc": 520,
      "topic": 9,
      "similarity": 0.7992633386897989
    },
    {
      "doc": 520,
      "topic": 11,
      "similarity": 0.7661150928692197
    },
    {
      "doc": 520,
      "topic": 13,
      "similarity": 0.7525282787471825
    },
    {
      "doc": 520,
      "topic": 16,
      "similarity": 0.7810478986327699
    },
    {
      "doc": 520,
      "topic": 17,
      "similarity": 0.8053683215476602
    },
    {
      "doc": 520,
      "topic": 18,
      "similarity": 0.7666240941524332
    },
    {
      "doc": 520,
      "topic": 19,
      "similarity": 0.7923079727983555
    },
    {
      "doc": 520,
      "topic": 20,
      "similarity": 0.7678305719831519
    },
    {
      "doc": 520,
      "topic": 21,
      "similarity": 0.7929176998913245
    },
    {
      "doc": 520,
      "topic": 23,
      "similarity": 0.7518036613183295
    },
    {
      "doc": 520,
      "topic": 24,
      "similarity": 0.7534922696073871
    },
    {
      "doc": 521,
      "topic": 2,
      "similarity": 0.7668173945081174
    },
    {
      "doc": 521,
      "topic": 3,
      "similarity": 0.7882472081408622
    },
    {
      "doc": 521,
      "topic": 5,
      "similarity": 0.7775747292200624
    },
    {
      "doc": 521,
      "topic": 7,
      "similarity": 0.7685010151087341
    },
    {
      "doc": 521,
      "topic": 8,
      "similarity": 0.7524894711494445
    },
    {
      "doc": 521,
      "topic": 9,
      "similarity": 0.8022941491945299
    },
    {
      "doc": 521,
      "topic": 10,
      "similarity": 0.7691039712063064
    },
    {
      "doc": 521,
      "topic": 11,
      "similarity": 0.7725159270029718
    },
    {
      "doc": 521,
      "topic": 12,
      "similarity": 0.7712028842430353
    },
    {
      "doc": 521,
      "topic": 13,
      "similarity": 0.763009604070664
    },
    {
      "doc": 521,
      "topic": 15,
      "similarity": 0.7697705203495642
    },
    {
      "doc": 521,
      "topic": 16,
      "similarity": 0.794882914397677
    },
    {
      "doc": 521,
      "topic": 17,
      "similarity": 0.8107755035968759
    },
    {
      "doc": 521,
      "topic": 18,
      "similarity": 0.7709619506279981
    },
    {
      "doc": 521,
      "topic": 19,
      "similarity": 0.8120623497962975
    },
    {
      "doc": 521,
      "topic": 20,
      "similarity": 0.8202574917601583
    },
    {
      "doc": 521,
      "topic": 21,
      "similarity": 0.7862597026422994
    },
    {
      "doc": 521,
      "topic": 22,
      "similarity": 0.7771027246869794
    },
    {
      "doc": 522,
      "topic": 1,
      "similarity": 0.7738196560370032
    },
    {
      "doc": 522,
      "topic": 2,
      "similarity": 0.7966317347034341
    },
    {
      "doc": 522,
      "topic": 3,
      "similarity": 0.8346971952622151
    },
    {
      "doc": 522,
      "topic": 4,
      "similarity": 0.7722930110110783
    },
    {
      "doc": 522,
      "topic": 5,
      "similarity": 0.8046998977109996
    },
    {
      "doc": 522,
      "topic": 7,
      "similarity": 0.7995046693924274
    },
    {
      "doc": 522,
      "topic": 8,
      "similarity": 0.7858971044831637
    },
    {
      "doc": 522,
      "topic": 9,
      "similarity": 0.8394775429068353
    },
    {
      "doc": 522,
      "topic": 10,
      "similarity": 0.8009449686716912
    },
    {
      "doc": 522,
      "topic": 11,
      "similarity": 0.8050978256622862
    },
    {
      "doc": 522,
      "topic": 13,
      "similarity": 0.7827333907393348
    },
    {
      "doc": 522,
      "topic": 14,
      "similarity": 0.8052228575409933
    },
    {
      "doc": 522,
      "topic": 15,
      "similarity": 0.7971321842025033
    },
    {
      "doc": 522,
      "topic": 16,
      "similarity": 0.8222510962740556
    },
    {
      "doc": 522,
      "topic": 17,
      "similarity": 0.8114462292520993
    },
    {
      "doc": 522,
      "topic": 18,
      "similarity": 0.824154754365544
    },
    {
      "doc": 522,
      "topic": 19,
      "similarity": 0.8659294597665592
    },
    {
      "doc": 522,
      "topic": 20,
      "similarity": 0.7788794160128958
    },
    {
      "doc": 522,
      "topic": 21,
      "similarity": 0.8169939349941407
    },
    {
      "doc": 522,
      "topic": 23,
      "similarity": 0.7680206614251218
    },
    {
      "doc": 522,
      "topic": 24,
      "similarity": 0.7503911076245373
    },
    {
      "doc": 523,
      "topic": 3,
      "similarity": 0.7708711785193645
    },
    {
      "doc": 523,
      "topic": 5,
      "similarity": 0.780168563067775
    },
    {
      "doc": 523,
      "topic": 7,
      "similarity": 0.754434224610974
    },
    {
      "doc": 523,
      "topic": 8,
      "similarity": 0.7504309207015124
    },
    {
      "doc": 523,
      "topic": 9,
      "similarity": 0.8177072808679763
    },
    {
      "doc": 523,
      "topic": 10,
      "similarity": 0.7639755037988466
    },
    {
      "doc": 523,
      "topic": 11,
      "similarity": 0.7566806900494752
    },
    {
      "doc": 523,
      "topic": 14,
      "similarity": 0.7540766766650263
    },
    {
      "doc": 523,
      "topic": 15,
      "similarity": 0.7619202718655511
    },
    {
      "doc": 523,
      "topic": 16,
      "similarity": 0.7655727122912547
    },
    {
      "doc": 523,
      "topic": 17,
      "similarity": 0.7854643103175756
    },
    {
      "doc": 523,
      "topic": 18,
      "similarity": 0.7566390961557793
    },
    {
      "doc": 523,
      "topic": 19,
      "similarity": 0.7831406014184367
    },
    {
      "doc": 523,
      "topic": 20,
      "similarity": 0.7751199747437376
    },
    {
      "doc": 523,
      "topic": 21,
      "similarity": 0.803979592336551
    },
    {
      "doc": 523,
      "topic": 23,
      "similarity": 0.7716987875782078
    },
    {
      "doc": 523,
      "topic": 24,
      "similarity": 0.7577841855234048
    },
    {
      "doc": 524,
      "topic": 2,
      "similarity": 0.7556009610487097
    },
    {
      "doc": 524,
      "topic": 3,
      "similarity": 0.8019011187276617
    },
    {
      "doc": 524,
      "topic": 5,
      "similarity": 0.7761781104258623
    },
    {
      "doc": 524,
      "topic": 7,
      "similarity": 0.7736383340474743
    },
    {
      "doc": 524,
      "topic": 8,
      "similarity": 0.7684366528844289
    },
    {
      "doc": 524,
      "topic": 9,
      "similarity": 0.8005672530516211
    },
    {
      "doc": 524,
      "topic": 10,
      "similarity": 0.7630959037551782
    },
    {
      "doc": 524,
      "topic": 11,
      "similarity": 0.7747779183439051
    },
    {
      "doc": 524,
      "topic": 12,
      "similarity": 0.7582250988840015
    },
    {
      "doc": 524,
      "topic": 15,
      "similarity": 0.7582654957731219
    },
    {
      "doc": 524,
      "topic": 16,
      "similarity": 0.8128706708324247
    },
    {
      "doc": 524,
      "topic": 17,
      "similarity": 0.7670465099908303
    },
    {
      "doc": 524,
      "topic": 18,
      "similarity": 0.766278508390548
    },
    {
      "doc": 524,
      "topic": 19,
      "similarity": 0.7844785976701953
    },
    {
      "doc": 524,
      "topic": 20,
      "similarity": 0.7678033555771437
    },
    {
      "doc": 524,
      "topic": 21,
      "similarity": 0.7819999814255609
    },
    {
      "doc": 524,
      "topic": 24,
      "similarity": 0.7682685634247559
    },
    {
      "doc": 525,
      "topic": 2,
      "similarity": 0.7578829473037596
    },
    {
      "doc": 525,
      "topic": 3,
      "similarity": 0.7826348119225521
    },
    {
      "doc": 525,
      "topic": 5,
      "similarity": 0.7717460232526209
    },
    {
      "doc": 525,
      "topic": 6,
      "similarity": 0.8228781794058995
    },
    {
      "doc": 525,
      "topic": 7,
      "similarity": 0.7965124165109662
    },
    {
      "doc": 525,
      "topic": 8,
      "similarity": 0.7720269111620525
    },
    {
      "doc": 525,
      "topic": 9,
      "similarity": 0.8109658612407452
    },
    {
      "doc": 525,
      "topic": 10,
      "similarity": 0.753350712417244
    },
    {
      "doc": 525,
      "topic": 11,
      "similarity": 0.7577931882002236
    },
    {
      "doc": 525,
      "topic": 13,
      "similarity": 0.7782630718276756
    },
    {
      "doc": 525,
      "topic": 14,
      "similarity": 0.764518545465193
    },
    {
      "doc": 525,
      "topic": 15,
      "similarity": 0.7615001559260787
    },
    {
      "doc": 525,
      "topic": 16,
      "similarity": 0.772524080512584
    },
    {
      "doc": 525,
      "topic": 17,
      "similarity": 0.79550556336914
    },
    {
      "doc": 525,
      "topic": 19,
      "similarity": 0.7835990283231551
    },
    {
      "doc": 525,
      "topic": 21,
      "similarity": 0.7844101840716386
    },
    {
      "doc": 526,
      "topic": 3,
      "similarity": 0.7598674403891399
    },
    {
      "doc": 526,
      "topic": 5,
      "similarity": 0.7562091217066286
    },
    {
      "doc": 526,
      "topic": 9,
      "similarity": 0.7850183951696955
    },
    {
      "doc": 526,
      "topic": 16,
      "similarity": 0.7715757430503438
    },
    {
      "doc": 526,
      "topic": 17,
      "similarity": 0.7600235829280165
    },
    {
      "doc": 526,
      "topic": 19,
      "similarity": 0.7680466503981335
    },
    {
      "doc": 526,
      "topic": 21,
      "similarity": 0.7736665689295114
    },
    {
      "doc": 527,
      "topic": 2,
      "similarity": 0.7657551446480156
    },
    {
      "doc": 527,
      "topic": 3,
      "similarity": 0.768619835974795
    },
    {
      "doc": 527,
      "topic": 5,
      "similarity": 0.7811615400590788
    },
    {
      "doc": 527,
      "topic": 8,
      "similarity": 0.7692706026364351
    },
    {
      "doc": 527,
      "topic": 9,
      "similarity": 0.8062234669234418
    },
    {
      "doc": 527,
      "topic": 10,
      "similarity": 0.7559729746603812
    },
    {
      "doc": 527,
      "topic": 12,
      "similarity": 0.7672518165568666
    },
    {
      "doc": 527,
      "topic": 14,
      "similarity": 0.756182177776398
    },
    {
      "doc": 527,
      "topic": 15,
      "similarity": 0.7621331760228003
    },
    {
      "doc": 527,
      "topic": 16,
      "similarity": 0.7689440622034469
    },
    {
      "doc": 527,
      "topic": 17,
      "similarity": 0.7745380272840235
    },
    {
      "doc": 527,
      "topic": 18,
      "similarity": 0.7673169905122833
    },
    {
      "doc": 527,
      "topic": 19,
      "similarity": 0.7947061704834977
    },
    {
      "doc": 527,
      "topic": 20,
      "similarity": 0.799507825138805
    },
    {
      "doc": 527,
      "topic": 21,
      "similarity": 0.7785142366567891
    },
    {
      "doc": 527,
      "topic": 22,
      "similarity": 0.7658886286390546
    },
    {
      "doc": 527,
      "topic": 23,
      "similarity": 0.7819749301068808
    },
    {
      "doc": 528,
      "topic": 1,
      "similarity": 0.7687495919441106
    },
    {
      "doc": 528,
      "topic": 3,
      "similarity": 0.8044579875583703
    },
    {
      "doc": 528,
      "topic": 5,
      "similarity": 0.7577429100110564
    },
    {
      "doc": 528,
      "topic": 7,
      "similarity": 0.7527371210568797
    },
    {
      "doc": 528,
      "topic": 8,
      "similarity": 0.7506270655520693
    },
    {
      "doc": 528,
      "topic": 9,
      "similarity": 0.7872221271072096
    },
    {
      "doc": 528,
      "topic": 11,
      "similarity": 0.7759067408816849
    },
    {
      "doc": 528,
      "topic": 15,
      "similarity": 0.7560332301704654
    },
    {
      "doc": 528,
      "topic": 16,
      "similarity": 0.7700796624686436
    },
    {
      "doc": 528,
      "topic": 17,
      "similarity": 0.7781234762257769
    },
    {
      "doc": 528,
      "topic": 19,
      "similarity": 0.7929095242818113
    },
    {
      "doc": 528,
      "topic": 20,
      "similarity": 0.7554006830505093
    },
    {
      "doc": 528,
      "topic": 21,
      "similarity": 0.7654768813673296
    },
    {
      "doc": 528,
      "topic": 23,
      "similarity": 0.7521172974481857
    },
    {
      "doc": 529,
      "topic": 1,
      "similarity": 0.7612742888906018
    },
    {
      "doc": 529,
      "topic": 2,
      "similarity": 0.7643341921764802
    },
    {
      "doc": 529,
      "topic": 3,
      "similarity": 0.814717488791465
    },
    {
      "doc": 529,
      "topic": 4,
      "similarity": 0.7626656915659488
    },
    {
      "doc": 529,
      "topic": 5,
      "similarity": 0.8081838773850984
    },
    {
      "doc": 529,
      "topic": 7,
      "similarity": 0.8323773343432807
    },
    {
      "doc": 529,
      "topic": 8,
      "similarity": 0.7806525871382906
    },
    {
      "doc": 529,
      "topic": 9,
      "similarity": 0.8718805988936076
    },
    {
      "doc": 529,
      "topic": 10,
      "similarity": 0.7941251093818651
    },
    {
      "doc": 529,
      "topic": 11,
      "similarity": 0.7918377258858259
    },
    {
      "doc": 529,
      "topic": 13,
      "similarity": 0.7670399441423847
    },
    {
      "doc": 529,
      "topic": 14,
      "similarity": 0.7814148608192084
    },
    {
      "doc": 529,
      "topic": 15,
      "similarity": 0.7902599965126591
    },
    {
      "doc": 529,
      "topic": 16,
      "similarity": 0.7972033466328207
    },
    {
      "doc": 529,
      "topic": 17,
      "similarity": 0.7988053529984016
    },
    {
      "doc": 529,
      "topic": 19,
      "similarity": 0.8151882797731906
    },
    {
      "doc": 529,
      "topic": 20,
      "similarity": 0.7616295590480768
    },
    {
      "doc": 529,
      "topic": 21,
      "similarity": 0.806122023724775
    },
    {
      "doc": 530,
      "topic": 1,
      "similarity": 0.7564528043696723
    },
    {
      "doc": 530,
      "topic": 2,
      "similarity": 0.776031049777513
    },
    {
      "doc": 530,
      "topic": 3,
      "similarity": 0.8064089468778374
    },
    {
      "doc": 530,
      "topic": 4,
      "similarity": 0.7822645007569465
    },
    {
      "doc": 530,
      "topic": 5,
      "similarity": 0.8284626567574008
    },
    {
      "doc": 530,
      "topic": 7,
      "similarity": 0.7924617544553768
    },
    {
      "doc": 530,
      "topic": 8,
      "similarity": 0.8080850512597392
    },
    {
      "doc": 530,
      "topic": 9,
      "similarity": 0.8306994275709794
    },
    {
      "doc": 530,
      "topic": 10,
      "similarity": 0.8295416768930459
    },
    {
      "doc": 530,
      "topic": 11,
      "similarity": 0.7938186910600961
    },
    {
      "doc": 530,
      "topic": 13,
      "similarity": 0.8002865925908582
    },
    {
      "doc": 530,
      "topic": 14,
      "similarity": 0.7885116395218189
    },
    {
      "doc": 530,
      "topic": 15,
      "similarity": 0.7738255635052996
    },
    {
      "doc": 530,
      "topic": 16,
      "similarity": 0.8207026525449652
    },
    {
      "doc": 530,
      "topic": 17,
      "similarity": 0.802220795523074
    },
    {
      "doc": 530,
      "topic": 18,
      "similarity": 0.759597036652324
    },
    {
      "doc": 530,
      "topic": 19,
      "similarity": 0.8172193084462942
    },
    {
      "doc": 530,
      "topic": 20,
      "similarity": 0.7901083342974141
    },
    {
      "doc": 530,
      "topic": 21,
      "similarity": 0.8121494336008491
    },
    {
      "doc": 530,
      "topic": 23,
      "similarity": 0.7582647039004039
    },
    {
      "doc": 530,
      "topic": 24,
      "similarity": 0.762582022524647
    },
    {
      "doc": 531,
      "topic": 1,
      "similarity": 0.7670530487048663
    },
    {
      "doc": 531,
      "topic": 2,
      "similarity": 0.7764295605263063
    },
    {
      "doc": 531,
      "topic": 3,
      "similarity": 0.7951301297638382
    },
    {
      "doc": 531,
      "topic": 4,
      "similarity": 0.7519372157448877
    },
    {
      "doc": 531,
      "topic": 5,
      "similarity": 0.7989705315724259
    },
    {
      "doc": 531,
      "topic": 6,
      "similarity": 0.8172520476248974
    },
    {
      "doc": 531,
      "topic": 7,
      "similarity": 0.7625161825231042
    },
    {
      "doc": 531,
      "topic": 8,
      "similarity": 0.7680586821373365
    },
    {
      "doc": 531,
      "topic": 9,
      "similarity": 0.7927436223230621
    },
    {
      "doc": 531,
      "topic": 10,
      "similarity": 0.809016034700613
    },
    {
      "doc": 531,
      "topic": 11,
      "similarity": 0.7796855475493089
    },
    {
      "doc": 531,
      "topic": 13,
      "similarity": 0.7760639781291039
    },
    {
      "doc": 531,
      "topic": 14,
      "similarity": 0.7778255512277383
    },
    {
      "doc": 531,
      "topic": 15,
      "similarity": 0.7704224325769286
    },
    {
      "doc": 531,
      "topic": 16,
      "similarity": 0.797125504858824
    },
    {
      "doc": 531,
      "topic": 17,
      "similarity": 0.8018804585089359
    },
    {
      "doc": 531,
      "topic": 18,
      "similarity": 0.8072762470080124
    },
    {
      "doc": 531,
      "topic": 19,
      "similarity": 0.8070635482630564
    },
    {
      "doc": 531,
      "topic": 20,
      "similarity": 0.7644158803959293
    },
    {
      "doc": 531,
      "topic": 21,
      "similarity": 0.7954540912663596
    },
    {
      "doc": 531,
      "topic": 23,
      "similarity": 0.7739328647660298
    },
    {
      "doc": 532,
      "topic": 1,
      "similarity": 0.7629547177917018
    },
    {
      "doc": 532,
      "topic": 2,
      "similarity": 0.7869975328430063
    },
    {
      "doc": 532,
      "topic": 3,
      "similarity": 0.834333775855054
    },
    {
      "doc": 532,
      "topic": 4,
      "similarity": 0.7558957848059562
    },
    {
      "doc": 532,
      "topic": 5,
      "similarity": 0.7983454630147928
    },
    {
      "doc": 532,
      "topic": 7,
      "similarity": 0.7958161454732238
    },
    {
      "doc": 532,
      "topic": 8,
      "similarity": 0.775260766334613
    },
    {
      "doc": 532,
      "topic": 9,
      "similarity": 0.8299540168988448
    },
    {
      "doc": 532,
      "topic": 10,
      "similarity": 0.8028073498944981
    },
    {
      "doc": 532,
      "topic": 11,
      "similarity": 0.7875067402543405
    },
    {
      "doc": 532,
      "topic": 13,
      "similarity": 0.7760100582148133
    },
    {
      "doc": 532,
      "topic": 14,
      "similarity": 0.8046062167878179
    },
    {
      "doc": 532,
      "topic": 15,
      "similarity": 0.789295292871923
    },
    {
      "doc": 532,
      "topic": 16,
      "similarity": 0.8046399724161075
    },
    {
      "doc": 532,
      "topic": 17,
      "similarity": 0.7907148591288385
    },
    {
      "doc": 532,
      "topic": 18,
      "similarity": 0.8090071038178156
    },
    {
      "doc": 532,
      "topic": 19,
      "similarity": 0.8482588902250383
    },
    {
      "doc": 532,
      "topic": 20,
      "similarity": 0.7884861669540328
    },
    {
      "doc": 532,
      "topic": 21,
      "similarity": 0.7960507974635805
    },
    {
      "doc": 532,
      "topic": 23,
      "similarity": 0.768560524355552
    },
    {
      "doc": 533,
      "topic": 1,
      "similarity": 0.751136550428401
    },
    {
      "doc": 533,
      "topic": 2,
      "similarity": 0.7800136218153815
    },
    {
      "doc": 533,
      "topic": 3,
      "similarity": 0.7953374750733364
    },
    {
      "doc": 533,
      "topic": 5,
      "similarity": 0.7833730891487154
    },
    {
      "doc": 533,
      "topic": 7,
      "similarity": 0.7579154701235983
    },
    {
      "doc": 533,
      "topic": 8,
      "similarity": 0.7850606898639537
    },
    {
      "doc": 533,
      "topic": 9,
      "similarity": 0.8013876874513873
    },
    {
      "doc": 533,
      "topic": 10,
      "similarity": 0.7722508757162585
    },
    {
      "doc": 533,
      "topic": 11,
      "similarity": 0.7646715004633402
    },
    {
      "doc": 533,
      "topic": 12,
      "similarity": 0.756597590652914
    },
    {
      "doc": 533,
      "topic": 13,
      "similarity": 0.7667354823619492
    },
    {
      "doc": 533,
      "topic": 14,
      "similarity": 0.7598132396818676
    },
    {
      "doc": 533,
      "topic": 15,
      "similarity": 0.7959329977147409
    },
    {
      "doc": 533,
      "topic": 16,
      "similarity": 0.789349592093577
    },
    {
      "doc": 533,
      "topic": 17,
      "similarity": 0.7964440567947677
    },
    {
      "doc": 533,
      "topic": 18,
      "similarity": 0.8073412002658948
    },
    {
      "doc": 533,
      "topic": 19,
      "similarity": 0.8276356249542353
    },
    {
      "doc": 533,
      "topic": 20,
      "similarity": 0.8103751297349313
    },
    {
      "doc": 533,
      "topic": 21,
      "similarity": 0.8067047517587105
    },
    {
      "doc": 533,
      "topic": 22,
      "similarity": 0.7525545975593885
    },
    {
      "doc": 533,
      "topic": 23,
      "similarity": 0.7725545521010897
    },
    {
      "doc": 534,
      "topic": 1,
      "similarity": 0.7701709371579533
    },
    {
      "doc": 534,
      "topic": 2,
      "similarity": 0.7690555044473499
    },
    {
      "doc": 534,
      "topic": 3,
      "similarity": 0.7950135223807666
    },
    {
      "doc": 534,
      "topic": 4,
      "similarity": 0.7537991876145037
    },
    {
      "doc": 534,
      "topic": 5,
      "similarity": 0.7924650607515412
    },
    {
      "doc": 534,
      "topic": 6,
      "similarity": 0.8496925622517489
    },
    {
      "doc": 534,
      "topic": 7,
      "similarity": 0.76816158862504
    },
    {
      "doc": 534,
      "topic": 8,
      "similarity": 0.7561209844598915
    },
    {
      "doc": 534,
      "topic": 9,
      "similarity": 0.8265515029441751
    },
    {
      "doc": 534,
      "topic": 10,
      "similarity": 0.7980702210106885
    },
    {
      "doc": 534,
      "topic": 11,
      "similarity": 0.776267650357498
    },
    {
      "doc": 534,
      "topic": 13,
      "similarity": 0.7806536946536187
    },
    {
      "doc": 534,
      "topic": 14,
      "similarity": 0.7851099530272037
    },
    {
      "doc": 534,
      "topic": 15,
      "similarity": 0.7817495974595926
    },
    {
      "doc": 534,
      "topic": 16,
      "similarity": 0.8068271688634971
    },
    {
      "doc": 534,
      "topic": 17,
      "similarity": 0.8058427688430454
    },
    {
      "doc": 534,
      "topic": 18,
      "similarity": 0.7779501025044633
    },
    {
      "doc": 534,
      "topic": 19,
      "similarity": 0.8190050347946635
    },
    {
      "doc": 534,
      "topic": 20,
      "similarity": 0.7703763014185632
    },
    {
      "doc": 534,
      "topic": 21,
      "similarity": 0.8043814940054683
    },
    {
      "doc": 534,
      "topic": 23,
      "similarity": 0.8070648694042935
    },
    {
      "doc": 535,
      "topic": 1,
      "similarity": 0.7647867364760946
    },
    {
      "doc": 535,
      "topic": 2,
      "similarity": 0.7953698442915315
    },
    {
      "doc": 535,
      "topic": 3,
      "similarity": 0.8186633212834884
    },
    {
      "doc": 535,
      "topic": 4,
      "similarity": 0.7554652031610753
    },
    {
      "doc": 535,
      "topic": 5,
      "similarity": 0.8033818269462653
    },
    {
      "doc": 535,
      "topic": 7,
      "similarity": 0.8209967209189264
    },
    {
      "doc": 535,
      "topic": 8,
      "similarity": 0.8039145757969812
    },
    {
      "doc": 535,
      "topic": 9,
      "similarity": 0.8273259228683384
    },
    {
      "doc": 535,
      "topic": 10,
      "similarity": 0.8195847838046842
    },
    {
      "doc": 535,
      "topic": 11,
      "similarity": 0.7918424525579133
    },
    {
      "doc": 535,
      "topic": 12,
      "similarity": 0.76092022712137
    },
    {
      "doc": 535,
      "topic": 13,
      "similarity": 0.7718419603267098
    },
    {
      "doc": 535,
      "topic": 14,
      "similarity": 0.7984120631326997
    },
    {
      "doc": 535,
      "topic": 15,
      "similarity": 0.7735343523814794
    },
    {
      "doc": 535,
      "topic": 16,
      "similarity": 0.7935797569701966
    },
    {
      "doc": 535,
      "topic": 17,
      "similarity": 0.789123235320576
    },
    {
      "doc": 535,
      "topic": 18,
      "similarity": 0.7860888519339102
    },
    {
      "doc": 535,
      "topic": 19,
      "similarity": 0.8246939489445899
    },
    {
      "doc": 535,
      "topic": 20,
      "similarity": 0.7831856703633361
    },
    {
      "doc": 535,
      "topic": 21,
      "similarity": 0.7933968413025977
    },
    {
      "doc": 535,
      "topic": 22,
      "similarity": 0.7504019033175351
    },
    {
      "doc": 535,
      "topic": 23,
      "similarity": 0.7908851427705743
    },
    {
      "doc": 536,
      "topic": 1,
      "similarity": 0.7732108762638842
    },
    {
      "doc": 536,
      "topic": 2,
      "similarity": 0.7837893107297735
    },
    {
      "doc": 536,
      "topic": 3,
      "similarity": 0.8099387154146152
    },
    {
      "doc": 536,
      "topic": 4,
      "similarity": 0.7525499390151555
    },
    {
      "doc": 536,
      "topic": 5,
      "similarity": 0.8069816557723565
    },
    {
      "doc": 536,
      "topic": 6,
      "similarity": 0.758164166853998
    },
    {
      "doc": 536,
      "topic": 7,
      "similarity": 0.7825304028517217
    },
    {
      "doc": 536,
      "topic": 8,
      "similarity": 0.7857495436298633
    },
    {
      "doc": 536,
      "topic": 9,
      "similarity": 0.8257055586048562
    },
    {
      "doc": 536,
      "topic": 10,
      "similarity": 0.795622060941224
    },
    {
      "doc": 536,
      "topic": 11,
      "similarity": 0.8048723885289216
    },
    {
      "doc": 536,
      "topic": 12,
      "similarity": 0.7703920157139706
    },
    {
      "doc": 536,
      "topic": 13,
      "similarity": 0.7808339095029653
    },
    {
      "doc": 536,
      "topic": 14,
      "similarity": 0.7868568386063572
    },
    {
      "doc": 536,
      "topic": 15,
      "similarity": 0.7969709427678168
    },
    {
      "doc": 536,
      "topic": 16,
      "similarity": 0.8333587386216306
    },
    {
      "doc": 536,
      "topic": 17,
      "similarity": 0.8240540218106529
    },
    {
      "doc": 536,
      "topic": 18,
      "similarity": 0.7985198912784373
    },
    {
      "doc": 536,
      "topic": 19,
      "similarity": 0.863073609704719
    },
    {
      "doc": 536,
      "topic": 20,
      "similarity": 0.7994881877385139
    },
    {
      "doc": 536,
      "topic": 21,
      "similarity": 0.8016331642361
    },
    {
      "doc": 536,
      "topic": 22,
      "similarity": 0.758931434668085
    },
    {
      "doc": 536,
      "topic": 23,
      "similarity": 0.7622321607736301
    },
    {
      "doc": 537,
      "topic": 0,
      "similarity": 0.772584884163484
    },
    {
      "doc": 537,
      "topic": 2,
      "similarity": 0.7525855351636932
    },
    {
      "doc": 537,
      "topic": 3,
      "similarity": 0.7782014281020094
    },
    {
      "doc": 537,
      "topic": 5,
      "similarity": 0.7671918017313306
    },
    {
      "doc": 537,
      "topic": 6,
      "similarity": 0.7511510516793671
    },
    {
      "doc": 537,
      "topic": 7,
      "similarity": 0.7657334317193015
    },
    {
      "doc": 537,
      "topic": 9,
      "similarity": 0.8379331470007629
    },
    {
      "doc": 537,
      "topic": 10,
      "similarity": 0.755937106680348
    },
    {
      "doc": 537,
      "topic": 11,
      "similarity": 0.8034804580832812
    },
    {
      "doc": 537,
      "topic": 14,
      "similarity": 0.7629280866374273
    },
    {
      "doc": 537,
      "topic": 15,
      "similarity": 0.7711025640658908
    },
    {
      "doc": 537,
      "topic": 16,
      "similarity": 0.7817497522490573
    },
    {
      "doc": 537,
      "topic": 17,
      "similarity": 0.7650578310007838
    },
    {
      "doc": 537,
      "topic": 18,
      "similarity": 0.7675632322583597
    },
    {
      "doc": 537,
      "topic": 19,
      "similarity": 0.7809369355181188
    },
    {
      "doc": 537,
      "topic": 20,
      "similarity": 0.7817639359078578
    },
    {
      "doc": 537,
      "topic": 21,
      "similarity": 0.7859123504076078
    },
    {
      "doc": 537,
      "topic": 23,
      "similarity": 0.7841507405221931
    },
    {
      "doc": 537,
      "topic": 24,
      "similarity": 0.7744865926970445
    },
    {
      "doc": 538,
      "topic": 2,
      "similarity": 0.7705334066398687
    },
    {
      "doc": 538,
      "topic": 3,
      "similarity": 0.7835397549178744
    },
    {
      "doc": 538,
      "topic": 5,
      "similarity": 0.7569765031117669
    },
    {
      "doc": 538,
      "topic": 7,
      "similarity": 0.7535839858345397
    },
    {
      "doc": 538,
      "topic": 9,
      "similarity": 0.8200258299073265
    },
    {
      "doc": 538,
      "topic": 10,
      "similarity": 0.7759156521884705
    },
    {
      "doc": 538,
      "topic": 11,
      "similarity": 0.764763203862811
    },
    {
      "doc": 538,
      "topic": 14,
      "similarity": 0.774418703672819
    },
    {
      "doc": 538,
      "topic": 15,
      "similarity": 0.7875095004256532
    },
    {
      "doc": 538,
      "topic": 16,
      "similarity": 0.7801635062304156
    },
    {
      "doc": 538,
      "topic": 17,
      "similarity": 0.775849681609082
    },
    {
      "doc": 538,
      "topic": 18,
      "similarity": 0.755466487301594
    },
    {
      "doc": 538,
      "topic": 19,
      "similarity": 0.7968849723285151
    },
    {
      "doc": 538,
      "topic": 20,
      "similarity": 0.772576792509727
    },
    {
      "doc": 538,
      "topic": 21,
      "similarity": 0.7860441861878894
    },
    {
      "doc": 538,
      "topic": 23,
      "similarity": 0.7576666144322407
    },
    {
      "doc": 538,
      "topic": 24,
      "similarity": 0.769947887744106
    },
    {
      "doc": 539,
      "topic": 1,
      "similarity": 0.7749617955666336
    },
    {
      "doc": 539,
      "topic": 2,
      "similarity": 0.7709685224101034
    },
    {
      "doc": 539,
      "topic": 3,
      "similarity": 0.8203649193554554
    },
    {
      "doc": 539,
      "topic": 4,
      "similarity": 0.7645709293391986
    },
    {
      "doc": 539,
      "topic": 5,
      "similarity": 0.7901886671324276
    },
    {
      "doc": 539,
      "topic": 7,
      "similarity": 0.7877053484543536
    },
    {
      "doc": 539,
      "topic": 8,
      "similarity": 0.7748685104122182
    },
    {
      "doc": 539,
      "topic": 9,
      "similarity": 0.8131180988072794
    },
    {
      "doc": 539,
      "topic": 10,
      "similarity": 0.7938610444631122
    },
    {
      "doc": 539,
      "topic": 11,
      "similarity": 0.7916688373501188
    },
    {
      "doc": 539,
      "topic": 13,
      "similarity": 0.7696327237178985
    },
    {
      "doc": 539,
      "topic": 14,
      "similarity": 0.788790217845606
    },
    {
      "doc": 539,
      "topic": 15,
      "similarity": 0.7799754649130216
    },
    {
      "doc": 539,
      "topic": 16,
      "similarity": 0.8028575199378751
    },
    {
      "doc": 539,
      "topic": 17,
      "similarity": 0.7776836356664061
    },
    {
      "doc": 539,
      "topic": 18,
      "similarity": 0.7936802514441124
    },
    {
      "doc": 539,
      "topic": 19,
      "similarity": 0.8364280376307338
    },
    {
      "doc": 539,
      "topic": 20,
      "similarity": 0.7578400917377274
    },
    {
      "doc": 539,
      "topic": 21,
      "similarity": 0.7878829892456306
    },
    {
      "doc": 539,
      "topic": 23,
      "similarity": 0.7825327418836096
    },
    {
      "doc": 540,
      "topic": 2,
      "similarity": 0.7721185901921076
    },
    {
      "doc": 540,
      "topic": 3,
      "similarity": 0.7746280575206616
    },
    {
      "doc": 540,
      "topic": 5,
      "similarity": 0.7776288651467134
    },
    {
      "doc": 540,
      "topic": 7,
      "similarity": 0.8108525633148207
    },
    {
      "doc": 540,
      "topic": 8,
      "similarity": 0.7905995428322949
    },
    {
      "doc": 540,
      "topic": 9,
      "similarity": 0.8144575710554661
    },
    {
      "doc": 540,
      "topic": 10,
      "similarity": 0.764054883503866
    },
    {
      "doc": 540,
      "topic": 11,
      "similarity": 0.7676466666309398
    },
    {
      "doc": 540,
      "topic": 13,
      "similarity": 0.7674529425830298
    },
    {
      "doc": 540,
      "topic": 15,
      "similarity": 0.7666243894707452
    },
    {
      "doc": 540,
      "topic": 16,
      "similarity": 0.8147539665562381
    },
    {
      "doc": 540,
      "topic": 17,
      "similarity": 0.7864919506348969
    },
    {
      "doc": 540,
      "topic": 19,
      "similarity": 0.7793755445898808
    },
    {
      "doc": 540,
      "topic": 21,
      "similarity": 0.8070957438470586
    },
    {
      "doc": 541,
      "topic": 3,
      "similarity": 0.7754986267091144
    },
    {
      "doc": 541,
      "topic": 5,
      "similarity": 0.7532763596275389
    },
    {
      "doc": 541,
      "topic": 7,
      "similarity": 0.7535012628244515
    },
    {
      "doc": 541,
      "topic": 9,
      "similarity": 0.7892110482768573
    },
    {
      "doc": 541,
      "topic": 17,
      "similarity": 0.7587108111374248
    },
    {
      "doc": 541,
      "topic": 18,
      "similarity": 0.7565974320115485
    },
    {
      "doc": 541,
      "topic": 19,
      "similarity": 0.7830207929756287
    },
    {
      "doc": 541,
      "topic": 20,
      "similarity": 0.7560557929334509
    },
    {
      "doc": 541,
      "topic": 21,
      "similarity": 0.7504325540022756
    },
    {
      "doc": 541,
      "topic": 23,
      "similarity": 0.7508373279887316
    },
    {
      "doc": 542,
      "topic": 1,
      "similarity": 0.7632481244216786
    },
    {
      "doc": 542,
      "topic": 2,
      "similarity": 0.7749307714324167
    },
    {
      "doc": 542,
      "topic": 3,
      "similarity": 0.8175146992868617
    },
    {
      "doc": 542,
      "topic": 4,
      "similarity": 0.762244816043663
    },
    {
      "doc": 542,
      "topic": 5,
      "similarity": 0.8029346152979192
    },
    {
      "doc": 542,
      "topic": 7,
      "similarity": 0.7807955795227113
    },
    {
      "doc": 542,
      "topic": 8,
      "similarity": 0.7898032967449055
    },
    {
      "doc": 542,
      "topic": 9,
      "similarity": 0.8251789003767384
    },
    {
      "doc": 542,
      "topic": 10,
      "similarity": 0.7869568049252063
    },
    {
      "doc": 542,
      "topic": 11,
      "similarity": 0.8085354996674339
    },
    {
      "doc": 542,
      "topic": 13,
      "similarity": 0.7823082443282393
    },
    {
      "doc": 542,
      "topic": 14,
      "similarity": 0.7860998930071579
    },
    {
      "doc": 542,
      "topic": 15,
      "similarity": 0.7878277778880465
    },
    {
      "doc": 542,
      "topic": 16,
      "similarity": 0.805937245368926
    },
    {
      "doc": 542,
      "topic": 17,
      "similarity": 0.790672712378291
    },
    {
      "doc": 542,
      "topic": 18,
      "similarity": 0.7628345879228496
    },
    {
      "doc": 542,
      "topic": 19,
      "similarity": 0.8141450319492973
    },
    {
      "doc": 542,
      "topic": 20,
      "similarity": 0.7809049740181934
    },
    {
      "doc": 542,
      "topic": 21,
      "similarity": 0.8067210253870566
    },
    {
      "doc": 542,
      "topic": 23,
      "similarity": 0.7680672954228944
    },
    {
      "doc": 543,
      "topic": 2,
      "similarity": 0.7605042825394581
    },
    {
      "doc": 543,
      "topic": 3,
      "similarity": 0.7777149243310356
    },
    {
      "doc": 543,
      "topic": 4,
      "similarity": 0.7605368470445651
    },
    {
      "doc": 543,
      "topic": 5,
      "similarity": 0.8011226896521166
    },
    {
      "doc": 543,
      "topic": 7,
      "similarity": 0.7671436026471926
    },
    {
      "doc": 543,
      "topic": 8,
      "similarity": 0.7623613278151163
    },
    {
      "doc": 543,
      "topic": 9,
      "similarity": 0.8033899479516511
    },
    {
      "doc": 543,
      "topic": 10,
      "similarity": 0.8259285868656637
    },
    {
      "doc": 543,
      "topic": 11,
      "similarity": 0.7554434658595613
    },
    {
      "doc": 543,
      "topic": 13,
      "similarity": 0.7513807832451782
    },
    {
      "doc": 543,
      "topic": 14,
      "similarity": 0.7677677330605875
    },
    {
      "doc": 543,
      "topic": 15,
      "similarity": 0.7682146070640186
    },
    {
      "doc": 543,
      "topic": 16,
      "similarity": 0.7764148034237176
    },
    {
      "doc": 543,
      "topic": 17,
      "similarity": 0.7765884908650296
    },
    {
      "doc": 543,
      "topic": 18,
      "similarity": 0.7756658241643208
    },
    {
      "doc": 543,
      "topic": 19,
      "similarity": 0.8048066088751797
    },
    {
      "doc": 543,
      "topic": 20,
      "similarity": 0.7572729395212872
    },
    {
      "doc": 543,
      "topic": 21,
      "similarity": 0.7935280419202306
    },
    {
      "doc": 544,
      "topic": 2,
      "similarity": 0.7652595006959355
    },
    {
      "doc": 544,
      "topic": 3,
      "similarity": 0.7895997545714061
    },
    {
      "doc": 544,
      "topic": 5,
      "similarity": 0.7837103763618097
    },
    {
      "doc": 544,
      "topic": 6,
      "similarity": 0.7576929671650576
    },
    {
      "doc": 544,
      "topic": 7,
      "similarity": 0.7643820395404295
    },
    {
      "doc": 544,
      "topic": 8,
      "similarity": 0.7679507568451008
    },
    {
      "doc": 544,
      "topic": 9,
      "similarity": 0.7985903926763267
    },
    {
      "doc": 544,
      "topic": 10,
      "similarity": 0.7699066202712288
    },
    {
      "doc": 544,
      "topic": 11,
      "similarity": 0.7854505174606788
    },
    {
      "doc": 544,
      "topic": 13,
      "similarity": 0.7562888540946195
    },
    {
      "doc": 544,
      "topic": 14,
      "similarity": 0.7605949373299438
    },
    {
      "doc": 544,
      "topic": 15,
      "similarity": 0.7814108145959321
    },
    {
      "doc": 544,
      "topic": 16,
      "similarity": 0.8069006744412249
    },
    {
      "doc": 544,
      "topic": 17,
      "similarity": 0.8069998228228805
    },
    {
      "doc": 544,
      "topic": 18,
      "similarity": 0.8026124908700372
    },
    {
      "doc": 544,
      "topic": 19,
      "similarity": 0.8372345954578584
    },
    {
      "doc": 544,
      "topic": 20,
      "similarity": 0.7814974466009498
    },
    {
      "doc": 544,
      "topic": 21,
      "similarity": 0.7905945797321942
    },
    {
      "doc": 544,
      "topic": 23,
      "similarity": 0.7720888397672196
    },
    {
      "doc": 545,
      "topic": 3,
      "similarity": 0.7509699973418736
    },
    {
      "doc": 545,
      "topic": 5,
      "similarity": 0.7603495685116247
    },
    {
      "doc": 545,
      "topic": 9,
      "similarity": 0.8250123172684624
    },
    {
      "doc": 545,
      "topic": 16,
      "similarity": 0.7677780393922045
    },
    {
      "doc": 545,
      "topic": 17,
      "similarity": 0.7572825934350761
    },
    {
      "doc": 545,
      "topic": 19,
      "similarity": 0.7641336474650723
    },
    {
      "doc": 545,
      "topic": 21,
      "similarity": 0.7703290835820862
    },
    {
      "doc": 545,
      "topic": 24,
      "similarity": 0.7713135356933811
    },
    {
      "doc": 546,
      "topic": 1,
      "similarity": 0.75810010517603
    },
    {
      "doc": 546,
      "topic": 2,
      "similarity": 0.7900040227613444
    },
    {
      "doc": 546,
      "topic": 3,
      "similarity": 0.8219585451896647
    },
    {
      "doc": 546,
      "topic": 4,
      "similarity": 0.770074183511585
    },
    {
      "doc": 546,
      "topic": 5,
      "similarity": 0.8031944883729306
    },
    {
      "doc": 546,
      "topic": 7,
      "similarity": 0.8059564506095239
    },
    {
      "doc": 546,
      "topic": 8,
      "similarity": 0.7942295475435531
    },
    {
      "doc": 546,
      "topic": 9,
      "similarity": 0.8382356947491263
    },
    {
      "doc": 546,
      "topic": 10,
      "similarity": 0.7883275497134339
    },
    {
      "doc": 546,
      "topic": 11,
      "similarity": 0.8038506043576211
    },
    {
      "doc": 546,
      "topic": 12,
      "similarity": 0.7529061196036302
    },
    {
      "doc": 546,
      "topic": 13,
      "similarity": 0.7841654536805405
    },
    {
      "doc": 546,
      "topic": 14,
      "similarity": 0.7861623333803777
    },
    {
      "doc": 546,
      "topic": 15,
      "similarity": 0.8115855066483684
    },
    {
      "doc": 546,
      "topic": 16,
      "similarity": 0.8217882421052891
    },
    {
      "doc": 546,
      "topic": 17,
      "similarity": 0.8170746252032183
    },
    {
      "doc": 546,
      "topic": 18,
      "similarity": 0.7998772182081167
    },
    {
      "doc": 546,
      "topic": 19,
      "similarity": 0.843853428167744
    },
    {
      "doc": 546,
      "topic": 20,
      "similarity": 0.7927885959924412
    },
    {
      "doc": 546,
      "topic": 21,
      "similarity": 0.8083317899688766
    },
    {
      "doc": 546,
      "topic": 22,
      "similarity": 0.756078164269377
    },
    {
      "doc": 547,
      "topic": 1,
      "similarity": 0.7796265745317663
    },
    {
      "doc": 547,
      "topic": 2,
      "similarity": 0.8065094904396313
    },
    {
      "doc": 547,
      "topic": 3,
      "similarity": 0.8117893538049517
    },
    {
      "doc": 547,
      "topic": 4,
      "similarity": 0.7809945272656584
    },
    {
      "doc": 547,
      "topic": 5,
      "similarity": 0.8247254567190551
    },
    {
      "doc": 547,
      "topic": 7,
      "similarity": 0.8138152829928259
    },
    {
      "doc": 547,
      "topic": 8,
      "similarity": 0.8026021381217912
    },
    {
      "doc": 547,
      "topic": 9,
      "similarity": 0.8551190763467685
    },
    {
      "doc": 547,
      "topic": 10,
      "similarity": 0.8053097723151004
    },
    {
      "doc": 547,
      "topic": 11,
      "similarity": 0.8098916212768522
    },
    {
      "doc": 547,
      "topic": 13,
      "similarity": 0.7836005518640217
    },
    {
      "doc": 547,
      "topic": 14,
      "similarity": 0.7973536449346601
    },
    {
      "doc": 547,
      "topic": 15,
      "similarity": 0.7980276004295034
    },
    {
      "doc": 547,
      "topic": 16,
      "similarity": 0.8365022061237198
    },
    {
      "doc": 547,
      "topic": 17,
      "similarity": 0.8297475337790761
    },
    {
      "doc": 547,
      "topic": 18,
      "similarity": 0.753514381573973
    },
    {
      "doc": 547,
      "topic": 19,
      "similarity": 0.813554527167302
    },
    {
      "doc": 547,
      "topic": 20,
      "similarity": 0.7944674850068187
    },
    {
      "doc": 547,
      "topic": 21,
      "similarity": 0.8275390971725686
    },
    {
      "doc": 547,
      "topic": 24,
      "similarity": 0.768121664784693
    },
    {
      "doc": 548,
      "topic": 3,
      "similarity": 0.7622843022024184
    },
    {
      "doc": 548,
      "topic": 5,
      "similarity": 0.7655804050689182
    },
    {
      "doc": 548,
      "topic": 9,
      "similarity": 0.7872702850137541
    },
    {
      "doc": 548,
      "topic": 10,
      "similarity": 0.7982606326431658
    },
    {
      "doc": 548,
      "topic": 15,
      "similarity": 0.7536642722169578
    },
    {
      "doc": 548,
      "topic": 16,
      "similarity": 0.7696498380267305
    },
    {
      "doc": 548,
      "topic": 17,
      "similarity": 0.785846935758951
    },
    {
      "doc": 548,
      "topic": 19,
      "similarity": 0.7763429638065953
    },
    {
      "doc": 548,
      "topic": 20,
      "similarity": 0.7713258521971103
    },
    {
      "doc": 548,
      "topic": 21,
      "similarity": 0.7799631817595193
    },
    {
      "doc": 549,
      "topic": 2,
      "similarity": 0.7582329437267856
    },
    {
      "doc": 549,
      "topic": 3,
      "similarity": 0.7946204601004921
    },
    {
      "doc": 549,
      "topic": 4,
      "similarity": 0.7506808003400878
    },
    {
      "doc": 549,
      "topic": 5,
      "similarity": 0.7737618953285849
    },
    {
      "doc": 549,
      "topic": 7,
      "similarity": 0.7749118313806731
    },
    {
      "doc": 549,
      "topic": 8,
      "similarity": 0.7589941550778526
    },
    {
      "doc": 549,
      "topic": 9,
      "similarity": 0.8390061147901429
    },
    {
      "doc": 549,
      "topic": 10,
      "similarity": 0.7796076616950063
    },
    {
      "doc": 549,
      "topic": 11,
      "similarity": 0.7779169920309363
    },
    {
      "doc": 549,
      "topic": 14,
      "similarity": 0.7605593646280376
    },
    {
      "doc": 549,
      "topic": 15,
      "similarity": 0.7733780417145478
    },
    {
      "doc": 549,
      "topic": 16,
      "similarity": 0.7801396543011632
    },
    {
      "doc": 549,
      "topic": 17,
      "similarity": 0.7814223758210209
    },
    {
      "doc": 549,
      "topic": 18,
      "similarity": 0.7504042927697734
    },
    {
      "doc": 549,
      "topic": 19,
      "similarity": 0.7921665912033455
    },
    {
      "doc": 549,
      "topic": 21,
      "similarity": 0.7901460357243497
    },
    {
      "doc": 549,
      "topic": 23,
      "similarity": 0.789586600320924
    },
    {
      "doc": 549,
      "topic": 24,
      "similarity": 0.7706093339536245
    },
    {
      "doc": 550,
      "topic": 3,
      "similarity": 0.7779343248335924
    },
    {
      "doc": 550,
      "topic": 4,
      "similarity": 0.7744846183525029
    },
    {
      "doc": 550,
      "topic": 5,
      "similarity": 0.7953212574634576
    },
    {
      "doc": 550,
      "topic": 7,
      "similarity": 0.7679098532930577
    },
    {
      "doc": 550,
      "topic": 8,
      "similarity": 0.750850988492418
    },
    {
      "doc": 550,
      "topic": 9,
      "similarity": 0.7894392903559965
    },
    {
      "doc": 550,
      "topic": 10,
      "similarity": 0.7599672853171779
    },
    {
      "doc": 550,
      "topic": 11,
      "similarity": 0.7564416651886536
    },
    {
      "doc": 550,
      "topic": 13,
      "similarity": 0.7538555887752042
    },
    {
      "doc": 550,
      "topic": 15,
      "similarity": 0.7511106742864972
    },
    {
      "doc": 550,
      "topic": 16,
      "similarity": 0.7958440167285379
    },
    {
      "doc": 550,
      "topic": 17,
      "similarity": 0.7786978703879934
    },
    {
      "doc": 550,
      "topic": 19,
      "similarity": 0.780070720129049
    },
    {
      "doc": 550,
      "topic": 20,
      "similarity": 0.789840920423883
    },
    {
      "doc": 550,
      "topic": 21,
      "similarity": 0.7918737388727584
    },
    {
      "doc": 550,
      "topic": 23,
      "similarity": 0.7519320156040085
    },
    {
      "doc": 550,
      "topic": 24,
      "similarity": 0.7523293152526446
    },
    {
      "doc": 551,
      "topic": 1,
      "similarity": 0.7643793909860984
    },
    {
      "doc": 551,
      "topic": 2,
      "similarity": 0.7836004573011227
    },
    {
      "doc": 551,
      "topic": 3,
      "similarity": 0.8111868249079741
    },
    {
      "doc": 551,
      "topic": 4,
      "similarity": 0.7759586212735572
    },
    {
      "doc": 551,
      "topic": 5,
      "similarity": 0.8215474691237594
    },
    {
      "doc": 551,
      "topic": 7,
      "similarity": 0.7882990378635283
    },
    {
      "doc": 551,
      "topic": 8,
      "similarity": 0.7887951187127149
    },
    {
      "doc": 551,
      "topic": 9,
      "similarity": 0.8300998445457886
    },
    {
      "doc": 551,
      "topic": 10,
      "similarity": 0.8083343614292539
    },
    {
      "doc": 551,
      "topic": 11,
      "similarity": 0.7893778188532902
    },
    {
      "doc": 551,
      "topic": 13,
      "similarity": 0.788915523715018
    },
    {
      "doc": 551,
      "topic": 14,
      "similarity": 0.7821795400458443
    },
    {
      "doc": 551,
      "topic": 15,
      "similarity": 0.7850984268748908
    },
    {
      "doc": 551,
      "topic": 16,
      "similarity": 0.8284138923779388
    },
    {
      "doc": 551,
      "topic": 17,
      "similarity": 0.8377812349659693
    },
    {
      "doc": 551,
      "topic": 18,
      "similarity": 0.750696884199416
    },
    {
      "doc": 551,
      "topic": 19,
      "similarity": 0.8129185788995175
    },
    {
      "doc": 551,
      "topic": 20,
      "similarity": 0.7880425016313526
    },
    {
      "doc": 551,
      "topic": 21,
      "similarity": 0.8587550729451486
    },
    {
      "doc": 552,
      "topic": 2,
      "similarity": 0.7639086794751553
    },
    {
      "doc": 552,
      "topic": 3,
      "similarity": 0.7936945932413991
    },
    {
      "doc": 552,
      "topic": 5,
      "similarity": 0.7881577753714996
    },
    {
      "doc": 552,
      "topic": 7,
      "similarity": 0.7551108853591444
    },
    {
      "doc": 552,
      "topic": 8,
      "similarity": 0.7646335073496955
    },
    {
      "doc": 552,
      "topic": 9,
      "similarity": 0.7980344426459477
    },
    {
      "doc": 552,
      "topic": 10,
      "similarity": 0.7552506646700133
    },
    {
      "doc": 552,
      "topic": 11,
      "similarity": 0.7835923798963794
    },
    {
      "doc": 552,
      "topic": 14,
      "similarity": 0.7548553009296015
    },
    {
      "doc": 552,
      "topic": 15,
      "similarity": 0.7602148308816165
    },
    {
      "doc": 552,
      "topic": 16,
      "similarity": 0.773562963018327
    },
    {
      "doc": 552,
      "topic": 17,
      "similarity": 0.7682875750701565
    },
    {
      "doc": 552,
      "topic": 18,
      "similarity": 0.7808594358926771
    },
    {
      "doc": 552,
      "topic": 19,
      "similarity": 0.8189710768053194
    },
    {
      "doc": 552,
      "topic": 20,
      "similarity": 0.754474129114878
    },
    {
      "doc": 552,
      "topic": 21,
      "similarity": 0.7842275096841501
    },
    {
      "doc": 553,
      "topic": 1,
      "similarity": 0.7533571796669616
    },
    {
      "doc": 553,
      "topic": 2,
      "similarity": 0.7787379203304285
    },
    {
      "doc": 553,
      "topic": 3,
      "similarity": 0.7945099528549695
    },
    {
      "doc": 553,
      "topic": 4,
      "similarity": 0.7649045435479602
    },
    {
      "doc": 553,
      "topic": 5,
      "similarity": 0.8050754170749761
    },
    {
      "doc": 553,
      "topic": 7,
      "similarity": 0.7905727721195808
    },
    {
      "doc": 553,
      "topic": 8,
      "similarity": 0.7815653097907012
    },
    {
      "doc": 553,
      "topic": 9,
      "similarity": 0.8324334528071662
    },
    {
      "doc": 553,
      "topic": 10,
      "similarity": 0.7661226174781174
    },
    {
      "doc": 553,
      "topic": 11,
      "similarity": 0.7813659581946536
    },
    {
      "doc": 553,
      "topic": 13,
      "similarity": 0.7835455083041217
    },
    {
      "doc": 553,
      "topic": 14,
      "similarity": 0.7651430466871716
    },
    {
      "doc": 553,
      "topic": 15,
      "similarity": 0.7793456240238386
    },
    {
      "doc": 553,
      "topic": 16,
      "similarity": 0.8020545231797158
    },
    {
      "doc": 553,
      "topic": 17,
      "similarity": 0.8274348293783287
    },
    {
      "doc": 553,
      "topic": 19,
      "similarity": 0.80815906942547
    },
    {
      "doc": 553,
      "topic": 20,
      "similarity": 0.7649004396260434
    },
    {
      "doc": 553,
      "topic": 21,
      "similarity": 0.8164029300038221
    },
    {
      "doc": 554,
      "topic": 1,
      "similarity": 0.7590403087304957
    },
    {
      "doc": 554,
      "topic": 2,
      "similarity": 0.7890737908877977
    },
    {
      "doc": 554,
      "topic": 3,
      "similarity": 0.7991887907169613
    },
    {
      "doc": 554,
      "topic": 4,
      "similarity": 0.7777036393578918
    },
    {
      "doc": 554,
      "topic": 5,
      "similarity": 0.8044779374372687
    },
    {
      "doc": 554,
      "topic": 7,
      "similarity": 0.7936616869935501
    },
    {
      "doc": 554,
      "topic": 8,
      "similarity": 0.7905890353399745
    },
    {
      "doc": 554,
      "topic": 9,
      "similarity": 0.8148324665721889
    },
    {
      "doc": 554,
      "topic": 10,
      "similarity": 0.7833112163581641
    },
    {
      "doc": 554,
      "topic": 11,
      "similarity": 0.7849125944259243
    },
    {
      "doc": 554,
      "topic": 13,
      "similarity": 0.7773770197445132
    },
    {
      "doc": 554,
      "topic": 14,
      "similarity": 0.7732134421281752
    },
    {
      "doc": 554,
      "topic": 15,
      "similarity": 0.7889143529363043
    },
    {
      "doc": 554,
      "topic": 16,
      "similarity": 0.8260620760731562
    },
    {
      "doc": 554,
      "topic": 17,
      "similarity": 0.8220482685750804
    },
    {
      "doc": 554,
      "topic": 18,
      "similarity": 0.7600469052545521
    },
    {
      "doc": 554,
      "topic": 19,
      "similarity": 0.8113433890023066
    },
    {
      "doc": 554,
      "topic": 20,
      "similarity": 0.7830910289605998
    },
    {
      "doc": 554,
      "topic": 21,
      "similarity": 0.8176543680435935
    },
    {
      "doc": 555,
      "topic": 1,
      "similarity": 0.7791462715610671
    },
    {
      "doc": 555,
      "topic": 2,
      "similarity": 0.7921517045020702
    },
    {
      "doc": 555,
      "topic": 3,
      "similarity": 0.8150353664735224
    },
    {
      "doc": 555,
      "topic": 5,
      "similarity": 0.780531362004045
    },
    {
      "doc": 555,
      "topic": 7,
      "similarity": 0.7769373503541211
    },
    {
      "doc": 555,
      "topic": 8,
      "similarity": 0.7902661299263906
    },
    {
      "doc": 555,
      "topic": 9,
      "similarity": 0.8197874898528655
    },
    {
      "doc": 555,
      "topic": 10,
      "similarity": 0.7798265471311268
    },
    {
      "doc": 555,
      "topic": 11,
      "similarity": 0.7653967060491135
    },
    {
      "doc": 555,
      "topic": 13,
      "similarity": 0.7773773285078767
    },
    {
      "doc": 555,
      "topic": 14,
      "similarity": 0.7817143183580327
    },
    {
      "doc": 555,
      "topic": 15,
      "similarity": 0.7707050906968451
    },
    {
      "doc": 555,
      "topic": 16,
      "similarity": 0.7919321048400149
    },
    {
      "doc": 555,
      "topic": 17,
      "similarity": 0.7806711557556373
    },
    {
      "doc": 555,
      "topic": 19,
      "similarity": 0.7924826138557527
    },
    {
      "doc": 555,
      "topic": 20,
      "similarity": 0.7661330462511047
    },
    {
      "doc": 555,
      "topic": 21,
      "similarity": 0.7901026534479818
    },
    {
      "doc": 555,
      "topic": 23,
      "similarity": 0.7752785242628496
    },
    {
      "doc": 556,
      "topic": 3,
      "similarity": 0.7809152705112545
    },
    {
      "doc": 556,
      "topic": 4,
      "similarity": 0.8071794252564226
    },
    {
      "doc": 556,
      "topic": 5,
      "similarity": 0.7719596239743188
    },
    {
      "doc": 556,
      "topic": 7,
      "similarity": 0.7725630667319446
    },
    {
      "doc": 556,
      "topic": 9,
      "similarity": 0.78264509229648
    },
    {
      "doc": 556,
      "topic": 10,
      "similarity": 0.7546730445155136
    },
    {
      "doc": 556,
      "topic": 11,
      "similarity": 0.7684602398599728
    },
    {
      "doc": 556,
      "topic": 15,
      "similarity": 0.7569963458127076
    },
    {
      "doc": 556,
      "topic": 16,
      "similarity": 0.7924737435709153
    },
    {
      "doc": 556,
      "topic": 17,
      "similarity": 0.7880644828766068
    },
    {
      "doc": 556,
      "topic": 19,
      "similarity": 0.7917016327506359
    },
    {
      "doc": 556,
      "topic": 20,
      "similarity": 0.7573773241168043
    },
    {
      "doc": 556,
      "topic": 21,
      "similarity": 0.7691639912046526
    },
    {
      "doc": 556,
      "topic": 24,
      "similarity": 0.7662987385987701
    },
    {
      "doc": 557,
      "topic": 3,
      "similarity": 0.7800833857982995
    },
    {
      "doc": 557,
      "topic": 5,
      "similarity": 0.7898263671570368
    },
    {
      "doc": 557,
      "topic": 7,
      "similarity": 0.7628615283013999
    },
    {
      "doc": 557,
      "topic": 8,
      "similarity": 0.7558228441260533
    },
    {
      "doc": 557,
      "topic": 9,
      "similarity": 0.821404305485376
    },
    {
      "doc": 557,
      "topic": 11,
      "similarity": 0.7838050183851802
    },
    {
      "doc": 557,
      "topic": 15,
      "similarity": 0.756107589047636
    },
    {
      "doc": 557,
      "topic": 16,
      "similarity": 0.7803907457240864
    },
    {
      "doc": 557,
      "topic": 17,
      "similarity": 0.7751448780582216
    },
    {
      "doc": 557,
      "topic": 19,
      "similarity": 0.7717499305377442
    },
    {
      "doc": 557,
      "topic": 20,
      "similarity": 0.7574391314002569
    },
    {
      "doc": 557,
      "topic": 21,
      "similarity": 0.7872067380280571
    },
    {
      "doc": 557,
      "topic": 23,
      "similarity": 0.7510224569439021
    },
    {
      "doc": 557,
      "topic": 24,
      "similarity": 0.7738255599539665
    },
    {
      "doc": 558,
      "topic": 1,
      "similarity": 0.7612890101244811
    },
    {
      "doc": 558,
      "topic": 2,
      "similarity": 0.8126111727548471
    },
    {
      "doc": 558,
      "topic": 3,
      "similarity": 0.8065429188367027
    },
    {
      "doc": 558,
      "topic": 4,
      "similarity": 0.7529498852932818
    },
    {
      "doc": 558,
      "topic": 5,
      "similarity": 0.8061852397133707
    },
    {
      "doc": 558,
      "topic": 7,
      "similarity": 0.7906829912580134
    },
    {
      "doc": 558,
      "topic": 8,
      "similarity": 0.7976424336048097
    },
    {
      "doc": 558,
      "topic": 9,
      "similarity": 0.827556473223958
    },
    {
      "doc": 558,
      "topic": 10,
      "similarity": 0.7971836349409832
    },
    {
      "doc": 558,
      "topic": 11,
      "similarity": 0.783901830231213
    },
    {
      "doc": 558,
      "topic": 13,
      "similarity": 0.7844841284798065
    },
    {
      "doc": 558,
      "topic": 14,
      "similarity": 0.8032991586803901
    },
    {
      "doc": 558,
      "topic": 15,
      "similarity": 0.7946928035710074
    },
    {
      "doc": 558,
      "topic": 16,
      "similarity": 0.8086658264092136
    },
    {
      "doc": 558,
      "topic": 17,
      "similarity": 0.7892981092254147
    },
    {
      "doc": 558,
      "topic": 18,
      "similarity": 0.7958755726112778
    },
    {
      "doc": 558,
      "topic": 19,
      "similarity": 0.82388890053033
    },
    {
      "doc": 558,
      "topic": 20,
      "similarity": 0.795457826952417
    },
    {
      "doc": 558,
      "topic": 21,
      "similarity": 0.7944543365889666
    },
    {
      "doc": 558,
      "topic": 22,
      "similarity": 0.753194116480388
    },
    {
      "doc": 558,
      "topic": 23,
      "similarity": 0.7626466156666176
    },
    {
      "doc": 559,
      "topic": 1,
      "similarity": 0.7500161153298964
    },
    {
      "doc": 559,
      "topic": 2,
      "similarity": 0.7819524042940352
    },
    {
      "doc": 559,
      "topic": 3,
      "similarity": 0.8036335942028184
    },
    {
      "doc": 559,
      "topic": 5,
      "similarity": 0.8076746380135764
    },
    {
      "doc": 559,
      "topic": 7,
      "similarity": 0.7785949236821338
    },
    {
      "doc": 559,
      "topic": 8,
      "similarity": 0.7891873597567309
    },
    {
      "doc": 559,
      "topic": 9,
      "similarity": 0.8141105497611328
    },
    {
      "doc": 559,
      "topic": 10,
      "similarity": 0.7731911528504295
    },
    {
      "doc": 559,
      "topic": 11,
      "similarity": 0.7747978331904294
    },
    {
      "doc": 559,
      "topic": 12,
      "similarity": 0.7563055977515405
    },
    {
      "doc": 559,
      "topic": 13,
      "similarity": 0.7647910493521012
    },
    {
      "doc": 559,
      "topic": 14,
      "similarity": 0.7796727925978953
    },
    {
      "doc": 559,
      "topic": 15,
      "similarity": 0.78454935449846
    },
    {
      "doc": 559,
      "topic": 16,
      "similarity": 0.8122375792097379
    },
    {
      "doc": 559,
      "topic": 17,
      "similarity": 0.8081036874442821
    },
    {
      "doc": 559,
      "topic": 18,
      "similarity": 0.7549821007801061
    },
    {
      "doc": 559,
      "topic": 19,
      "similarity": 0.8263915262981355
    },
    {
      "doc": 559,
      "topic": 20,
      "similarity": 0.7963595742700973
    },
    {
      "doc": 559,
      "topic": 21,
      "similarity": 0.775440830142759
    },
    {
      "doc": 559,
      "topic": 22,
      "similarity": 0.7938825223484144
    },
    {
      "doc": 559,
      "topic": 23,
      "similarity": 0.7878405491623098
    },
    {
      "doc": 560,
      "topic": 1,
      "similarity": 0.7579493421765543
    },
    {
      "doc": 560,
      "topic": 2,
      "similarity": 0.7830078519788463
    },
    {
      "doc": 560,
      "topic": 3,
      "similarity": 0.8058847733849678
    },
    {
      "doc": 560,
      "topic": 4,
      "similarity": 0.7574269311774801
    },
    {
      "doc": 560,
      "topic": 5,
      "similarity": 0.8096284012060653
    },
    {
      "doc": 560,
      "topic": 7,
      "similarity": 0.7807153464202192
    },
    {
      "doc": 560,
      "topic": 8,
      "similarity": 0.8126648635218078
    },
    {
      "doc": 560,
      "topic": 9,
      "similarity": 0.8241351630456827
    },
    {
      "doc": 560,
      "topic": 10,
      "similarity": 0.7775013759769005
    },
    {
      "doc": 560,
      "topic": 11,
      "similarity": 0.7882290914097942
    },
    {
      "doc": 560,
      "topic": 12,
      "similarity": 0.7636955229502009
    },
    {
      "doc": 560,
      "topic": 13,
      "similarity": 0.7868423618112494
    },
    {
      "doc": 560,
      "topic": 14,
      "similarity": 0.7962492286541397
    },
    {
      "doc": 560,
      "topic": 15,
      "similarity": 0.8114634479062869
    },
    {
      "doc": 560,
      "topic": 16,
      "similarity": 0.8016347436869047
    },
    {
      "doc": 560,
      "topic": 17,
      "similarity": 0.8109582947029809
    },
    {
      "doc": 560,
      "topic": 18,
      "similarity": 0.7737651206457611
    },
    {
      "doc": 560,
      "topic": 19,
      "similarity": 0.8281569346167204
    },
    {
      "doc": 560,
      "topic": 20,
      "similarity": 0.7849907492400341
    },
    {
      "doc": 560,
      "topic": 21,
      "similarity": 0.8011597917734444
    },
    {
      "doc": 560,
      "topic": 22,
      "similarity": 0.7908207614970562
    },
    {
      "doc": 560,
      "topic": 23,
      "similarity": 0.7675562828100679
    },
    {
      "doc": 561,
      "topic": 1,
      "similarity": 0.7588447668647376
    },
    {
      "doc": 561,
      "topic": 2,
      "similarity": 0.776942812337601
    },
    {
      "doc": 561,
      "topic": 3,
      "similarity": 0.794076061312151
    },
    {
      "doc": 561,
      "topic": 4,
      "similarity": 0.7562115942671103
    },
    {
      "doc": 561,
      "topic": 5,
      "similarity": 0.8154654846793459
    },
    {
      "doc": 561,
      "topic": 7,
      "similarity": 0.7831701567931671
    },
    {
      "doc": 561,
      "topic": 8,
      "similarity": 0.7982521900881931
    },
    {
      "doc": 561,
      "topic": 9,
      "similarity": 0.8327151993617402
    },
    {
      "doc": 561,
      "topic": 10,
      "similarity": 0.8128191163211225
    },
    {
      "doc": 561,
      "topic": 11,
      "similarity": 0.78788486687086
    },
    {
      "doc": 561,
      "topic": 12,
      "similarity": 0.7588101872785521
    },
    {
      "doc": 561,
      "topic": 13,
      "similarity": 0.7720679039498481
    },
    {
      "doc": 561,
      "topic": 14,
      "similarity": 0.7762512813130933
    },
    {
      "doc": 561,
      "topic": 15,
      "similarity": 0.8074139608640944
    },
    {
      "doc": 561,
      "topic": 16,
      "similarity": 0.8044517965692616
    },
    {
      "doc": 561,
      "topic": 17,
      "similarity": 0.8223335924041344
    },
    {
      "doc": 561,
      "topic": 18,
      "similarity": 0.8108990112902921
    },
    {
      "doc": 561,
      "topic": 19,
      "similarity": 0.8326935182937955
    },
    {
      "doc": 561,
      "topic": 20,
      "similarity": 0.7668385805330528
    },
    {
      "doc": 561,
      "topic": 21,
      "similarity": 0.7987601908382472
    },
    {
      "doc": 561,
      "topic": 22,
      "similarity": 0.7715708829115344
    },
    {
      "doc": 562,
      "topic": 1,
      "similarity": 0.7532151585955877
    },
    {
      "doc": 562,
      "topic": 2,
      "similarity": 0.7819165446604208
    },
    {
      "doc": 562,
      "topic": 3,
      "similarity": 0.8117449648590667
    },
    {
      "doc": 562,
      "topic": 4,
      "similarity": 0.7602847235524262
    },
    {
      "doc": 562,
      "topic": 5,
      "similarity": 0.815379173395388
    },
    {
      "doc": 562,
      "topic": 7,
      "similarity": 0.7818817868938303
    },
    {
      "doc": 562,
      "topic": 8,
      "similarity": 0.7904542187945521
    },
    {
      "doc": 562,
      "topic": 9,
      "similarity": 0.819108912413471
    },
    {
      "doc": 562,
      "topic": 10,
      "similarity": 0.7732023286277414
    },
    {
      "doc": 562,
      "topic": 11,
      "similarity": 0.8115734619970829
    },
    {
      "doc": 562,
      "topic": 12,
      "similarity": 0.7501576842487891
    },
    {
      "doc": 562,
      "topic": 13,
      "similarity": 0.7576664923652014
    },
    {
      "doc": 562,
      "topic": 14,
      "similarity": 0.7637283829000427
    },
    {
      "doc": 562,
      "topic": 15,
      "similarity": 0.8451109327194715
    },
    {
      "doc": 562,
      "topic": 16,
      "similarity": 0.8120267388556198
    },
    {
      "doc": 562,
      "topic": 17,
      "similarity": 0.8284425561761063
    },
    {
      "doc": 562,
      "topic": 18,
      "similarity": 0.7744988892072098
    },
    {
      "doc": 562,
      "topic": 19,
      "similarity": 0.8230071903756806
    },
    {
      "doc": 562,
      "topic": 20,
      "similarity": 0.8015725618535963
    },
    {
      "doc": 562,
      "topic": 21,
      "similarity": 0.8132469572435613
    },
    {
      "doc": 562,
      "topic": 23,
      "similarity": 0.7553364447269293
    },
    {
      "doc": 562,
      "topic": 24,
      "similarity": 0.7648446997020736
    },
    {
      "doc": 563,
      "topic": 2,
      "similarity": 0.7759064472826513
    },
    {
      "doc": 563,
      "topic": 3,
      "similarity": 0.7852410708407216
    },
    {
      "doc": 563,
      "topic": 5,
      "similarity": 0.7815812480018328
    },
    {
      "doc": 563,
      "topic": 7,
      "similarity": 0.7959761964795921
    },
    {
      "doc": 563,
      "topic": 8,
      "similarity": 0.7952085492483103
    },
    {
      "doc": 563,
      "topic": 9,
      "similarity": 0.8593933186076579
    },
    {
      "doc": 563,
      "topic": 10,
      "similarity": 0.7747703811202504
    },
    {
      "doc": 563,
      "topic": 11,
      "similarity": 0.7795649022699315
    },
    {
      "doc": 563,
      "topic": 12,
      "similarity": 0.7577267014324798
    },
    {
      "doc": 563,
      "topic": 13,
      "similarity": 0.7880361922107657
    },
    {
      "doc": 563,
      "topic": 14,
      "similarity": 0.7853641175747
    },
    {
      "doc": 563,
      "topic": 15,
      "similarity": 0.7745093137647951
    },
    {
      "doc": 563,
      "topic": 16,
      "similarity": 0.7828739057237788
    },
    {
      "doc": 563,
      "topic": 17,
      "similarity": 0.7731302598129904
    },
    {
      "doc": 563,
      "topic": 18,
      "similarity": 0.7588775953598991
    },
    {
      "doc": 563,
      "topic": 19,
      "similarity": 0.7835656045446523
    },
    {
      "doc": 563,
      "topic": 20,
      "similarity": 0.7827549903487512
    },
    {
      "doc": 563,
      "topic": 21,
      "similarity": 0.7958597065041877
    },
    {
      "doc": 563,
      "topic": 22,
      "similarity": 0.7644506892795027
    },
    {
      "doc": 563,
      "topic": 23,
      "similarity": 0.759572809409867
    },
    {
      "doc": 563,
      "topic": 24,
      "similarity": 0.7675755829909334
    },
    {
      "doc": 564,
      "topic": 3,
      "similarity": 0.7791328330389555
    },
    {
      "doc": 564,
      "topic": 5,
      "similarity": 0.7766213966979655
    },
    {
      "doc": 564,
      "topic": 7,
      "similarity": 0.755525538239757
    },
    {
      "doc": 564,
      "topic": 8,
      "similarity": 0.7520159940827097
    },
    {
      "doc": 564,
      "topic": 9,
      "similarity": 0.8117012838824035
    },
    {
      "doc": 564,
      "topic": 10,
      "similarity": 0.7612634397641344
    },
    {
      "doc": 564,
      "topic": 11,
      "similarity": 0.7622831128979318
    },
    {
      "doc": 564,
      "topic": 13,
      "similarity": 0.7570160815528795
    },
    {
      "doc": 564,
      "topic": 15,
      "similarity": 0.7690741346349141
    },
    {
      "doc": 564,
      "topic": 16,
      "similarity": 0.7783794165914957
    },
    {
      "doc": 564,
      "topic": 17,
      "similarity": 0.7932326970199899
    },
    {
      "doc": 564,
      "topic": 19,
      "similarity": 0.8019278286108723
    },
    {
      "doc": 564,
      "topic": 20,
      "similarity": 0.771399475908175
    },
    {
      "doc": 564,
      "topic": 21,
      "similarity": 0.8092253116451992
    },
    {
      "doc": 564,
      "topic": 24,
      "similarity": 0.7537783000474423
    },
    {
      "doc": 565,
      "topic": 0,
      "similarity": 0.763678044448658
    },
    {
      "doc": 565,
      "topic": 3,
      "similarity": 0.7823459977989059
    },
    {
      "doc": 565,
      "topic": 5,
      "similarity": 0.8030482066020546
    },
    {
      "doc": 565,
      "topic": 7,
      "similarity": 0.7568930589192454
    },
    {
      "doc": 565,
      "topic": 8,
      "similarity": 0.7540135502088968
    },
    {
      "doc": 565,
      "topic": 9,
      "similarity": 0.8040319607923178
    },
    {
      "doc": 565,
      "topic": 10,
      "similarity": 0.7524267836233338
    },
    {
      "doc": 565,
      "topic": 11,
      "similarity": 0.8078990149460443
    },
    {
      "doc": 565,
      "topic": 15,
      "similarity": 0.7668085432816729
    },
    {
      "doc": 565,
      "topic": 16,
      "similarity": 0.783228591744602
    },
    {
      "doc": 565,
      "topic": 17,
      "similarity": 0.7888913644652426
    },
    {
      "doc": 565,
      "topic": 18,
      "similarity": 0.7598842979419412
    },
    {
      "doc": 565,
      "topic": 19,
      "similarity": 0.7859810657042096
    },
    {
      "doc": 565,
      "topic": 20,
      "similarity": 0.7712385878833631
    },
    {
      "doc": 565,
      "topic": 21,
      "similarity": 0.7919028768238126
    },
    {
      "doc": 565,
      "topic": 23,
      "similarity": 0.7566365863480942
    },
    {
      "doc": 565,
      "topic": 24,
      "similarity": 0.768428281499241
    },
    {
      "doc": 566,
      "topic": 2,
      "similarity": 0.7557493781463233
    },
    {
      "doc": 566,
      "topic": 3,
      "similarity": 0.781693137520636
    },
    {
      "doc": 566,
      "topic": 4,
      "similarity": 0.7589896122116502
    },
    {
      "doc": 566,
      "topic": 5,
      "similarity": 0.7938236626095785
    },
    {
      "doc": 566,
      "topic": 7,
      "similarity": 0.7988139798923592
    },
    {
      "doc": 566,
      "topic": 8,
      "similarity": 0.7899796805695624
    },
    {
      "doc": 566,
      "topic": 9,
      "similarity": 0.8344445581306335
    },
    {
      "doc": 566,
      "topic": 10,
      "similarity": 0.7707064747187886
    },
    {
      "doc": 566,
      "topic": 11,
      "similarity": 0.7822919182572737
    },
    {
      "doc": 566,
      "topic": 13,
      "similarity": 0.7739101122790463
    },
    {
      "doc": 566,
      "topic": 14,
      "similarity": 0.7610175404476022
    },
    {
      "doc": 566,
      "topic": 15,
      "similarity": 0.7637777099718629
    },
    {
      "doc": 566,
      "topic": 16,
      "similarity": 0.7934836485536675
    },
    {
      "doc": 566,
      "topic": 17,
      "similarity": 0.7867109218161082
    },
    {
      "doc": 566,
      "topic": 18,
      "similarity": 0.7685710149434747
    },
    {
      "doc": 566,
      "topic": 19,
      "similarity": 0.8080332621255016
    },
    {
      "doc": 566,
      "topic": 21,
      "similarity": 0.775347559614045
    },
    {
      "doc": 567,
      "topic": 1,
      "similarity": 0.7617394282844757
    },
    {
      "doc": 567,
      "topic": 2,
      "similarity": 0.7732488489112211
    },
    {
      "doc": 567,
      "topic": 3,
      "similarity": 0.813458651900657
    },
    {
      "doc": 567,
      "topic": 4,
      "similarity": 0.760488823479058
    },
    {
      "doc": 567,
      "topic": 5,
      "similarity": 0.8144725971167297
    },
    {
      "doc": 567,
      "topic": 7,
      "similarity": 0.7879827195624146
    },
    {
      "doc": 567,
      "topic": 8,
      "similarity": 0.7934548008849795
    },
    {
      "doc": 567,
      "topic": 9,
      "similarity": 0.829374915222469
    },
    {
      "doc": 567,
      "topic": 10,
      "similarity": 0.7978468484145543
    },
    {
      "doc": 567,
      "topic": 11,
      "similarity": 0.7899865012127095
    },
    {
      "doc": 567,
      "topic": 13,
      "similarity": 0.7766137998123893
    },
    {
      "doc": 567,
      "topic": 14,
      "similarity": 0.7917212935994216
    },
    {
      "doc": 567,
      "topic": 15,
      "similarity": 0.777254757542535
    },
    {
      "doc": 567,
      "topic": 16,
      "similarity": 0.8190698577013173
    },
    {
      "doc": 567,
      "topic": 17,
      "similarity": 0.8005321566400286
    },
    {
      "doc": 567,
      "topic": 18,
      "similarity": 0.7771397584750678
    },
    {
      "doc": 567,
      "topic": 19,
      "similarity": 0.8273307703497336
    },
    {
      "doc": 567,
      "topic": 20,
      "similarity": 0.8110429908844088
    },
    {
      "doc": 567,
      "topic": 21,
      "similarity": 0.8059119258603359
    },
    {
      "doc": 567,
      "topic": 23,
      "similarity": 0.7606502831446766
    },
    {
      "doc": 567,
      "topic": 24,
      "similarity": 0.758545451495352
    },
    {
      "doc": 568,
      "topic": 3,
      "similarity": 0.7761239927759284
    },
    {
      "doc": 568,
      "topic": 5,
      "similarity": 0.7699007685205629
    },
    {
      "doc": 568,
      "topic": 7,
      "similarity": 0.8374728248134179
    },
    {
      "doc": 568,
      "topic": 8,
      "similarity": 0.7873536243251428
    },
    {
      "doc": 568,
      "topic": 9,
      "similarity": 0.7952528342517444
    },
    {
      "doc": 568,
      "topic": 11,
      "similarity": 0.7610448413557352
    },
    {
      "doc": 568,
      "topic": 16,
      "similarity": 0.7981293265436484
    },
    {
      "doc": 568,
      "topic": 17,
      "similarity": 0.7593036197198311
    },
    {
      "doc": 568,
      "topic": 19,
      "similarity": 0.7820949734533778
    },
    {
      "doc": 568,
      "topic": 21,
      "similarity": 0.7542841649304686
    },
    {
      "doc": 569,
      "topic": 1,
      "similarity": 0.7654787630719244
    },
    {
      "doc": 569,
      "topic": 2,
      "similarity": 0.7849448647591621
    },
    {
      "doc": 569,
      "topic": 3,
      "similarity": 0.7965665398951771
    },
    {
      "doc": 569,
      "topic": 4,
      "similarity": 0.7811827732336055
    },
    {
      "doc": 569,
      "topic": 5,
      "similarity": 0.8037260555697529
    },
    {
      "doc": 569,
      "topic": 7,
      "similarity": 0.8000888449114483
    },
    {
      "doc": 569,
      "topic": 8,
      "similarity": 0.8050429391929994
    },
    {
      "doc": 569,
      "topic": 9,
      "similarity": 0.8312253097269414
    },
    {
      "doc": 569,
      "topic": 10,
      "similarity": 0.8457506413218143
    },
    {
      "doc": 569,
      "topic": 11,
      "similarity": 0.802296090188169
    },
    {
      "doc": 569,
      "topic": 12,
      "similarity": 0.752358500156399
    },
    {
      "doc": 569,
      "topic": 13,
      "similarity": 0.7709196910639132
    },
    {
      "doc": 569,
      "topic": 14,
      "similarity": 0.7864450668213366
    },
    {
      "doc": 569,
      "topic": 15,
      "similarity": 0.8001658607296906
    },
    {
      "doc": 569,
      "topic": 16,
      "similarity": 0.8088347447230483
    },
    {
      "doc": 569,
      "topic": 17,
      "similarity": 0.8080616917687551
    },
    {
      "doc": 569,
      "topic": 18,
      "similarity": 0.7826351562556227
    },
    {
      "doc": 569,
      "topic": 19,
      "similarity": 0.8168874726487093
    },
    {
      "doc": 569,
      "topic": 20,
      "similarity": 0.7840776729263792
    },
    {
      "doc": 569,
      "topic": 21,
      "similarity": 0.8086108694842915
    },
    {
      "doc": 569,
      "topic": 23,
      "similarity": 0.7514980004128734
    },
    {
      "doc": 569,
      "topic": 24,
      "similarity": 0.7583487594197172
    },
    {
      "doc": 570,
      "topic": 1,
      "similarity": 0.7507165237786331
    },
    {
      "doc": 570,
      "topic": 2,
      "similarity": 0.7747184100635833
    },
    {
      "doc": 570,
      "topic": 3,
      "similarity": 0.78125121505444
    },
    {
      "doc": 570,
      "topic": 5,
      "similarity": 0.799957148652558
    },
    {
      "doc": 570,
      "topic": 6,
      "similarity": 0.7514510904140665
    },
    {
      "doc": 570,
      "topic": 7,
      "similarity": 0.7629991329144362
    },
    {
      "doc": 570,
      "topic": 8,
      "similarity": 0.7718834274262726
    },
    {
      "doc": 570,
      "topic": 9,
      "similarity": 0.8114801837774092
    },
    {
      "doc": 570,
      "topic": 10,
      "similarity": 0.7767585999930416
    },
    {
      "doc": 570,
      "topic": 11,
      "similarity": 0.7826622807629388
    },
    {
      "doc": 570,
      "topic": 13,
      "similarity": 0.7918839949089986
    },
    {
      "doc": 570,
      "topic": 14,
      "similarity": 0.7831145846290539
    },
    {
      "doc": 570,
      "topic": 15,
      "similarity": 0.77595205417247
    },
    {
      "doc": 570,
      "topic": 16,
      "similarity": 0.8218097588536828
    },
    {
      "doc": 570,
      "topic": 17,
      "similarity": 0.8072657953571316
    },
    {
      "doc": 570,
      "topic": 18,
      "similarity": 0.7789743979453491
    },
    {
      "doc": 570,
      "topic": 19,
      "similarity": 0.8415861899154998
    },
    {
      "doc": 570,
      "topic": 20,
      "similarity": 0.808696506955636
    },
    {
      "doc": 570,
      "topic": 21,
      "similarity": 0.7950991372986645
    },
    {
      "doc": 570,
      "topic": 22,
      "similarity": 0.7702395000116365
    },
    {
      "doc": 570,
      "topic": 23,
      "similarity": 0.7725970262119914
    },
    {
      "doc": 571,
      "topic": 3,
      "similarity": 0.7759203629011796
    },
    {
      "doc": 571,
      "topic": 5,
      "similarity": 0.7698854990705271
    },
    {
      "doc": 571,
      "topic": 7,
      "similarity": 0.75658607833779
    },
    {
      "doc": 571,
      "topic": 8,
      "similarity": 0.757489864852129
    },
    {
      "doc": 571,
      "topic": 9,
      "similarity": 0.8102536584339349
    },
    {
      "doc": 571,
      "topic": 10,
      "similarity": 0.7537550746841944
    },
    {
      "doc": 571,
      "topic": 11,
      "similarity": 0.7669826058805468
    },
    {
      "doc": 571,
      "topic": 13,
      "similarity": 0.7525530644201055
    },
    {
      "doc": 571,
      "topic": 15,
      "similarity": 0.7717225584012949
    },
    {
      "doc": 571,
      "topic": 16,
      "similarity": 0.7860378259556664
    },
    {
      "doc": 571,
      "topic": 17,
      "similarity": 0.7946818918086362
    },
    {
      "doc": 571,
      "topic": 19,
      "similarity": 0.7725451709837541
    },
    {
      "doc": 571,
      "topic": 20,
      "similarity": 0.7773574554702599
    },
    {
      "doc": 571,
      "topic": 21,
      "similarity": 0.8420752414214175
    },
    {
      "doc": 571,
      "topic": 24,
      "similarity": 0.7758533757982711
    },
    {
      "doc": 572,
      "topic": 1,
      "similarity": 0.7716780847476953
    },
    {
      "doc": 572,
      "topic": 2,
      "similarity": 0.762688861430934
    },
    {
      "doc": 572,
      "topic": 3,
      "similarity": 0.8219136219495787
    },
    {
      "doc": 572,
      "topic": 5,
      "similarity": 0.7826837746854878
    },
    {
      "doc": 572,
      "topic": 7,
      "similarity": 0.7880710468169747
    },
    {
      "doc": 572,
      "topic": 8,
      "similarity": 0.7619844616447653
    },
    {
      "doc": 572,
      "topic": 9,
      "similarity": 0.8102832125503286
    },
    {
      "doc": 572,
      "topic": 10,
      "similarity": 0.7846447447927682
    },
    {
      "doc": 572,
      "topic": 11,
      "similarity": 0.8046414500280759
    },
    {
      "doc": 572,
      "topic": 13,
      "similarity": 0.7652514429200498
    },
    {
      "doc": 572,
      "topic": 14,
      "similarity": 0.7562719968644198
    },
    {
      "doc": 572,
      "topic": 15,
      "similarity": 0.7688793836648474
    },
    {
      "doc": 572,
      "topic": 16,
      "similarity": 0.8136671347761011
    },
    {
      "doc": 572,
      "topic": 17,
      "similarity": 0.7829521583574844
    },
    {
      "doc": 572,
      "topic": 18,
      "similarity": 0.7537833418456522
    },
    {
      "doc": 572,
      "topic": 19,
      "similarity": 0.8116926690048222
    },
    {
      "doc": 572,
      "topic": 20,
      "similarity": 0.7704891605761781
    },
    {
      "doc": 572,
      "topic": 21,
      "similarity": 0.7849981332205431
    },
    {
      "doc": 573,
      "topic": 2,
      "similarity": 0.7620893425304391
    },
    {
      "doc": 573,
      "topic": 3,
      "similarity": 0.8051960766563985
    },
    {
      "doc": 573,
      "topic": 4,
      "similarity": 0.7642655421282463
    },
    {
      "doc": 573,
      "topic": 5,
      "similarity": 0.8102608326549795
    },
    {
      "doc": 573,
      "topic": 7,
      "similarity": 0.7848102823889529
    },
    {
      "doc": 573,
      "topic": 8,
      "similarity": 0.7896760359111774
    },
    {
      "doc": 573,
      "topic": 9,
      "similarity": 0.8396270811798469
    },
    {
      "doc": 573,
      "topic": 10,
      "similarity": 0.7939851454918891
    },
    {
      "doc": 573,
      "topic": 11,
      "similarity": 0.7997055281700896
    },
    {
      "doc": 573,
      "topic": 12,
      "similarity": 0.7602546459847566
    },
    {
      "doc": 573,
      "topic": 13,
      "similarity": 0.7659183848279505
    },
    {
      "doc": 573,
      "topic": 14,
      "similarity": 0.7701188642305455
    },
    {
      "doc": 573,
      "topic": 15,
      "similarity": 0.8086646228088017
    },
    {
      "doc": 573,
      "topic": 16,
      "similarity": 0.7919818935765957
    },
    {
      "doc": 573,
      "topic": 17,
      "similarity": 0.8000772826696284
    },
    {
      "doc": 573,
      "topic": 18,
      "similarity": 0.7613510493014961
    },
    {
      "doc": 573,
      "topic": 19,
      "similarity": 0.7937642721671264
    },
    {
      "doc": 573,
      "topic": 20,
      "similarity": 0.7589901760593588
    },
    {
      "doc": 573,
      "topic": 21,
      "similarity": 0.796553793256328
    },
    {
      "doc": 573,
      "topic": 23,
      "similarity": 0.7626951494728075
    },
    {
      "doc": 573,
      "topic": 24,
      "similarity": 0.7778044412434574
    },
    {
      "doc": 574,
      "topic": 1,
      "similarity": 0.8129628797311028
    },
    {
      "doc": 574,
      "topic": 2,
      "similarity": 0.7885254809044445
    },
    {
      "doc": 574,
      "topic": 3,
      "similarity": 0.8132411875352178
    },
    {
      "doc": 574,
      "topic": 5,
      "similarity": 0.7828511152933896
    },
    {
      "doc": 574,
      "topic": 7,
      "similarity": 0.7878085353845184
    },
    {
      "doc": 574,
      "topic": 8,
      "similarity": 0.7876813261260883
    },
    {
      "doc": 574,
      "topic": 9,
      "similarity": 0.8316404769148225
    },
    {
      "doc": 574,
      "topic": 10,
      "similarity": 0.7894392257413694
    },
    {
      "doc": 574,
      "topic": 11,
      "similarity": 0.7899585670933853
    },
    {
      "doc": 574,
      "topic": 12,
      "similarity": 0.765144185551967
    },
    {
      "doc": 574,
      "topic": 13,
      "similarity": 0.7698788988253542
    },
    {
      "doc": 574,
      "topic": 14,
      "similarity": 0.7908829719666711
    },
    {
      "doc": 574,
      "topic": 15,
      "similarity": 0.7923036090477501
    },
    {
      "doc": 574,
      "topic": 16,
      "similarity": 0.8115513732848094
    },
    {
      "doc": 574,
      "topic": 17,
      "similarity": 0.7975186870325156
    },
    {
      "doc": 574,
      "topic": 18,
      "similarity": 0.7657551967963571
    },
    {
      "doc": 574,
      "topic": 19,
      "similarity": 0.8271312290186641
    },
    {
      "doc": 574,
      "topic": 20,
      "similarity": 0.771233270496501
    },
    {
      "doc": 574,
      "topic": 21,
      "similarity": 0.7979063587454498
    },
    {
      "doc": 574,
      "topic": 22,
      "similarity": 0.7761977705884234
    },
    {
      "doc": 574,
      "topic": 23,
      "similarity": 0.7669322135874042
    },
    {
      "doc": 575,
      "topic": 2,
      "similarity": 0.7707240222450967
    },
    {
      "doc": 575,
      "topic": 3,
      "similarity": 0.8051568473964839
    },
    {
      "doc": 575,
      "topic": 4,
      "similarity": 0.757401662125719
    },
    {
      "doc": 575,
      "topic": 5,
      "similarity": 0.7955070830999382
    },
    {
      "doc": 575,
      "topic": 6,
      "similarity": 0.762664429370775
    },
    {
      "doc": 575,
      "topic": 7,
      "similarity": 0.7810213378548673
    },
    {
      "doc": 575,
      "topic": 8,
      "similarity": 0.7673103479310229
    },
    {
      "doc": 575,
      "topic": 9,
      "similarity": 0.8204307457998149
    },
    {
      "doc": 575,
      "topic": 10,
      "similarity": 0.7724384132116826
    },
    {
      "doc": 575,
      "topic": 11,
      "similarity": 0.781740236947619
    },
    {
      "doc": 575,
      "topic": 13,
      "similarity": 0.7636737680094284
    },
    {
      "doc": 575,
      "topic": 14,
      "similarity": 0.7624605258029528
    },
    {
      "doc": 575,
      "topic": 15,
      "similarity": 0.7807896388420397
    },
    {
      "doc": 575,
      "topic": 16,
      "similarity": 0.8093879159584089
    },
    {
      "doc": 575,
      "topic": 17,
      "similarity": 0.8102864110152213
    },
    {
      "doc": 575,
      "topic": 18,
      "similarity": 0.7653513909134428
    },
    {
      "doc": 575,
      "topic": 19,
      "similarity": 0.8168978487810008
    },
    {
      "doc": 575,
      "topic": 20,
      "similarity": 0.798204319634503
    },
    {
      "doc": 575,
      "topic": 21,
      "similarity": 0.8102653789709213
    },
    {
      "doc": 575,
      "topic": 23,
      "similarity": 0.7578849276493907
    },
    {
      "doc": 575,
      "topic": 24,
      "similarity": 0.7664622300988346
    },
    {
      "doc": 576,
      "topic": 2,
      "similarity": 0.7627802695169286
    },
    {
      "doc": 576,
      "topic": 3,
      "similarity": 0.7814271394773377
    },
    {
      "doc": 576,
      "topic": 4,
      "similarity": 0.7509870484457131
    },
    {
      "doc": 576,
      "topic": 5,
      "similarity": 0.7670217037698244
    },
    {
      "doc": 576,
      "topic": 6,
      "similarity": 0.7645911378338647
    },
    {
      "doc": 576,
      "topic": 8,
      "similarity": 0.7500692039922592
    },
    {
      "doc": 576,
      "topic": 9,
      "similarity": 0.790696981992278
    },
    {
      "doc": 576,
      "topic": 10,
      "similarity": 0.7644625338291825
    },
    {
      "doc": 576,
      "topic": 11,
      "similarity": 0.7568024283294583
    },
    {
      "doc": 576,
      "topic": 13,
      "similarity": 0.7828561108916371
    },
    {
      "doc": 576,
      "topic": 14,
      "similarity": 0.7629401951243742
    },
    {
      "doc": 576,
      "topic": 15,
      "similarity": 0.7594436778343627
    },
    {
      "doc": 576,
      "topic": 16,
      "similarity": 0.7741759381112413
    },
    {
      "doc": 576,
      "topic": 17,
      "similarity": 0.7742058011722152
    },
    {
      "doc": 576,
      "topic": 18,
      "similarity": 0.7581208757817625
    },
    {
      "doc": 576,
      "topic": 19,
      "similarity": 0.7770423859631854
    },
    {
      "doc": 576,
      "topic": 20,
      "similarity": 0.7541020668241046
    },
    {
      "doc": 576,
      "topic": 21,
      "similarity": 0.7969747898358353
    },
    {
      "doc": 576,
      "topic": 24,
      "similarity": 0.754520061510541
    },
    {
      "doc": 577,
      "topic": 2,
      "similarity": 0.7636659881477533
    },
    {
      "doc": 577,
      "topic": 3,
      "similarity": 0.8048969139158899
    },
    {
      "doc": 577,
      "topic": 4,
      "similarity": 0.7517320090948592
    },
    {
      "doc": 577,
      "topic": 5,
      "similarity": 0.8319461467694472
    },
    {
      "doc": 577,
      "topic": 7,
      "similarity": 0.7796548538861787
    },
    {
      "doc": 577,
      "topic": 8,
      "similarity": 0.7615432347322959
    },
    {
      "doc": 577,
      "topic": 9,
      "similarity": 0.8102563315749047
    },
    {
      "doc": 577,
      "topic": 10,
      "similarity": 0.7743469805079086
    },
    {
      "doc": 577,
      "topic": 11,
      "similarity": 0.7744541748265983
    },
    {
      "doc": 577,
      "topic": 13,
      "similarity": 0.759597307421104
    },
    {
      "doc": 577,
      "topic": 14,
      "similarity": 0.7552103176233346
    },
    {
      "doc": 577,
      "topic": 15,
      "similarity": 0.764743111094719
    },
    {
      "doc": 577,
      "topic": 16,
      "similarity": 0.8140346016228233
    },
    {
      "doc": 577,
      "topic": 17,
      "similarity": 0.797836148478365
    },
    {
      "doc": 577,
      "topic": 18,
      "similarity": 0.7515529456897004
    },
    {
      "doc": 577,
      "topic": 19,
      "similarity": 0.8089713948010282
    },
    {
      "doc": 577,
      "topic": 20,
      "similarity": 0.7936454125570651
    },
    {
      "doc": 577,
      "topic": 21,
      "similarity": 0.8002919359547783
    },
    {
      "doc": 577,
      "topic": 23,
      "similarity": 0.7580071197826512
    },
    {
      "doc": 577,
      "topic": 24,
      "similarity": 0.7566630913023366
    },
    {
      "doc": 578,
      "topic": 3,
      "similarity": 0.7859905130589043
    },
    {
      "doc": 578,
      "topic": 5,
      "similarity": 0.7815724264850475
    },
    {
      "doc": 578,
      "topic": 7,
      "similarity": 0.7754957087045914
    },
    {
      "doc": 578,
      "topic": 9,
      "similarity": 0.7887366969588494
    },
    {
      "doc": 578,
      "topic": 11,
      "similarity": 0.7687971844906217
    },
    {
      "doc": 578,
      "topic": 15,
      "similarity": 0.7608812619756323
    },
    {
      "doc": 578,
      "topic": 16,
      "similarity": 0.8030919091800108
    },
    {
      "doc": 578,
      "topic": 17,
      "similarity": 0.7722448096330166
    },
    {
      "doc": 578,
      "topic": 19,
      "similarity": 0.7757419535739275
    },
    {
      "doc": 578,
      "topic": 20,
      "similarity": 0.7853299014086577
    },
    {
      "doc": 578,
      "topic": 21,
      "similarity": 0.7861456407400318
    },
    {
      "doc": 579,
      "topic": 1,
      "similarity": 0.7530264517237938
    },
    {
      "doc": 579,
      "topic": 2,
      "similarity": 0.7747579730672173
    },
    {
      "doc": 579,
      "topic": 3,
      "similarity": 0.8022402648894221
    },
    {
      "doc": 579,
      "topic": 4,
      "similarity": 0.7680490492957117
    },
    {
      "doc": 579,
      "topic": 5,
      "similarity": 0.8149016108086888
    },
    {
      "doc": 579,
      "topic": 7,
      "similarity": 0.779601276613143
    },
    {
      "doc": 579,
      "topic": 8,
      "similarity": 0.7811481785420543
    },
    {
      "doc": 579,
      "topic": 9,
      "similarity": 0.8295093519840621
    },
    {
      "doc": 579,
      "topic": 10,
      "similarity": 0.8008225436792822
    },
    {
      "doc": 579,
      "topic": 11,
      "similarity": 0.7947223003495366
    },
    {
      "doc": 579,
      "topic": 13,
      "similarity": 0.7603408360196703
    },
    {
      "doc": 579,
      "topic": 14,
      "similarity": 0.7706799631578543
    },
    {
      "doc": 579,
      "topic": 15,
      "similarity": 0.7950727152285586
    },
    {
      "doc": 579,
      "topic": 16,
      "similarity": 0.8050757860589824
    },
    {
      "doc": 579,
      "topic": 17,
      "similarity": 0.8051359286658117
    },
    {
      "doc": 579,
      "topic": 18,
      "similarity": 0.7517993801498494
    },
    {
      "doc": 579,
      "topic": 19,
      "similarity": 0.8117077462382875
    },
    {
      "doc": 579,
      "topic": 20,
      "similarity": 0.7560710071595126
    },
    {
      "doc": 579,
      "topic": 21,
      "similarity": 0.8031388214970416
    },
    {
      "doc": 579,
      "topic": 23,
      "similarity": 0.756091019276425
    },
    {
      "doc": 579,
      "topic": 24,
      "similarity": 0.7813866051323295
    },
    {
      "doc": 580,
      "topic": 2,
      "similarity": 0.7611965636696046
    },
    {
      "doc": 580,
      "topic": 3,
      "similarity": 0.781099705114614
    },
    {
      "doc": 580,
      "topic": 5,
      "similarity": 0.7944432534522353
    },
    {
      "doc": 580,
      "topic": 7,
      "similarity": 0.7746244392013183
    },
    {
      "doc": 580,
      "topic": 8,
      "similarity": 0.7658053930534217
    },
    {
      "doc": 580,
      "topic": 9,
      "similarity": 0.8223478580218553
    },
    {
      "doc": 580,
      "topic": 10,
      "similarity": 0.773978194095235
    },
    {
      "doc": 580,
      "topic": 11,
      "similarity": 0.7686512226815578
    },
    {
      "doc": 580,
      "topic": 13,
      "similarity": 0.7588994496869885
    },
    {
      "doc": 580,
      "topic": 14,
      "similarity": 0.761599436154851
    },
    {
      "doc": 580,
      "topic": 15,
      "similarity": 0.7875858954736389
    },
    {
      "doc": 580,
      "topic": 16,
      "similarity": 0.7993150257778843
    },
    {
      "doc": 580,
      "topic": 17,
      "similarity": 0.8003327688648795
    },
    {
      "doc": 580,
      "topic": 19,
      "similarity": 0.7945714716444182
    },
    {
      "doc": 580,
      "topic": 20,
      "similarity": 0.7955356032427648
    },
    {
      "doc": 580,
      "topic": 21,
      "similarity": 0.8192977871014954
    },
    {
      "doc": 580,
      "topic": 23,
      "similarity": 0.7507069577320308
    },
    {
      "doc": 580,
      "topic": 24,
      "similarity": 0.7514167304469153
    },
    {
      "doc": 581,
      "topic": 2,
      "similarity": 0.7648253995505006
    },
    {
      "doc": 581,
      "topic": 3,
      "similarity": 0.7874904704706879
    },
    {
      "doc": 581,
      "topic": 4,
      "similarity": 0.7502332554342896
    },
    {
      "doc": 581,
      "topic": 5,
      "similarity": 0.7944634407122658
    },
    {
      "doc": 581,
      "topic": 7,
      "similarity": 0.814784448935644
    },
    {
      "doc": 581,
      "topic": 8,
      "similarity": 0.8037423595857516
    },
    {
      "doc": 581,
      "topic": 9,
      "similarity": 0.8454501820308402
    },
    {
      "doc": 581,
      "topic": 10,
      "similarity": 0.7771193967311438
    },
    {
      "doc": 581,
      "topic": 11,
      "similarity": 0.7794486793924248
    },
    {
      "doc": 581,
      "topic": 13,
      "similarity": 0.7870536717305374
    },
    {
      "doc": 581,
      "topic": 14,
      "similarity": 0.7775574695864773
    },
    {
      "doc": 581,
      "topic": 15,
      "similarity": 0.7726552641588899
    },
    {
      "doc": 581,
      "topic": 16,
      "similarity": 0.7959875095580756
    },
    {
      "doc": 581,
      "topic": 17,
      "similarity": 0.8266354277563795
    },
    {
      "doc": 581,
      "topic": 18,
      "similarity": 0.7615868442661442
    },
    {
      "doc": 581,
      "topic": 19,
      "similarity": 0.805888382221395
    },
    {
      "doc": 581,
      "topic": 20,
      "similarity": 0.7850891698651855
    },
    {
      "doc": 581,
      "topic": 21,
      "similarity": 0.8072322160035564
    },
    {
      "doc": 581,
      "topic": 24,
      "similarity": 0.7576104256451912
    },
    {
      "doc": 582,
      "topic": 1,
      "similarity": 0.7543390072373265
    },
    {
      "doc": 582,
      "topic": 2,
      "similarity": 0.7969248543461134
    },
    {
      "doc": 582,
      "topic": 3,
      "similarity": 0.7990731051879968
    },
    {
      "doc": 582,
      "topic": 5,
      "similarity": 0.7834968175241998
    },
    {
      "doc": 582,
      "topic": 7,
      "similarity": 0.791718282187617
    },
    {
      "doc": 582,
      "topic": 8,
      "similarity": 0.7758651128479408
    },
    {
      "doc": 582,
      "topic": 9,
      "similarity": 0.8138448803150754
    },
    {
      "doc": 582,
      "topic": 10,
      "similarity": 0.7812033930324732
    },
    {
      "doc": 582,
      "topic": 11,
      "similarity": 0.7702176220164398
    },
    {
      "doc": 582,
      "topic": 12,
      "similarity": 0.7619019638537863
    },
    {
      "doc": 582,
      "topic": 13,
      "similarity": 0.7864317612159629
    },
    {
      "doc": 582,
      "topic": 14,
      "similarity": 0.7893807140778569
    },
    {
      "doc": 582,
      "topic": 15,
      "similarity": 0.7741753387835792
    },
    {
      "doc": 582,
      "topic": 16,
      "similarity": 0.7882165630522531
    },
    {
      "doc": 582,
      "topic": 17,
      "similarity": 0.7860447922754228
    },
    {
      "doc": 582,
      "topic": 18,
      "similarity": 0.7710237539801549
    },
    {
      "doc": 582,
      "topic": 19,
      "similarity": 0.8077442495449154
    },
    {
      "doc": 582,
      "topic": 20,
      "similarity": 0.7783761883240246
    },
    {
      "doc": 582,
      "topic": 21,
      "similarity": 0.7984670829716848
    },
    {
      "doc": 582,
      "topic": 22,
      "similarity": 0.7670494127674315
    },
    {
      "doc": 582,
      "topic": 23,
      "similarity": 0.7712825403262299
    },
    {
      "doc": 583,
      "topic": 2,
      "similarity": 0.759549521808651
    },
    {
      "doc": 583,
      "topic": 3,
      "similarity": 0.8017817416927364
    },
    {
      "doc": 583,
      "topic": 5,
      "similarity": 0.785232737873904
    },
    {
      "doc": 583,
      "topic": 7,
      "similarity": 0.790698914004927
    },
    {
      "doc": 583,
      "topic": 8,
      "similarity": 0.7944763497189326
    },
    {
      "doc": 583,
      "topic": 9,
      "similarity": 0.8351640477052366
    },
    {
      "doc": 583,
      "topic": 10,
      "similarity": 0.7706431371672641
    },
    {
      "doc": 583,
      "topic": 11,
      "similarity": 0.7620548184329613
    },
    {
      "doc": 583,
      "topic": 12,
      "similarity": 0.7762277473375309
    },
    {
      "doc": 583,
      "topic": 14,
      "similarity": 0.7520154020933297
    },
    {
      "doc": 583,
      "topic": 15,
      "similarity": 0.7816403889470082
    },
    {
      "doc": 583,
      "topic": 16,
      "similarity": 0.7938587816389399
    },
    {
      "doc": 583,
      "topic": 17,
      "similarity": 0.793330377690976
    },
    {
      "doc": 583,
      "topic": 18,
      "similarity": 0.7511644194380896
    },
    {
      "doc": 583,
      "topic": 19,
      "similarity": 0.7983262028287346
    },
    {
      "doc": 583,
      "topic": 20,
      "similarity": 0.7718798909074359
    },
    {
      "doc": 583,
      "topic": 21,
      "similarity": 0.8207737635375699
    },
    {
      "doc": 583,
      "topic": 24,
      "similarity": 0.7726345147240842
    },
    {
      "doc": 584,
      "topic": 2,
      "similarity": 0.7682958363150474
    },
    {
      "doc": 584,
      "topic": 3,
      "similarity": 0.7947752163396495
    },
    {
      "doc": 584,
      "topic": 4,
      "similarity": 0.7519813375422824
    },
    {
      "doc": 584,
      "topic": 5,
      "similarity": 0.8029331841778108
    },
    {
      "doc": 584,
      "topic": 6,
      "similarity": 0.7635954067364628
    },
    {
      "doc": 584,
      "topic": 7,
      "similarity": 0.7769600087226219
    },
    {
      "doc": 584,
      "topic": 8,
      "similarity": 0.7749699115155768
    },
    {
      "doc": 584,
      "topic": 9,
      "similarity": 0.8173967135649333
    },
    {
      "doc": 584,
      "topic": 10,
      "similarity": 0.7950795530451527
    },
    {
      "doc": 584,
      "topic": 11,
      "similarity": 0.7885973455986419
    },
    {
      "doc": 584,
      "topic": 13,
      "similarity": 0.7739820968074527
    },
    {
      "doc": 584,
      "topic": 14,
      "similarity": 0.7726986530384753
    },
    {
      "doc": 584,
      "topic": 15,
      "similarity": 0.7904386725455604
    },
    {
      "doc": 584,
      "topic": 16,
      "similarity": 0.8191624149558362
    },
    {
      "doc": 584,
      "topic": 17,
      "similarity": 0.8543789953646014
    },
    {
      "doc": 584,
      "topic": 18,
      "similarity": 0.7909173805488127
    },
    {
      "doc": 584,
      "topic": 19,
      "similarity": 0.809818781566881
    },
    {
      "doc": 584,
      "topic": 20,
      "similarity": 0.8052858897660591
    },
    {
      "doc": 584,
      "topic": 21,
      "similarity": 0.8312903749983972
    },
    {
      "doc": 584,
      "topic": 23,
      "similarity": 0.7606898237907933
    },
    {
      "doc": 584,
      "topic": 24,
      "similarity": 0.7627695382991351
    },
    {
      "doc": 585,
      "topic": 1,
      "similarity": 0.757619245655764
    },
    {
      "doc": 585,
      "topic": 2,
      "similarity": 0.7825862851845362
    },
    {
      "doc": 585,
      "topic": 3,
      "similarity": 0.7864601037745033
    },
    {
      "doc": 585,
      "topic": 5,
      "similarity": 0.7894469737561044
    },
    {
      "doc": 585,
      "topic": 7,
      "similarity": 0.7551280092208547
    },
    {
      "doc": 585,
      "topic": 8,
      "similarity": 0.7517808307670488
    },
    {
      "doc": 585,
      "topic": 9,
      "similarity": 0.796418520704709
    },
    {
      "doc": 585,
      "topic": 10,
      "similarity": 0.8047053031007348
    },
    {
      "doc": 585,
      "topic": 11,
      "similarity": 0.7734080190875623
    },
    {
      "doc": 585,
      "topic": 13,
      "similarity": 0.7713110774926137
    },
    {
      "doc": 585,
      "topic": 14,
      "similarity": 0.7774066352200074
    },
    {
      "doc": 585,
      "topic": 15,
      "similarity": 0.7617489947875629
    },
    {
      "doc": 585,
      "topic": 16,
      "similarity": 0.800874821228515
    },
    {
      "doc": 585,
      "topic": 17,
      "similarity": 0.7949176668857296
    },
    {
      "doc": 585,
      "topic": 18,
      "similarity": 0.7755155380699706
    },
    {
      "doc": 585,
      "topic": 19,
      "similarity": 0.808487328438844
    },
    {
      "doc": 585,
      "topic": 20,
      "similarity": 0.796103481173747
    },
    {
      "doc": 585,
      "topic": 21,
      "similarity": 0.7972201544195457
    },
    {
      "doc": 585,
      "topic": 23,
      "similarity": 0.7744191053895632
    },
    {
      "doc": 586,
      "topic": 2,
      "similarity": 0.7749511977444774
    },
    {
      "doc": 586,
      "topic": 3,
      "similarity": 0.7968222151406077
    },
    {
      "doc": 586,
      "topic": 4,
      "similarity": 0.7540812815256684
    },
    {
      "doc": 586,
      "topic": 5,
      "similarity": 0.801025006688901
    },
    {
      "doc": 586,
      "topic": 7,
      "similarity": 0.7818288168843247
    },
    {
      "doc": 586,
      "topic": 8,
      "similarity": 0.7777969807715248
    },
    {
      "doc": 586,
      "topic": 9,
      "similarity": 0.8225755233125043
    },
    {
      "doc": 586,
      "topic": 10,
      "similarity": 0.7820438663406495
    },
    {
      "doc": 586,
      "topic": 11,
      "similarity": 0.7647007020473359
    },
    {
      "doc": 586,
      "topic": 13,
      "similarity": 0.7663441809404248
    },
    {
      "doc": 586,
      "topic": 14,
      "similarity": 0.7776817324842854
    },
    {
      "doc": 586,
      "topic": 15,
      "similarity": 0.7948348863595568
    },
    {
      "doc": 586,
      "topic": 16,
      "similarity": 0.7985023731009292
    },
    {
      "doc": 586,
      "topic": 17,
      "similarity": 0.8273272070594324
    },
    {
      "doc": 586,
      "topic": 18,
      "similarity": 0.7695921603205375
    },
    {
      "doc": 586,
      "topic": 19,
      "similarity": 0.8058836906915917
    },
    {
      "doc": 586,
      "topic": 20,
      "similarity": 0.7966150483180641
    },
    {
      "doc": 586,
      "topic": 21,
      "similarity": 0.8259340634604869
    },
    {
      "doc": 586,
      "topic": 23,
      "similarity": 0.7574473123572736
    },
    {
      "doc": 586,
      "topic": 24,
      "similarity": 0.7737534132673716
    },
    {
      "doc": 587,
      "topic": 1,
      "similarity": 0.7870838205558586
    },
    {
      "doc": 587,
      "topic": 2,
      "similarity": 0.7649127846392528
    },
    {
      "doc": 587,
      "topic": 3,
      "similarity": 0.8010738909489193
    },
    {
      "doc": 587,
      "topic": 4,
      "similarity": 0.7567930467295596
    },
    {
      "doc": 587,
      "topic": 5,
      "similarity": 0.7774422209317197
    },
    {
      "doc": 587,
      "topic": 6,
      "similarity": 0.7685995682305673
    },
    {
      "doc": 587,
      "topic": 7,
      "similarity": 0.786251198852391
    },
    {
      "doc": 587,
      "topic": 8,
      "similarity": 0.7837723264468572
    },
    {
      "doc": 587,
      "topic": 9,
      "similarity": 0.8171251620948871
    },
    {
      "doc": 587,
      "topic": 10,
      "similarity": 0.7739336058557134
    },
    {
      "doc": 587,
      "topic": 11,
      "similarity": 0.753487590453374
    },
    {
      "doc": 587,
      "topic": 13,
      "similarity": 0.7683024490515848
    },
    {
      "doc": 587,
      "topic": 14,
      "similarity": 0.7685750999876029
    },
    {
      "doc": 587,
      "topic": 15,
      "similarity": 0.7572793483063219
    },
    {
      "doc": 587,
      "topic": 16,
      "similarity": 0.7817436491635151
    },
    {
      "doc": 587,
      "topic": 17,
      "similarity": 0.7770655297044788
    },
    {
      "doc": 587,
      "topic": 18,
      "similarity": 0.7509277528185738
    },
    {
      "doc": 587,
      "topic": 19,
      "similarity": 0.804071171796397
    },
    {
      "doc": 587,
      "topic": 20,
      "similarity": 0.7636266564986305
    },
    {
      "doc": 587,
      "topic": 21,
      "similarity": 0.7742997342995875
    },
    {
      "doc": 587,
      "topic": 23,
      "similarity": 0.7502316019999903
    },
    {
      "doc": 588,
      "topic": 0,
      "similarity": 0.8102167407444365
    },
    {
      "doc": 588,
      "topic": 1,
      "similarity": 0.7630457711635106
    },
    {
      "doc": 588,
      "topic": 2,
      "similarity": 0.7822574872938245
    },
    {
      "doc": 588,
      "topic": 3,
      "similarity": 0.7919368360879724
    },
    {
      "doc": 588,
      "topic": 5,
      "similarity": 0.7889296664979271
    },
    {
      "doc": 588,
      "topic": 7,
      "similarity": 0.7778368404374347
    },
    {
      "doc": 588,
      "topic": 8,
      "similarity": 0.7688175562273831
    },
    {
      "doc": 588,
      "topic": 9,
      "similarity": 0.8122048845542016
    },
    {
      "doc": 588,
      "topic": 10,
      "similarity": 0.7721445630296082
    },
    {
      "doc": 588,
      "topic": 11,
      "similarity": 0.8137605714460389
    },
    {
      "doc": 588,
      "topic": 12,
      "similarity": 0.7532342676658175
    },
    {
      "doc": 588,
      "topic": 13,
      "similarity": 0.7515325406702332
    },
    {
      "doc": 588,
      "topic": 14,
      "similarity": 0.7619816690262433
    },
    {
      "doc": 588,
      "topic": 15,
      "similarity": 0.7808384802600881
    },
    {
      "doc": 588,
      "topic": 16,
      "similarity": 0.8114534389621446
    },
    {
      "doc": 588,
      "topic": 17,
      "similarity": 0.792992223034371
    },
    {
      "doc": 588,
      "topic": 19,
      "similarity": 0.7881915894139986
    },
    {
      "doc": 588,
      "topic": 20,
      "similarity": 0.7769641950548517
    },
    {
      "doc": 588,
      "topic": 21,
      "similarity": 0.7925457701935417
    },
    {
      "doc": 588,
      "topic": 23,
      "similarity": 0.767794445934758
    },
    {
      "doc": 588,
      "topic": 24,
      "similarity": 0.7557938792898775
    },
    {
      "doc": 589,
      "topic": 3,
      "similarity": 0.77840168436627
    },
    {
      "doc": 589,
      "topic": 5,
      "similarity": 0.800723848176014
    },
    {
      "doc": 589,
      "topic": 7,
      "similarity": 0.7810711309881693
    },
    {
      "doc": 589,
      "topic": 8,
      "similarity": 0.763187713285114
    },
    {
      "doc": 589,
      "topic": 9,
      "similarity": 0.827112789423804
    },
    {
      "doc": 589,
      "topic": 10,
      "similarity": 0.7746606493960438
    },
    {
      "doc": 589,
      "topic": 11,
      "similarity": 0.7711356402211909
    },
    {
      "doc": 589,
      "topic": 14,
      "similarity": 0.7501202865906932
    },
    {
      "doc": 589,
      "topic": 15,
      "similarity": 0.7786071662830153
    },
    {
      "doc": 589,
      "topic": 16,
      "similarity": 0.7961981369529729
    },
    {
      "doc": 589,
      "topic": 17,
      "similarity": 0.8205498192220303
    },
    {
      "doc": 589,
      "topic": 18,
      "similarity": 0.7525642596048326
    },
    {
      "doc": 589,
      "topic": 19,
      "similarity": 0.7949458902364039
    },
    {
      "doc": 589,
      "topic": 20,
      "similarity": 0.7826727281314844
    },
    {
      "doc": 589,
      "topic": 21,
      "similarity": 0.7980687030472823
    },
    {
      "doc": 590,
      "topic": 1,
      "similarity": 0.7946070879742901
    },
    {
      "doc": 590,
      "topic": 2,
      "similarity": 0.7982486470201355
    },
    {
      "doc": 590,
      "topic": 3,
      "similarity": 0.8255658082629079
    },
    {
      "doc": 590,
      "topic": 4,
      "similarity": 0.7826175209281448
    },
    {
      "doc": 590,
      "topic": 5,
      "similarity": 0.839795933830422
    },
    {
      "doc": 590,
      "topic": 7,
      "similarity": 0.8269339641793073
    },
    {
      "doc": 590,
      "topic": 8,
      "similarity": 0.7927452336888741
    },
    {
      "doc": 590,
      "topic": 9,
      "similarity": 0.8454319288125507
    },
    {
      "doc": 590,
      "topic": 10,
      "similarity": 0.8025366989583628
    },
    {
      "doc": 590,
      "topic": 11,
      "similarity": 0.8094900777705585
    },
    {
      "doc": 590,
      "topic": 13,
      "similarity": 0.8004254539495573
    },
    {
      "doc": 590,
      "topic": 14,
      "similarity": 0.7922522085740095
    },
    {
      "doc": 590,
      "topic": 15,
      "similarity": 0.794950793870626
    },
    {
      "doc": 590,
      "topic": 16,
      "similarity": 0.8641898838386584
    },
    {
      "doc": 590,
      "topic": 17,
      "similarity": 0.8222588052285602
    },
    {
      "doc": 590,
      "topic": 18,
      "similarity": 0.7629411154971696
    },
    {
      "doc": 590,
      "topic": 19,
      "similarity": 0.8159721227931348
    },
    {
      "doc": 590,
      "topic": 20,
      "similarity": 0.7885514594462384
    },
    {
      "doc": 590,
      "topic": 21,
      "similarity": 0.8227551007785261
    },
    {
      "doc": 590,
      "topic": 23,
      "similarity": 0.7526313873788186
    },
    {
      "doc": 590,
      "topic": 24,
      "similarity": 0.752772833847319
    },
    {
      "doc": 591,
      "topic": 1,
      "similarity": 0.7698957938242934
    },
    {
      "doc": 591,
      "topic": 2,
      "similarity": 0.768103888191976
    },
    {
      "doc": 591,
      "topic": 3,
      "similarity": 0.8124809168753322
    },
    {
      "doc": 591,
      "topic": 4,
      "similarity": 0.7560747267219098
    },
    {
      "doc": 591,
      "topic": 5,
      "similarity": 0.8061592772308384
    },
    {
      "doc": 591,
      "topic": 7,
      "similarity": 0.7897648187926041
    },
    {
      "doc": 591,
      "topic": 8,
      "similarity": 0.7835355425677529
    },
    {
      "doc": 591,
      "topic": 9,
      "similarity": 0.8185229651436332
    },
    {
      "doc": 591,
      "topic": 10,
      "similarity": 0.7882585153274002
    },
    {
      "doc": 591,
      "topic": 11,
      "similarity": 0.7854068363963754
    },
    {
      "doc": 591,
      "topic": 13,
      "similarity": 0.782123500772983
    },
    {
      "doc": 591,
      "topic": 14,
      "similarity": 0.7861211569077196
    },
    {
      "doc": 591,
      "topic": 15,
      "similarity": 0.7720494285291244
    },
    {
      "doc": 591,
      "topic": 16,
      "similarity": 0.805759688504404
    },
    {
      "doc": 591,
      "topic": 17,
      "similarity": 0.7921767310884438
    },
    {
      "doc": 591,
      "topic": 19,
      "similarity": 0.802665232451548
    },
    {
      "doc": 591,
      "topic": 20,
      "similarity": 0.763321514678732
    },
    {
      "doc": 591,
      "topic": 21,
      "similarity": 0.7923741560621117
    },
    {
      "doc": 591,
      "topic": 23,
      "similarity": 0.7672191860298778
    },
    {
      "doc": 592,
      "topic": 2,
      "similarity": 0.7805799310605087
    },
    {
      "doc": 592,
      "topic": 3,
      "similarity": 0.7873669350487371
    },
    {
      "doc": 592,
      "topic": 5,
      "similarity": 0.7876623168468604
    },
    {
      "doc": 592,
      "topic": 7,
      "similarity": 0.7698228437497294
    },
    {
      "doc": 592,
      "topic": 8,
      "similarity": 0.7736387073495141
    },
    {
      "doc": 592,
      "topic": 9,
      "similarity": 0.8189176784518576
    },
    {
      "doc": 592,
      "topic": 10,
      "similarity": 0.7621414442638181
    },
    {
      "doc": 592,
      "topic": 11,
      "similarity": 0.7634154842777595
    },
    {
      "doc": 592,
      "topic": 13,
      "similarity": 0.7735133941231824
    },
    {
      "doc": 592,
      "topic": 14,
      "similarity": 0.7602457021169166
    },
    {
      "doc": 592,
      "topic": 15,
      "similarity": 0.7820444178398198
    },
    {
      "doc": 592,
      "topic": 16,
      "similarity": 0.8052808177759472
    },
    {
      "doc": 592,
      "topic": 17,
      "similarity": 0.8180672602332464
    },
    {
      "doc": 592,
      "topic": 19,
      "similarity": 0.7982992461952674
    },
    {
      "doc": 592,
      "topic": 20,
      "similarity": 0.7899918547462232
    },
    {
      "doc": 592,
      "topic": 21,
      "similarity": 0.8528833893556416
    },
    {
      "doc": 593,
      "topic": 2,
      "similarity": 0.7635005356828948
    },
    {
      "doc": 593,
      "topic": 3,
      "similarity": 0.7894809297362201
    },
    {
      "doc": 593,
      "topic": 5,
      "similarity": 0.7747975998794377
    },
    {
      "doc": 593,
      "topic": 7,
      "similarity": 0.751736826060776
    },
    {
      "doc": 593,
      "topic": 8,
      "similarity": 0.7693293672132552
    },
    {
      "doc": 593,
      "topic": 9,
      "similarity": 0.8128073764415122
    },
    {
      "doc": 593,
      "topic": 10,
      "similarity": 0.7676179977641454
    },
    {
      "doc": 593,
      "topic": 11,
      "similarity": 0.7647739985434894
    },
    {
      "doc": 593,
      "topic": 13,
      "similarity": 0.7535162832017877
    },
    {
      "doc": 593,
      "topic": 14,
      "similarity": 0.7547281851825779
    },
    {
      "doc": 593,
      "topic": 15,
      "similarity": 0.7729234192184221
    },
    {
      "doc": 593,
      "topic": 16,
      "similarity": 0.7957970191277751
    },
    {
      "doc": 593,
      "topic": 17,
      "similarity": 0.8065561622626953
    },
    {
      "doc": 593,
      "topic": 19,
      "similarity": 0.7892835400408528
    },
    {
      "doc": 593,
      "topic": 20,
      "similarity": 0.7744922108346375
    },
    {
      "doc": 593,
      "topic": 21,
      "similarity": 0.8555384112993805
    },
    {
      "doc": 593,
      "topic": 24,
      "similarity": 0.7642742418345888
    },
    {
      "doc": 594,
      "topic": 1,
      "similarity": 0.7744821288736076
    },
    {
      "doc": 594,
      "topic": 2,
      "similarity": 0.7887719981106641
    },
    {
      "doc": 594,
      "topic": 3,
      "similarity": 0.7978655075941739
    },
    {
      "doc": 594,
      "topic": 4,
      "similarity": 0.7592237868970622
    },
    {
      "doc": 594,
      "topic": 5,
      "similarity": 0.7789847340671842
    },
    {
      "doc": 594,
      "topic": 7,
      "similarity": 0.8019084793278187
    },
    {
      "doc": 594,
      "topic": 8,
      "similarity": 0.7774429389836076
    },
    {
      "doc": 594,
      "topic": 9,
      "similarity": 0.8483174378981778
    },
    {
      "doc": 594,
      "topic": 10,
      "similarity": 0.7674374088672522
    },
    {
      "doc": 594,
      "topic": 11,
      "similarity": 0.7712967188089811
    },
    {
      "doc": 594,
      "topic": 12,
      "similarity": 0.7535856539249153
    },
    {
      "doc": 594,
      "topic": 13,
      "similarity": 0.7729492634162033
    },
    {
      "doc": 594,
      "topic": 14,
      "similarity": 0.7885998072157596
    },
    {
      "doc": 594,
      "topic": 15,
      "similarity": 0.7680119558702242
    },
    {
      "doc": 594,
      "topic": 16,
      "similarity": 0.7848692203961661
    },
    {
      "doc": 594,
      "topic": 17,
      "similarity": 0.7782780962352024
    },
    {
      "doc": 594,
      "topic": 18,
      "similarity": 0.7514152238117018
    },
    {
      "doc": 594,
      "topic": 19,
      "similarity": 0.800051809404487
    },
    {
      "doc": 594,
      "topic": 21,
      "similarity": 0.7793167567497197
    },
    {
      "doc": 594,
      "topic": 23,
      "similarity": 0.7756726140694283
    },
    {
      "doc": 594,
      "topic": 24,
      "similarity": 0.7561422362944575
    },
    {
      "doc": 595,
      "topic": 1,
      "similarity": 0.7538942208363458
    },
    {
      "doc": 595,
      "topic": 2,
      "similarity": 0.7841777754974535
    },
    {
      "doc": 595,
      "topic": 3,
      "similarity": 0.8129523337509309
    },
    {
      "doc": 595,
      "topic": 4,
      "similarity": 0.761167193913286
    },
    {
      "doc": 595,
      "topic": 5,
      "similarity": 0.7943932345346525
    },
    {
      "doc": 595,
      "topic": 7,
      "similarity": 0.8102251119364888
    },
    {
      "doc": 595,
      "topic": 8,
      "similarity": 0.7897935275375227
    },
    {
      "doc": 595,
      "topic": 9,
      "similarity": 0.8686772340856889
    },
    {
      "doc": 595,
      "topic": 10,
      "similarity": 0.7810826768173239
    },
    {
      "doc": 595,
      "topic": 11,
      "similarity": 0.8006703565112543
    },
    {
      "doc": 595,
      "topic": 13,
      "similarity": 0.7796213205850552
    },
    {
      "doc": 595,
      "topic": 14,
      "similarity": 0.7801964785403745
    },
    {
      "doc": 595,
      "topic": 15,
      "similarity": 0.7995353719659326
    },
    {
      "doc": 595,
      "topic": 16,
      "similarity": 0.7986713582694344
    },
    {
      "doc": 595,
      "topic": 17,
      "similarity": 0.791990630896249
    },
    {
      "doc": 595,
      "topic": 18,
      "similarity": 0.7631327773420805
    },
    {
      "doc": 595,
      "topic": 19,
      "similarity": 0.8030139449546431
    },
    {
      "doc": 595,
      "topic": 20,
      "similarity": 0.7826349221700272
    },
    {
      "doc": 595,
      "topic": 21,
      "similarity": 0.8166603620138644
    },
    {
      "doc": 595,
      "topic": 24,
      "similarity": 0.7684927276399087
    },
    {
      "doc": 596,
      "topic": 2,
      "similarity": 0.7504665122533373
    },
    {
      "doc": 596,
      "topic": 3,
      "similarity": 0.8014611569914862
    },
    {
      "doc": 596,
      "topic": 4,
      "similarity": 0.7559890409933637
    },
    {
      "doc": 596,
      "topic": 5,
      "similarity": 0.7931153816459922
    },
    {
      "doc": 596,
      "topic": 6,
      "similarity": 0.7612207934834084
    },
    {
      "doc": 596,
      "topic": 7,
      "similarity": 0.7609126961736195
    },
    {
      "doc": 596,
      "topic": 8,
      "similarity": 0.7685458899565125
    },
    {
      "doc": 596,
      "topic": 9,
      "similarity": 0.8265774231616034
    },
    {
      "doc": 596,
      "topic": 10,
      "similarity": 0.7702966467464963
    },
    {
      "doc": 596,
      "topic": 11,
      "similarity": 0.7808517210031277
    },
    {
      "doc": 596,
      "topic": 13,
      "similarity": 0.7576590529458556
    },
    {
      "doc": 596,
      "topic": 14,
      "similarity": 0.7503114845492378
    },
    {
      "doc": 596,
      "topic": 15,
      "similarity": 0.7736221929997017
    },
    {
      "doc": 596,
      "topic": 16,
      "similarity": 0.8040425029762038
    },
    {
      "doc": 596,
      "topic": 17,
      "similarity": 0.8176286819448412
    },
    {
      "doc": 596,
      "topic": 18,
      "similarity": 0.7646573734880732
    },
    {
      "doc": 596,
      "topic": 19,
      "similarity": 0.7971765128358895
    },
    {
      "doc": 596,
      "topic": 20,
      "similarity": 0.7887342645864405
    },
    {
      "doc": 596,
      "topic": 21,
      "similarity": 0.8242219493211587
    },
    {
      "doc": 596,
      "topic": 23,
      "similarity": 0.7641354676542563
    },
    {
      "doc": 596,
      "topic": 24,
      "similarity": 0.7837423208366604
    },
    {
      "doc": 597,
      "topic": 2,
      "similarity": 0.7551384295897374
    },
    {
      "doc": 597,
      "topic": 3,
      "similarity": 0.7644212217910535
    },
    {
      "doc": 597,
      "topic": 5,
      "similarity": 0.7680683795357379
    },
    {
      "doc": 597,
      "topic": 6,
      "similarity": 0.758835954284682
    },
    {
      "doc": 597,
      "topic": 9,
      "similarity": 0.7792406934132845
    },
    {
      "doc": 597,
      "topic": 10,
      "similarity": 0.7623831304776418
    },
    {
      "doc": 597,
      "topic": 11,
      "similarity": 0.76348268007606
    },
    {
      "doc": 597,
      "topic": 15,
      "similarity": 0.7767452933531476
    },
    {
      "doc": 597,
      "topic": 16,
      "similarity": 0.7771708589583872
    },
    {
      "doc": 597,
      "topic": 17,
      "similarity": 0.818733584352814
    },
    {
      "doc": 597,
      "topic": 18,
      "similarity": 0.7892840547067186
    },
    {
      "doc": 597,
      "topic": 19,
      "similarity": 0.8064206275609522
    },
    {
      "doc": 597,
      "topic": 20,
      "similarity": 0.7976707592556174
    },
    {
      "doc": 597,
      "topic": 21,
      "similarity": 0.8090621742225744
    },
    {
      "doc": 597,
      "topic": 23,
      "similarity": 0.7651718744894531
    },
    {
      "doc": 598,
      "topic": 3,
      "similarity": 0.7785216179134068
    },
    {
      "doc": 598,
      "topic": 5,
      "similarity": 0.7832540148246498
    },
    {
      "doc": 598,
      "topic": 7,
      "similarity": 0.7673702446433898
    },
    {
      "doc": 598,
      "topic": 8,
      "similarity": 0.7550484860958473
    },
    {
      "doc": 598,
      "topic": 9,
      "similarity": 0.7900493090849763
    },
    {
      "doc": 598,
      "topic": 10,
      "similarity": 0.7542140232617572
    },
    {
      "doc": 598,
      "topic": 11,
      "similarity": 0.7745364964816089
    },
    {
      "doc": 598,
      "topic": 13,
      "similarity": 0.7585384304529194
    },
    {
      "doc": 598,
      "topic": 14,
      "similarity": 0.7544705133514595
    },
    {
      "doc": 598,
      "topic": 15,
      "similarity": 0.7652037489303647
    },
    {
      "doc": 598,
      "topic": 16,
      "similarity": 0.7994798163772157
    },
    {
      "doc": 598,
      "topic": 17,
      "similarity": 0.7830847448983159
    },
    {
      "doc": 598,
      "topic": 18,
      "similarity": 0.7561816854466721
    },
    {
      "doc": 598,
      "topic": 19,
      "similarity": 0.789153692890771
    },
    {
      "doc": 598,
      "topic": 20,
      "similarity": 0.7884653016748021
    },
    {
      "doc": 598,
      "topic": 21,
      "similarity": 0.8023367702185868
    },
    {
      "doc": 599,
      "topic": 2,
      "similarity": 0.7553993555912012
    },
    {
      "doc": 599,
      "topic": 3,
      "similarity": 0.7874311834443537
    },
    {
      "doc": 599,
      "topic": 5,
      "similarity": 0.7989735863288759
    },
    {
      "doc": 599,
      "topic": 7,
      "similarity": 0.7695315083459932
    },
    {
      "doc": 599,
      "topic": 8,
      "similarity": 0.7719055166689504
    },
    {
      "doc": 599,
      "topic": 9,
      "similarity": 0.8231149039901743
    },
    {
      "doc": 599,
      "topic": 10,
      "similarity": 0.7814412490880273
    },
    {
      "doc": 599,
      "topic": 11,
      "similarity": 0.7694287305669385
    },
    {
      "doc": 599,
      "topic": 13,
      "similarity": 0.7616389176051486
    },
    {
      "doc": 599,
      "topic": 14,
      "similarity": 0.7538731471072451
    },
    {
      "doc": 599,
      "topic": 15,
      "similarity": 0.7751523248673026
    },
    {
      "doc": 599,
      "topic": 16,
      "similarity": 0.8045300755595933
    },
    {
      "doc": 599,
      "topic": 17,
      "similarity": 0.7973681930491604
    },
    {
      "doc": 599,
      "topic": 19,
      "similarity": 0.7852677730691829
    },
    {
      "doc": 599,
      "topic": 20,
      "similarity": 0.794365817781662
    },
    {
      "doc": 599,
      "topic": 21,
      "similarity": 0.8205804428651297
    },
    {
      "doc": 600,
      "topic": 1,
      "similarity": 0.7593811355964344
    },
    {
      "doc": 600,
      "topic": 2,
      "similarity": 0.7854980423449023
    },
    {
      "doc": 600,
      "topic": 3,
      "similarity": 0.8063726978796775
    },
    {
      "doc": 600,
      "topic": 5,
      "similarity": 0.8098073233590242
    },
    {
      "doc": 600,
      "topic": 7,
      "similarity": 0.7872476444828566
    },
    {
      "doc": 600,
      "topic": 8,
      "similarity": 0.795386770370411
    },
    {
      "doc": 600,
      "topic": 9,
      "similarity": 0.8312048069911094
    },
    {
      "doc": 600,
      "topic": 10,
      "similarity": 0.7957032113636734
    },
    {
      "doc": 600,
      "topic": 11,
      "similarity": 0.8017893241521807
    },
    {
      "doc": 600,
      "topic": 12,
      "similarity": 0.8075160950864573
    },
    {
      "doc": 600,
      "topic": 13,
      "similarity": 0.7844468472694022
    },
    {
      "doc": 600,
      "topic": 14,
      "similarity": 0.7740891685605911
    },
    {
      "doc": 600,
      "topic": 15,
      "similarity": 0.8158185622738915
    },
    {
      "doc": 600,
      "topic": 16,
      "similarity": 0.8147906148914229
    },
    {
      "doc": 600,
      "topic": 17,
      "similarity": 0.8241726839607031
    },
    {
      "doc": 600,
      "topic": 18,
      "similarity": 0.793041913501397
    },
    {
      "doc": 600,
      "topic": 19,
      "similarity": 0.8283233230069147
    },
    {
      "doc": 600,
      "topic": 20,
      "similarity": 0.8238813942393733
    },
    {
      "doc": 600,
      "topic": 21,
      "similarity": 0.8069680135191946
    },
    {
      "doc": 600,
      "topic": 22,
      "similarity": 0.7613495251116262
    },
    {
      "doc": 600,
      "topic": 24,
      "similarity": 0.7556682096617903
    },
    {
      "doc": 601,
      "topic": 1,
      "similarity": 0.7577173716418657
    },
    {
      "doc": 601,
      "topic": 2,
      "similarity": 0.7681946356619018
    },
    {
      "doc": 601,
      "topic": 3,
      "similarity": 0.8047350171845636
    },
    {
      "doc": 601,
      "topic": 4,
      "similarity": 0.7558771357523095
    },
    {
      "doc": 601,
      "topic": 5,
      "similarity": 0.7853652008170117
    },
    {
      "doc": 601,
      "topic": 6,
      "similarity": 0.7848130646715172
    },
    {
      "doc": 601,
      "topic": 7,
      "similarity": 0.7769236551670081
    },
    {
      "doc": 601,
      "topic": 8,
      "similarity": 0.7777913059997781
    },
    {
      "doc": 601,
      "topic": 9,
      "similarity": 0.8132226864613148
    },
    {
      "doc": 601,
      "topic": 10,
      "similarity": 0.7647708867112275
    },
    {
      "doc": 601,
      "topic": 11,
      "similarity": 0.7689391768308584
    },
    {
      "doc": 601,
      "topic": 13,
      "similarity": 0.7683742127123208
    },
    {
      "doc": 601,
      "topic": 14,
      "similarity": 0.7771537347590853
    },
    {
      "doc": 601,
      "topic": 15,
      "similarity": 0.771520477239736
    },
    {
      "doc": 601,
      "topic": 16,
      "similarity": 0.7811948455064368
    },
    {
      "doc": 601,
      "topic": 17,
      "similarity": 0.7937333585598288
    },
    {
      "doc": 601,
      "topic": 19,
      "similarity": 0.8019107188709721
    },
    {
      "doc": 601,
      "topic": 20,
      "similarity": 0.761922213748946
    },
    {
      "doc": 601,
      "topic": 21,
      "similarity": 0.7886576195043402
    },
    {
      "doc": 601,
      "topic": 23,
      "similarity": 0.7597949463184686
    },
    {
      "doc": 602,
      "topic": 1,
      "similarity": 0.7729062929681487
    },
    {
      "doc": 602,
      "topic": 2,
      "similarity": 0.7838769328260758
    },
    {
      "doc": 602,
      "topic": 3,
      "similarity": 0.8157991309770409
    },
    {
      "doc": 602,
      "topic": 4,
      "similarity": 0.7734238709779359
    },
    {
      "doc": 602,
      "topic": 5,
      "similarity": 0.8197118838663243
    },
    {
      "doc": 602,
      "topic": 7,
      "similarity": 0.7984842515149854
    },
    {
      "doc": 602,
      "topic": 8,
      "similarity": 0.7804190717447798
    },
    {
      "doc": 602,
      "topic": 9,
      "similarity": 0.824178047358813
    },
    {
      "doc": 602,
      "topic": 10,
      "similarity": 0.8173518857935689
    },
    {
      "doc": 602,
      "topic": 11,
      "similarity": 0.787767726197157
    },
    {
      "doc": 602,
      "topic": 13,
      "similarity": 0.7816083575072807
    },
    {
      "doc": 602,
      "topic": 14,
      "similarity": 0.7910756095698649
    },
    {
      "doc": 602,
      "topic": 15,
      "similarity": 0.7889704521229783
    },
    {
      "doc": 602,
      "topic": 16,
      "similarity": 0.8213921457022149
    },
    {
      "doc": 602,
      "topic": 17,
      "similarity": 0.8180982373745256
    },
    {
      "doc": 602,
      "topic": 18,
      "similarity": 0.7896929780832
    },
    {
      "doc": 602,
      "topic": 19,
      "similarity": 0.8436309427956199
    },
    {
      "doc": 602,
      "topic": 20,
      "similarity": 0.7997845299168503
    },
    {
      "doc": 602,
      "topic": 21,
      "similarity": 0.8196534466529483
    },
    {
      "doc": 602,
      "topic": 23,
      "similarity": 0.7597613446347273
    },
    {
      "doc": 603,
      "topic": 1,
      "similarity": 0.7714976724482107
    },
    {
      "doc": 603,
      "topic": 2,
      "similarity": 0.7752788867501477
    },
    {
      "doc": 603,
      "topic": 3,
      "similarity": 0.8151606416957793
    },
    {
      "doc": 603,
      "topic": 4,
      "similarity": 0.7881046037762759
    },
    {
      "doc": 603,
      "topic": 5,
      "similarity": 0.8159559465283902
    },
    {
      "doc": 603,
      "topic": 7,
      "similarity": 0.7974975636738015
    },
    {
      "doc": 603,
      "topic": 8,
      "similarity": 0.7951403889620982
    },
    {
      "doc": 603,
      "topic": 9,
      "similarity": 0.8409825880825106
    },
    {
      "doc": 603,
      "topic": 10,
      "similarity": 0.8303916207168507
    },
    {
      "doc": 603,
      "topic": 11,
      "similarity": 0.7942580267412722
    },
    {
      "doc": 603,
      "topic": 13,
      "similarity": 0.7787939533459386
    },
    {
      "doc": 603,
      "topic": 14,
      "similarity": 0.7853811125757069
    },
    {
      "doc": 603,
      "topic": 15,
      "similarity": 0.7904557503683254
    },
    {
      "doc": 603,
      "topic": 16,
      "similarity": 0.8169388449211927
    },
    {
      "doc": 603,
      "topic": 17,
      "similarity": 0.8075298686091334
    },
    {
      "doc": 603,
      "topic": 18,
      "similarity": 0.7817104149500366
    },
    {
      "doc": 603,
      "topic": 19,
      "similarity": 0.8279000759927619
    },
    {
      "doc": 603,
      "topic": 20,
      "similarity": 0.7845854497094858
    },
    {
      "doc": 603,
      "topic": 21,
      "similarity": 0.8172273910672767
    },
    {
      "doc": 603,
      "topic": 23,
      "similarity": 0.7634003491250614
    },
    {
      "doc": 603,
      "topic": 24,
      "similarity": 0.7542448218954799
    },
    {
      "doc": 604,
      "topic": 1,
      "similarity": 0.7519140304041845
    },
    {
      "doc": 604,
      "topic": 2,
      "similarity": 0.7829931268205718
    },
    {
      "doc": 604,
      "topic": 3,
      "similarity": 0.8043453923650372
    },
    {
      "doc": 604,
      "topic": 5,
      "similarity": 0.8092053891440247
    },
    {
      "doc": 604,
      "topic": 7,
      "similarity": 0.7803782862244745
    },
    {
      "doc": 604,
      "topic": 8,
      "similarity": 0.779890961220585
    },
    {
      "doc": 604,
      "topic": 9,
      "similarity": 0.8465455032221576
    },
    {
      "doc": 604,
      "topic": 10,
      "similarity": 0.7940317658261355
    },
    {
      "doc": 604,
      "topic": 11,
      "similarity": 0.7959265446172016
    },
    {
      "doc": 604,
      "topic": 12,
      "similarity": 0.7538292867558597
    },
    {
      "doc": 604,
      "topic": 13,
      "similarity": 0.7760067762386714
    },
    {
      "doc": 604,
      "topic": 14,
      "similarity": 0.7766343562117066
    },
    {
      "doc": 604,
      "topic": 15,
      "similarity": 0.8034258067481677
    },
    {
      "doc": 604,
      "topic": 16,
      "similarity": 0.8156938681480774
    },
    {
      "doc": 604,
      "topic": 17,
      "similarity": 0.8188491712948852
    },
    {
      "doc": 604,
      "topic": 18,
      "similarity": 0.7738018611980326
    },
    {
      "doc": 604,
      "topic": 19,
      "similarity": 0.7987971404284033
    },
    {
      "doc": 604,
      "topic": 20,
      "similarity": 0.7984742196130081
    },
    {
      "doc": 604,
      "topic": 21,
      "similarity": 0.8156972886618362
    },
    {
      "doc": 604,
      "topic": 22,
      "similarity": 0.7506151852420084
    },
    {
      "doc": 604,
      "topic": 23,
      "similarity": 0.7558101372531101
    },
    {
      "doc": 604,
      "topic": 24,
      "similarity": 0.7722239440185648
    },
    {
      "doc": 605,
      "topic": 2,
      "similarity": 0.7680177134889608
    },
    {
      "doc": 605,
      "topic": 3,
      "similarity": 0.786917577423422
    },
    {
      "doc": 605,
      "topic": 4,
      "similarity": 0.7502521462563786
    },
    {
      "doc": 605,
      "topic": 5,
      "similarity": 0.7928030964783357
    },
    {
      "doc": 605,
      "topic": 7,
      "similarity": 0.7618134768365361
    },
    {
      "doc": 605,
      "topic": 8,
      "similarity": 0.7519045434831075
    },
    {
      "doc": 605,
      "topic": 9,
      "similarity": 0.8101376021579612
    },
    {
      "doc": 605,
      "topic": 10,
      "similarity": 0.7738024529126951
    },
    {
      "doc": 605,
      "topic": 11,
      "similarity": 0.7756683925905772
    },
    {
      "doc": 605,
      "topic": 13,
      "similarity": 0.7575547715765247
    },
    {
      "doc": 605,
      "topic": 14,
      "similarity": 0.7720648469233236
    },
    {
      "doc": 605,
      "topic": 15,
      "similarity": 0.7734335222563883
    },
    {
      "doc": 605,
      "topic": 16,
      "similarity": 0.7982322247921962
    },
    {
      "doc": 605,
      "topic": 17,
      "similarity": 0.7890933544638092
    },
    {
      "doc": 605,
      "topic": 19,
      "similarity": 0.7951375514668074
    },
    {
      "doc": 605,
      "topic": 20,
      "similarity": 0.7727423037463366
    },
    {
      "doc": 605,
      "topic": 21,
      "similarity": 0.8100231221362647
    },
    {
      "doc": 606,
      "topic": 1,
      "similarity": 0.7672664873890116
    },
    {
      "doc": 606,
      "topic": 2,
      "similarity": 0.779206056280173
    },
    {
      "doc": 606,
      "topic": 3,
      "similarity": 0.8006545647699067
    },
    {
      "doc": 606,
      "topic": 5,
      "similarity": 0.7878073389971672
    },
    {
      "doc": 606,
      "topic": 7,
      "similarity": 0.7857032930238245
    },
    {
      "doc": 606,
      "topic": 8,
      "similarity": 0.7790965971565535
    },
    {
      "doc": 606,
      "topic": 9,
      "similarity": 0.8073250998543933
    },
    {
      "doc": 606,
      "topic": 10,
      "similarity": 0.7621800715112889
    },
    {
      "doc": 606,
      "topic": 11,
      "similarity": 0.7597816812877387
    },
    {
      "doc": 606,
      "topic": 13,
      "similarity": 0.7631914092480385
    },
    {
      "doc": 606,
      "topic": 14,
      "similarity": 0.7767782200663366
    },
    {
      "doc": 606,
      "topic": 15,
      "similarity": 0.7626136747841731
    },
    {
      "doc": 606,
      "topic": 16,
      "similarity": 0.7801347954108254
    },
    {
      "doc": 606,
      "topic": 17,
      "similarity": 0.7722471756367179
    },
    {
      "doc": 606,
      "topic": 19,
      "similarity": 0.797842549215132
    },
    {
      "doc": 606,
      "topic": 20,
      "similarity": 0.7591661767569198
    },
    {
      "doc": 606,
      "topic": 21,
      "similarity": 0.7717703304014125
    },
    {
      "doc": 606,
      "topic": 23,
      "similarity": 0.7617063383217112
    },
    {
      "doc": 607,
      "topic": 1,
      "similarity": 0.782313664689149
    },
    {
      "doc": 607,
      "topic": 2,
      "similarity": 0.7918519420478083
    },
    {
      "doc": 607,
      "topic": 3,
      "similarity": 0.816202712072933
    },
    {
      "doc": 607,
      "topic": 4,
      "similarity": 0.7686625499659369
    },
    {
      "doc": 607,
      "topic": 5,
      "similarity": 0.8313247265811649
    },
    {
      "doc": 607,
      "topic": 7,
      "similarity": 0.8063946946275983
    },
    {
      "doc": 607,
      "topic": 8,
      "similarity": 0.8080694136723574
    },
    {
      "doc": 607,
      "topic": 9,
      "similarity": 0.8540699109021471
    },
    {
      "doc": 607,
      "topic": 10,
      "similarity": 0.8216566575381589
    },
    {
      "doc": 607,
      "topic": 11,
      "similarity": 0.8159028078597659
    },
    {
      "doc": 607,
      "topic": 13,
      "similarity": 0.7886967034903714
    },
    {
      "doc": 607,
      "topic": 14,
      "similarity": 0.7900063750985109
    },
    {
      "doc": 607,
      "topic": 15,
      "similarity": 0.8039260024005311
    },
    {
      "doc": 607,
      "topic": 16,
      "similarity": 0.8300425806240157
    },
    {
      "doc": 607,
      "topic": 17,
      "similarity": 0.8261458551492291
    },
    {
      "doc": 607,
      "topic": 18,
      "similarity": 0.7662859642794506
    },
    {
      "doc": 607,
      "topic": 19,
      "similarity": 0.8453521321155971
    },
    {
      "doc": 607,
      "topic": 20,
      "similarity": 0.8074282009720667
    },
    {
      "doc": 607,
      "topic": 21,
      "similarity": 0.822877231374657
    },
    {
      "doc": 607,
      "topic": 24,
      "similarity": 0.7555573337619433
    },
    {
      "doc": 608,
      "topic": 0,
      "similarity": 0.7615662607401128
    },
    {
      "doc": 608,
      "topic": 3,
      "similarity": 0.7721740777344607
    },
    {
      "doc": 608,
      "topic": 5,
      "similarity": 0.7618645217728351
    },
    {
      "doc": 608,
      "topic": 7,
      "similarity": 0.7576485985121217
    },
    {
      "doc": 608,
      "topic": 9,
      "similarity": 0.786130106350269
    },
    {
      "doc": 608,
      "topic": 10,
      "similarity": 0.7634018556017748
    },
    {
      "doc": 608,
      "topic": 11,
      "similarity": 0.7632287419531655
    },
    {
      "doc": 608,
      "topic": 16,
      "similarity": 0.7787601216045601
    },
    {
      "doc": 608,
      "topic": 17,
      "similarity": 0.7569047968388241
    },
    {
      "doc": 608,
      "topic": 19,
      "similarity": 0.7827732431147802
    },
    {
      "doc": 608,
      "topic": 20,
      "similarity": 0.7539895840485659
    },
    {
      "doc": 608,
      "topic": 21,
      "similarity": 0.7563689437213842
    },
    {
      "doc": 608,
      "topic": 23,
      "similarity": 0.7526706508655239
    },
    {
      "doc": 609,
      "topic": 1,
      "similarity": 0.7502242197873586
    },
    {
      "doc": 609,
      "topic": 2,
      "similarity": 0.7705922349691872
    },
    {
      "doc": 609,
      "topic": 3,
      "similarity": 0.7990170116757155
    },
    {
      "doc": 609,
      "topic": 4,
      "similarity": 0.7607146246936329
    },
    {
      "doc": 609,
      "topic": 5,
      "similarity": 0.7975377138352364
    },
    {
      "doc": 609,
      "topic": 7,
      "similarity": 0.7884799496281872
    },
    {
      "doc": 609,
      "topic": 8,
      "similarity": 0.7861784941440124
    },
    {
      "doc": 609,
      "topic": 9,
      "similarity": 0.834792881503395
    },
    {
      "doc": 609,
      "topic": 10,
      "similarity": 0.7865814122914703
    },
    {
      "doc": 609,
      "topic": 11,
      "similarity": 0.7968546769084389
    },
    {
      "doc": 609,
      "topic": 13,
      "similarity": 0.778971410911649
    },
    {
      "doc": 609,
      "topic": 14,
      "similarity": 0.7883745185401629
    },
    {
      "doc": 609,
      "topic": 15,
      "similarity": 0.7751282241888967
    },
    {
      "doc": 609,
      "topic": 16,
      "similarity": 0.8159551410417489
    },
    {
      "doc": 609,
      "topic": 17,
      "similarity": 0.7872615825679774
    },
    {
      "doc": 609,
      "topic": 18,
      "similarity": 0.8070882861208488
    },
    {
      "doc": 609,
      "topic": 19,
      "similarity": 0.8435102478441259
    },
    {
      "doc": 609,
      "topic": 20,
      "similarity": 0.7662632861507064
    },
    {
      "doc": 609,
      "topic": 21,
      "similarity": 0.792638557595416
    },
    {
      "doc": 609,
      "topic": 23,
      "similarity": 0.7531588311778916
    },
    {
      "doc": 610,
      "topic": 1,
      "similarity": 0.7680698145866849
    },
    {
      "doc": 610,
      "topic": 2,
      "similarity": 0.7697786966607587
    },
    {
      "doc": 610,
      "topic": 3,
      "similarity": 0.8111094493701865
    },
    {
      "doc": 610,
      "topic": 4,
      "similarity": 0.7601434318936592
    },
    {
      "doc": 610,
      "topic": 5,
      "similarity": 0.8124940584599464
    },
    {
      "doc": 610,
      "topic": 7,
      "similarity": 0.7881898011546878
    },
    {
      "doc": 610,
      "topic": 8,
      "similarity": 0.7735767161641396
    },
    {
      "doc": 610,
      "topic": 9,
      "similarity": 0.8350750842640073
    },
    {
      "doc": 610,
      "topic": 10,
      "similarity": 0.7901341499624907
    },
    {
      "doc": 610,
      "topic": 11,
      "similarity": 0.806069594633933
    },
    {
      "doc": 610,
      "topic": 13,
      "similarity": 0.7814102916998228
    },
    {
      "doc": 610,
      "topic": 14,
      "similarity": 0.7784139345232771
    },
    {
      "doc": 610,
      "topic": 15,
      "similarity": 0.788453809267822
    },
    {
      "doc": 610,
      "topic": 16,
      "similarity": 0.8205756970743404
    },
    {
      "doc": 610,
      "topic": 17,
      "similarity": 0.8209185419929199
    },
    {
      "doc": 610,
      "topic": 18,
      "similarity": 0.7734424782593702
    },
    {
      "doc": 610,
      "topic": 19,
      "similarity": 0.8090763044372826
    },
    {
      "doc": 610,
      "topic": 20,
      "similarity": 0.7976139200460824
    },
    {
      "doc": 610,
      "topic": 21,
      "similarity": 0.8241160560928351
    },
    {
      "doc": 610,
      "topic": 23,
      "similarity": 0.7755067645867115
    },
    {
      "doc": 610,
      "topic": 24,
      "similarity": 0.7867438316122245
    },
    {
      "doc": 611,
      "topic": 2,
      "similarity": 0.7566838883692288
    },
    {
      "doc": 611,
      "topic": 3,
      "similarity": 0.784235949770353
    },
    {
      "doc": 611,
      "topic": 4,
      "similarity": 0.755118721505862
    },
    {
      "doc": 611,
      "topic": 5,
      "similarity": 0.7750789973309475
    },
    {
      "doc": 611,
      "topic": 7,
      "similarity": 0.7581814010978717
    },
    {
      "doc": 611,
      "topic": 8,
      "similarity": 0.7629929552671104
    },
    {
      "doc": 611,
      "topic": 9,
      "similarity": 0.8076076350759883
    },
    {
      "doc": 611,
      "topic": 10,
      "similarity": 0.8034591368926255
    },
    {
      "doc": 611,
      "topic": 11,
      "similarity": 0.7852391340644773
    },
    {
      "doc": 611,
      "topic": 13,
      "similarity": 0.7514476787054001
    },
    {
      "doc": 611,
      "topic": 15,
      "similarity": 0.7784260539790386
    },
    {
      "doc": 611,
      "topic": 16,
      "similarity": 0.7788351686054892
    },
    {
      "doc": 611,
      "topic": 17,
      "similarity": 0.7941107859218935
    },
    {
      "doc": 611,
      "topic": 18,
      "similarity": 0.7650402793533982
    },
    {
      "doc": 611,
      "topic": 19,
      "similarity": 0.7913526280089598
    },
    {
      "doc": 611,
      "topic": 20,
      "similarity": 0.7730907594512411
    },
    {
      "doc": 611,
      "topic": 21,
      "similarity": 0.8103743922557454
    },
    {
      "doc": 611,
      "topic": 23,
      "similarity": 0.7544226099413652
    },
    {
      "doc": 611,
      "topic": 24,
      "similarity": 0.7886376323873572
    },
    {
      "doc": 612,
      "topic": 2,
      "similarity": 0.7681400033386013
    },
    {
      "doc": 612,
      "topic": 3,
      "similarity": 0.8168563133404835
    },
    {
      "doc": 612,
      "topic": 4,
      "similarity": 0.7954555236069144
    },
    {
      "doc": 612,
      "topic": 5,
      "similarity": 0.7884722437238948
    },
    {
      "doc": 612,
      "topic": 7,
      "similarity": 0.7949031214496446
    },
    {
      "doc": 612,
      "topic": 8,
      "similarity": 0.7748463623768499
    },
    {
      "doc": 612,
      "topic": 9,
      "similarity": 0.8526071793550188
    },
    {
      "doc": 612,
      "topic": 10,
      "similarity": 0.7728770548783304
    },
    {
      "doc": 612,
      "topic": 11,
      "similarity": 0.7738788811688073
    },
    {
      "doc": 612,
      "topic": 13,
      "similarity": 0.7649214621915368
    },
    {
      "doc": 612,
      "topic": 14,
      "similarity": 0.7701887350193641
    },
    {
      "doc": 612,
      "topic": 15,
      "similarity": 0.7636899466741202
    },
    {
      "doc": 612,
      "topic": 16,
      "similarity": 0.796236752372352
    },
    {
      "doc": 612,
      "topic": 17,
      "similarity": 0.7948097326322199
    },
    {
      "doc": 612,
      "topic": 19,
      "similarity": 0.7881590852254146
    },
    {
      "doc": 612,
      "topic": 20,
      "similarity": 0.7626292415280986
    },
    {
      "doc": 612,
      "topic": 21,
      "similarity": 0.8097475067776788
    },
    {
      "doc": 612,
      "topic": 23,
      "similarity": 0.7750199647526067
    },
    {
      "doc": 612,
      "topic": 24,
      "similarity": 0.7890189714959914
    },
    {
      "doc": 613,
      "topic": 1,
      "similarity": 0.7727945895868034
    },
    {
      "doc": 613,
      "topic": 2,
      "similarity": 0.791255117399541
    },
    {
      "doc": 613,
      "topic": 3,
      "similarity": 0.8008233141057273
    },
    {
      "doc": 613,
      "topic": 4,
      "similarity": 0.756286329436573
    },
    {
      "doc": 613,
      "topic": 5,
      "similarity": 0.8213501034052145
    },
    {
      "doc": 613,
      "topic": 7,
      "similarity": 0.7944953348995349
    },
    {
      "doc": 613,
      "topic": 8,
      "similarity": 0.7733110374892773
    },
    {
      "doc": 613,
      "topic": 9,
      "similarity": 0.8117538766613891
    },
    {
      "doc": 613,
      "topic": 10,
      "similarity": 0.8035974732088671
    },
    {
      "doc": 613,
      "topic": 11,
      "similarity": 0.797640615800407
    },
    {
      "doc": 613,
      "topic": 13,
      "similarity": 0.793459938406955
    },
    {
      "doc": 613,
      "topic": 14,
      "similarity": 0.8064429193110311
    },
    {
      "doc": 613,
      "topic": 15,
      "similarity": 0.800845001095815
    },
    {
      "doc": 613,
      "topic": 16,
      "similarity": 0.8138878971217041
    },
    {
      "doc": 613,
      "topic": 17,
      "similarity": 0.8072982838882965
    },
    {
      "doc": 613,
      "topic": 18,
      "similarity": 0.7661745982163368
    },
    {
      "doc": 613,
      "topic": 19,
      "similarity": 0.8169896876037183
    },
    {
      "doc": 613,
      "topic": 20,
      "similarity": 0.77198920909544
    },
    {
      "doc": 613,
      "topic": 21,
      "similarity": 0.8050781033579242
    },
    {
      "doc": 614,
      "topic": 3,
      "similarity": 0.781960469671511
    },
    {
      "doc": 614,
      "topic": 5,
      "similarity": 0.784600592411092
    },
    {
      "doc": 614,
      "topic": 6,
      "similarity": 0.8525050147513611
    },
    {
      "doc": 614,
      "topic": 7,
      "similarity": 0.7502754392867447
    },
    {
      "doc": 614,
      "topic": 9,
      "similarity": 0.7857662498599788
    },
    {
      "doc": 614,
      "topic": 10,
      "similarity": 0.7569747302467243
    },
    {
      "doc": 614,
      "topic": 11,
      "similarity": 0.7595092720180309
    },
    {
      "doc": 614,
      "topic": 13,
      "similarity": 0.7661188809597392
    },
    {
      "doc": 614,
      "topic": 15,
      "similarity": 0.7606807801573843
    },
    {
      "doc": 614,
      "topic": 16,
      "similarity": 0.7786079191952715
    },
    {
      "doc": 614,
      "topic": 17,
      "similarity": 0.7781391109534541
    },
    {
      "doc": 614,
      "topic": 18,
      "similarity": 0.7514304320736576
    },
    {
      "doc": 614,
      "topic": 19,
      "similarity": 0.7923644819085196
    },
    {
      "doc": 614,
      "topic": 21,
      "similarity": 0.7797965245840338
    },
    {
      "doc": 614,
      "topic": 23,
      "similarity": 0.7786207115018166
    },
    {
      "doc": 615,
      "topic": 1,
      "similarity": 0.7580642341980703
    },
    {
      "doc": 615,
      "topic": 2,
      "similarity": 0.7746722825782217
    },
    {
      "doc": 615,
      "topic": 3,
      "similarity": 0.7937757843023052
    },
    {
      "doc": 615,
      "topic": 4,
      "similarity": 0.7604476958246098
    },
    {
      "doc": 615,
      "topic": 5,
      "similarity": 0.7958597464069341
    },
    {
      "doc": 615,
      "topic": 7,
      "similarity": 0.7803656625469856
    },
    {
      "doc": 615,
      "topic": 8,
      "similarity": 0.796178607758158
    },
    {
      "doc": 615,
      "topic": 9,
      "similarity": 0.8418109315958526
    },
    {
      "doc": 615,
      "topic": 10,
      "similarity": 0.8188002830717471
    },
    {
      "doc": 615,
      "topic": 11,
      "similarity": 0.7992073541867718
    },
    {
      "doc": 615,
      "topic": 12,
      "similarity": 0.7656236568564642
    },
    {
      "doc": 615,
      "topic": 13,
      "similarity": 0.7727763392792346
    },
    {
      "doc": 615,
      "topic": 14,
      "similarity": 0.7767514830856838
    },
    {
      "doc": 615,
      "topic": 15,
      "similarity": 0.8112987750735805
    },
    {
      "doc": 615,
      "topic": 16,
      "similarity": 0.8111011456069156
    },
    {
      "doc": 615,
      "topic": 17,
      "similarity": 0.8083677073757047
    },
    {
      "doc": 615,
      "topic": 18,
      "similarity": 0.7631524363340864
    },
    {
      "doc": 615,
      "topic": 19,
      "similarity": 0.8072146994787663
    },
    {
      "doc": 615,
      "topic": 20,
      "similarity": 0.7879213272220528
    },
    {
      "doc": 615,
      "topic": 21,
      "similarity": 0.8121271911408976
    },
    {
      "doc": 615,
      "topic": 24,
      "similarity": 0.751720214834629
    },
    {
      "doc": 616,
      "topic": 1,
      "similarity": 0.7607544746761736
    },
    {
      "doc": 616,
      "topic": 2,
      "similarity": 0.7560525459019998
    },
    {
      "doc": 616,
      "topic": 3,
      "similarity": 0.7937193305495615
    },
    {
      "doc": 616,
      "topic": 5,
      "similarity": 0.8001985509471647
    },
    {
      "doc": 616,
      "topic": 7,
      "similarity": 0.7755121090617976
    },
    {
      "doc": 616,
      "topic": 8,
      "similarity": 0.775353269828276
    },
    {
      "doc": 616,
      "topic": 9,
      "similarity": 0.8255978992361943
    },
    {
      "doc": 616,
      "topic": 10,
      "similarity": 0.7785906513088264
    },
    {
      "doc": 616,
      "topic": 11,
      "similarity": 0.7900329703808108
    },
    {
      "doc": 616,
      "topic": 12,
      "similarity": 0.755620878161919
    },
    {
      "doc": 616,
      "topic": 13,
      "similarity": 0.7635618378250586
    },
    {
      "doc": 616,
      "topic": 14,
      "similarity": 0.7721561383704524
    },
    {
      "doc": 616,
      "topic": 15,
      "similarity": 0.7766055736221
    },
    {
      "doc": 616,
      "topic": 16,
      "similarity": 0.8035784759400603
    },
    {
      "doc": 616,
      "topic": 17,
      "similarity": 0.8043448187242416
    },
    {
      "doc": 616,
      "topic": 18,
      "similarity": 0.7848020998786311
    },
    {
      "doc": 616,
      "topic": 19,
      "similarity": 0.8270126271094966
    },
    {
      "doc": 616,
      "topic": 20,
      "similarity": 0.7932767743588525
    },
    {
      "doc": 616,
      "topic": 21,
      "similarity": 0.8040177667788782
    },
    {
      "doc": 616,
      "topic": 23,
      "similarity": 0.7517701682653237
    },
    {
      "doc": 616,
      "topic": 24,
      "similarity": 0.7555886558413234
    },
    {
      "doc": 617,
      "topic": 2,
      "similarity": 0.7501741886256049
    },
    {
      "doc": 617,
      "topic": 3,
      "similarity": 0.7762420762977916
    },
    {
      "doc": 617,
      "topic": 5,
      "similarity": 0.7781693225101561
    },
    {
      "doc": 617,
      "topic": 7,
      "similarity": 0.7563112293828349
    },
    {
      "doc": 617,
      "topic": 8,
      "similarity": 0.7759898075946465
    },
    {
      "doc": 617,
      "topic": 9,
      "similarity": 0.8216549448534164
    },
    {
      "doc": 617,
      "topic": 10,
      "similarity": 0.7746375209183519
    },
    {
      "doc": 617,
      "topic": 11,
      "similarity": 0.7705120817873492
    },
    {
      "doc": 617,
      "topic": 12,
      "similarity": 0.7635242082969839
    },
    {
      "doc": 617,
      "topic": 13,
      "similarity": 0.7625965660108425
    },
    {
      "doc": 617,
      "topic": 14,
      "similarity": 0.7613861819160983
    },
    {
      "doc": 617,
      "topic": 15,
      "similarity": 0.7706198273617527
    },
    {
      "doc": 617,
      "topic": 16,
      "similarity": 0.7912908456777269
    },
    {
      "doc": 617,
      "topic": 17,
      "similarity": 0.7977642854936168
    },
    {
      "doc": 617,
      "topic": 18,
      "similarity": 0.791978118156405
    },
    {
      "doc": 617,
      "topic": 19,
      "similarity": 0.8147445970778648
    },
    {
      "doc": 617,
      "topic": 20,
      "similarity": 0.7845677286181594
    },
    {
      "doc": 617,
      "topic": 21,
      "similarity": 0.7899632217326479
    },
    {
      "doc": 617,
      "topic": 24,
      "similarity": 0.7656156930295507
    },
    {
      "doc": 618,
      "topic": 1,
      "similarity": 0.8228143802957006
    },
    {
      "doc": 618,
      "topic": 2,
      "similarity": 0.8003804970982182
    },
    {
      "doc": 618,
      "topic": 3,
      "similarity": 0.8318228960829662
    },
    {
      "doc": 618,
      "topic": 4,
      "similarity": 0.7834604904429647
    },
    {
      "doc": 618,
      "topic": 5,
      "similarity": 0.8227368123857691
    },
    {
      "doc": 618,
      "topic": 7,
      "similarity": 0.8148706576322711
    },
    {
      "doc": 618,
      "topic": 8,
      "similarity": 0.8052664982845248
    },
    {
      "doc": 618,
      "topic": 9,
      "similarity": 0.833210174795496
    },
    {
      "doc": 618,
      "topic": 10,
      "similarity": 0.8099846436522379
    },
    {
      "doc": 618,
      "topic": 11,
      "similarity": 0.7949432436452296
    },
    {
      "doc": 618,
      "topic": 13,
      "similarity": 0.8076010503259701
    },
    {
      "doc": 618,
      "topic": 14,
      "similarity": 0.8062049659587428
    },
    {
      "doc": 618,
      "topic": 15,
      "similarity": 0.7934671183208062
    },
    {
      "doc": 618,
      "topic": 16,
      "similarity": 0.8295090623973681
    },
    {
      "doc": 618,
      "topic": 17,
      "similarity": 0.8393554681923665
    },
    {
      "doc": 618,
      "topic": 18,
      "similarity": 0.767912931042232
    },
    {
      "doc": 618,
      "topic": 19,
      "similarity": 0.8280590678976439
    },
    {
      "doc": 618,
      "topic": 20,
      "similarity": 0.8029631000817572
    },
    {
      "doc": 618,
      "topic": 21,
      "similarity": 0.8319913507752641
    },
    {
      "doc": 618,
      "topic": 23,
      "similarity": 0.7664557149195416
    },
    {
      "doc": 618,
      "topic": 24,
      "similarity": 0.7534787546424757
    },
    {
      "doc": 619,
      "topic": 3,
      "similarity": 0.7842771369062549
    },
    {
      "doc": 619,
      "topic": 5,
      "similarity": 0.7737978670063498
    },
    {
      "doc": 619,
      "topic": 8,
      "similarity": 0.7520925819764468
    },
    {
      "doc": 619,
      "topic": 9,
      "similarity": 0.7984531650208181
    },
    {
      "doc": 619,
      "topic": 10,
      "similarity": 0.764416540252834
    },
    {
      "doc": 619,
      "topic": 11,
      "similarity": 0.7595267355528396
    },
    {
      "doc": 619,
      "topic": 14,
      "similarity": 0.7559909842313887
    },
    {
      "doc": 619,
      "topic": 15,
      "similarity": 0.7523360405923388
    },
    {
      "doc": 619,
      "topic": 16,
      "similarity": 0.7801511265713008
    },
    {
      "doc": 619,
      "topic": 17,
      "similarity": 0.7677639540525419
    },
    {
      "doc": 619,
      "topic": 18,
      "similarity": 0.7824901948766253
    },
    {
      "doc": 619,
      "topic": 19,
      "similarity": 0.8190335743313973
    },
    {
      "doc": 619,
      "topic": 20,
      "similarity": 0.7648420530430756
    },
    {
      "doc": 619,
      "topic": 21,
      "similarity": 0.7850657097187638
    },
    {
      "doc": 620,
      "topic": 2,
      "similarity": 0.7774329278485159
    },
    {
      "doc": 620,
      "topic": 3,
      "similarity": 0.7814479238975927
    },
    {
      "doc": 620,
      "topic": 4,
      "similarity": 0.7520926092268052
    },
    {
      "doc": 620,
      "topic": 5,
      "similarity": 0.7929543586792269
    },
    {
      "doc": 620,
      "topic": 7,
      "similarity": 0.7611624118195922
    },
    {
      "doc": 620,
      "topic": 8,
      "similarity": 0.7576290642944125
    },
    {
      "doc": 620,
      "topic": 9,
      "similarity": 0.7967045200606259
    },
    {
      "doc": 620,
      "topic": 10,
      "similarity": 0.7566672865152075
    },
    {
      "doc": 620,
      "topic": 11,
      "similarity": 0.7700026629679727
    },
    {
      "doc": 620,
      "topic": 13,
      "similarity": 0.7569386273394836
    },
    {
      "doc": 620,
      "topic": 14,
      "similarity": 0.7503183725145457
    },
    {
      "doc": 620,
      "topic": 15,
      "similarity": 0.7863037727946931
    },
    {
      "doc": 620,
      "topic": 16,
      "similarity": 0.8084435914605168
    },
    {
      "doc": 620,
      "topic": 17,
      "similarity": 0.8565629555106296
    },
    {
      "doc": 620,
      "topic": 19,
      "similarity": 0.7964291288985644
    },
    {
      "doc": 620,
      "topic": 20,
      "similarity": 0.7888481604521985
    },
    {
      "doc": 620,
      "topic": 21,
      "similarity": 0.8137040160873198
    },
    {
      "doc": 621,
      "topic": 0,
      "similarity": 0.7553029571499358
    },
    {
      "doc": 621,
      "topic": 1,
      "similarity": 0.7739648748211576
    },
    {
      "doc": 621,
      "topic": 2,
      "similarity": 0.8036320392585377
    },
    {
      "doc": 621,
      "topic": 3,
      "similarity": 0.8103183157772144
    },
    {
      "doc": 621,
      "topic": 4,
      "similarity": 0.7604802274646155
    },
    {
      "doc": 621,
      "topic": 5,
      "similarity": 0.8158263326445827
    },
    {
      "doc": 621,
      "topic": 6,
      "similarity": 0.7522326155176565
    },
    {
      "doc": 621,
      "topic": 7,
      "similarity": 0.8021748544781072
    },
    {
      "doc": 621,
      "topic": 8,
      "similarity": 0.8122173235507498
    },
    {
      "doc": 621,
      "topic": 9,
      "similarity": 0.840624036900589
    },
    {
      "doc": 621,
      "topic": 10,
      "similarity": 0.8052923009491834
    },
    {
      "doc": 621,
      "topic": 11,
      "similarity": 0.8390742094966964
    },
    {
      "doc": 621,
      "topic": 12,
      "similarity": 0.77146537447892
    },
    {
      "doc": 621,
      "topic": 13,
      "similarity": 0.7886347465578614
    },
    {
      "doc": 621,
      "topic": 14,
      "similarity": 0.8065151023453182
    },
    {
      "doc": 621,
      "topic": 15,
      "similarity": 0.8269695489996272
    },
    {
      "doc": 621,
      "topic": 16,
      "similarity": 0.8263474437111556
    },
    {
      "doc": 621,
      "topic": 17,
      "similarity": 0.8287523765412916
    },
    {
      "doc": 621,
      "topic": 18,
      "similarity": 0.8099530759447553
    },
    {
      "doc": 621,
      "topic": 19,
      "similarity": 0.849640865503186
    },
    {
      "doc": 621,
      "topic": 20,
      "similarity": 0.7964961218710754
    },
    {
      "doc": 621,
      "topic": 21,
      "similarity": 0.8127888492304836
    },
    {
      "doc": 621,
      "topic": 22,
      "similarity": 0.7653516553469587
    },
    {
      "doc": 621,
      "topic": 23,
      "similarity": 0.7761691037054109
    },
    {
      "doc": 621,
      "topic": 24,
      "similarity": 0.7506487061615486
    },
    {
      "doc": 622,
      "topic": 3,
      "similarity": 0.7706691949964147
    },
    {
      "doc": 622,
      "topic": 4,
      "similarity": 0.8272156922047498
    },
    {
      "doc": 622,
      "topic": 7,
      "similarity": 0.7505659907379307
    },
    {
      "doc": 622,
      "topic": 9,
      "similarity": 0.7573806945483357
    },
    {
      "doc": 622,
      "topic": 16,
      "similarity": 0.7682455718851404
    },
    {
      "doc": 622,
      "topic": 17,
      "similarity": 0.7519984714266352
    },
    {
      "doc": 622,
      "topic": 19,
      "similarity": 0.7550304353139389
    },
    {
      "doc": 622,
      "topic": 21,
      "similarity": 0.7550658930960724
    },
    {
      "doc": 623,
      "topic": 1,
      "similarity": 0.7583542824060947
    },
    {
      "doc": 623,
      "topic": 2,
      "similarity": 0.7640358703705796
    },
    {
      "doc": 623,
      "topic": 3,
      "similarity": 0.7720288963038991
    },
    {
      "doc": 623,
      "topic": 5,
      "similarity": 0.7626215968507054
    },
    {
      "doc": 623,
      "topic": 8,
      "similarity": 0.7618093362491637
    },
    {
      "doc": 623,
      "topic": 9,
      "similarity": 0.7804967575854743
    },
    {
      "doc": 623,
      "topic": 10,
      "similarity": 0.7761218584288523
    },
    {
      "doc": 623,
      "topic": 11,
      "similarity": 0.7556173596562613
    },
    {
      "doc": 623,
      "topic": 12,
      "similarity": 0.7530726789429942
    },
    {
      "doc": 623,
      "topic": 13,
      "similarity": 0.7626869163370684
    },
    {
      "doc": 623,
      "topic": 14,
      "similarity": 0.7579070169198138
    },
    {
      "doc": 623,
      "topic": 15,
      "similarity": 0.7888995597148405
    },
    {
      "doc": 623,
      "topic": 16,
      "similarity": 0.7956321975684228
    },
    {
      "doc": 623,
      "topic": 17,
      "similarity": 0.7957971525463549
    },
    {
      "doc": 623,
      "topic": 18,
      "similarity": 0.752262413330741
    },
    {
      "doc": 623,
      "topic": 19,
      "similarity": 0.8007302881736884
    },
    {
      "doc": 623,
      "topic": 20,
      "similarity": 0.7700562230033758
    },
    {
      "doc": 623,
      "topic": 21,
      "similarity": 0.7779808121127711
    },
    {
      "doc": 623,
      "topic": 22,
      "similarity": 0.7536703319186835
    },
    {
      "doc": 624,
      "topic": 0,
      "similarity": 0.7627464961859823
    },
    {
      "doc": 624,
      "topic": 1,
      "similarity": 0.7679834847699447
    },
    {
      "doc": 624,
      "topic": 2,
      "similarity": 0.7663625238853725
    },
    {
      "doc": 624,
      "topic": 3,
      "similarity": 0.8110666743688223
    },
    {
      "doc": 624,
      "topic": 5,
      "similarity": 0.7702909834393215
    },
    {
      "doc": 624,
      "topic": 7,
      "similarity": 0.7720465409838773
    },
    {
      "doc": 624,
      "topic": 8,
      "similarity": 0.7626880487910033
    },
    {
      "doc": 624,
      "topic": 9,
      "similarity": 0.8102879455121708
    },
    {
      "doc": 624,
      "topic": 10,
      "similarity": 0.76873903830995
    },
    {
      "doc": 624,
      "topic": 11,
      "similarity": 0.7783096494168495
    },
    {
      "doc": 624,
      "topic": 13,
      "similarity": 0.7612121951399619
    },
    {
      "doc": 624,
      "topic": 14,
      "similarity": 0.7808951961457853
    },
    {
      "doc": 624,
      "topic": 15,
      "similarity": 0.7589900936099679
    },
    {
      "doc": 624,
      "topic": 16,
      "similarity": 0.7949295279299828
    },
    {
      "doc": 624,
      "topic": 17,
      "similarity": 0.7770427635390669
    },
    {
      "doc": 624,
      "topic": 18,
      "similarity": 0.7566192591688043
    },
    {
      "doc": 624,
      "topic": 19,
      "similarity": 0.8136097416740052
    },
    {
      "doc": 624,
      "topic": 20,
      "similarity": 0.7815533687200865
    },
    {
      "doc": 624,
      "topic": 21,
      "similarity": 0.7881134031768537
    },
    {
      "doc": 624,
      "topic": 22,
      "similarity": 0.7732768630880416
    },
    {
      "doc": 624,
      "topic": 23,
      "similarity": 0.7725941004879968
    },
    {
      "doc": 625,
      "topic": 0,
      "similarity": 0.7531018225773028
    },
    {
      "doc": 625,
      "topic": 3,
      "similarity": 0.7658156003652365
    },
    {
      "doc": 625,
      "topic": 5,
      "similarity": 0.7817712170151225
    },
    {
      "doc": 625,
      "topic": 7,
      "similarity": 0.7619099554120476
    },
    {
      "doc": 625,
      "topic": 9,
      "similarity": 0.8174286940020205
    },
    {
      "doc": 625,
      "topic": 11,
      "similarity": 0.808881082602971
    },
    {
      "doc": 625,
      "topic": 15,
      "similarity": 0.7748680380687856
    },
    {
      "doc": 625,
      "topic": 16,
      "similarity": 0.7770972903836534
    },
    {
      "doc": 625,
      "topic": 17,
      "similarity": 0.8050561340820938
    },
    {
      "doc": 625,
      "topic": 19,
      "similarity": 0.7741379188583786
    },
    {
      "doc": 625,
      "topic": 20,
      "similarity": 0.7678340854457689
    },
    {
      "doc": 625,
      "topic": 21,
      "similarity": 0.793892381211319
    },
    {
      "doc": 626,
      "topic": 1,
      "similarity": 0.7523481374126977
    },
    {
      "doc": 626,
      "topic": 2,
      "similarity": 0.7631495076706415
    },
    {
      "doc": 626,
      "topic": 3,
      "similarity": 0.7818417665844513
    },
    {
      "doc": 626,
      "topic": 4,
      "similarity": 0.7531102760197695
    },
    {
      "doc": 626,
      "topic": 5,
      "similarity": 0.771636549497373
    },
    {
      "doc": 626,
      "topic": 7,
      "similarity": 0.800091324002206
    },
    {
      "doc": 626,
      "topic": 8,
      "similarity": 0.7695350655684465
    },
    {
      "doc": 626,
      "topic": 9,
      "similarity": 0.7898671088348586
    },
    {
      "doc": 626,
      "topic": 10,
      "similarity": 0.7543735481343481
    },
    {
      "doc": 626,
      "topic": 11,
      "similarity": 0.7912332564860267
    },
    {
      "doc": 626,
      "topic": 14,
      "similarity": 0.7591751918561599
    },
    {
      "doc": 626,
      "topic": 15,
      "similarity": 0.7855955475404718
    },
    {
      "doc": 626,
      "topic": 16,
      "similarity": 0.8298932900285786
    },
    {
      "doc": 626,
      "topic": 17,
      "similarity": 0.807012867305245
    },
    {
      "doc": 626,
      "topic": 19,
      "similarity": 0.8104495001696527
    },
    {
      "doc": 626,
      "topic": 21,
      "similarity": 0.7812780774493137
    },
    {
      "doc": 627,
      "topic": 1,
      "similarity": 0.7595814768797416
    },
    {
      "doc": 627,
      "topic": 2,
      "similarity": 0.7864080186710513
    },
    {
      "doc": 627,
      "topic": 3,
      "similarity": 0.7954052842717279
    },
    {
      "doc": 627,
      "topic": 4,
      "similarity": 0.7662043631690325
    },
    {
      "doc": 627,
      "topic": 5,
      "similarity": 0.7996897661649567
    },
    {
      "doc": 627,
      "topic": 7,
      "similarity": 0.7842633176030284
    },
    {
      "doc": 627,
      "topic": 8,
      "similarity": 0.7693292827540916
    },
    {
      "doc": 627,
      "topic": 9,
      "similarity": 0.8325524293193948
    },
    {
      "doc": 627,
      "topic": 10,
      "similarity": 0.778706382503788
    },
    {
      "doc": 627,
      "topic": 11,
      "similarity": 0.7924095255726231
    },
    {
      "doc": 627,
      "topic": 13,
      "similarity": 0.7642495082651682
    },
    {
      "doc": 627,
      "topic": 14,
      "similarity": 0.767318086432216
    },
    {
      "doc": 627,
      "topic": 15,
      "similarity": 0.7925094229741888
    },
    {
      "doc": 627,
      "topic": 16,
      "similarity": 0.8185937694976769
    },
    {
      "doc": 627,
      "topic": 17,
      "similarity": 0.8412990599360299
    },
    {
      "doc": 627,
      "topic": 19,
      "similarity": 0.7977232300452994
    },
    {
      "doc": 627,
      "topic": 20,
      "similarity": 0.7798501535949259
    },
    {
      "doc": 627,
      "topic": 21,
      "similarity": 0.8371170242326791
    },
    {
      "doc": 628,
      "topic": 3,
      "similarity": 0.7787650330160711
    },
    {
      "doc": 628,
      "topic": 5,
      "similarity": 0.7745009064110602
    },
    {
      "doc": 628,
      "topic": 7,
      "similarity": 0.7729848425540145
    },
    {
      "doc": 628,
      "topic": 9,
      "similarity": 0.8060564415115423
    },
    {
      "doc": 628,
      "topic": 10,
      "similarity": 0.7556787727451869
    },
    {
      "doc": 628,
      "topic": 11,
      "similarity": 0.7657092895043226
    },
    {
      "doc": 628,
      "topic": 13,
      "similarity": 0.7569620477785102
    },
    {
      "doc": 628,
      "topic": 14,
      "similarity": 0.7526097221306007
    },
    {
      "doc": 628,
      "topic": 15,
      "similarity": 0.7667354025358921
    },
    {
      "doc": 628,
      "topic": 16,
      "similarity": 0.8075469769008807
    },
    {
      "doc": 628,
      "topic": 17,
      "similarity": 0.7912448398567257
    },
    {
      "doc": 628,
      "topic": 18,
      "similarity": 0.7657480443933524
    },
    {
      "doc": 628,
      "topic": 19,
      "similarity": 0.7911796945667632
    },
    {
      "doc": 628,
      "topic": 20,
      "similarity": 0.7903389626696466
    },
    {
      "doc": 628,
      "topic": 21,
      "similarity": 0.7945158482473518
    },
    {
      "doc": 628,
      "topic": 24,
      "similarity": 0.7535051920685472
    },
    {
      "doc": 629,
      "topic": 3,
      "similarity": 0.762869750692072
    },
    {
      "doc": 629,
      "topic": 4,
      "similarity": 0.8038080306786213
    },
    {
      "doc": 629,
      "topic": 5,
      "similarity": 0.755684415515825
    },
    {
      "doc": 629,
      "topic": 7,
      "similarity": 0.7547050123052197
    },
    {
      "doc": 629,
      "topic": 9,
      "similarity": 0.7672498097488587
    },
    {
      "doc": 629,
      "topic": 16,
      "similarity": 0.7750309355668806
    },
    {
      "doc": 629,
      "topic": 17,
      "similarity": 0.7708003827089082
    },
    {
      "doc": 629,
      "topic": 19,
      "similarity": 0.781294913607424
    },
    {
      "doc": 629,
      "topic": 20,
      "similarity": 0.7507380562486395
    },
    {
      "doc": 629,
      "topic": 21,
      "similarity": 0.7670198627660327
    },
    {
      "doc": 630,
      "topic": 2,
      "similarity": 0.7619786168964541
    },
    {
      "doc": 630,
      "topic": 3,
      "similarity": 0.8004560389154087
    },
    {
      "doc": 630,
      "topic": 4,
      "similarity": 0.7618311890674684
    },
    {
      "doc": 630,
      "topic": 5,
      "similarity": 0.810168952313539
    },
    {
      "doc": 630,
      "topic": 7,
      "similarity": 0.7781864128472884
    },
    {
      "doc": 630,
      "topic": 8,
      "similarity": 0.758332285657427
    },
    {
      "doc": 630,
      "topic": 9,
      "similarity": 0.8186579049392356
    },
    {
      "doc": 630,
      "topic": 10,
      "similarity": 0.7979106789203648
    },
    {
      "doc": 630,
      "topic": 11,
      "similarity": 0.7813103970068184
    },
    {
      "doc": 630,
      "topic": 13,
      "similarity": 0.7585129163010369
    },
    {
      "doc": 630,
      "topic": 14,
      "similarity": 0.7685743452277572
    },
    {
      "doc": 630,
      "topic": 15,
      "similarity": 0.7731233713762004
    },
    {
      "doc": 630,
      "topic": 16,
      "similarity": 0.7974269942504458
    },
    {
      "doc": 630,
      "topic": 17,
      "similarity": 0.8011628151975737
    },
    {
      "doc": 630,
      "topic": 18,
      "similarity": 0.7872884441170218
    },
    {
      "doc": 630,
      "topic": 19,
      "similarity": 0.81384067323767
    },
    {
      "doc": 630,
      "topic": 20,
      "similarity": 0.7947157949892361
    },
    {
      "doc": 630,
      "topic": 21,
      "similarity": 0.806492706411892
    },
    {
      "doc": 630,
      "topic": 23,
      "similarity": 0.7733023187503575
    },
    {
      "doc": 630,
      "topic": 24,
      "similarity": 0.7592628480173952
    },
    {
      "doc": 631,
      "topic": 1,
      "similarity": 0.766611545348463
    },
    {
      "doc": 631,
      "topic": 2,
      "similarity": 0.7784279011917518
    },
    {
      "doc": 631,
      "topic": 3,
      "similarity": 0.8024405153145087
    },
    {
      "doc": 631,
      "topic": 4,
      "similarity": 0.7560048951135392
    },
    {
      "doc": 631,
      "topic": 5,
      "similarity": 0.8203860395072262
    },
    {
      "doc": 631,
      "topic": 7,
      "similarity": 0.772068495279096
    },
    {
      "doc": 631,
      "topic": 8,
      "similarity": 0.7789114303198997
    },
    {
      "doc": 631,
      "topic": 9,
      "similarity": 0.8221195897227419
    },
    {
      "doc": 631,
      "topic": 10,
      "similarity": 0.8031407088374751
    },
    {
      "doc": 631,
      "topic": 11,
      "similarity": 0.8077739729658107
    },
    {
      "doc": 631,
      "topic": 13,
      "similarity": 0.7694757715485483
    },
    {
      "doc": 631,
      "topic": 14,
      "similarity": 0.7808983864811074
    },
    {
      "doc": 631,
      "topic": 15,
      "similarity": 0.785570432954724
    },
    {
      "doc": 631,
      "topic": 16,
      "similarity": 0.8224497869911371
    },
    {
      "doc": 631,
      "topic": 17,
      "similarity": 0.8192567155101954
    },
    {
      "doc": 631,
      "topic": 18,
      "similarity": 0.7657381224137432
    },
    {
      "doc": 631,
      "topic": 19,
      "similarity": 0.8056782720085236
    },
    {
      "doc": 631,
      "topic": 20,
      "similarity": 0.7673888667671653
    },
    {
      "doc": 631,
      "topic": 21,
      "similarity": 0.8129217684638665
    },
    {
      "doc": 631,
      "topic": 23,
      "similarity": 0.7616349303633767
    },
    {
      "doc": 631,
      "topic": 24,
      "similarity": 0.7716197992191878
    },
    {
      "doc": 632,
      "topic": 1,
      "similarity": 0.7667299878617664
    },
    {
      "doc": 632,
      "topic": 2,
      "similarity": 0.7954824967305112
    },
    {
      "doc": 632,
      "topic": 3,
      "similarity": 0.812963984060812
    },
    {
      "doc": 632,
      "topic": 4,
      "similarity": 0.7755663086907679
    },
    {
      "doc": 632,
      "topic": 5,
      "similarity": 0.8040237504652057
    },
    {
      "doc": 632,
      "topic": 7,
      "similarity": 0.7968044474052722
    },
    {
      "doc": 632,
      "topic": 8,
      "similarity": 0.7779058817214912
    },
    {
      "doc": 632,
      "topic": 9,
      "similarity": 0.8132156666339782
    },
    {
      "doc": 632,
      "topic": 10,
      "similarity": 0.7834515631055264
    },
    {
      "doc": 632,
      "topic": 11,
      "similarity": 0.7814253263643473
    },
    {
      "doc": 632,
      "topic": 13,
      "similarity": 0.7632358505507114
    },
    {
      "doc": 632,
      "topic": 14,
      "similarity": 0.7718541983277087
    },
    {
      "doc": 632,
      "topic": 15,
      "similarity": 0.7891629515981563
    },
    {
      "doc": 632,
      "topic": 16,
      "similarity": 0.8245723981361047
    },
    {
      "doc": 632,
      "topic": 17,
      "similarity": 0.8166994612668735
    },
    {
      "doc": 632,
      "topic": 19,
      "similarity": 0.7949852517757394
    },
    {
      "doc": 632,
      "topic": 20,
      "similarity": 0.7855159531929983
    },
    {
      "doc": 632,
      "topic": 21,
      "similarity": 0.8117789537724232
    },
    {
      "doc": 632,
      "topic": 23,
      "similarity": 0.7565891782403346
    },
    {
      "doc": 633,
      "topic": 1,
      "similarity": 0.773027079351325
    },
    {
      "doc": 633,
      "topic": 2,
      "similarity": 0.8042715193466973
    },
    {
      "doc": 633,
      "topic": 3,
      "similarity": 0.8074440447859711
    },
    {
      "doc": 633,
      "topic": 4,
      "similarity": 0.7653455188916453
    },
    {
      "doc": 633,
      "topic": 5,
      "similarity": 0.8052060714234798
    },
    {
      "doc": 633,
      "topic": 6,
      "similarity": 0.7560823875232807
    },
    {
      "doc": 633,
      "topic": 7,
      "similarity": 0.7885103407986448
    },
    {
      "doc": 633,
      "topic": 8,
      "similarity": 0.7812716840476797
    },
    {
      "doc": 633,
      "topic": 9,
      "similarity": 0.8255314583434779
    },
    {
      "doc": 633,
      "topic": 10,
      "similarity": 0.8041650059893171
    },
    {
      "doc": 633,
      "topic": 11,
      "similarity": 0.7803110571042682
    },
    {
      "doc": 633,
      "topic": 13,
      "similarity": 0.781196117758753
    },
    {
      "doc": 633,
      "topic": 14,
      "similarity": 0.801859800765257
    },
    {
      "doc": 633,
      "topic": 15,
      "similarity": 0.789991905214058
    },
    {
      "doc": 633,
      "topic": 16,
      "similarity": 0.8294786309540872
    },
    {
      "doc": 633,
      "topic": 17,
      "similarity": 0.8208628732330926
    },
    {
      "doc": 633,
      "topic": 18,
      "similarity": 0.7717472871412001
    },
    {
      "doc": 633,
      "topic": 19,
      "similarity": 0.8229613388848067
    },
    {
      "doc": 633,
      "topic": 20,
      "similarity": 0.7984240728138737
    },
    {
      "doc": 633,
      "topic": 21,
      "similarity": 0.8200535556091053
    },
    {
      "doc": 633,
      "topic": 23,
      "similarity": 0.7877311183421691
    },
    {
      "doc": 634,
      "topic": 1,
      "similarity": 0.7899497507653596
    },
    {
      "doc": 634,
      "topic": 2,
      "similarity": 0.76358766077056
    },
    {
      "doc": 634,
      "topic": 3,
      "similarity": 0.7982550969566452
    },
    {
      "doc": 634,
      "topic": 5,
      "similarity": 0.7707878571792174
    },
    {
      "doc": 634,
      "topic": 7,
      "similarity": 0.7679370451036215
    },
    {
      "doc": 634,
      "topic": 8,
      "similarity": 0.7696837759994801
    },
    {
      "doc": 634,
      "topic": 9,
      "similarity": 0.8037486014967047
    },
    {
      "doc": 634,
      "topic": 10,
      "similarity": 0.7720556150028433
    },
    {
      "doc": 634,
      "topic": 11,
      "similarity": 0.7938656326761949
    },
    {
      "doc": 634,
      "topic": 12,
      "similarity": 0.7651180265792734
    },
    {
      "doc": 634,
      "topic": 13,
      "similarity": 0.756473509379162
    },
    {
      "doc": 634,
      "topic": 14,
      "similarity": 0.7661114895320656
    },
    {
      "doc": 634,
      "topic": 15,
      "similarity": 0.7888662976824966
    },
    {
      "doc": 634,
      "topic": 16,
      "similarity": 0.7791910720151
    },
    {
      "doc": 634,
      "topic": 17,
      "similarity": 0.780783788761581
    },
    {
      "doc": 634,
      "topic": 19,
      "similarity": 0.7987451827682122
    },
    {
      "doc": 634,
      "topic": 20,
      "similarity": 0.7649630546520072
    },
    {
      "doc": 634,
      "topic": 21,
      "similarity": 0.7807065955505801
    },
    {
      "doc": 635,
      "topic": 1,
      "similarity": 0.774398362292166
    },
    {
      "doc": 635,
      "topic": 2,
      "similarity": 0.7750985634031762
    },
    {
      "doc": 635,
      "topic": 3,
      "similarity": 0.8059509179846087
    },
    {
      "doc": 635,
      "topic": 5,
      "similarity": 0.8019684616282451
    },
    {
      "doc": 635,
      "topic": 7,
      "similarity": 0.7842736385482847
    },
    {
      "doc": 635,
      "topic": 8,
      "similarity": 0.7785645176781094
    },
    {
      "doc": 635,
      "topic": 9,
      "similarity": 0.8251858087260598
    },
    {
      "doc": 635,
      "topic": 10,
      "similarity": 0.8009369307231579
    },
    {
      "doc": 635,
      "topic": 11,
      "similarity": 0.8121888798109214
    },
    {
      "doc": 635,
      "topic": 12,
      "similarity": 0.7865283376261674
    },
    {
      "doc": 635,
      "topic": 13,
      "similarity": 0.7846295992093152
    },
    {
      "doc": 635,
      "topic": 14,
      "similarity": 0.7800011694431915
    },
    {
      "doc": 635,
      "topic": 15,
      "similarity": 0.802955062562186
    },
    {
      "doc": 635,
      "topic": 16,
      "similarity": 0.8292972461626105
    },
    {
      "doc": 635,
      "topic": 17,
      "similarity": 0.8361885631110577
    },
    {
      "doc": 635,
      "topic": 18,
      "similarity": 0.7835510342372788
    },
    {
      "doc": 635,
      "topic": 19,
      "similarity": 0.8291527538411353
    },
    {
      "doc": 635,
      "topic": 20,
      "similarity": 0.8010220913554483
    },
    {
      "doc": 635,
      "topic": 21,
      "similarity": 0.8180399263162447
    },
    {
      "doc": 635,
      "topic": 22,
      "similarity": 0.7566545937620921
    },
    {
      "doc": 636,
      "topic": 3,
      "similarity": 0.7831697660283805
    },
    {
      "doc": 636,
      "topic": 5,
      "similarity": 0.7751552044078125
    },
    {
      "doc": 636,
      "topic": 8,
      "similarity": 0.7611104067834777
    },
    {
      "doc": 636,
      "topic": 9,
      "similarity": 0.7920518838749191
    },
    {
      "doc": 636,
      "topic": 10,
      "similarity": 0.75499815746197
    },
    {
      "doc": 636,
      "topic": 11,
      "similarity": 0.7627755956571827
    },
    {
      "doc": 636,
      "topic": 15,
      "similarity": 0.767066243367655
    },
    {
      "doc": 636,
      "topic": 16,
      "similarity": 0.7899508854516454
    },
    {
      "doc": 636,
      "topic": 17,
      "similarity": 0.8096170766366481
    },
    {
      "doc": 636,
      "topic": 19,
      "similarity": 0.785390193493552
    },
    {
      "doc": 636,
      "topic": 20,
      "similarity": 0.769790822323507
    },
    {
      "doc": 636,
      "topic": 21,
      "similarity": 0.7954008276140628
    },
    {
      "doc": 636,
      "topic": 24,
      "similarity": 0.75642404339407
    },
    {
      "doc": 637,
      "topic": 2,
      "similarity": 0.762114495310761
    },
    {
      "doc": 637,
      "topic": 3,
      "similarity": 0.7991202878104272
    },
    {
      "doc": 637,
      "topic": 5,
      "similarity": 0.7985971511950559
    },
    {
      "doc": 637,
      "topic": 6,
      "similarity": 0.7520378258347399
    },
    {
      "doc": 637,
      "topic": 7,
      "similarity": 0.7697243167402469
    },
    {
      "doc": 637,
      "topic": 8,
      "similarity": 0.7613135473819012
    },
    {
      "doc": 637,
      "topic": 9,
      "similarity": 0.7988489893575099
    },
    {
      "doc": 637,
      "topic": 10,
      "similarity": 0.785265944667437
    },
    {
      "doc": 637,
      "topic": 11,
      "similarity": 0.7711433029117364
    },
    {
      "doc": 637,
      "topic": 12,
      "similarity": 0.7532843415391905
    },
    {
      "doc": 637,
      "topic": 13,
      "similarity": 0.7524882077397361
    },
    {
      "doc": 637,
      "topic": 15,
      "similarity": 0.7674078406956264
    },
    {
      "doc": 637,
      "topic": 16,
      "similarity": 0.7955433142440252
    },
    {
      "doc": 637,
      "topic": 17,
      "similarity": 0.7856545933192872
    },
    {
      "doc": 637,
      "topic": 18,
      "similarity": 0.7636505929279341
    },
    {
      "doc": 637,
      "topic": 19,
      "similarity": 0.8101601969919816
    },
    {
      "doc": 637,
      "topic": 20,
      "similarity": 0.7835861909710364
    },
    {
      "doc": 637,
      "topic": 21,
      "similarity": 0.7822772503661948
    },
    {
      "doc": 637,
      "topic": 23,
      "similarity": 0.7789572480711883
    },
    {
      "doc": 638,
      "topic": 1,
      "similarity": 0.7902691791901714
    },
    {
      "doc": 638,
      "topic": 2,
      "similarity": 0.7921400169718901
    },
    {
      "doc": 638,
      "topic": 3,
      "similarity": 0.8304774312907579
    },
    {
      "doc": 638,
      "topic": 4,
      "similarity": 0.7698482144189637
    },
    {
      "doc": 638,
      "topic": 5,
      "similarity": 0.8177727303272119
    },
    {
      "doc": 638,
      "topic": 7,
      "similarity": 0.7976901940579479
    },
    {
      "doc": 638,
      "topic": 8,
      "similarity": 0.7877822455144232
    },
    {
      "doc": 638,
      "topic": 9,
      "similarity": 0.8400752844876761
    },
    {
      "doc": 638,
      "topic": 10,
      "similarity": 0.8061214069273982
    },
    {
      "doc": 638,
      "topic": 11,
      "similarity": 0.7989399627743231
    },
    {
      "doc": 638,
      "topic": 13,
      "similarity": 0.7896207950079523
    },
    {
      "doc": 638,
      "topic": 14,
      "similarity": 0.7840131905171472
    },
    {
      "doc": 638,
      "topic": 15,
      "similarity": 0.7807687306062361
    },
    {
      "doc": 638,
      "topic": 16,
      "similarity": 0.8087935462933967
    },
    {
      "doc": 638,
      "topic": 17,
      "similarity": 0.8101025434668746
    },
    {
      "doc": 638,
      "topic": 18,
      "similarity": 0.7551489766057924
    },
    {
      "doc": 638,
      "topic": 19,
      "similarity": 0.8077135341353614
    },
    {
      "doc": 638,
      "topic": 20,
      "similarity": 0.7720785015932947
    },
    {
      "doc": 638,
      "topic": 21,
      "similarity": 0.822189403532248
    },
    {
      "doc": 638,
      "topic": 23,
      "similarity": 0.7736990595965789
    },
    {
      "doc": 638,
      "topic": 24,
      "similarity": 0.7701971237607922
    },
    {
      "doc": 639,
      "topic": 2,
      "similarity": 0.7599211210661084
    },
    {
      "doc": 639,
      "topic": 3,
      "similarity": 0.786689267731972
    },
    {
      "doc": 639,
      "topic": 4,
      "similarity": 0.755920961176826
    },
    {
      "doc": 639,
      "topic": 5,
      "similarity": 0.8084409452061204
    },
    {
      "doc": 639,
      "topic": 7,
      "similarity": 0.7967319629094185
    },
    {
      "doc": 639,
      "topic": 8,
      "similarity": 0.7876812325191774
    },
    {
      "doc": 639,
      "topic": 9,
      "similarity": 0.8461708110048712
    },
    {
      "doc": 639,
      "topic": 10,
      "similarity": 0.7801475094502707
    },
    {
      "doc": 639,
      "topic": 11,
      "similarity": 0.7760396981455893
    },
    {
      "doc": 639,
      "topic": 13,
      "similarity": 0.7801256919825992
    },
    {
      "doc": 639,
      "topic": 14,
      "similarity": 0.7738672171296683
    },
    {
      "doc": 639,
      "topic": 15,
      "similarity": 0.7651195788795275
    },
    {
      "doc": 639,
      "topic": 16,
      "similarity": 0.7966520497452421
    },
    {
      "doc": 639,
      "topic": 17,
      "similarity": 0.7829544683952396
    },
    {
      "doc": 639,
      "topic": 19,
      "similarity": 0.7800424904573136
    },
    {
      "doc": 639,
      "topic": 20,
      "similarity": 0.772798208557879
    },
    {
      "doc": 639,
      "topic": 21,
      "similarity": 0.795339928317959
    },
    {
      "doc": 640,
      "topic": 1,
      "similarity": 0.7688927233207964
    },
    {
      "doc": 640,
      "topic": 2,
      "similarity": 0.7830118681578458
    },
    {
      "doc": 640,
      "topic": 3,
      "similarity": 0.8092419864366507
    },
    {
      "doc": 640,
      "topic": 4,
      "similarity": 0.7766583553866475
    },
    {
      "doc": 640,
      "topic": 5,
      "similarity": 0.8050889178043457
    },
    {
      "doc": 640,
      "topic": 7,
      "similarity": 0.7889744526546753
    },
    {
      "doc": 640,
      "topic": 8,
      "similarity": 0.7887775118094327
    },
    {
      "doc": 640,
      "topic": 9,
      "similarity": 0.8295176280511939
    },
    {
      "doc": 640,
      "topic": 10,
      "similarity": 0.7984325265350082
    },
    {
      "doc": 640,
      "topic": 11,
      "similarity": 0.7952358579636725
    },
    {
      "doc": 640,
      "topic": 13,
      "similarity": 0.7824725034750004
    },
    {
      "doc": 640,
      "topic": 14,
      "similarity": 0.7849839197699736
    },
    {
      "doc": 640,
      "topic": 15,
      "similarity": 0.8081912195288583
    },
    {
      "doc": 640,
      "topic": 16,
      "similarity": 0.8104914034381235
    },
    {
      "doc": 640,
      "topic": 17,
      "similarity": 0.8188876326712536
    },
    {
      "doc": 640,
      "topic": 18,
      "similarity": 0.7872549677941489
    },
    {
      "doc": 640,
      "topic": 19,
      "similarity": 0.8303046877469443
    },
    {
      "doc": 640,
      "topic": 20,
      "similarity": 0.7922466606049534
    },
    {
      "doc": 640,
      "topic": 21,
      "similarity": 0.8194394259979835
    },
    {
      "doc": 640,
      "topic": 23,
      "similarity": 0.7568226792945236
    },
    {
      "doc": 640,
      "topic": 24,
      "similarity": 0.7726145602000596
    },
    {
      "doc": 641,
      "topic": 2,
      "similarity": 0.7824949727687552
    },
    {
      "doc": 641,
      "topic": 3,
      "similarity": 0.7968841364227925
    },
    {
      "doc": 641,
      "topic": 4,
      "similarity": 0.7510241353027843
    },
    {
      "doc": 641,
      "topic": 5,
      "similarity": 0.8116488048970892
    },
    {
      "doc": 641,
      "topic": 7,
      "similarity": 0.7698336550328763
    },
    {
      "doc": 641,
      "topic": 8,
      "similarity": 0.774577445379394
    },
    {
      "doc": 641,
      "topic": 9,
      "similarity": 0.8269853503766859
    },
    {
      "doc": 641,
      "topic": 10,
      "similarity": 0.8125367183197437
    },
    {
      "doc": 641,
      "topic": 11,
      "similarity": 0.7792666290390813
    },
    {
      "doc": 641,
      "topic": 13,
      "similarity": 0.7679497626936239
    },
    {
      "doc": 641,
      "topic": 14,
      "similarity": 0.7662870856610073
    },
    {
      "doc": 641,
      "topic": 15,
      "similarity": 0.7838920972494876
    },
    {
      "doc": 641,
      "topic": 16,
      "similarity": 0.8255146892444493
    },
    {
      "doc": 641,
      "topic": 17,
      "similarity": 0.8110764754772704
    },
    {
      "doc": 641,
      "topic": 18,
      "similarity": 0.7742876513708719
    },
    {
      "doc": 641,
      "topic": 19,
      "similarity": 0.817209851364235
    },
    {
      "doc": 641,
      "topic": 20,
      "similarity": 0.8049775384385153
    },
    {
      "doc": 641,
      "topic": 21,
      "similarity": 0.822034254857922
    },
    {
      "doc": 641,
      "topic": 23,
      "similarity": 0.7682143728852499
    },
    {
      "doc": 641,
      "topic": 24,
      "similarity": 0.7598459560808369
    },
    {
      "doc": 642,
      "topic": 2,
      "similarity": 0.751534934516237
    },
    {
      "doc": 642,
      "topic": 3,
      "similarity": 0.7898113209812034
    },
    {
      "doc": 642,
      "topic": 4,
      "similarity": 0.7556376614099851
    },
    {
      "doc": 642,
      "topic": 5,
      "similarity": 0.784902462689988
    },
    {
      "doc": 642,
      "topic": 8,
      "similarity": 0.7516658880409476
    },
    {
      "doc": 642,
      "topic": 9,
      "similarity": 0.7984250199460143
    },
    {
      "doc": 642,
      "topic": 10,
      "similarity": 0.7580717482461211
    },
    {
      "doc": 642,
      "topic": 11,
      "similarity": 0.7815779111029929
    },
    {
      "doc": 642,
      "topic": 12,
      "similarity": 0.750503754972637
    },
    {
      "doc": 642,
      "topic": 15,
      "similarity": 0.7793674143183745
    },
    {
      "doc": 642,
      "topic": 16,
      "similarity": 0.7955706336322467
    },
    {
      "doc": 642,
      "topic": 17,
      "similarity": 0.8479111292476114
    },
    {
      "doc": 642,
      "topic": 18,
      "similarity": 0.7537940415229168
    },
    {
      "doc": 642,
      "topic": 19,
      "similarity": 0.8090761570871472
    },
    {
      "doc": 642,
      "topic": 20,
      "similarity": 0.8018039295791055
    },
    {
      "doc": 642,
      "topic": 21,
      "similarity": 0.8070320666814523
    },
    {
      "doc": 643,
      "topic": 0,
      "similarity": 0.7556266111167662
    },
    {
      "doc": 643,
      "topic": 3,
      "similarity": 0.7744736154324211
    },
    {
      "doc": 643,
      "topic": 5,
      "similarity": 0.7786006989389748
    },
    {
      "doc": 643,
      "topic": 8,
      "similarity": 0.7501031224880375
    },
    {
      "doc": 643,
      "topic": 9,
      "similarity": 0.8064055989561925
    },
    {
      "doc": 643,
      "topic": 10,
      "similarity": 0.7588749041907357
    },
    {
      "doc": 643,
      "topic": 11,
      "similarity": 0.8153716698010158
    },
    {
      "doc": 643,
      "topic": 15,
      "similarity": 0.7615065746765062
    },
    {
      "doc": 643,
      "topic": 16,
      "similarity": 0.7798993195035315
    },
    {
      "doc": 643,
      "topic": 17,
      "similarity": 0.7882739732555478
    },
    {
      "doc": 643,
      "topic": 18,
      "similarity": 0.7903244577673979
    },
    {
      "doc": 643,
      "topic": 19,
      "similarity": 0.7827938713925229
    },
    {
      "doc": 643,
      "topic": 20,
      "similarity": 0.7697908589325281
    },
    {
      "doc": 643,
      "topic": 21,
      "similarity": 0.7819045325423295
    },
    {
      "doc": 643,
      "topic": 23,
      "similarity": 0.7621286082269976
    },
    {
      "doc": 643,
      "topic": 24,
      "similarity": 0.7818271671350975
    },
    {
      "doc": 644,
      "topic": 2,
      "similarity": 0.7713750851236275
    },
    {
      "doc": 644,
      "topic": 3,
      "similarity": 0.7970008837392287
    },
    {
      "doc": 644,
      "topic": 4,
      "similarity": 0.7569617150251118
    },
    {
      "doc": 644,
      "topic": 5,
      "similarity": 0.8031416893409847
    },
    {
      "doc": 644,
      "topic": 7,
      "similarity": 0.7784443851992027
    },
    {
      "doc": 644,
      "topic": 8,
      "similarity": 0.7745598808821651
    },
    {
      "doc": 644,
      "topic": 9,
      "similarity": 0.8429548384253807
    },
    {
      "doc": 644,
      "topic": 10,
      "similarity": 0.777880742484463
    },
    {
      "doc": 644,
      "topic": 11,
      "similarity": 0.7963833825919768
    },
    {
      "doc": 644,
      "topic": 13,
      "similarity": 0.7671445671621596
    },
    {
      "doc": 644,
      "topic": 14,
      "similarity": 0.7746766858782467
    },
    {
      "doc": 644,
      "topic": 15,
      "similarity": 0.7686659749264856
    },
    {
      "doc": 644,
      "topic": 16,
      "similarity": 0.8081016845621019
    },
    {
      "doc": 644,
      "topic": 17,
      "similarity": 0.7912883625983815
    },
    {
      "doc": 644,
      "topic": 18,
      "similarity": 0.7538886170992783
    },
    {
      "doc": 644,
      "topic": 19,
      "similarity": 0.7922659812577542
    },
    {
      "doc": 644,
      "topic": 20,
      "similarity": 0.7904212921801567
    },
    {
      "doc": 644,
      "topic": 21,
      "similarity": 0.8284399962799187
    },
    {
      "doc": 644,
      "topic": 23,
      "similarity": 0.7511633469517005
    },
    {
      "doc": 644,
      "topic": 24,
      "similarity": 0.753584396721761
    },
    {
      "doc": 645,
      "topic": 3,
      "similarity": 0.7858239667672741
    },
    {
      "doc": 645,
      "topic": 4,
      "similarity": 0.810869876022231
    },
    {
      "doc": 645,
      "topic": 5,
      "similarity": 0.7713564954943767
    },
    {
      "doc": 645,
      "topic": 7,
      "similarity": 0.776621725297199
    },
    {
      "doc": 645,
      "topic": 8,
      "similarity": 0.7528758532824088
    },
    {
      "doc": 645,
      "topic": 9,
      "similarity": 0.793849395592995
    },
    {
      "doc": 645,
      "topic": 10,
      "similarity": 0.7642633546030649
    },
    {
      "doc": 645,
      "topic": 11,
      "similarity": 0.7557189486343846
    },
    {
      "doc": 645,
      "topic": 16,
      "similarity": 0.7956389529699613
    },
    {
      "doc": 645,
      "topic": 17,
      "similarity": 0.7765866736646857
    },
    {
      "doc": 645,
      "topic": 19,
      "similarity": 0.777703841011159
    },
    {
      "doc": 645,
      "topic": 21,
      "similarity": 0.7615379434481506
    },
    {
      "doc": 645,
      "topic": 24,
      "similarity": 0.7516378418178965
    },
    {
      "doc": 646,
      "topic": 3,
      "similarity": 0.7705125540606255
    },
    {
      "doc": 646,
      "topic": 5,
      "similarity": 0.7607970898874789
    },
    {
      "doc": 646,
      "topic": 7,
      "similarity": 0.76291924882898
    },
    {
      "doc": 646,
      "topic": 8,
      "similarity": 0.770286884743616
    },
    {
      "doc": 646,
      "topic": 9,
      "similarity": 0.8050117997962773
    },
    {
      "doc": 646,
      "topic": 10,
      "similarity": 0.7558435088454779
    },
    {
      "doc": 646,
      "topic": 12,
      "similarity": 0.8042700056061637
    },
    {
      "doc": 646,
      "topic": 16,
      "similarity": 0.7818466985532219
    },
    {
      "doc": 646,
      "topic": 17,
      "similarity": 0.7830720213564015
    },
    {
      "doc": 646,
      "topic": 19,
      "similarity": 0.777732928786334
    },
    {
      "doc": 646,
      "topic": 20,
      "similarity": 0.7793638377662303
    },
    {
      "doc": 646,
      "topic": 21,
      "similarity": 0.7869426448650839
    },
    {
      "doc": 646,
      "topic": 24,
      "similarity": 0.768624833837074
    },
    {
      "doc": 647,
      "topic": 2,
      "similarity": 0.7689526880635966
    },
    {
      "doc": 647,
      "topic": 3,
      "similarity": 0.7923217384283365
    },
    {
      "doc": 647,
      "topic": 5,
      "similarity": 0.8032345927825137
    },
    {
      "doc": 647,
      "topic": 7,
      "similarity": 0.7703504120944423
    },
    {
      "doc": 647,
      "topic": 8,
      "similarity": 0.772409731652564
    },
    {
      "doc": 647,
      "topic": 9,
      "similarity": 0.8266754221585063
    },
    {
      "doc": 647,
      "topic": 10,
      "similarity": 0.75953269639628
    },
    {
      "doc": 647,
      "topic": 11,
      "similarity": 0.7728485600555013
    },
    {
      "doc": 647,
      "topic": 13,
      "similarity": 0.7625029607762495
    },
    {
      "doc": 647,
      "topic": 14,
      "similarity": 0.7668258529111462
    },
    {
      "doc": 647,
      "topic": 15,
      "similarity": 0.7569545610442611
    },
    {
      "doc": 647,
      "topic": 16,
      "similarity": 0.7980141280289824
    },
    {
      "doc": 647,
      "topic": 17,
      "similarity": 0.7916639193425061
    },
    {
      "doc": 647,
      "topic": 19,
      "similarity": 0.8043484036266534
    },
    {
      "doc": 647,
      "topic": 20,
      "similarity": 0.7765261684322593
    },
    {
      "doc": 647,
      "topic": 21,
      "similarity": 0.7940666577495472
    },
    {
      "doc": 647,
      "topic": 23,
      "similarity": 0.7528189862179913
    },
    {
      "doc": 647,
      "topic": 24,
      "similarity": 0.7590455525090094
    },
    {
      "doc": 648,
      "topic": 3,
      "similarity": 0.7916365377898565
    },
    {
      "doc": 648,
      "topic": 4,
      "similarity": 0.7516615683504178
    },
    {
      "doc": 648,
      "topic": 5,
      "similarity": 0.7674672508175607
    },
    {
      "doc": 648,
      "topic": 7,
      "similarity": 0.7508316005641266
    },
    {
      "doc": 648,
      "topic": 8,
      "similarity": 0.7502279446272373
    },
    {
      "doc": 648,
      "topic": 9,
      "similarity": 0.7929502464521098
    },
    {
      "doc": 648,
      "topic": 11,
      "similarity": 0.7673026114412205
    },
    {
      "doc": 648,
      "topic": 16,
      "similarity": 0.7766087340914215
    },
    {
      "doc": 648,
      "topic": 17,
      "similarity": 0.770701437568194
    },
    {
      "doc": 648,
      "topic": 18,
      "similarity": 0.7851488181202648
    },
    {
      "doc": 648,
      "topic": 19,
      "similarity": 0.8114771211873134
    },
    {
      "doc": 648,
      "topic": 20,
      "similarity": 0.7568227420039001
    },
    {
      "doc": 648,
      "topic": 21,
      "similarity": 0.7745481094535857
    },
    {
      "doc": 648,
      "topic": 24,
      "similarity": 0.7521592033740405
    },
    {
      "doc": 649,
      "topic": 1,
      "similarity": 0.7571108095619133
    },
    {
      "doc": 649,
      "topic": 3,
      "similarity": 0.7943743562187127
    },
    {
      "doc": 649,
      "topic": 4,
      "similarity": 0.751928892884144
    },
    {
      "doc": 649,
      "topic": 5,
      "similarity": 0.7797237106881854
    },
    {
      "doc": 649,
      "topic": 7,
      "similarity": 0.7857174301902996
    },
    {
      "doc": 649,
      "topic": 8,
      "similarity": 0.772590507025894
    },
    {
      "doc": 649,
      "topic": 9,
      "similarity": 0.8043412442524183
    },
    {
      "doc": 649,
      "topic": 10,
      "similarity": 0.7592734306931112
    },
    {
      "doc": 649,
      "topic": 11,
      "similarity": 0.7699449631545724
    },
    {
      "doc": 649,
      "topic": 13,
      "similarity": 0.7590081712354285
    },
    {
      "doc": 649,
      "topic": 14,
      "similarity": 0.7581734661964744
    },
    {
      "doc": 649,
      "topic": 15,
      "similarity": 0.7647216581073132
    },
    {
      "doc": 649,
      "topic": 16,
      "similarity": 0.7995866215513718
    },
    {
      "doc": 649,
      "topic": 17,
      "similarity": 0.7682504689583326
    },
    {
      "doc": 649,
      "topic": 18,
      "similarity": 0.7663682405945567
    },
    {
      "doc": 649,
      "topic": 19,
      "similarity": 0.8059474781512022
    },
    {
      "doc": 649,
      "topic": 20,
      "similarity": 0.7767201503540867
    },
    {
      "doc": 649,
      "topic": 21,
      "similarity": 0.7802855348686532
    },
    {
      "doc": 649,
      "topic": 23,
      "similarity": 0.7562225182962594
    },
    {
      "doc": 650,
      "topic": 3,
      "similarity": 0.7783187162993274
    },
    {
      "doc": 650,
      "topic": 5,
      "similarity": 0.7903072500322987
    },
    {
      "doc": 650,
      "topic": 8,
      "similarity": 0.7584038343984396
    },
    {
      "doc": 650,
      "topic": 9,
      "similarity": 0.7844128664740201
    },
    {
      "doc": 650,
      "topic": 10,
      "similarity": 0.7541676093847066
    },
    {
      "doc": 650,
      "topic": 15,
      "similarity": 0.7513388121356732
    },
    {
      "doc": 650,
      "topic": 16,
      "similarity": 0.7761640259928068
    },
    {
      "doc": 650,
      "topic": 17,
      "similarity": 0.7783718473938238
    },
    {
      "doc": 650,
      "topic": 18,
      "similarity": 0.7715214224871941
    },
    {
      "doc": 650,
      "topic": 19,
      "similarity": 0.8089961991947554
    },
    {
      "doc": 650,
      "topic": 20,
      "similarity": 0.7504444425916418
    },
    {
      "doc": 650,
      "topic": 21,
      "similarity": 0.7680261677239689
    },
    {
      "doc": 650,
      "topic": 24,
      "similarity": 0.7550387331376839
    },
    {
      "doc": 651,
      "topic": 1,
      "similarity": 0.7520698553159878
    },
    {
      "doc": 651,
      "topic": 2,
      "similarity": 0.7634613395514781
    },
    {
      "doc": 651,
      "topic": 3,
      "similarity": 0.7769995355277868
    },
    {
      "doc": 651,
      "topic": 5,
      "similarity": 0.7832414626513333
    },
    {
      "doc": 651,
      "topic": 8,
      "similarity": 0.7544613466697464
    },
    {
      "doc": 651,
      "topic": 9,
      "similarity": 0.7965700804756541
    },
    {
      "doc": 651,
      "topic": 10,
      "similarity": 0.7871904330445029
    },
    {
      "doc": 651,
      "topic": 11,
      "similarity": 0.7660949107790075
    },
    {
      "doc": 651,
      "topic": 13,
      "similarity": 0.7564185195510131
    },
    {
      "doc": 651,
      "topic": 14,
      "similarity": 0.776245508776365
    },
    {
      "doc": 651,
      "topic": 15,
      "similarity": 0.7738643554555605
    },
    {
      "doc": 651,
      "topic": 16,
      "similarity": 0.7803810957944691
    },
    {
      "doc": 651,
      "topic": 17,
      "similarity": 0.7684333934036854
    },
    {
      "doc": 651,
      "topic": 18,
      "similarity": 0.7596062568988813
    },
    {
      "doc": 651,
      "topic": 19,
      "similarity": 0.8059364887932473
    },
    {
      "doc": 651,
      "topic": 20,
      "similarity": 0.7742596687272758
    },
    {
      "doc": 651,
      "topic": 21,
      "similarity": 0.7725097841215729
    },
    {
      "doc": 651,
      "topic": 23,
      "similarity": 0.8049580095652146
    },
    {
      "doc": 652,
      "topic": 1,
      "similarity": 0.759789416130494
    },
    {
      "doc": 652,
      "topic": 2,
      "similarity": 0.7864803594169612
    },
    {
      "doc": 652,
      "topic": 3,
      "similarity": 0.7914692315392822
    },
    {
      "doc": 652,
      "topic": 4,
      "similarity": 0.7871187442151315
    },
    {
      "doc": 652,
      "topic": 5,
      "similarity": 0.8179406277950355
    },
    {
      "doc": 652,
      "topic": 7,
      "similarity": 0.7827948051360648
    },
    {
      "doc": 652,
      "topic": 8,
      "similarity": 0.7988004384268629
    },
    {
      "doc": 652,
      "topic": 9,
      "similarity": 0.8265384428821734
    },
    {
      "doc": 652,
      "topic": 10,
      "similarity": 0.7634852818679656
    },
    {
      "doc": 652,
      "topic": 11,
      "similarity": 0.7848901024205116
    },
    {
      "doc": 652,
      "topic": 13,
      "similarity": 0.7947280652520807
    },
    {
      "doc": 652,
      "topic": 14,
      "similarity": 0.7844232687088047
    },
    {
      "doc": 652,
      "topic": 15,
      "similarity": 0.7834979038252887
    },
    {
      "doc": 652,
      "topic": 16,
      "similarity": 0.8198678111279067
    },
    {
      "doc": 652,
      "topic": 17,
      "similarity": 0.7976759683926463
    },
    {
      "doc": 652,
      "topic": 19,
      "similarity": 0.806443247942591
    },
    {
      "doc": 652,
      "topic": 20,
      "similarity": 0.7719831681766737
    },
    {
      "doc": 652,
      "topic": 21,
      "similarity": 0.8015502529280852
    },
    {
      "doc": 652,
      "topic": 23,
      "similarity": 0.7592849817681951
    },
    {
      "doc": 652,
      "topic": 24,
      "similarity": 0.7602996133307552
    }
  ]
}
